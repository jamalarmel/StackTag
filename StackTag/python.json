[
{"body": "What is the use of the  keyword in Python? What does it do?For example, I'm trying to understand this code:And this is the caller:What happens when the method  is called?\nIs a list returned? A single element? Is it called again? When will subsequent calls stop?To understand what  does, you must understand what  are. And before generators come .When you create a list, you can read its items one by one. Reading its items one by one is called iteration: is an . When you use a list comprehension, you create a list, and so an iterable:Everything you can use \"\" on is an iterable; , , files...These iterables are handy because you can read them as much as you wish, but you store all the values in memory and this is not always what you want when you have a lot of values.Generators are iterators, but . It's because they do not store all the values in memory, :It is just the same except you used  instead of . BUT, you  perform  a second time since generators can only be used once: they calculate 0, then forget about it and calculate 1, and end calculating 4, one by one. is a keyword that is used like , except the function will return a generator.Here it's a useless example, but it's handy when you know your function will return a huge set of values that you will only need to read once.To master , you must understand that  The function only returns the generator object, this is a bit tricky :-)Then, your code will be run each time the  uses the generator.Now the hard part:The first time the  calls the generator object created from your function, it will run the code in your function from the beginning until it hits , then it'll return the first value of the loop. Then, each other call will run the loop you have written in the function one more time, and return the next value, until there is no value to return.The generator is considered empty once the function runs but does not hit  anymore. It can be because the loop had come to an end, or because you do not satisfy an  anymore.Generator:Caller:This code contains several smart parts:Usually we pass a list to it:But in your code it gets a generator, which is good because:And it works because Python does not care if the argument of a method is a list or not. Python expects iterables so it will work with strings, lists, tuples and generators! This is called duck typing and is one of the reason why Python is so cool. But this is another story, for another question...You can stop here, or read a little bit to see an advanced use of a generator:It can be useful for various things like controlling access to a resource.The itertools module contains special functions to manipulate iterables. Ever wish to duplicate a generator?\nChain two generators? Group values in a nested list with a one liner?  without creating another list?Then just .An example? Let's see the possible orders of arrival for a 4 horse race:Iteration is a process implying iterables (implementing the  method) and iterators (implementing the  method).\nIterables are any objects you can get an iterator from. Iterators are objects that let you iterate on iterables.More about it in this article about .When you see a function with  statements, apply this easy trick to understand what will happen:This trick may give you an idea of the logic behind the function, but what actually happens with  is significantly different that what happens in the list based approach. In many cases the yield approach will be a lot more memory efficient and faster too. In other cases this trick will get you stuck in an infinite loop, even though the original function works just fine. Read on to learn more...First, the  - when you writePython performs the following two steps:The truth is Python performs the above two steps anytime it wants to  the contents of an object - so it could be a for loop, but it could also be code like  (where  is a Python list).Here  is an  because it implements the iterator protocol. In a user defined class, you can implement the  method to make instances of your class iterable. This method should return an . An iterator is an object with a  method. It is possible to implement both  and  on the same class, and have  return . This will work for simple cases, but not when you want two iterators looping over the same object at the same time.So that's the iterator protocol, many objects implement this protocol:Note that a  loop doesn't know what kind of object it's dealing with - it just follows the iterator protocol, and is happy to get item after item as it calls . Built-in lists return their items one by one, dictionaries return the  one by one, files return the  one by one, etc. And generators return... well that's where  comes in:Instead of  statements, if you had three  statements in  only the first would get executed, and the function would exit. But  is no ordinary function. When  is called, it  return any of the values in the yield statements! It returns a generator object. Also, the function does not really exit - it goes into a suspended state. When the  loop tries to loop over the generator object, the function resumes from its suspended state at the very next line after the  it previously returned from, executes the next line of code, in this case a  statement, and returns that as the next item. This happens until the function exits, at which point the generator raises , and the loop exits. So the generator object is sort of like an adapter - at one end it exhibits the iterator protocol, by exposing  and  methods to keep the  loop happy. At the other end however, it runs the function just enough to get the next value out of it, and puts it back in suspended mode.Usually you can write code that doesn't use generators but implements the same logic. One option is to use the temporary list 'trick' I mentioned before. That will not work in all cases, for e.g. if you have infinite loops, or it may make inefficient use of memory when you have a really long list. The other approach is to implement a new iterable class  that keeps state in instance members and performs the next logical step in it's  (or  in Python 3) method. Depending on the logic, the code inside the  method may end up looking very complex and be prone to bugs. Here generators provide a clean and easy solution.Think of it this way:An iterator is just a fancy sounding term for an object that has a next() method.  So a yield-ed function ends up being something like this:Original version:This is basically what the python interpreter does with the above code:For more insight as to what's happening behind the scenes, the for loop can be rewritten to this:Does that make more sense or just confuse you more?  :) I should note that this IS an oversimplification for illustrative purposes.  :) Forgot to throw the StopIteration exceptionThe  keyword is reduced to two simple facts:In a nutshell: , and  the generator should incrementally spit out.Let's define a function  that's just like Python's . Calling  RETURNS A GENERATOR:To force the generator to immediately return its pending values, you can pass it into  (just like you could any iterable):The above example can be thought of as merely creating a list which you append to and return:There is one major difference, though; see the last section.An iterable is the last part of a list comprehension, and all generators are iterable, so they're often used like so:To get a better feel for generators, you can play around with the  module (be sure to use  rather than  when warranted). For example, you might even use generators to implement infinitely-long lazy lists like . You could implement your own , or alternatively do so with the  keyword in a while-loop.Please note: generators can actually be used for many more things, such as  or non-deterministic programming or other elegant things. However, the \"lazy lists\" viewpoint I present here is the most common use you will find.This is how the \"Python iteration protocol\" works. That is, what is going on when you do . This is what I describe earlier as a \"lazy, incremental list\".The built-in function  just calls the objects  function, which is a part of the \"iteration protocol\" and is found on all iterators. You can manually use the  function (and other parts of the iteration protocol) to implement fancy things, usually at the expense of readability, so try to avoid doing that...Normally, most people would not care about the following distinctions and probably want to stop reading here.In Python-speak, an  is any object which \"understands the concept of a for-loop\" like a list , and an  is a specific instance of the requested for-loop like . A  is exactly the same as any iterator, except for the way it was written (with function syntax).When you request an iterator from a list, it creates a new iterator. However, when you request an iterator from an iterator (which you would rarely do), it just gives you a copy of itself.Thus, in the unlikely event that you are failing to do something like this...... then remember that a generator is an ; that is, it is one-time-use. If you want to reuse it, you should call  again. If you need to use the result twice, convert the result to a list and store it in a variable . Those who absolutely need to clone a generator (for example, who are doing terrifyingly hackish metaprogramming) can use  if absolutely necessary, since the copyable iterator Python  standards proposal has been deferred. is just like  - it returns whatever you tell it to.  The only difference is that the next time you call the function, execution starts from the last call to the  statement.In the case of your code, the function  is acting like an iterator so that when you extend your list, it adds one element at a time to the new list. calls an iterator until it's exhausted.  In the case of the code sample you posted, it would be much clearer to just return a tuple and append that to the list.There's one extra thing to mention: a function that yields doesn't actually have to terminate. I've written code like this:Then I can use it in other code like this:It really helps simplify some problems, and makes some things easier to work with.  is only legal inside of a function definition, and The idea for generators comes from other languages (see footnote 1) with varying implementations. In Python's Generators, the execution of the code is  at the point of the yield. When the generator is called (methods are discussed below) execution resumes and then freezes at the next yield. provides an \neasy way of , defined by the following two methods: \n and  (Python 2) or  (Python 3).  Both of those methods\nmake an object an iterator that you could type-check with the  Abstract Base \nClass from the  module.The generator type is a sub-type of iterator:And if necessary, we can type-check like this:A feature of an  , you can't reuse or reset it:You'll have to make another if you want to use its functionality again (see footnote 2):One can yield data programmatically, for example:The above simple generator is also equivalent to the below - as of Python 3.3 (and not available in Python 2), you can use :However,  also allows for delegation to subgenerators, \nwhich will be explained in the following section on cooperative delegation with sub-coroutines. forms an expression that allows data to be sent into the generator (see footnote 3)Here is an example, take note of the  variable, which will point to the data that is sent to the generator:First, we must queue up the generator with the builtin function, . It will \ncall the appropriate  or  method, depending on the version of\nPython you are using:And now we can send data into the generator. (.) :Now, recall that  is available in Python 3. This allows us to delegate\ncoroutines to a subcoroutine:And now we can delegate functionality to a sub-generator and it can be used\nby a generator just as above:You can read more about the precise semantics of  in The  method raises  at the point the function \nexecution was frozen. This will also be called by  so you \ncan put any cleanup code where you handle the :You can also throw an exception which can be handled in the generator\nor propagated back to the user:I believe I have covered all aspects of the following question:It turns out that  does a lot. I'm sure I could add even more \nthorough examples to this. If you want more or have some constructive criticism, let me know by commenting\nbelow.In :An  is basically any number of expressions separated by commas - essentially, in Python 2, you can stop the generator with , but you can't return a value.In : For those who prefer a minimal working example, meditate on this interactive  session:Yield gives you a generator. As you can see, in the first case foo holds the entire list in memory at once. It's not a big deal for a list with 5 elements, but what if you want a list of 5 million? Not only is this a huge memory eater, it also costs a lot of time to build at the time that the function is called. In the second case, bar just gives you a generator. A generator is an iterable--which means you can use it in a for loop, etc, but each value can only be accessed once. All the values are also not stored in memory at the same time; the generator object \"remembers\" where it was in the looping the last time you called it--this way, if you're using an iterable to (say) count to 50 billion, you don't have to count to 50 billion all at once and store the 50 billion numbers to count through. Again, this is a pretty contrived example, you probably would use itertools if you really wanted to count to 50 billion. :)This is the most simple use case of generators. As you said, it can be used to write efficient permutations, using yield to push things up through the call stack instead of using some sort of stack variable. Generators can also be used for specialized tree traversal, and all manner of other things.There is one type of answer that I don't feel has been given yet, among the many great answers that describe how to use generators.  Here is the PL theory answer:The  statement in python returns a generator.  A generator in python is a function that returns  (and specifically a type of coroutine, but continuations represent the more general mechanism to understand what is going on).Continuations in programming languages theory are a much more fundamental kind of computation, but they are not often used because they are extremely hard to reason about and also very difficult to implement.  But the idea of what a continuation is, is straightforward: it is the state of a computation that has not yet finished. In this state are saved the current values of variables and the operations that have yet to be performed, and so on. Then at some point later in the program the continuation can be invoked, such that the program's variables are reset to that state and the operations that were saved are carried out.Continuations, in this more general form, can be implemented in two ways. In the  way, the program's stack is literally saved and then when the continuation is invoked, the stack is restored.In continuation passing style (CPS), continuations are just normal functions (only in languages where functions are first class) which the programmer explicitly manages and passes around to subroutines. In this style, program state is represented by closures (and the variables that happen to be encoded in them) rather than variables that reside somewhere on the stack. Functions that manage control flow accept continuation as arguments (in some variations of CPS, functions may accept multiple continuations) and manipulate control flow by invoking them by simply calling them and returning afterwards. A very simple example of continuation passing style is as follows:In this (very simplistic) example, the programmer saves the operation of actually writing the file into a continuation (which can potentially be a very complex operation with many details to write out), and then passes that continuation (i.e, as a first-class closure) to another operator which does some more processing, and then calls it if necessary. (I use this design pattern a lot in actual GUI programming, either because it saves me lines of code or, more importantly, to manage control flow after GUI events trigger)The rest of this post will, without loss of generality, conceptualize continuations as CPS, because it is a hell of a lot easier to understand and read.Now let's talk about generators in python. Generators are a specific subtype of continuation. Whereas  (i.e., the program's call stack), . Although, this definition is slightly misleading for certain use cases of generators. For instance:This is clearly a reasonable iterable whose behavior is well defined -- each time the generator iterates over it, it returns 4 (and does so forever). But it isn't probably the prototypical type of iterable that comes to mind when thinking of iterators (i.e., ). This example illustrates the power of generators: if anything is an iterator, a generator can save the state of its iteration.To reiterate: Continuations can save the state of a program's stack and generators can save the state of iteration. This means that continuations are more a lot powerful than generators, but also that generators are a lot, lot easier. They are easier for the language designer to implement, and they are easier for the programmer to use (if you have some time to burn, try to read and understand ).But you could easily implement (and conceptualize) generators as a simple, specific case of continuation passing style: Whenever  is called, it tells the function to return a continuation.  When the function is called again, it starts from wherever it left off. So, in pseudo-pseudocode  (i.e., not pseudocode but not code) the generator's  method is basically as follows: where  keyword is actually syntactic sugar for the real generator function, basically something like:Remember that this is just pseudocode and the actual implementation of generators in python is more complex. But as an exercise to understand what is going on, try to use continuation passing style to implement generator objects without use of the  keyword.It's returning a generator. I'm not particularly familiar with Python, but I believe it's the same kind of thing as  if you're familiar with those.There's an  which explains it reasonably well (for Python) as far as I can see.The key idea is that the compiler/interpreter/whatever does some trickery so that as far as the caller is concerned, they can keep calling next() and it will keep returning values - . Now obviously you can't really \"pause\" a method, so the compiler builds a state machine for you to remember where you currently are and what the local variables etc look like. This is much easier than writing an iterator yourself.An example in plain language. I will provide a correspondence between high-level human concepts to low-level python concepts.I want to operate on a sequence of numbers, but I don't want to bother my self with the creation of that sequence, I want only to focus on the operation I want to do. So, I do the following:This is what a generator does (a function that contains a ); it starts executing, pauses whenever it does a , and when asked for a  value it continues from the point it was last. It fits perfectly by design with the iterator protocol of python, which describes how to sequentially request for values.The most famous user of the iterator protocol is the  command in python. So, whenever you do a:it doesn't matter if  is a list, a string, a dictionary or a generator  like described above; the result is the same: you read items off a sequence one by one.Note that ining a function which contains a  keyword is not the only way to create a generator; it's just the easiest way to create one.For more accurate information, read about , the  and  in the Python documentation.While a lot of answers show why you'd use a  to create a generator, there are more uses for .  It's quite easy to make a coroutine, which enables the passing of information between two blocks of code.  I won't repeat any of the fine examples that have already been given about using  to create a generator.To help understand what a  does in the following code, you can use your finger to trace the cycle through any code that has a .  Every time your finger hits the , you have to wait for a  or a  to be entered.  When a  is called, you trace through the code until you hit the \u2026 the code on the right of the  is evaluated and returned to the caller\u2026 then you wait.  When  is called again, you perform another loop through the code.  However, you'll note that in a coroutine,  can also be used with a \u2026 which will send a value from the caller  the yielding function. If a  is given, then  receives the value sent, and spits it out the left hand side\u2026 then the trace through the code progresses until you hit the  again (returning the value at the end, as if  was called).For example:There is another  use and meaning (since python 3.3):moreover  will introduce (since python 3.5):to avoid coroutines confused with regular generator (today  is used in both).This was my first aha-moment with yield. is a sugary way to say Same behavior:Different behavior:Yield is : you can only iterate through once. Conceptually the yield-function returns an ordered container of things. But it's revealing that we call any function with a yield in it a . And the term for what it returns is an .Yield is , it puts off computation until you need it. A function with a yield in it  when you call it. The iterator object it returns uses  to maintain the function's internal context. Each time you call  on the iterator (as happens in a for-loop), execution inches forward to the next yield. (Or , which raises  and ends the series.)Yield is . It can do infinite loops:If you need  and the series isn't humongous, just pass the iterator to Brilliant choice of the word  because  apply:...provide the next data in the series....relinquish CPU execution until the iterator advances.I was going to post \"read page 19 of Beazley's 'Python: Essential Reference' for a quick description of generators\", but so many others have posted good descriptions already.Also, note that  can be used in coroutines as the dual of their use in generator functions.  Although it isn't the same use as your code snippet,  can be used as an expression in a function.  When a caller sends a value to the method using the  method, then the coroutine will execute until the next  statement is encountered.Generators and coroutines are a cool way to set up data-flow type applications.  I thought it would be worthwhile knowing about the other use of the  statement in functions.Here are some  as if Python did not provide syntactic sugar for them (or in a language without native syntax, like ). Snippets from that link is below. (because )Here is a simple example:output :I am not a Python developer, but it looks to me  holds the position of program flow and the next loop start from \"yield\" position. It seems like it is waiting at that position, and just before that, returning a value outside, and next time continues to work.Seems to me an interesting and nice ability :D From a programming viewpoint, the iterators are implemented as  To implement iterators/generators/thread pools for concurrent execution/etc as thunks (also called anonymous functions), one uses messages sent to a closure object, which has a dispatcher, and the dispatcher answers to \"messages\".\"\" is a message sent to a closure, created by \"\" call.There are lots of ways to implement this computation.  I used mutation but it is easy to do it without mutation, by returning the current value and the next yielder.Here is a demonstration which uses the structure of R6RS but the semantics is absolutely identical as in python, it's the same model of computation, only a change in syntax is required to rewrite it in python.Here is a mental image of what  does.I like to think of a thread as having a stack (even when it's not implemented that way).When a normal function is called, it puts its local variables on the stack, does some computation, then clears the stack and returns. The values of its local variables are never seen again.With a  function, when its code begins to run (i.e. after the function is called, returning a generator object, whose  method is then invoked), it similarly puts its local variables onto the stack and computes for a while. But then, when it hits the  statement, before clearing its part of the stack and returning, it takes a snapshot of its local variables and stores them in the generator object. It also writes down the place where it's currently up to in its code (i.e. the particular  statement).So it's a kind of a frozen function that the generator is hanging onto.When  is called subsequently, it retrieves the function's belongings onto the stack and re-animates it. The function continues to compute from where it left off, oblivious to the fact that it had just spent an eternity in cold storage.Compare the following examples:When we call the second function, it behaves very differently to the first. The  statement might be unreachable, but if it's present anywhere, it changes the nature of what we're dealing with.Calling  doesn't run its code, but makes a generator out of the code. (Maybe it's a good idea to name such things with the  prefix for readability.)The  and  fields are where the frozen state is stored. Exploring them with , we can confirm that our mental model above is credible.Like every answer suggests,  is used for creating a sequence generator. It's used for generating some sequence dynamically. Eg. While reading a file line by line on a network, you can use the  function as follows:You can use it in your code as follows :The execution control will be transferred from getNextLines() to the for loop when yield is executed. Thus, every time getNextLines() is invoked, execution begins from the point where it was paused last time.Thus in short, a function with the following codewill print I hope this helps you. is like a return element for a function. The difference is, that the  element turns a function into a generator. A generator behaves just like a function until something is 'yielded'. The generator stops until it is next called, and continues from exactly the same point as it started. You can get a sequence of all the 'yielded' values in one, by calling .A  in a function will return a single value.If you want  use .More importantly,  is a  i.eIt will run the code in your function from the beginning until it hits . Then, it\u2019ll return the first value of the loop. \nThen, every other call will run the loop you have written in the function one more time, returning the next value until there is no value to return.(My below answer only speaks from the perspective of using Python generator, not the , which involves some tricks of stack and heap manipulation.)When  is used instead of a  in a python function, that function is turned into something special called . That function will return an object of  type.  Normal functions will terminate once some value is returned from it. But with the help of the compiler, the generator function  as resumable. That is, the execution context will be restored and the execution will continue from last run. Until you explicitly call return, which will raise a  exception (which is also part of the iterator protocol), or reach the end of the function. I found a lot of references about  but this  from the  is the most digestable.(Now I want to talk about the rationale behind , and the  based on my own understanding. I hope this can help you grasp the  of iterator and generator. Such concept shows up in other languages as well such as C#.)As I understand, when we want to process a bunch of data, we usually first store the data somewhere and then process it one by one. But this  approach is problematic. If the data volume is huge, it's expensive to store them as a whole beforehand. . There are 2 approaches to wrap such metadata.Either way, an iterator is created, i.e. some object that can give you the data you want. The OO approach may be a bit complex. Anyway, which one to use is up to you.The  keyword simply collects returning results. Think of  like Many people use  rather than  but in some cases  can be more efficient and easier to work with.Here is an example which  is definitely best for:Both functions do the same thing but  uses 3 lines instead of 5 and has one less variable to worry about.As you can see both functions do the same thing, the only difference is  gives a list and  gives a generatorA real life example would be something like reading a file line by line or if you just want to make a generatorMost questions regarding the  statement and the semantics/functionality that it introduces are present in . The collective knowledge from all previous answers is amazing but I'll add an answer that references the official presentation.So first of, the form of the  statement:consist of the   along with an optional . Syntactically yield can only appear inside a function definition and its presence alone is responsible for tranforming a function into a generator object:So after you define your generator you're left with a generator function that is waiting to be called: So parameters are bound in the same way as they do for all callable but the body of the generator object is not executed, what happens is:We get back an object that comforms to the  this means that the  object implements  and  and as such can be used in  loops like any object that supports iteration. The key difference that  makes is here, specifically:So everything  the  is executed and then execution stops, at that point what happens is: So in the case of a  loop:  the value of  is going to be equal to  as previously stated.But \"frozen\" you may ask, what does that mean? This is further explained as: So state is retained when  is encountered thereby allowing consequent calls to  to continue smoothly. When a  call is made the generator is going to execute everything until it finds another  statement. That cicle is repeated until no  (i.e control flows off the end of the generator) or a  is found in which case a  exception is raised signalling that the generator has been exhausted.In summary, the  statement transforms your function into a factory that produces a special object called a  which wraps around the body of your original function. When the  is iterated, it executes your function  until it reaches the next  then suspends execution and evaluates to the value passed to . It repeats this process on each iteration until the path of execution exits the function. For instance;simply outputs ;The power comes from using the generator with a loop that calculates a sequence, the generator executes the loop stopping each time to 'yield' the next result of the calculation, in this way it calculates a list on the fly, the benefit being the memory saved for especially large calculationsSay you wanted to create a your own  function that produces an iterable range of numbers, you could do it like so,and use it like this;but this is ineffecient becauseLuckily Guido and his team were generous enough to develop generators so we could just do this;Now upon each iteration a function on the generator called  executes the function until it either reaches a 'yield' statement in which it stops and  'yields' the value or reaches the end of the function. In this case on the first call,  executes up to the yield statement and yield 'n', on the next call it will execute the  increment statement, jump back to the 'while', evaluate it, and if true, it will stop and yield 'n' again, it will continue that way until the while condition returns false and the generator jumps to the end of the function. At a glance, the yield statement is used to define generators, replacing the return of a function to provide a result to its caller without destroying local variables. Unlike a function, where on each call it starts with new set of variables, a generator will resume the execution where it was left off.About Python Generators\nSince the yield keyword is only used with generators, it makes sense to recall the concept of generators first.The idea of generators is to calculate a series of results one-by-one on demand (on the fly). In the simplest case, a generator can be used as a list, where each element is calculated lazily. Let's compare a list and a generator that do the same thing - return powers of two:Iterating over the list and the generator looks completely the same. However, although the generator is iterable, it is not a collection and thus has no length. Collections (lists, tuples, sets, etc) keep all values in memory and we can access them whenever needed. A generator calculates the values on the fly and forgets them, so it does not have any overview about the own result set.Generators are especially useful for memory-intensive tasks, where there is no need to keep all of the elements of a memory-heavy list accessible at the same time. Calculating a series of values one-by-one can also be useful in situations where the complete result is never needed, yielding intermediate results to the caller until some requirement is satisfied and further processing stops.Using the Python \"yield\" keyword\nA good example is a search task, where typically there is no need to wait for all results to be found. Performing a file-system search, a user would be happier to receive results on-the-fly, rather the wait for a search engine to go through every single file and only afterwards return results. Are there any people who really navigate through all Google search results until the last page?Since a search functionality cannot be created using list-comprehensions, we are going to define a generator using a function with the yield statement/keyword. The yield instruction should be put into a place where the generator returns an intermediate result to the caller and sleeps until the next invocation occurs. So far the most practical aspects of Python generators have been described. For more detailed info and an interesting discussion take a look at the Python Enhancement Proposal 255, which discusses the feature of the language in detail.Happy Pythoning!\nYet another TL;DR:  returns the next element of the list:  will compute the next element on the fly (execute code)You can see the yield/generator as a way to manually run the  from outside (like continue loop 1 step), by calling next, however complex the flow.NOTE: the generator is  a normal function, it remembers previous state like local variables (stack), see other answers or articles for detailed explanation, the generator can only be .\nYou could do without  but it would not be as nice, so it can be considered 'very nice' language sugar."},
{"body": "Is there a simple way to determine if a variable is a list, dictionary, or something else? I am getting an object back that may be either type and I need to be able to tell the difference.To get the type of an object, you can use the built-in  function. Passing an object as the only parameter will return the type object of that object:This of course also works for custom types:Note that  will only return the immediate type of the object, but won\u2019t be able to tell you about type inheritance.To cover that, you should use the  function. This of course also works for built-in types: is usually the preferred way to ensure the type of an object because it will also accept derived types. So unless you actually need the type object (for whatever reason), using  is preferred over .The second parameter of  also accepts a tuple of types, so it\u2019s possible to check for multiple types at once.  will then return true, if the object is of any of those types:You can do that using :It might be more Pythonic to use a ... block. That way, if you have a class which quacks like a list, or quacks like a dict, it will behave properly regardless of what its type  is.To clarify, the preferred method of \"telling the difference\" between variable types is with something called : as long as the methods (and return types) that a variable responds to are what your subroutine expects, treat it like what you expect it to be. For example, if you have a class that overloads the bracket operators with  and , but uses some funny internal scheme, it would be appropriate for it to behave as a dictionary if that's what it's trying to emulate.The other problem with the  checking is that if  is a subclass of , it evaluates to  when, programmatically, you would hope it would be . If an object is a subclass of a list, it should work like a list: checking the type as presented in the other answer will prevent this. ( will work, however).On instances of object you also have the:attribute. Here is a sample taken from Python 3.3 consoleBeware that in python 3.x and in New-Style classes (aviable optionally from Python 2.6) class and type have been merged and this can sometime lead to unexpected results. Mainly for this reason my favorite way of testing types/classes is to the  built in function.You can use  or .Be warned that you can clobber  or any other type by assigning a variable in the current scope of the same name.Above we see that  gets reassigned to a string, therefore the test:...fails.To get around this and use  more cautiously:Determine the type of an object with Although it works, avoid double underscore attributes like  - they're not semantically public, and, while perhaps not in this case, the builtin functions usually have better behavior.Well that's a different question, don't use type - use :This covers the case where your user might be doing something clever or sensible by subclassing  - according to the principle of Liskov Substitution, you want to be able to use subclass instances without breaking your code - and  supports this. Even better, you might look for a specific Abstract Base Class from  or :Or, perhaps best of all, use duck-typing, and don't explicitly type-check your code. Duck-typing supports Liskov Substitution with more elegance and less verbosity. While the questions is pretty old, I stumbled across this while finding out a proper way myself, and I think it still needs clarifying,  (did not check on Python 3, but since the issue arises with classic classes which are gone on such version, it probably doesn't matter).Here I'm trying to answer the title's question: ? Other suggestions about using or not using isinstance are fine in many comments and answers, but I'm not addressing those concerns.The main issue with the  approach is that :Executing this snippet would yield:Which, I argue, is not what most people would expect.The  approach is the most close to correctness, but it won't work in one crucial case: when the passed-in object is an old-style  (not an instance!), since those objects lack such attribute.This is the smallest snippet of code I could think of that satisfies such legitimate question in a consistent fashion:As an aside to the previous answers, it's worth mentioning the existence of  which contains several abstract base classes (ABCs) that complement duck-typing.For example, instead of explicitly checking if something is a list with: you could, if you're only interested in seeing if the object you have allows getting items, use :if you're strictly interested in objects that allow getting, setting  deleting items (i.e  sequences), you'd opt for .Many other ABCs are defined there,  for objects that can be used as maps, , , et cetera. A full list of all these can be seen in "},
{"body": "In the following method definitions, what does the  and  do for ?The  and  is a common idiom to allow arbitrary number of arguments to functions as described in the section  in the Python documentation.The  will give you all function parameters :The  will give you all \n except for those corresponding to a formal parameter as a dictionary.Both idioms can be mixed with normal arguments to allow a set of fixed and some variable arguments:Another usage of the  idiom is to  when calling a function.In Python 3 it is possible to use  on the left side of an assignment (), though it gives a list instead of a tuple in this context:Also Python 3 adds new semantic (refer ):Such function accepts only 3 positional arguments, and everything after  can only be passed as keyword arguments.It's also worth noting that you can use * and ** when calling functions as well. This is a shortcut that allows you to pass multiple arguments to a function directly using either a list/tuple or a dictionary. For example, if you have the following function:You can do things like:The single * means that there can be any number of extra positional arguments.  can be invoked like . In the body of foo() param2 is a sequence containing 2-5.The double ** means there can be any number of extra named parameters.  can be invoked like . In the body of bar() param2 is a dictionary containing {'a':2, 'b':3 }With the following code:the output isThey allow for  and for  any number of arguments, positional () and keyword (). allows for any number of optional positional arguments (parameters), which will be assigned to a tuple named .  allows for any number of optional keyword arguments (parameters), which will be in a dict named .You can (and should) choose any appropriate name, but if the intention is for the arguments to be of non-specific semantics,  and  are standard names.You can also use  and  to pass in parameters from lists (or any iterable) and dicts (or any mapping), respectively.The function recieving the parameters does not have to know that they are being expanded. For example, Python 2's xrange does not explicitly expect , but since it takes 3 integers as arguments:As another example, we can use dict expansion in :You can have  after the  - for example, here,  must be given as a keyword argument - not positionally:Usage:Also,  can be used by itself  to indicate that keyword only arguments follow, without allowing for unlimited positional arguments.Here,  again must be an explicitly named, keyword argument:And we can no longer accept unlimited positional arguments because we don't have :Again, more simply, here we require  to be given by name, not positionally:In this example, we see that if we try to pass  positionally, we get an error:We must explicitly pass the  parameter as a keyword argument. (typically said \"star-args\") and  (stars can be implied by saying \"kwargs\", but be explicit with \"double-star kwargs\") are common idioms of Python for using the  and  notation. These specific variable names aren't required (e.g. you could use  and ), but a departure from convention is likely to enrage your fellow Python coders. We typically use these when we don't know what our function is going to receive or how many arguments we may be passing, and sometimes even when naming every variable separately would get very messy and redundant (but this is a case where usually explicit is better than implicit).The following function describes how they can be used, and demonstrates behavior. Note the named  argument will be consumed by the second positional argument before :We can check the online help for the function's signature, with , which tells us Let's call this function with  which prints:We can also call it using another function, into which we just provide : prints:OK, so maybe we're not seeing the utility yet. So imagine you have several functions with redundant code before and/or after the differentiating code. The following named functions are just pseudo-code for illustrative purposes.We might be able to handle this differently, but we can certainly extract the redundancy with a decorator, and so our below example demonstrates how  and  can be very useful:And now every wrapped function can be written much more succinctly, as we've factored out the redundancy:And by factoring out our code, which  and  allows us to do, we reduce lines of code, improve readability and maintainability, and have sole canonical locations for the logic in our program. If we need to change any part of this structure, we have one place in which to make each change.Let us first understand what are positional arguments and keyword arguments.\nBelow is an example of function definition with So this is a function definition with positional arguments.\nYou can call it with keyword/named arguments as well:Now let us study an example of function definition with :You can call this function with positional arguments as well:So we now know function definitions with positional as well as keyword arguments.Now let us study the '*' operator and '**' operator.Please note these operators can be used in 2 areas:a) b) The use of '*' operator and '**' operator in  Let us get straight to an example and then discuss it.So remember when the '*' or '**' operator is used in a  -'*' operator unpacks data structure such as a list or tuple  into arguments needed by function definition.'**' operator unpacks a dictionary into arguments needed by function definition.Now let us study the '*' operator use in .\nExample:In function  the '*' operator packs the received arguments into a tuple.Now let us see an example of '**' used in function definition:In function  The '**' operator packs the received arguments into a dictionary.So remember:In a  the '*'  data structure of tuple or list into positional or keyword arguments to be received by function definition.In a  the '**'  data structure of dictionary into positional or keyword arguments to be received by function definition.In a  the '*'  positional arguments into a tuple.In a  the '**'  keyword arguments into a dictionary. and  have special usage in the function argument list. \nimplies that the argument is a list and  implies that the argument\nis a dictionary. This allows functions to take arbitrary number of\nargumentsFrom the Python documentation:In Python 3.5, you can also use this syntax in , , , and  displays (also sometimes called literals). See .It also allows multiple iterables to be unpacked in a single function call.(Thanks to mgilson for the PEP link.)In addition to function calls, *args and **kwargs are useful in class hierarchies and also avoid having to write  method in Python. Similar usage is seen in frameworks like Django code.For example,A subclass can then beThe subclass then be called as Also, a subclass with a new attribute which makes sense only to that subclass instance can call the Base class  to offload the attributes setting.\nThis is done through *args and **kwargs. kwargs mainly used so that code is readable using named arguments. For example,which can be instatiated asThe complete code is I want to give an example which others haven't  mentioned* can also unpack a An example from Python3 Documentunzip_x will be [1, 2, 3], unzip_y will be [4, 5, 6]The zip() receives multiple iretable args, and return a generator. A good example of using both in a function is:This example would help you remember ,  and even  and inheritance in Python at once."},
{"body": "Is it possible to upgrade all Python packages at one time with ?Note that there is  for this on the official issue tracker.There isn't a built-in flag yet, but you can useNote: there are infinite potential variations for this. I'm trying to keep this answer short and simple, but please do suggest variations in the comments!Relevant edits:You can use the following Python code. Unlike , this will not print warnings and FIXME errors.To upgrade all local packages; you could use : is a fork of . See  mentioned by .  package works but  package no longer works. works on Windows .Works on Windows. Should be good for others too.\n($ is whatever directory you're in, in command prompt. eg. C:/Users/Username>)dothen doIf you have a problem with a certain package stalling the upgrade (numpy sometimes), just go to the directory ($), comment out the name (add a # before it) and run the upgrade again. You can later uncomment that section back.Windows version after consulting excellent  for  by Rob van der WoudeYou can just print the packages that are outdatedThe following one-liner might prove of help: keeps going if an error occurs. If you need more \"fine grained\" control over what is omitted and what raises an error you should not add the  flag and explicitly define the errors to ignore, by \"piping\" the following line for each separate error:Here is an example:This option seems to me more straightforward and readable:( selects the first word of the line (separated by a space))And this version allows for the suppression of warning message from :( removes line containing a given pattern. In my case the warning messages include \"Could not\" and \"ignored\" respectively)From  :however you need to get yolk first:This seems more concise.Explanation: gets lines like theseIn ,  sets \"space\" as the delimiter,  means to get the first column. So the above lines becomes:then pass them to  to run the command, , with each line as appending arguments limits the number of arguments passed to each command  to be 1One-liner version of @Ramana's answer.`when using a virtualenv and if you just want to upgrade packages  to your virtualenv, you may want to do:You can try this :@Ramana's worked the best for me, of those here, but I had to add a few catches:The  check excludes my development packages, because they are not located in the system site-packages directory. The try-except simply skips packages that have been removed from PyPI.@endolith: I was hoping for an easy , too, but it doesn't look like pip was meant to be used by anything but the command line (the docs don't mention the internal API, and the pip developers didn't use docstrings).Sent through ; in the meantime use this pip library solution I wrote:This seemed to work for me...I used  with a space afterwards to properly separate the package names.The rather amazing yolk makes this easy.For more info on yolk: It can do lots of things you'll probably find useful.I had the same problem with upgrading. Thing is, i never upgrade all packages. I upgrade only what i need, because project may break.Because there was no easy way for upgrading package by package, and updating the requirements.txt file, i wrote this  which  for the packages chosen (or all packages).Activate your virtualenv (important, because it will also install the new versions of upgraded packages in current virtualenv). into your project directory, then run:If the requirements are placed in a non-standard location, send them as arguments:If you already know what package you want to upgrade, simply send them as arguments:If you need to upgrade to  pre-release / post-release version, add  argument to your command.Full disclosure: I wrote this package.Windows Powershell solutionMy script:Here is my variation on rbp's answer, which bypasses \"editable\" and development distributions. It shares two flaws of the original: it re-downloads and reinstalls unnecessarily; and an error on one package will prevent the upgrade of every package after that.Related bug reports, a bit disjointed after the migration from bitbucket:I have tried the code of Ramana and I found out on Ubuntu you have to write  for each command. Here is my script which works fine on ubuntu 13.10:Isn't this more effective?here is another way of doing with a script in pythonHere is a scripts that only updates the outdated packages.    I've been using  lately. It's simple and to the point. It updates your  file to reflect the upgrades and you can then upgrade with your  file as usual.For pip3 use this:For pip, just remove the 3s as such:This solution is well designed and tested, whereas there are problems with even the most popular solutions.The above command uses the simplest and most portable pip syntax in combination with sed and sh to overcome these issues completely.  Details of sed operation can be scrutinized with the commented version.[1] Tested and regularly used in a Linux 4.8.16-200.fc24.x86_64 cluster and tested on five other Linux/Unix flavors.  It also runs on Cygwin64 installed on Windows 10.  Testing on iOS is needed.[2] To see the anatomy of the command more clearly, this is the exact equivalent of the above pip3 command with comments: [3] Upgrading a Python or PIP component that is also used in the upgrading of a Python or PIP component can be a potential cause of a deadlock or package database corruption.or even:Works fast as it is not constantly launching a shell.  I would love to find the time to get this actually using the list outdated to speed things up still more.Here is my variation:I took @Ramana's answer and made it pip3 friendly."},
{"body": "This has always confused me. It seems like this would be nicer:Than this:Is there a specific reason it is like this?It's because any iterable can be joined, not just lists, but the result and the \"joiner\" are always strings.E.G:Because the  method is in the string class, instead of the list class?I agree it looks funny.See :This was discussed in the  thread in the Python-Dev achive, and was accepted by Guido. This thread began in Jun 1999, and  was included in Python 1.6 (which supported Unicode) was released in Sep 2000. Python 2.0 (supported  methods including ) was released in Oct 2000.Here are some additional thoughts (my own, and my friend's):Guido's decision is recorded in a , deciding on :I agree that it's counterintuitive at first, but there's a good reason. Join can't be a method of a list because:There are actually two join methods (Python 3.0):If join was a method of a list, then it would have to inspect its arguments to decide which one of them to call. And you can't join byte and str together, so the way they have it now makes sense. This is because  is a \"string\" method! It creates a string from any iterable. If we stuck the method on lists, what about when we have iterables that aren't lists? What if you have a tuple of strings? If this were a  method, you would have to cast every such iterator of strings as a  before you could join the elements into a single string! For example:Let's roll our own list join method:And to use it, note that we have to first create a list from each iterable to join the strings in that iterable, wasting both memory and processing power:So we see we have to add an extra step to use our list method, instead of just using the builtin string method:The algorithm Python uses to create the final string with  actually has to pass over the iterable twice, so if you provide it a generator expression, it has to materialize it into a list first before it can create the final string. Thus, while passing around generators is usually better than list comprehensions,  is an exception:Nevertheless, the  operation is still semantically a \"string\" operation, so it still makes sense to have it on the  object than on miscellaneous iterables.Think of it as the natural orthogonal operation to split.I understand why it is applicable to anything iterable and so can't easily be implemented  on list.For readability, I'd like to see it in the language but I don't think that is actually feasible - if iterability were an interface then it could be added to the interface but it is just a convention and so there's no central way to add it to the set of things which are iterable.Primarily because the result of a  is a string.The sequence (list or tuple or whatever) doesn't appear in the result, just a string.  Because the result is a string, it makes sense as a method of a string."},
{"body": "What is the Python equivalent of Perl's  function, which removes the last character of a value?Try the method  (see doc  and )Python's  method strips  kinds of trailing whitespace by default, not just one newline as Perl does with .To strip only newlines:There are also the methods  and :And I would say the \"pythonic\" way to get lines without trailing newline characters is splitlines().The canonical way to strip end-of-line (EOL) characters is to use the string rstrip() method removing any trailing \\r or \\n.  Here are examples for Mac, Windows, and Unix EOL characters.Using '\\r\\n' as the parameter to rstrip means that it will strip out any trailing combination of '\\r' or '\\n'.  That's why it works in all three cases above.This nuance matters in rare cases.  For example, I once had to process a text file which contained an HL7 message.  The HL7 standard requires a trailing '\\r' as its EOL character.  The Windows machine on which I was using this message had appended its own '\\r\\n' EOL character.  Therefore, the end of each line looked like '\\r\\r\\n'.  Using rstrip('\\r\\n') would have taken off the entire '\\r\\r\\n' which is not what I wanted.  In that case, I simply sliced off the last two characters instead.Note that unlike Perl's  function, this will strip all specified characters at the end of the string, not just one:Note that rstrip doesn't act exactly like Perl's chomp() because it doesn't modify the string. That is, in Perl:results in  being .but in Python:will mean that the value of  is  . Even  doesn't always give the same result, as it strips all whitespace from the end of the string, not just one newline at most.I might use something like this:I think the problem with  is that you'll probably want to make sure the line separator is portable. (some antiquated systems are rumored to use ). The other gotcha is that  will strip out repeated whitespace. Hopefully  will contain the right characters. the above works for me.You may use . This will strip all newlines from the end of the string, not just one.will remove all newlines at the end of the string . The assignment is needed because  returns a new string instead of modifying the original string. Careful with : That will only chomp the newline characters for the platform where your Python is being executed. Imagine you're chimping the lines of a Windows file under Linux, for instance:Use  instead, as Mike says above.or you could always get geekier with regexps :)have fun!you can use strip:demo:An  simply uses .Perl's  function removes one linebreak sequence from the end of a string only if it's actually there.Here is how I plan to do that in Python, if  is conceptually the function that I need in order to do something useful to each line from this file:rstrip doesn't do the same thing as chomp, on so many levels. Read  and see that chomp is very complex indeed.However, my main point is that chomp removes at most 1 line ending, whereas rstrip will remove as many as it can.Here you can see rstrip removing all the newlines:A much closer approximation of typical Perl chomp usage can be accomplished with re.sub, like this:I don't program in Python, but I came across an  at python.org advocating S.rstrip(\"\\r\\n\") for python 2.2 or later.workaround solution for special case:if the newline character is the last character (as is the case with most file inputs), then for any element in the collection you can index as follows: to slice out your newline character. If your question is to clean up all the line breaks in a multiple line str object (oldstr), you can split it into a list according to the delimiter '\\n' and then join this list into a new str(newstr).    This would replicate exactly perl's chomp (minus behavior on arrays) for \"\\n\" line terminator:(Note: it does not modify string 'in place'; it does not strip extra trailing whitespace; takes \\r\\n in account)Here we go Official Complete Documentation  Just use : orYou don't need any of this complicated stuffA catch all:There are three types of line endings that we normally encounter: ,  and . A rather simple regular expression in , namely , is able to catch them all.(And we , am I right?)With the last argument, we limit the number of occurences replaced to one, mimicking chomp to some extent. Example:... where  is .I find it convenient to have be able to get the chomped lines via in iterator, parallel to the way you can get the un-chomped lines from a file object. You can do so with the following code:Sample usage:If you are concerned about speed (say you have a looong list of strings) and you know the nature of the newline char, string slicing is actually faster than rstrip. A little test to illustrate this:Output:It looks like there is not a perfect analog for perl's .  In particular,  cannot handle multi-character newline delimiters like . However,  does .\nFollowing  on a different question, you can combine  and  to remove/replace all newlines from a string :The following removes  newline (as chomp would, I believe). Passing  as the  argument to splitlines retain the delimiters.  Then, splitlines is called again to remove the delimiters on just the last \"line\": "},
{"body": "Is it possible to have static methods in Python so I can call them without initializing a class, like:Yep, using the  decoratorNote that some code might use the old method of defining a static method, using  as a function rather than a decorator. This should only be used if you have to support ancient versions of Python (2.2 and 2.3)This is entirely identical to the first example (using ), just not using the nice decorator syntaxFinally, use  sparingly! There are very few situations where static-methods are necessary in Python, and I've seen them used many times where a separate \"top-level\" function would have been clearer.:I think that Steven is actually right. To answer the original question, then, in order to set up a class method, simply assume that the first argument is not going to be a calling instance, and then make sure that you only call the method from the class.(Note that this answer refers to Python 3.x. In Python 2.x you'll get a  for calling the method on the class itself.)For example:In this code, the \"rollCall\" method assumes that the first argument is not an instance (as it would be if it were called by an instance instead of a class). As long as \"rollCall\" is called from the class rather than an instance, the code will work fine. If we try to call \"rollCall\" from an instance, e.g.:however, it would cause an exception to be raised because it would send two arguments: itself and -1, and \"rollCall\" is only defined to accept one argument.Incidentally, rex.rollCall() would send the correct number of arguments, but would also cause an exception to be raised because now n would be representing a Dog instance (i.e., rex) when the function expects n to be numerical.This is where the decoration comes in:\nIf we precede the \"rollCall\" method withthen, by explicitly stating that the method is static, we can even call it from an instance. Now, would work. The insertion of @staticmethod before a method definition, then, stops an instance from sending itself as an argument.You can verify this by trying the following code with and without the @staticmethod line commented out.Yes, check out the  decorator:You don't really need to use the  decorator. Just declaring a method (that doesn't expect the self parameter) and call it from the class. The decorator is only there in case you want to be able to call it from an instance as well (which was not what you wanted to do)Mostly, you just use functions though...Yes, static methods can be created like this (although it's a bit more  to use underscores instead of CamelCase for methods):The above uses the decorator syntax. This syntax is equivalent to This can be used just as you described:A builtin example of a static method is  in Python 3, which was a function in the  module in Python 2.Another option that can be used as you describe is the , the difference is that the classmethod gets the class as an implicit first argument, and if subclassed, then it gets the subclass as the implicit first argument.Note that  is not a required name for the first argument, but most experienced Python coders will consider it badly done if you use anything else.These are typically used as alternative constructors. A builtin example is :Aside from the particularities of how  behave, there is a certain kind of beauty you can strike with them when it comes to organizing your module-level code..........It now becomes a bit more intuitive and self-documenting in which context certain components are meant to be used and it pans out ideally for naming distinct test cases as well as having a straightforward approach to how test modules map to actual modules under tests for purists.I frequently find it viable to apply this approach to organizing a project's utility code. Quite often, people immediately rush and create a  package and end up with 9 modules of which one has 120 LOC and the rest are two dozen LOC at best. I prefer to start with this and convert it to a package and create modules only for the beasts that truly deserve them:Perhaps the simplest option is just to put those functions outside of the class:Using this method, functions which modify or use internal object state (have side effects) can be kept in the class, and the reusable utility functions can be moved outside.Let's say this file is called . To use these, you'd call  instead of .If you really need a static method to be part of the class, you can use the  decorator."},
{"body": "I'm mainly a C# developer, but I'm currently working on a project in Python.How can I represent the equivalent of an Enum in Python?  Enums have been added to Python 3.4 as described in .  It has also been  on pypi.  For more advanced Enum techniques try the  (2.7, 3.3+, same author as . Code is not perfectly compatible between py2 and py3, e.g. you'll need ).Installing  (no numbers) will install a completely different and incompatible version.or equivalently:In earlier versions, one way of accomplishing enums is:which is used like so:You can also easily support automatic enumeration with something like this:and used like so:Support for converting the values back to names can be added this way:This overwrites anything with that name, but it is useful for rendering your enums in output.  It will throw KeyError if the reverse mapping doesn't exist.  With the first example:Before PEP 435, Python didn't have an equivalent but you could implement your own.Myself, I like keeping it simple (I've seen some horribly complex examples on the net), something like this ...In Python 3.4 (), you can make Enum the base class.  This gets you a little bit of extra functionality, described in the PEP.  For example, enum values are distinct from integers.If you don't want to type the values, use the following shortcut:Here is one implementation:Here is its usage:If you need the numeric values, here's the quickest way:In Python 3.x you can also add a starred placeholder at the end, which will soak up all the remaining values of the range in case you don't mind wasting memory and cannot count:The best solution for you would depend on what you require from your  .If you need the  as only a list of  identifying different , the solution by  (above) is great:Using a  also allows you to set any :In addition to the above, if you also require that the items belong to a  of some sort, then embed them in a class:To use the enum item, you would now need to use the container name and the item name:For long lists of enum or more complicated uses of enum, these solutions will not suffice. You could look to the recipe by Will Ware for  published in the . An online version of that is available . has the interesting details of a proposal for enum in Python and why it was rejected.The typesafe enum pattern which was used in Java pre-JDK 5 has a\nnumber of advantages. Much like in Alexandru's answer, you create a\nclass and class level fields are the enum values; however, the enum\nvalues are instances of the class rather than small integers. This has\nthe advantage that your enum values don't inadvertently compare equal\nto small integers, you can control how they're printed, add arbitrary\nmethods if that's useful and make assertions using isinstance:A recent  pointed out there are a couple of enum libraries in the wild, including:An Enum class can be a one-liner.How to use it (forward and reverse lookup, keys, values, items, etc.)Python doesn't have a built-in equivalent to , and other answers have ideas for implementing your own (you may also be interested in the  in the Python cookbook).However, in situations where an  would be called for in C, I usually end up : because of the way objects/attributes are implemented, (C)Python is optimized to work very fast with short strings anyway, so there wouldn't really be any performance benefit to using integers. To guard against typos / invalid values you can insert checks in selected places.(One disadvantage compared to using a class is that you lose the benefit of autocomplete)So, I agree. Let's not enforce type safety in Python, but I would like to protect myself from silly mistakes. So what do we think about this?It keeps me from value-collision in defining my enums.There's another handy advantage: really fast reverse lookups:Use it like this:  if you just want unique symbols and don't care about the values, replace this line:  with this:I prefer to define enums in Python like so:It's more bug-proof than using integers since you don't have to worry about ensuring that the integers are unique (e.g. if you said Dog = 1 and Cat = 1 you'd be screwed).It's more bug-proof than using strings since you don't have to worry about typos (e.g.\nx == \"catt\" fails silently, but x == Animal.Catt is a runtime exception).On 2013-05-10, Guido agreed to accept  into the Python 3.4 standard library. This means that Python finally has builtin support for enumerations!There is a backport available for Python 3.3, 3.2, 3.1, 2.7, 2.6, 2.5, and 2.4.  It's on Pypi as .Declaration:Representation:Iteration:Programmatic access:For more information, refer to . Official documentation will probably follow soon.Another, very simple, implementation of an enum in Python, using :or, alternatively,Like the method above that subclasses , this allows:But has more flexibility as it can have different keys and values. This allowsto act as is expected if you use the version that fills in sequential number values.Hmmm... I suppose the closest thing to an enum would be a dictionary, defined either like this:orThen, you can use the symbolic name for the constants like this:There are other options, like a list of tuples, or a tuple of tuples, but the dictionary is the only one that provides you with a \"symbolic\" (constant string) way to access the \nvalue.Edit: I like Alexandru's answer too!What I use:How to use:So this gives you integer constants like state.PUBLISHED and the two-tuples to use as choices in Django models.davidg recommends using dicts.  I'd go one step further and use sets:Now you can test whether a value matches one of the values in the set like this:like dF, though, I usually just use string constants in place of enums.This is the best one I have seen: \"First Class Enums in Python\"It gives you a class, and the class contains all the enums. The enums can be compared to each other, but don't have any particular value; you can't use them as an integer value. (I resisted this at first because I am used to C enums, which are integer values. But if you can't use it as an integer, you can't use it as an integer by mistake so overall I think it is a win.) Each enum is a unique value. You can print enums, you can iterate over them, you can test that an enum value is \"in\" the enum. It's pretty complete and slick.Edit (cfi): The above link is not Python 3 compatible. Here's my port of enum.py to Python 3:From Python 3.4 there will be official support for enums. You can find documentation and examples .I have had occasion to need of an Enum class, for the purpose of decoding a binary file format. The features I happened to want is concise enum definition, the ability to freely create instances of the enum by either integer value or string, and a useful esentation.  Here's what I ended up with:A whimsical example of using it:Key features:Keep it simple:Then:I really like Alec Thomas' solution (http://stackoverflow.com/a/1695250):It's elegant and clean looking, but it's just a function that creates a class with the specified attributes.With a little modification to the function, we can get it to act a little more 'enumy':This creates an enum based off a specified type. In addition to giving attribute access like the previous function, it behaves as you would expect an Enum to with respect to types.  It also inherits the base class.For example, integer enums:Another interesting thing that can be done with this method is customize specific behavior by overriding built-in methods:The new standard in Python is , so an Enum class will be available in future versions of Python:However to begin using it now you can install the  that motivated the PEP:Then you :If you name it, is your problem, but if not creating objects instead of values allows you to do this:When using other implementations sited here (also when using named instances in my example) you must be sure you never try to compare objects from different enums. For here's a possible pitfall:Yikes!Alexandru's suggestion of using class constants for enums works quite well. I also like to add a dictionary for each set of constants to lookup a human-readable string representation. This serves two purposes: a) it provides a simple way to pretty-print your enum and b) the dictionary logically groups the constants so that you can test for membership.The enum package from  provides a robust implementation of enums. An earlier answer mentioned PEP 354; this was rejected but the proposal was implemented \n.Usage is easy and elegant:This solution is a simple way of getting a class for the enumeration defined as a list (no more annoying integer assignments):enumeration.py:example.py:While the original enum proposal, , was rejected years ago, it keeps coming back up. Some kind of enum was intended to be added to 3.2, but it got pushed back to 3.3 and then forgotten. And now there's a  intended for inclusion in Python 3.4. The reference implementation of PEP 435 is .As of April 2013, there seems to be a general consensus that  should be added to the standard library in 3.4\u2014as long as people can agree on what that \"something\" should be. That's the hard part. See the threads starting  and , and a half dozen other threads in the early months of 2013.Meanwhile, every time this comes up, a slew of new designs and implementations appear on PyPI, ActiveState, etc., so if you don't like the FLUFL design, try a .Here's an approach with some different characteristics I find valuable:and most importantly !Based closely on .Many doctests included here to illustrate what's different about this approach.I had need of some symbolic constants in pyparsing to represent left and right associativity of binary operators.  I used class constants like this:Now when client code wants to use these constants, they can import the entire enum using:The enumerations are unique, they can be tested with 'is' instead of '==', they don't take up a big footprint in my code for a minor concept, and they are easily imported into the client code.  They don't support any fancy str() behavior, but so far that is in the  category.Here is a variant on :"},
{"body": "I have a list of arbitrary length, and I need to split it up into equal size chunks and operate on it. There are some obvious ways to do this, like keeping a counter and two lists, and when the second list fills up, add it to the first list and empty the second list for the next round of data, but this is potentially extremely expensive.I was wondering if anyone had a good solution to this for lists of any length, e.g. using generators.I was looking for something useful in  but I couldn't find anything obviously useful. Might've missed it, though.Related question: Here's a generator that yields the chunks you want:If you're using Python 2, you should use  instead of :Also you can simply use list comprehension instead of write a function. Python 3:Python 2 version:If you want something super simple:Directly from the (old) Python documentation (recipes for itertools):The current version, as suggested by J.F.Sebastian:I guess Guido's time machine works\u2014worked\u2014will work\u2014will have worked\u2014was working again.These solutions work because  (or the equivalent in the earlier version) creates  iterator, repeated  times in the list.  then effectively performs a round-robin of \"each\" iterator; because this is the same iterator, it is advanced by each such call, resulting in each such zip-roundrobin generating one tuple of  items.Here is a generator that work on arbitrary iterables:Example:I know this is kind of old but I don't why nobody mentioned :I'm surprised nobody has thought of using 's :Demo:This works with any iterable and produces output lazily. It returns tuples rather than iterators, but I think it has a certain elegance nonetheless. It also doesn't pad; if you want padding, a simple variation on the above will suffice:Demo:Like the -based solutions, the above  pads. As far as I know, there's no one- or two-line itertools recipe for a function that  pads. By combining the above two approaches, this one comes pretty close:Demo:I believe this is the shortest chunker proposed that offers optional padding. Simple yet elegantor if you prefer:I saw the most awesome Python-ish answer in a  of this question:You can create n-tuple for any n.It also has a lot more things, including all the recipes in the itertools documentation.None of these answers are evenly sized chunks, they all leave a runt chunk at the end, so they're not completely balanced. If you were using these functions to distribute work, you've built-in the prospect of one likely finishing well before the others, so it would sit around doing nothing while the others continued working hard.For example, the current top answer ends with:I just hate that runt at the end!Others, like , and  both return: . The 's are just padding, and rather inelegant in my opinion. They are NOT evenly chunking the iterables.Why can't we divide these better?Here's a balanced solution, adapted from a function I've used in production (Note in Python 3 to replace  with ):And I created a generator that does the same if you put it into a list:And finally, since I see that all of the above functions return elements in a contiguous order (as they were given):To test them out:Which prints out:Notice that the contiguous generator provide chunks in the same length patterns as the other two, but the items are all in order, and they are as evenly divided as one may divide a list of discrete elements.If you had a chunk size of 3 for example, you could do:source:\nI would use this when my chunk size is fixed number I can type, e.g. '3', and would never change.A generator expression:eg.I like the Python doc's version proposed by tzot and J.F.Sebastian a lot,\n but it has two shortcomings:I'm using this one a lot in my code:UPDATE: A lazy chunks version:If you know list size:If you don't (an iterator):In the latter case, it can be rephrased in a more beautiful way if you can be sure that the sequence always contains a whole number of chunks of given size (i.e. there is no incomplete last chunk).The  library has the  function for this:At this point, I think we need a , just in case...In python 2:In python 3:Also, in case of massive Alien invasion, a  might become handy:usage:Where AA is array, SS is chunk size. For example:You may also use  function of  library as:You can install  via pip:.Consider using  piecesfor example:Another more explicit version.code:result:heh, one line versionI realise this question is old (stumbled over it on Google), but surely something like the following is far simpler and clearer than any of the huge complex suggestions and only uses slicing:See Python3If you are into brackets - I picked up a book on Erlang :)"},
{"body": "How do I concatenate two lists in Python?Example:Expected outcome:Python makes this ridiculously easy.It's also possible to create a generator that simply iterates over the items in both lists.  This allows you to chain lists (or any iterable) together for processing without copying the items to a new list:You can use sets to obtain merged list of unique valuesYou could also use  in order to add a  add the end of another one:This is quite simple, I think it was even shown in the :You could simply use the  or  operator as follows:Or:Also, if you want the values in the merged list to be unique you can do:Even though this is an old answer, another alternative has been introduced via the acceptance of  which deserves mentioning. The PEP, titled , generally reduced some syntactic restrictions when using the starred  expression in Python; with it, joining two lists (applies to any iterable) can now also be done with:This functionality ; from testing it in  I don't belive it has been backported to previous versions in the  family. In unsupported versions a  is going to be raised.The  to this approach is that you really don't need lists in order to perform it, anything that is iterable will do. As stated in the PEP:So while addition with  would raise a  due to type mismatch:The following won't:because it will first unpack the contents of the iterables and then simply create a  from the contents.It's worth noting that the  function accepts variable number of arguments:If an iterable (tuple, list, generator, etc.) is the input, the  class method may be used:This question directly asks about joining two lists. However it's pretty high in search even when you are looking for a way of joining many lists (including the case when you joining zero lists). Consider this more generic approach:Will output:Note, this also works correctly when  is  or .Consider better alternative suggested by Patrick Collins in the comments:With Python 3.3+ you can use :Or, if you want to support an arbitrary number of iterators:If you want to merge the two lists in sorted form, you can use merge function from the heapq library.If you don't want to or can't use the plus operator (), you can uses the   function:If you need to merge two ordered lists with complicated sorting rules, you might have to roll it yourself like in the following code (using a simple sorting rule for readability :-) ).As a more general way for more lists you can put them within a list and use  function which based on  answer is the best way for flatting a nested list :Joining two lists in Python:If you don't want any duplication:You could use the  method defined on  objects: The above code, does not preserve order, removes duplicate from each list (but not from the concatenated list)Yes, it's that simple.. This gives a new list that is the concatenation of  and .As already pointed out by many,  is the way to go if one needs to apply  to both lists. In my case, I had a label and a flag which were different from one list to the other, so I needed something slightly more complex. As it turns out, behind the scenes  simply does the following:(see ), so I took inspiration from here and wrote something along these lines:The main points to understand here are that lists are just a special case of iterable, which are objects like any other; and that  loops in python can work with tuple variables, so it is simple to loop on multiple variables at the same time. lst1 = [1,2]lst2 = [3,4]def list_combinationer(Bushisms, are_funny):list_combinationer(lst1, lst2)[1,2,3,4]you just take the values of the first and second and  add them to one variable. if I:I will have:If you wanted a new list whilst keeping the two old lists:"},
{"body": "I spent most of the day yesterday searching for a clear answer for installing  (package manager for Python). I can't find a good solution.How do I install it?All you need to do isYou can install it through Homebrew on OS X.  Why would you install Python with Homebrew?Homebrew is something of a package manager for OS X.  Find more details on the .  Once Homebrew is installed, run the following to install the latest Python, Pip & Setuptools:: All you have to do is:MacOS comes with  installed. But to make sure that you have  installed open the terminal and run the following command.If this command returns a version number that means  exists. That also means you already have access to  considering you are using .\u2139\ufe0f Now, all you have to do is run the following command.After that,  will be installed and you'll be able to use it for installing other packages.Let me know if you have any problems installing  this way.Complimentary GIF.Cheers! On Mac: is available on OS X via .\nOpen a terminal and type:When prompted for a password enter your normal login password.\nAfter the installation has completed you should be able to use  as expected.   The simplest solution is to follow the .Basically, this consists in:The main advantage of that solution is that it install pip for the python version that has been used to run , which means that if you use the default OS X installation of python to run  you will install pip for the python install from the system.Most solutions that use a package manager (homebrew or macport) on OS X create a redundant installation of python in the environment of the package manager which can create inconsistencies in your system since, depending on what you are doing, you may call one installation of python instead of another.I'm surprised no-one has mentioned this - it's a built-in way to install pip, without external tools or scripts:Works in pretty much the same way as , but worth knowing anyway.Installing a separate copy of Python is a popular option, even though Python already comes with MacOS. You take on the responsibility to make sure you're using the copy of Python you intend. But, the benefits are having the latest Python release and some protection from hosing your system if things go badly wrong.To install Python using :Now confirm that we're working with our newly installed Python:...should show a symbolic link to a path with \"Cellar\" in it like:Pip should be installed along with Python. You might want to upgrade it by typing:Now you're ready to install any of the 50,000+ packages on .Formerly, I've used . But, the docs warn that get-pip.py does not coordinate with package managers and may leave your system in an inconsistent state. Anyway, there's no need, given that pip is now .Note that pip isn't the only package manager for Python. There's also easy_install. It's no good to mix the two, so don't do it.Finally, if you have both Python 2 and 3 installed,  will point to whichever Python you installed last. Get in the habit of explicitly using either  or , so you're sure which Python is getting the new library.Happy hacking!You should install Brew first:Then brew install PythonThen  will workNEW 2016 December: This worked for me on  (El Capitan):Mac comes with , but not with pip.Requirements:With this I got these errors (but I've solved them in step 3):The directory  or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want .The directory  or its parent directory is not owned by the current user and caching wheels has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want .Finally you can install an app like:: If you install , pip will be installed automatically.You need only to upgrade pip, but before that you need create a virtual environment to work with Python 3. You can use a project folder or any folder:Check the versions:To deactivate the environment:Download this file: Then simply typeMake sure you are on the same directory as get-pip.py or you supply the correct path for that file.For details, you can visit: or,  Then update your PATH to include py27-pip bin directory (you can add this in ~/.bash_profile\nPATH=/opt/local/Library/Frameworks/Python.framework/Versions/2.7/bin:$PATHpip will be available in new terminal window.To install or upgrade , download  from Then run the following:\nFor example:Download python setup tools from the below website:Use the tar file.Once you download, go to the downloaded folder and run Once you do that,you will have easy_install.Use the below then to install pip:Install python3 first, then use pip3 to install packages.python3 will be installed, and pip is shipped with it. To use pip to install some package, run the followingNotice it's pip3 because you want to use python3.I recommend Anaconda to you. It`s the leading open data science platform powered by Python. There are many basic packages installed. "},
{"body": "I happened to find myself having a basic filtering need: I have a list and I have to filter it by an attribute of the items.My code looked like this:But then I thought, wouldn't it be better to write it like this?It's more readable, and if needed for performance the lambda could be taken out to gain something.  Question is: are there any caveats in using the second way? Any performance difference? Am I missing the Pythonic Way\u2122 entirely and should do it in yet another way (such as using itemgetter instead of the lambda)?It is strange how much beauty varies for different people. I find the list comprehension much clearer than the ugly +, but use whichever you find easier. However, do stop giving your variables names already used for builtins, that's just ugly, and not open for discussion.There are two things that may slow down your use of .The first is the function call overhead: as soon as you use a Python function (whether created by  or ) it is likely that filter will be slower than the list comprehension. It almost certainly is not enough to matter, and you shouldn't think much about performance until you've timed your code and found it to be a bottleneck, but the difference will be there.The other overhead that might apply is that the lambda is being forced to access a scoped variable (). That is slower than accessing a local variable and in Python 2.x the list comprehension only accesses local variables. If you are using Python 3.x the list comprehension runs in a separate function so it will also be accessing  through a closure and this difference won't apply.The other option to consider is to use a generator instead of a list comprehension:Then in your main code (which is where readability really matters) you've replaced both list comprehension and filter with a hopefully meaningful function name.This is a somewhat religious issue in Python. Even though , there was enough of a backlash that in the end only  was moved from built-ins to .Personally I find list comprehensions easier to read. It is more explicit what is happening from the expression  as all the behaviour is on the surface not inside the filter function.I would not worry too much about the performance difference between the two approaches as it is marginal. I would really only optimise this if it proved to be the bottleneck in your application which is unlikely.Also since the  wanted  gone from the language then surely that automatically makes list comprehensions more Pythonic ;-)Since any speed difference is bound to be miniscule, whether to use filters or list comprehensions comes down to a matter of taste. In general I'm inclined to use comprehensions (which seems to agree with most other answers here), but there is one case where I prefer . A very frequent use case is pulling out the values of some iterable X subject to a predicate P(x):but sometimes you want to apply some function to the values first:\nAs a specific example, considerI think this looks slightly better than using . But now considerIn this case we want to  against the post-computed value. Besides the issue of computing the cube twice (imagine a more expensive calculation), there is the issue of writing the expression twice, violating the  aesthetic. In this case I'd be apt to useAlthough  may be the \"faster way\", the \"Pythonic way\" would be not to care about such things unless performance is absolutely critical (in which case you wouldn't be using Python!).I find the second way more readable. It tells you exactly what the intention is: filter the list.\nPS: do not use 'list' as a variable nameAn important difference is that list comprehension will return a  while the filter returns a , which you cannot manipulate like a  (ie: call  on it, which does not work with the return of ).My own self-learning brought me to some similar issue.That being said, if there is a way to have the resulting  from a , a bit like you would do in .NET when you do , I am curious to know it.EDIT: This is the case for Python 3, not 2 (see discussion in comments). is just that. It filters out the elements of a list. You can see the definition mentions the same(in the official docs link I mentioned before). Whereas, list comprehension is something that produces a new list after acting upon  on the previous list.(Both filter and list comprehension creates new list and not perform operation in place of the older list. A new list here is something like a list with, say, an entirely new data type. Like converting integers to string ,etc)In your example, it is better to use filter than list comprehension, as per the definition. However, if you want, say other_attribute from the list elements, in your example is to be retrieved as a new list, then you can use list comprehension.This is how I actually remember about filter and list comprehension. Remove a few things within a list and keep the other elements intact, use filter. Use some logic on your own at the elements and create a watered down list suitable for some purpose, use list comprehension.generally  is slightly faster if using a builtin function.I would expect the list comprehension to be slightly faster in your case Here's a short piece I use when I need to filter on something  the list comprehension.  Just a combination of filter, lambda, and lists (otherwise known as the loyalty of a cat and the cleanliness of a dog).In this case I'm reading a file, stripping out blank lines, commented out lines, and anything after a comment on a line:I thought I'd just add that in python 3, filter() is actually an iterator object, so you'd have to pass your filter method call to list() in order to build the filtered list. So in python 2:lists b and c have the same values, and were completed in about the same time as filter() was equivalent [x for x in y if z]. However, in 3, this same code would leave list c containing a filter object, not a filtered list. To produce the same values in 3:The problem is that list() takes an iterable as it's argument, and creates a new list from that argument. The result is that using filter in this way in python 3 takes up to twice as long as the [x for x in y if z] method because you have to iterate over the output from filter() as well as the original list. My take"},
{"body": "Reading the , I found something... unexpected:I never heard about named tuples before, and I thought elements could either be indexed by numbers (like in tuples and lists) or by keys (like in dicts). I never expected they could be indexed both ways.Thus, my questions are:Named tuples are basically easy-to-create, lightweight object types.  Named tuple instances can be referenced using object-like variable dereferencing or the standard tuple syntax.  They can be used similarly to  or other common record types, except that they are immutable.  They were added in Python 2.6 and Python 3.0, although there is a .For example, it is common to represent a point as a tuple .  This leads to code like the following:Using a named tuple it becomes more readable:However, named tuples are still backwards compatible with normal tuples, so the following will still work:Thus, .  I personally have started using them to represent very simple value types, particularly when passing them as parameters to functions.  It makes the functions more readable, without seeing the context of the tuple packing.Furthermore, , only fields with them.  You can even use your named tuple types as base classes:However, as with tuples, attributes in named tuples are immutable:If you want to be able change the values, you need another type.  There is a handy recipe for  which allow you to set new values to attributes.I am not aware of any form of \"named list\" that lets you add new fields, however.  You may just want to use a dictionary in this situation. Named tuples can be converted to dictionaries using  which returns  and can be operated upon with all the usual dictionary functions.  As already noted, you should  for more information from which these examples were constructed. is a  for making a tuple class. With that class we can create tuples that are callable by name also.namedtuples are a great feature, they are perfect container for data. When you have to \"store\" data you would use tuples or dictionaries, like:or:The dictionary approach is overwhelming, since dict are mutable and slower than tuples. On the other hand, the tuples are immutable and lightweight but lack readability for a great number of entries in the data fields.namedtuples are the perfect compromise for the two approaches, the have great readability, lightweightness and immutability (plus they are polymorphic!).named tuples allow backward compatibility with code that checks for the version like thiswhile allowing future code to be more explicit by using this syntaxTo understand named tuples, you first need to know what a tuple is. A tuple is essentially an immutable (can't be changed in-place in memory) list.Here's how you might use a regular tuple:You can expand a tuple with iterable unpacking:Named tuples are tuples that allow their elements to be accessed by name instead of just index! You make a namedtuple like this:You can also use a single string with the names separated by spaces, a slightly more readable use of the API:You can do everything tuples can do (see above) as well as do the following:Use them when it improves your code to have the semantics of tuple elements expressed in your code. You can use them instead of an object if you would otherwise use an object with unchanging data attributes and no functionality. You can also :It would probably be a regression to switch from using named tuples to tuples. The upfront design decision centers around whether the cost from the extra code involved is worth the improved readability when the tuple is used. There is no extra memory used by named tuples versus tuples. You're looking for either a slotted object that implements all of the functionality of a statically sized list or a subclassed list that works like a named tuple (and that somehow blocks the list from changing in size.) An incomplete example of the first:And usage:What is namedtuple ?As the name suggests, namedtuple is a tuple with name.  In standard tuple, we access the elements using the index, whereas namedtuple allows user to define name for elements.  This is very handy especially processing csv (comma separated value) files and working with complex and large dataset, where the code becomes messy with the use of indices (not so pythonic).How to use them ?Reading Interesting Scenario in CSV Processing :In Python inside there is a good use of container called a named tuple, it can be used to create a definition of class and has all the features of the original tuple.Using named tuple will be directly applied to the default class template to generate a simple class, this method allows a lot of code to improve readability and it is also very convenient when defining a class.is one of the easiest ways to clean up your code and make it more readable. It self-documents what is happening in the tuple. Namedtuples instances are just as memory efficient as regular tuples as they do not have per-instance dictionaries, making them faster than dictionaries. Without naming each element in the tuple, it would read like this:It is so much harder to understand what is going on in the first example. With a namedtuple, each field has a name. And you access it by name rather than position or index. Instead of , we can call it p.saturation. It's easier to understand. And it looks cleaner.Creating an instance of the namedtuple is easier than creating a dictionary.You can still access namedtuples by their position, if you so choose. . It still unpacks like a regular tuple.All the  are supported. Ex: min(), max(), len(), in, not in, concatenation (+), index, slice, etc. And there are a few additional ones for namedtuple. Note: these all start with an underscore. , , .\nReturns a new instance of the named tuple replacing specified fields with new values.: The field names are not in quotes; they are keywords here.\n: Tuples are immutable - even if they are namedtuples and have the  method. The  produces a  instance; it does not modify the original or replace the old value. You can of course save the new result to the variable. Makes a new instance from an existing sequence or iterable.What happened with the last one? The item inside the parenthesis should be the iterable. So a list or tuple inside the parenthesis works, but the sequence of values without enclosing as an iterable returns an error.Returns a new  which maps field names to their corresponding values.: There is also named list which is similar to named tuple but mutable\n"},
{"body": "How do I remove leading and trailing whitespace from a string in Python?For example:Just one space, or all such spaces?  If the second, then strings already have a  method:If you need only to remove one space however, you could do it with:Also, note that  removes other whitespace characters as well (e.g. tabs and newlines).  To remove only spaces, you can specify the character to remove as an argument to , i.e.:As pointed out in answers above will remove all the leading and trailing whitespace characters such as \\n, \\r, \\t, \\f, space.For more flexibility use the followingMore details are available in the  is not limited to whitespace characters either:You want strip():"},
{"body": "When trying to delete a key from a dictionary, I write:Is there a one line way of doing this?Use :This will return  if  exists in the dictionary, and  otherwise. If the second parameter is not specified (ie. ) and  does not exist, a  is raised.Specifically to answer \"is there a one line way of doing this?\"...well, you  ;-)You should consider, though, that this way of deleting an object from a  is \u2014it is possible that  may be in  during the  statement, but may be deleted before  is executed, in which case  will fail with a .  Given this, it would be safest to either  or something along the lines ofwhich, of course, is definitely  a one-liner.It took me some time to figure out what exactly  is doing. So I'll add this as an answer to save others googling time:If you need to remove a lot of keys from a dictionary in one line of code, I think using map() is quite succinct and pythonic readable: And if you need to catch errors where you pop a value that isn't in the dictionary, use lambda inside map() like this:Works. and 'e' did not cause an error, even there myDict did not have an 'e' key.Timing of the three solutions described above.Small dictionary:Larger dictionary:Another way You can delete by conditions.\nNo error if key doesn't exist"},
{"body": "According to the documentation, they're pretty much interchangeable.  Is there a stylistic reason to use one over the other?I like to use double quotes around strings that are used for interpolation or that are natural language messages, and single quotes for small symbol-like strings, but will break the rules if the strings contain quotes, or if I forget. I use triple double quotes for docstrings and raw string literals for regular expressions even if they aren't needed.For example:Quoting the official docs at :So there is no difference. Instead, people will tell you to choose whichever style that matches the context, . And I would agree - adding that it is pointless to try to come up with \"conventions\" for this sort of thing because you'll only end up confusing any newcomers.I used to prefer , especially for , as I find . Also,  can be typed without the  key on my Swiss German keyboard.I have since changed to using triple quotes for , to conform to .I'm with Will: I'll stick with that even if it means a lot of escaping. I get the most value out of single quoted identifiers standing out because of the quotes. The rest of the practices are there just to give those single quoted identifiers some standing room. If the string you have contains one, then you should use the other.  For example, , or .  Other than that, you should simply be as consistent as you can (within a module, within a package, within a project, within an organisation).If your code is going to be read by people who work with C/C++ (or if you switch between those languages and Python), then using  for single-character strings, and  for longer strings might help ease the transition.  (Likewise for following other languages where they are not interchangeable).The Python code I've seen in the wild tends to favour  over , but only slightly.  The one exception is that  are much more common than , from what I have seen.Triple quoted comments are an interesting subtopic of this question. . I did a quick check using Google Code Search and found that  are about 10x as popular as  -- 1.3M vs 131K occurrences in the code Google indexes. So in the multi line case your code is probably going to be more familiar to people if it uses triple double quotes.For that simple reason, I always use double quotes on the outside. Speaking of fluff, what good is streamlining your string literals with ' if you're going to have to use escape characters to represent apostrophes? Does it offend coders to read novels? I can't imagine how painful high school English class was for you!Python uses quotes something like this:Which gives the following output:I use double quotes in general, but not for any specific reason - Probably just out of habit from Java.I guess you're also more likely to want apostrophes in an inline literal string than you are to want double quotes.Personally I stick with one or the other. It doesn't matter. And providing your own meaning to either quote is just to confuse other people when you collaborate.It's probably a stylistic preference more than anything.  I just checked PEP 8 and didn't see any mention of single versus double quotes.I prefer single quotes because its only one keystroke instead of two.  That is, I don't have to mash the shift key to make single quote.PHP makes the same distinction as Perl: content in single quotes will not be interpreted (not even \\n will be converted), as opposed to double quotes which can contain variables to have their value printed out.Python does not, I'm afraid. Technically seen, there is no $ token (or the like) to separate a name/text from a variable in Python. Both features make Python more readable, less confusing, after all. Single and double quotes can be used interchangeably in Python.I chose to use double quotes because they are easier to see.I just use whatever strikes my fancy at the time; it's convenient to be able to switch between the two at a whim!Of course, when quoting quote characetrs, switching between the two might not be so whimsical after all...Your team's taste or your project's coding guidelines.If you are in a multilanguage environment, you might wish to encourage the use of the same type of quotes for strings that the other language uses, for instance. Else, I personally like best the look of 'None as far as I know. Although if you look at some code, \" \" is commonly used for strings of text (I guess ' is more common inside text than \"), and ' ' appears in hashkeys and things like that.I aim to minimize both pixels and surprise. I typically prefer  in order to minimize pixels, but  instead if the string has an apostrophe, again to minimize pixels. For a docstring, however, I prefer  over  because the latter is non-standard, uncommon, and therefore surprising. If now I have a bunch of strings where I used  per the above logic, but also one that can get away with a , I may still use  in it to preserve consistency, only to minimize surprise.Perhaps it helps to think of the pixel minimization philosophy in the following way. Would you rather that English characters looked like  or ? The latter choice wastes 50% of the non-empty pixels.I use double quotes because I have been doing so for years in most languages (C++, Java, VB\u2026) except Bash, because I also use double quotes in normal text and because I'm using a (modified) non-English keyboard where both characters require the shift key. =  =  = example :  Results are the same=>> no, they're not the same.\nA single backslash will escape characters. You just happen to luck out in that example because  and  aren't valid escapes like  or  or  or If you want to use single backslashes (and have them interpreted as such), then you need to use a \"raw\" string. You can do this by putting an '' in front of the stringAs far as paths in Windows are concerned, forward slashes are interpreted the same way. Clearly the string itself is different though. I wouldn't guarantee that they're handled this way on an external device though."},
{"body": "What is the difference between old style and new style classes in Python?  Is there ever a reason to use old-style classes these days?From  :New-style classes inherit from object, or from another new-style class.Old-style classes don't.It was mentioned in other answers, but here goes a concrete example of the difference between classic MRO and C3 MRO (used in new style classes).The question is the order in which attributes (which include methods and member variables) are searched for in multiple inheritance. do a depth first search from left to right. Stop on first match. They do not have the  attribute. MRO is more complicated to synthesize in a single English sentence. It is explained in detail . One of its properties is that a Base class is only searched for once all its Derived classes have been. They have the  attribute which shows the search order. Around Python 2.5 many classes could be raised, around Python 2.6 this was removed. On Python 2.7.3:Old style classes are still marginally faster for attribute lookup. This is not usually important, but may be useful in performance-sensitive Python 2.x code:Guido has written , a really great article about new-style and old-style class in Python.Python 3 has only new-style class, even if you write an 'old-style class', it is implicitly derived from .New-style classes have some advanced features lacking in old-style classes, such as  and the new , some magical methods, etc.Here's a very practical, True/False difference. The only difference between the two versions of the following code is that in the second version Person inherits from object. Other than that the two versions are identical, but with different results :1) old-style classes2) new-style classes New-style classes inherit from  and must be written as such in Python 2.2 onwards (i.e.  instead of ). The core change is to unify types and classes, and the nice side-effect of this is that it allows you to inherit from built-in types.Read  for more details.Or rather, you should always use new-style classes,  you have code that needs to work with versions of Python older than 2.2.New style classes may use  where  is a class and  is the instance.And in Python 3.x you can simply use  inside a class with no parameters."},
{"body": "Is there a function that will trim not only spaces for whitespace, but also tabs?Whitespace on both sides:Whitespace on the right side:Whitespace on the left side:As  points out, you can provide an argument to strip arbitrary characters to any of these functions like this:This will strip any space, , , or  characters from the left-hand side, right-hand side, or both sides of the string. The examples above only remove strings from the left-hand and right-hand sides of strings. If you want to also remove characters from the middle of a string, try :That should print out:Python  method is called :For leading and trailing whitespace:Otherwise, a regular expression works:You can also use very simple, and basic function: , works with the whitespaces and tabs:Simple and easy.No one has posted these regex solutions yet.Matching:Searching (you have to handle the \"only spaces\" input case differently):If you use , you may remove inner whitespace, which could be undesirable.Whitespace includes . So an elegant and  string function we can use is .  if you want to be thoroughoutput:\n  please_remove_all_whitespacestry translateGenerally, I am using the following method:Note: This is only for removing \"\\n\", \"\\r\" and \"\\t\" only. It does not remove extra spaces.To remove , , , the better way is:Output:This is the easiest way to remove the above characters.\nIf any python package or library is available, then please let me know and also suggest how to remove character/??, occurs due to pressing Enter."},
{"body": "I have this JSON in a file:I wrote this script which prints all of the json text:How can I parse the file and extract single values?I think what Ignacio is saying is that your JSON file is incorrect. You have  when you should have .  are for lists,  are for dictionaries.Here's how your JSON file should look, your JSON file wouldn't even load for me:Then you can use your code:With data, you can now also find values like so:Try those out and see if it starts to make sense.Your  should look like this:Your code should be:Note that this only works in Python 2.6 and up, as it depends upon the . In Python 2.5 use , in Python <= 2.4, see , which this answer is based upon.You can now also access single values like this:@Justin Peel's answer is really helpful, but if you are using Python 3 reading JSON should be done like this:Note: use  instead of . In Python 3,  takes a string parameter.  takes a file-like object parameter.  returns a string object.\"Ultra JSON\" or simply \"ujson\" can handle having  in your JSON file input. If you're reading a JSON input file into your program as a list of JSON elements; such as,  ujson can handle any arbitrary order of lists of dictionaries, dictionaries of lists. You can find ujson in the  and the API is almost identical to Python's built-in  library. ujson is also much more faster if you're loading larger JSON files. You can see the performance details in comparison to other Python JSON libraries in the same link provided.if you are in python 3 here is how you can do itThe code should look like assuming connection.json file looks like above"},
{"body": "I'm building a web application with Django. The reasons I chose Django were:Now that I'm getting closer to thinking about publishing my work, I start being concerned about scale. The only information I found about the scaling capabilities of Django is provided by the Django team (I'm not saying anything to disregard them, but this is clearly not objective information...).My questions:There are, of course, many more sites and bloggers of interest, but I have got to stop somewhere!Blog post about  described as a .   and .We're doing load testing now.  We think we can support 240 concurrent requests (a sustained rate of 120 hits per second 24x7) without any significant degradation in the server performance.  That would be 432,000 hits per hour.  Response times aren't small (our transactions are large) but there's no degradation from our baseline performance as the load increases.We're using Apache front-ending Django and MySQL.  The OS is Red Hat Enterprise Linux (RHEL).  64-bit.  We use mod_wsgi in daemon mode for Django.  We've done no cache or database optimization other than to accept the defaults.  We're all in one VM on a 64-bit Dell with (I think) 32Gb RAM. Since performance is almost the same for 20 or 200 concurrent users, we don't need to spend huge amounts of time \"tweaking\".  Instead we simply need to keep our base performance up through ordinary SSL performance improvements, ordinary database design and implementation (indexing, etc.), ordinary firewall performance improvements, etc.What we do measure is our load test laptops struggling under the insane workload of 15 processes running 16 concurrent threads of requests.Not sure about the number of daily visits but here are a few examples of large Django sites:Screencast on how to deploy django with scaling in mind Here is a link to list of .In the US, . I'm told they handle roughly 10 million uniques a month. Abroad, the  network (a network of news, sports, and entertainment sites in Brazil); Alexa ranks them in to top 100 globally (around 80th currently).Other notable Django users include PBS, National Geographic, Discovery, NASA (actually a number of different divisions within NASA), and the Library of Congress.Yes -- but only if you've written your application right, and if you've got enough hardware. Django's not a magic bullet.Yes (but see above).Technology-wise, easily: see  for one attempt. Traffic-wise, compete pegs StackOverflow at under 1 million uniques per month. I can name at least dozen Django sites with more traffic than SO.Playing devil's advocate a little bit:You should check the , delivered by , titled \"Why I hate Django\" where he pretty much goes over everything Django is missing that you might want to do in a high traffic website. At the end of the day you have to take this all with an open mind because it  perfectly possible to write Django apps that scale, but I thought it was a good presentation and relevant to your question.The largest django site I know of is the , which would certainly indicate that it  scale well.Good design decisions probably have a bigger performance impact than anything else. Twitter is often cited as a site which embodies the performance issues with another dynamic interpreted language based web framework, Ruby on Rails - yet Twitter engineers have stated that the framework isn't as much an issue as some of the database design choices they made early on. Django works very nicely with memcached and provides some classes for managing the cache, which is where you would resolve the majority of your performance issues. What you deliver on the wire is almost more important than your backend in reality - using a tool like yslow is critical for a high performance web application. You can always throw more hardware at your backend, but you can't change your users bandwidth.Scaling Web apps is not about web frameworks or languages, is about your architecture.\nIt's about how you handle you browser cache, your database cache, how you use non-standard persistence providers (like ), how tuned is your database and a lot of other stuff...Don't bother...I was at the EuroDjangoCon conference the other week, and this was the subject of a couple of talks - including from the founders of what was the largest Django-based site, Pownce (slides from one talk ). The main message is that it's not Django you have to worry about, but things like proper caching, load balancing, database optimisation, etc.Django actually has hooks for most of those things - caching, in particular, is made very easy.I'm sure you're looking for a more solid answer, but the most obvious objective validation I can think of is that Google pushes Django for use with its  framework. If anybody knows about and deals with scalability on a regular basis, it's Google. From what I've read, the most limiting factor seems to be the database back-end, which is why Google uses their own...I think we might as well add Apple's App of the year for 2011, , to the list which uses django intensively.Chinese version of Stack Overflow is using Django: If you haven't already, I recommend reading the section on scaling in The Django Book:Or the newer version:Yes it can. It could be Django with Python or Ruby on Rails. It will still scale. There are few different techniques. First, caching is not scaling. You could have several application servers balanced with nginx as the front in addition to hardware balancer(s).\nTo scale on the database side you can go pretty far with read slave in MySQL / PostgreSQL if you go the RDBMS way.Some good examples of heavy traffic websites in Django could be:You can feel safe.Today we use many web apps and sites for our needs. Most of them are highly useful. I will show you some of them used by python or django.The Washington Post\u2019s website is a hugely popular online news source to accompany their daily paper. Its\u2019 huge amount of views and traffic can be easily handled by the Django web framework.\nThe National Aeronautics and Space Administration\u2019s official website is the place to find news, pictures, and videos about their ongoing space exploration. This Django website can easily handle huge amounts of views and traffic.\nThe Guardian is a British news and media website owned by the Guardian Media Group. It contains nearly all of the content of the newspapers The Guardian and The Observer. This huge data is handled by Django.\nWe all know YouTube as the place to upload cat videos and fails. As one of the most popular websites in existence, it provides us with endless hours of video entertainment. The Python programming language powers it and the features we love.DropBox started the online document storing revolution that has become part of daily life. We now store almost everything in the cloud. Dropbox allows us to store, sync, and share almost anything using the power of Python.Survey Monkey is the largest online survey company. They can handle over one million responses every day on their rewritten Python website.Quora is the number one place online to ask a question and receive answers from a community of individuals. On their Python website relevant results are answered, edited, and organized by these community members.A majority of the code for Bitly URL shortening services and analytics are all built with Python. Their service can handle hundreds of millions of events per day.Reddit is known as the front page of the internet. It is the place online to find information or entertainment based on thousands of different categories. Posts and links are user generated and are promoted to the top through votes. Many of Reddit\u2019s capabilities rely on Python for their functionality.Hipmunk is an online consumer travel site that compares the top travel sites to find you the best deals. This Python website\u2019s tools allow you to find the cheapest hotels and flights for your destination.Click here for more: \n, \nHere's a list of some relatively high-profile things built in Django:I imagine a number of these these sites probably gets well over 100k+ hits per day. Django can certainly do 100k hits/day and more. But YMMV in getting your particular site there depending on what you're building.There are caching options at the Django level (for example caching querysets and views in  can work wonders) and beyond (upstream caches like ). Database Server specifications will also be a factor (and usually the place to splurge), as is how well you've tuned it. Don't assume, for example, that Django's going set up indexes properly. Don't assume that the default  or  configuration is the right one.Furthermore, you always have the option of having multiple application servers running Django if that is the slow point, with a software or hardware load balancer in front.Finally, are you serving static content on the same server as Django? Are you using Apache or something like  or ? Can you afford to use a  for static content? These are things to think about, but it's all very speculative. 100k hits/day isn't the only variable: how much do you want to spend? How much expertise do you have managing all these components? How much time do you have to pull it all together?Note that if you're expecting 100K users per day, that are active for hours at a time (meaning max of 20K+ concurrent users), you're going to need A LOT of servers.  SO has ~15,000 registered users, and most of them are probably not active daily.  While the bulk of traffic comes from unregistered users, I'm guessing that very few of them stay on the site more than a couple minutes (i.e. they follow google search results then leave).  For that volume, expect at least 30 servers ... which is still a rather heavy 1,000 concurrent users per server.Another example is rasp.yandex.ru, Russian transport timetable service. Its attendance satisfies your requirements.I have been using Django for over a year now, and am very impressed with how it manages to combine modularity, scalability and speed of development. Like with any technology, it comes with a learning curve. However, this learning curve is made a lot less steep by the excellent documentation from the Django community. Django has been able to handle everything I have thrown at it really well. It looks like it will be able to scale well into the future. is a moderately sized Django powered website. It is a very dynamic website and does handle a good number of page views a day. If you have a site with some static content, then putting a  server in front will dramatically increase your performance. Even a single box can then easily spit out 100 Mbit/s of traffic.Note that with dynamic content, using something like Varnish becomes a lot more tricky.The developer advocate for YouTube gave a , which is also relevant to scaling Django.YouTube has more than a , and YouTube is built on Python.My experience with Django is minimal but I do remember in The Django Book they have a chapter where they interview people running some of the larger Django applications.    I guess it could provide some insights.It says curse.com is one of the largest Django applications with around 60-90 million page views in a month.You can definitely run a high-traffic site in Django. Check out this pre-Django 1.0 but still relevant post here: Check out this micro news aggregator called .It's entirely written in Django. In fact they are the people who developed the Django  framework itself.If you want to use Open source then there are many options for you. But python is best among them it have many libraries and a super awesome community.\nThese are reasons which might change your mind:Conclusion is a framework or language won't do everything for you. A better architecture, designing and strategy will give you a scalable website. Instagram is biggest example,this small team is managing such huge data. Here is one  about its architecture must read it.As stated in High Performance Django Book  \n and Go through this See further details as mentioned below:It\u2019s not uncommon to hear people say \u201cDjango doesn\u2019t scale\u201d. Depending on how you look at it, the statement is either completely true or patently false. Django, on its own, doesn\u2019t scale.The same can be said of Ruby on Rails, Flask, PHP, or any other language used by a database-driven dynamic website.The good news, however, is that Django interacts beautifully with a suite of caching and\nload balancing tools that will allow it to scale to as much traffic as you can throw at it.Contrary to what you may have read online, \nit can do so without replacing core components often labeled as \u201ctoo slow\u201d such as the database ORM or the template layer.Disqus serves over 8 billion page views per month. Those are some huge numbers. These teams have proven Django most certainly does scale.\n Our experience here at Lincoln Loop backs it up.We\u2019ve built big Django sites capable of spending the day on the Reddit homepage without breaking a sweat.Django\u2019s scaling success stories are almost too numerous to list at this point.It backs Disqus, Instagram, and Pinterest. Want some more proof? Instagram was able to sustain over 30 million users on Django with only 3 engineers (2 of which had no back-end developmentEven-though there have been a lot of great answers here, I just feel like pointing out, that nobody have put emphasis on.. If you application is light on writes, as in you are reading a lot more data from the DB than you are writing. Then scaling django should be fairly trivial, heck, it comes with some fairly decent output/view caching straight out of the box. Make use of that, and say, redis as a cache provider, put a load balancer in front of it, spin up n-instances and you should be able to deal with a VERY large amount of traffic.Now, if you have to do thousands of complex writes a second? Different story. Is Django going to be a bad choice? Well, not necessarily, depends on how you architect your solution really, and also, what your requirements are.Just my two cents :-)Spreading the tasks evenly, in short optimizing each and every aspect including DBs, Files, Images, CSS etc. and balancing the load with several other resources is necessary once your site/application starts growing. OR you make some more space for it to grow. Implementation of latest technologies like CDN, Cloud are must with huge sites. Just developing and tweaking an application won't give your the cent percent satisfation, other components also play an important role.I develop high traffic sites using Django for the national broadcaster in Ireland. It works well for us. Developing a high performance site is more than about just choosing a framework. A framework will only be one part of a system that is as strong as it's weakest link. Using the latest framework 'X' won't solve your performance issues if the problem is slow database queries or a badly configured server or network.The problem is not to know if django can scale or not. The right way is to understand and know which are the network design patterns and tools to put under your django/symfony/rails project to scale well.Some ideas can be :Hope it help a bit. This is my tiny rock to the mountain.I don't think the issue is really about Django scaling. I really suggest you look into your architecture that's what is going to help you with you scaling needs.If you get that wrong there is not point on how well Django performs. Performance != Scale. You can have a system that has amazing performance but does not scale and vice versa.Is your application database bound? If it is then your scale issues lie there as well. How are you planning on interacting with the database from Django? What happens when you database cannot process requests as fast as Django accepts them? What happens when your data outgrows one physical machine. You need to account for how you plan on dealing with those circumstances.Moreover, What happens when your traffic outgrows one app server? how you handle sessions in this case can be tricky, more often than not you'd probably require a shared nothing architecture. Again that depends on your application.In short Languages is not what determines scale, a language is responsible for performance(again depending on your applications different languages perform differently). It is your design and architecture that makes scaling a reality. I hope it helps, would be glad to help further if you have questions."},
{"body": "I'm using this code to get standard output from an external program:The communicate() method returns an array of bytes:However, I'd like to work with the output as a normal Python string. So that I could print it like this:I thought that's what the  method is for, but when I tried it, I got the same byte array again:Does anybody know how to convert the bytes value back to string? I mean, using the \"batteries\" instead of doing it manually. And I'd like it to be ok with Python 3.You need to decode the bytes object to produce a string:I think this way is easy:You need to decode the byte string and turn it in to a character (unicode) string. or If you don't know the encoding, then to read binary input into string in Python 3 and Python 2 compatible way, use ancient MS-DOS  encoding:Because encoding is unknown, expect non-English symbols to translate to characters of  (English chars are not translated, because they match in most single byte encodings and UTF-8).Decoding arbitrary binary input to UTF-8 is unsafe, because you may get this:The same applies to , which was popular (default?) for Python 2. See the missing points in  - it is where Python chokes with infamous .: There are rumors that Python 3 has  error strategy for encoding stuff into binary data without data loss and crashes, but it needs conversion tests  to validate both performance and reliability.: Thanks to comment by Nearoo - there is also a possibility to slash escape all unknown bytes with  error handler. That works only for Python 3, so even with this workaround you will still get inconsistent output from different Python versions:See  for details.: I decided to implement slash escaping decode that works for both Python 2 and Python 3. It should be slower that  solution, but it should produce  on every Python version.I think what you actually want is this:Aaron's answer was correct, except that you need to know WHICH encoding to use. And I believe that Windows uses 'windows-1252'. It will only matter if you have some unusual (non-ascii) characters in your content, but then it will make a difference.By the way, the fact that it DOES matter is the reason that Python moved to using two different types for binary and text data: it can't convert magically between them because it doesn't know the encoding unless you tell it! The only way YOU would know is to read the Windows documentation (or read it here).Set universal_newlines to True, i.e.In Python 3 you can use directly:which is equivalent tohere the default encoding is \"utf-8\", or you can check it by:While  just works, a user You can use has a To interpret a byte sequence as a text, you have to know the\ncorresponding character encoding:Example: command may produce output that can't be interpreted as text. File names\non Unix may be any sequence of bytes except slash  and zero\n:Trying to decode such byte soup using utf-8 encoding raises .It can be worse. The decoding may fail silently and produce \nif you use a wrong incompatible encoding:The data is corrupted but your program remains unaware that a failure\nhas occurred.In general, what character encoding to use is not embedded in the byte sequence itself. You have to communicate this info out-of-band. Some outcomes are more likely than others and therefore  module exists that can  the character encoding. A single Python script may use multiple character encodings in different places. output can be converted to a Python string using \nfunction that succeeds even for  (it uses\n and  error handler on\nUnix):To get the original bytes, you could use .If you pass  parameter then  uses\n to decode bytes e.g., it can be\n on Windows.To decode the byte stream on-the-fly,\n\ncould be used: .Different commands may use different character encodings for their\noutput e.g.,  internal command () may use cp437. To decode its\noutput, you could pass the encoding explicitly (Python 3.6+):The filenames may differ from  (which uses Windows\nUnicode API) e.g.,  can be substituted with \u2014Python's\ncp437 codec maps  to control character U+0014 instead of\nU+00B6 (\u00b6). To support filenames with arbitrary Unicode characters, see  From ,To write or read binary data from/to the standard streams, use the underlying binary buffer. For example, to write bytes to stdout, use sys.stdout.buffer.write(b'abc').For Python 3,this is a much safer and  approach to convert from  to :Output:I made a function to clean a listI made a function to clean a list"},
{"body": "I tried to install the Python package :But I get a cryptic error message:The same happens if I try installing the package manually:: Comments point out that the instructions here may be dangerous. Consider using the Visual C++ 2008 Express edition or the purpose-built  () and  using the original answer below. Original error message means the required version of Visual C++ is not installed.For Windows installations:While running setup.py for package installations, Python 2.7 searches for an installed Visual Studio 2008. You can trick Python to use a newer Visual Studio by setting the correct path in  environment variable before calling .Execute the following command based on the version of Visual Studio installed:WARNING: As noted below, this answer is unlikely to work if you are trying to compile python modules.See  for details.You can install compiled version from I found the solution. \nI had the exact same problem, and error, installing 'amara'. I had mingw32 installed, but distutils needed to be configured.Make sure environment is set by opening a new .What's going on? Python modules can be  (typically for speed). If you try to install such a package with Pip (or ), it has to compile that C/C++ from source. Out the box, Pip will brazenly assume you the compiler Microsoft Visual C++ installed. If you don't have it, you'll see this cryptic error message \"Error: Unable to find vcvarsall.bat\".The prescribed solution is to install a C/C++ compiler, either Microsoft Visual C++, or  (an open-source project). However, installing and configuring either is prohibitively difficult. (Edit 2014: Microsoft have published a special  for Python 2.7)The easiest solution is to use Christoph Gohlke's Windows installers (.msi) for popular Python packages. He builds installers for Python 2.x and 3.x, 32 bit and 64 bit. You can download them from If you too think \"Error: Unable to find vcvarsall.bat\" is a ludicrously cryptic and unhelpful message, then please comment on the bug at  to replace it with a more helpful and user-friendly message. For comparison, Ruby ships with a package manager Gem and offers a quasi-official C/C++ compiler, DevKit. If you try to install a package without it, you see this helpful friendly useful message:You can read a longer rant about Python packaging at You'll need to install a Microsoft compiler, compatible with the compiler used to build Python. This means you need Visual C++ 2008 (or newer, with ).Microsoft now supplies a bundled compiler and headers  to be able to compile Python extensions, at the memorable URL: This is a relatively small package; 85MB to download, installable without admin privileges, no reboot required. The name is a little misleading, the compiler will work for any Python version originally compiled with Visual C++ 2008, not just Python 2.7.If you start a Python interactive prompt or print , look for the  version string; if it is  you can use this tool.From the :Note that you need to have  installed (listed in the system requirements on the download page). The project you are installing must use , not  or the auto-detection won't work.Microsoft has stated that they want to keep the URL stable, so that automated scripts can reference it easily.I just had this same problem, so I'll tell my story here hoping it helps someone else with the same issues and save them the couple of hours I just spent:I have mingw (g++ (GCC) 4.6.1) and python 2.7.3 in a windows 7 box and I'm trying to install PyCrypto.It all started with this error when running setup.py install: Easily solved after googling the error by specifying mingw as the compiler of choice: The problem is that then I got a different error: It turns out that my anti-virus was blocking the execution of a freshly compiled .exe. I just disabled the anti-virus \"resident shield\" and went to the next error:This solved it: \"Either install a slightly older version of MinGW, or edit distutils\\cygwinccompiler.py in your Python directory to remove all instances of -mno-cygwin.\" (from )Now, I can finally start working.Looks like its looking for VC compilers, so you could try to mention compiler type with , since you have msysI have python 2.73 and windows 7 .The solution that worked for me was:To deal with MinGW not recognizing the -mno-cygwin flag anymore, remove the flag in C:\\Python27\\Lib\\distutils\\cygwincompiler.py line 322 to 326, so it looks like this:Look in the  file of the package you are trying to install. If it is an older package it may be importing  rather than .I ran in to this (in 2015) with a combination of these factors:If you use a recent version of pip, it will force (monkeypatch) the package to use setuptools, even if its  calls for distutils. However, if you are not using pip, and instead are just doing , the build process will use , which does not know about the compiler install location. Open the appropriate Visual C++ 2008 Command PromptOpen the Start menu or Start screen, and search for \"Visual C++ 2008 32-bit Command Prompt\" (if your python is 32-bit) or \"Visual C++ 2008 64-bit Command Prompt\" (if your python is 64-bit). Run it. The command prompt should say Visual C++ 2008 ... in the title bar. Set environment variablesSet these environment variables in the command prompt you just opened.Reference  Build and install to the package you want to build, and run , then . If you want to install in to a virtualenv, activate it before you build. Maybe somebody can be interested, the following worked for me for the py2exe package.\n(I have windows 7 64 bit and portable python 2.7, Visual Studio 2005 Express with Windows SDK for Windows 7 and .NET Framework 4)then:I tried all the above answers, and found all of them not to work, this was perhaps I was using Windows 8 and had installed Visual Studio 2012. In this case, this is what you do.The  file is located here:\nSimply select the file, and copy it.Then go to this directory:\nand paste the file. And then, all should be well.I spent almost 2 days figuring out how to fix this problem in my python 3.4 64 bit version: Python 3.4.3 (v3.4.3:9b73f1c3e601, Feb 24 2015, 22:44:40) [MSC v.1600 64 bit (AMD64)] on win32 (before reading this, read first Solution 2 below)\nFinally, this is what helped me:It took very long - several minutes for numpy to compile, I even thought that there was an error, but finally everything was ok.\n(I know this approach has already been mentioned in a highly voted , but let me repeat since it really is easier)\nAfter going through all of this work I understood that the best way for me is just to use already precompiled binaries from  in future. There is very small chance that I will ever need some package (or a version of a package) which this site doesn't contain. The installation process is also much quicker this way. For example, to install :You can download the free Visual C++ 2008 Express Edition from , which will set the VS90COMNTOOLS environment variable during installation and therefore build with a compatible compiler.As @PiotrDobrogost mentioned in a comment, his answer to this other question goes into details about why Visual C++ 2008 is the right thing to build with, but this can change as the Windows build of Python moves to newer versions of Visual Studio: I had this problem using , and unfortunately the packages I needed didn't have suitable exe or wheels that I could use. This system requires a few 'workarounds', which are detailed below (and ).Using the info in , I determined I needed Visual Studio C++ 2010 (sys.version return MSC v.1600), so I installed Visual C++ 2010 Express from the link in his answer, which is . I installed everything with updates, but as you can read below, this was a mistake. Only the original version of Express should be installed at this time (no updated anything).vcvarsall.bat was now present, but there was a new error when installing the package, . There are other stackoverflow questions with this error, such as I determined from that answer that 2010 Express only installs 32-bit compilers. To get 64-bit (and other) compilers, you need to install Windows 7.1 SDK. See This would not install for me though, and the installer returned the error . I found the solution at the following link: . In short, if newer versions of x86 and x64 Microsoft Visual C++ 2010 Redistributable's are installed, they conflict with the ones in SDK installer, and need uninstalling first.The SDK then installed, but I noticed vcvars64.bat still did not exist in , nor its subfolders. vcvarsall.bat runs the vcvars64 batch file, so without it, the python package still wouldn't install (I forgot the error that was shown at this time).I then found some instructions here: \nFollowing the instructions, I had already installed Express and 7.1 SDK, so installed SDK 7.1 SP1, and did the missing header file fix. I then manually created vcvars64.bat with the content . I will paste all those instructions here, so they don't get lost.My python package still did not install (can't recall error). I then found some instructions (copied below) to use the special SDK 7.1 Command Prompt, see: I opened the Windows SDK 7.1 Command Prompt as instructed, and used it to run easy_install on the python package. And at last, success!;I wanted to run pysph on Windows 10 under Python 2.7 and got vcvarsall.bat was not found (from distutils)My solution was the following:Install Microsoft Visual C++ for Python 2.7 (like @Michael suggested)On Windows 10 it was installed into (my username is Andreas):Set environment variable  to the installation path of Visual C++ for Python 2.7 (see above path).If it still doesn't work, then modifiy in the modulethe file . Find in it the function  and do following modification.Replace the line:withThis is where vcvarsall.bat resides in my case (check, where vcvarsall.bat is in your installation).I encountered this issue when I tried to install numpy library on my python 3.5. The solution is to install VS2015. I had VS2008, 2012, 2013, none of which is compatible with python 3.5. Apparently newer version of python has dependency on newer versions of VS.Also make sure C++ Common Tools are installed with Visual Studio.I tried many solutions but only one worked for me, the install of Microsoft Visual Studio  Express C++.I got this issue with a Python 2.7 module written in C (yEnc, which has other issues with MS VS). Note that Python 2.7 is built with MS VS 2008 version, not 2010!Despite the fact it's free, it is quite hard to find since MS is promoting VS 2010.\nStill, the MSDN official very direct links are still working: check  for download links.If you have mingw installedworks, forcing pip to build using the mingw compiler instead of Microsoft's. See here  for details (last post).Is Microsoft Visual C++ Compiler for Python 2.7 at  not a solution? The easiest way to solve this in 2016 is to install Chocolatey and then the   package. Open Powershell:You can use easy_install instead of pip it works for me.I don't know if it is too late, but I found  which readsHope this helps!If you're looking to install pyodbc on a Windows box that  have Visual Studio installed another option is to manually install pyodbc using the binary distribution.This is particularly useful if you do not have administrator privileges on the machine you're working with and are trying to set up a .Steps:With Python 3.4, the dependency is on Visual Studio 2010. Installing Visual C++ 2010 Express fixed the problem for me. Tricking it into using the VS 2008 or 2013 installs that I happened to have didn't work.The answer given by @monkey is one of the correct ones, but it is incomplete.In case you'd like to use , you should select the C, C++ and also other development tools suggested during the MinGW installation process to also get \"make.exe.\"You must also have the path set to make.exe in the env.To complete his answer, here are the steps:Make sure the environment variables is set by opening a new cmd.exe.I had the same error (which I find silly and not really helpful whatsoever as error messages go) and continued having problems, despite having a C compiler available. Surprising, what ended up working for me was simply . Hope this helps someone else out there.:If you have , the solution is simply to install \nVC++ 2010 since it is used to compile itself into.my python version is\nMSC v.1600 32 bit (intel)] on win32Go here: There are instructions to install anaconda which will provide a GUI and a silent install of a majority of the packages that seem to be causing this issue from .  I am aware of the solution for 2.7 here  but I did not see an option for Python 3.4.  After downloading and installing Anaconda you should be able to import a majority of the packages you need from scipy.Hope this helps some people.  Took me 45 minutes of scouring posts and sites.EDIT: Just wanted to note there is a Python34 link on the GUI page next to the OS symbols.Install Visual Studio 2015 Community Edition from ,\nthenfor Python 3.4"},
{"body": "I want to clear this up once and for all. Can someone please explain the exact meaning of having leading underscores before an object's name in Python? Also explain the difference between a single and a double leading underscore. Also, does that meaning stay the same whether the object in question is a variable, a function, a method, etc?Names, in a class, with a leading underscore are simply to indicate to other programmers that the attribute or method is intended to be private.  However, nothing special is done with the name itself.To quote :From :And a warning from the same page:Excellent answers so far but some tidbits are missing. A single leading underscore isn't exactly  a convention: if you use , and module  does not define an  list, the names imported from the module  include those with a leading underscore. Let's say it's  a convention, since this case is a pretty obscure corner;-).The leading-underscore convention is widely used not just for  names, but also for what C++ would call  ones -- for example, names of methods that are fully intended to be overridden by subclasses (even ones that  to be overridden since in the base class they !-) are often single-leading-underscore names to indicate to code  instances of that class (or subclasses) that said methods are not meant to be called directly.For example, to make a thread-safe queue with a different queueing discipline than FIFO, one imports Queue, subclasses Queue.Queue, and overrides such methods as  and ; \"client code\" never calls those (\"hook\") methods, but rather the (\"organizing\") public methods such as  and  (this is known as the  design pattern -- see e.g.  for an interesting presentation based on a video of a talk of mine on the subject, with the addition of synopses of the transcript).: this is just a convention, a way for the Python system to use names that won't conflict with user names.: this is just a convention, a way for the programmer to indicate that the variable is private (whatever that means in Python).: this has real meaning: the interpreter replaces this name with  as a way to ensure that the name will not overlap with a similar name in another class.No other form of underscores have meaning in the Python world.There's no difference between class, variable, global, etc in these conventions. is semiprivate and meant just for convention is often incorrectly considered superprivate, while it's actual meaning is just to namemangle to  is typically reserved for builtin methods or variablesYou can still access  variables if you desperately want to. The double underscores just namemangles, or renames, the variable to something like Example:t._b is accessible because it is only hidden by conventiont.__a isn't found because it no longer exists due to namemanglingBy accessing  instead of just the double underscore name, you can access the hidden valuePython doesn't have real private methods, so one underscore at the start of a method or attribute name means you shouldn't access this method, because it's not part of the API.code snippet taken from django source code (django/forms/forms.py). This means errors is a property, and it's part of the module, but the method this property calls, _get_errors, is \"private\", so you shouldn't access it.This causes a lot of confusion. It should not be used to create a private method. It should be used to avoid your method to be overridden by a subclass or accessed accidentally. Let's see an example:Output: Now create a subclass B and do customization for __test methodOutput will be....As we have seen, A.test() didn't call B.__test() methods, as we might expect. But in fact, this is the correct behavior for __. So when you create a method starting with __ it means that you don't want to anyone to be able to override it, it will be accessible only from inside the own class.When we see a method like , don't call it. Because it means it's a method which python calls, not by you. Let's take a look:There is always an operator or native function which calls these magic methods. Sometimes it's just a hook python calls in specific situations. For example  is called when the object is created after  is called to build the instance...Let's take an example...For more details  will help more.Please find more magic methods in python here. Sometimes you have what appears to be a tuple with a leading underscore as in In this case, what's going on is that _() is an alias for a localization function that operates on text to put it into the proper language, etc. based on the locale. For example, Sphinx does this, and you'll find among the importsand in sphinx.locale, _() is assigned as an alias of some localization function.If one really wants to make a variable read-only, IMHO the best way would be to use property() with only getter passed to it. With property() we can have complete control over the data.I understand that OP asked a little different question but since I found another question asking for 'how to set private variables' marked duplicate with this one, I thought of adding this additional info here.Single leading underscores is a convention. there is no difference from the interpreter's point of view if whether names starts with a single underscore or not. Double leading and trailing underscores are used for built-in methods, such as , , etc.Double leading underscores w/o trailing counterparts are a convention too, however, the class methods will be  by the interpreter. For variables or basic function names no difference exists.Your question is good, it is not only about methods. Functions and objects in modules are commonly prefixed with one underscore as well, and can be prefixed by two.But __double_underscore names are not name-mangled in modules, for example. What happens is that names beginning with one (or more) underscores are not imported if you import all from a module (from module import *), nor are the names shown in help(module).\u201cPrivate\u201d instance variables that cannot be accessed except from inside an object don\u2019t exist in Python. However, there is a convention that is followed by most Python code: a name prefixed with an underscore (e.g. _spam) should be treated as a non-public part of the API (whether it is a function, a method or a data member). It should be considered an implementation detail and subject to change without notice.reference\nHere is a simple illustrative example on how double underscore properties can affect an inherited class. So with the following setup:if you then create a child instance in the python REPL, you will see the belowThis may be obvious to some, but it caught me off guard in a much more complex environment"},
{"body": "How do I see the type of a variable whether it is unsigned 32 bit, signed 16 bit, etc.?How do I view it?Python doesn't have the same types as C/C++, which appears to be your question.Try this:The distinction between int and long goes away in Python 3.0, though.You may be looking for the  function.See the examples below, but there's no \"unsigned\" type in Python just like Java.Positive integer: positive integer:Negative integer:Literal sequence of characters:It is so simple. You do it like this.So if you have a variable, for example:You want to know its type?There are right ways and wrong ways to do just about everything in Python. Here's the right way: You can use the  attribute to get the name of the object. (This is one of the few special attributes that you need to use the  name to get to - there's not even a method for it in the  module.) (Except when absolutely necessary.)Since  gives us the class of the object, we should avoid getting this directly. :This is usually the first idea people have when accessing the type of an object in a method - they're already looking for attributes, so type seems weird. For example:Don't. Instead, do type(self):In Python, these specifics are implementation details. So, in general, we don't usually worry about this in Python. However, to sate your curiosity...In Python 2, int is usually a signed integer equal to the implementation's  width (limited by the system). It's usually implemented as a . When integers get bigger than this, we usually convert them to Python longs (with unlimited precision, not to be confused with C longs).For example, in a 32 bit Python 2, we can deduce that int is a signed 32 bit integer:In Python 3, the old int goes away, and we just use (Python's) long as int, which has We can also get some information about Python's floats, which are usually implemented as a  in C:Don't use , a semantically private attribute, to get the type of a variable. Use  instead. And don't worry too much about the implementation details of Python. I've not had to deal with issues around this myself. You probably won't either, and if you really do, you should know enough not to be looking to this answer for what to do.One more way using :The question is somewhat ambiguous -- I'm not sure what you mean by \"view\". If you are trying to  the type of a native Python object, 's answer will steer you in the right direction.However, if you are trying to  Python objects that have the semantics of primitive C-types, (such as , ), use the  module. You can determine the number of bits in a given C-type primitive thusly:This is also reflected in the  module, which can make arrays of these lower-level types:The maximum integer supported (Python 2's ) is given by .There is also , which returns the actual size of the  object in residual memory:For float data and precision data, use :Do you mean in  or using ?In the first case, you simply cannot - because Python does not have signed/unsigned, 16/32 bit integers.In the second case, you can use :For more reference on ctypes, an its type, see .It may be little irrelevant. but you can check types of an object with  as mentioned .I also highly recommend the  interactive interpreter when dealing with questions like this. It lets you type  and will return a whole list of information about the object including the type and the doc string for the type.e.g.Python doesn't have such types as you describe. There are two types used to represent integral values: , which corresponds to platform's int type in C, and , which is an arbitrary precision integer (i.e. it grows as needed and doesn't have an upper limit). s are silently converted to  if an expression produces result which cannot be stored in .Examples of simple type checking in Python:It really depends on what level you mean. In Python 2.x, there are two integer types,  (constrained to ) and  (unlimited precision), for historical reasons. In Python code, this shouldn't make a bit of difference because the interpreter automatically converts to long when a number is too large. If you want to know about the actual data types used in the underlying interpreter, that's implementation dependent. (CPython's are located in Objects/intobject.c and Objects/longobject.c.) To find out about the systems types look at cdleary answer for using the struct module.Simple, for python 3.4 and abovePython 2.7 and above"},
{"body": "I want to convert an integer to a string in Python. I am typecasting it in vain:When I try to convert it to string, it's showing an error like  doesn't have any attribute called .[Edit]Links to the documentation:\n\n[Edit]The problem seems to come from this line: \nConversion to string is done with the builtin  function, which basically calls the  method of its parameter.Also, it shouldn't be necessary to call . Try using the  operator.Try this:There is not typecast and no type coercion in Python. You have to convert your variable in an explicit way.To convert an object in string you use the  function. It works with any object that has a method  called  defined. In factis equivalent toThe same if you want to convert something to int, float, etc.You can use  which gives you a string object of .To manage non-integer inputs:Ok, if I take your latest code and rewrite a bit to get it working with Python:It gives me something like:Which is the first characters of the string result .\nWhat are we trying to do here?The most decent way in my opinion is ``."},
{"body": "What is the most pythonic way to pad a numeric string with zeroes to the left, i.e., so the numeric string has a specific length?Strings:And for numbers:.Just use the  method of the string object.This example will make a string of 10 characters long, padding as necessary.For numbers:See also: .: It's worth noting that as of  December 3rd, 2008, this method of formatting is deprecated in favour of the  string method:See  for details.if you want the opposite: will work with s, s, s... and is Python 2. and 3. compatible:Works in both Python 2 and Python 3:See the print documentation for all the exciting details!That last line should now be:I.e.  is now a function, not a statement. Note that I still prefer the Old School  style because, IMNSHO, it reads better, and because, um, I've been using that notation since January, 1980. Something ... old dogs .. something something ... new tricks.For the ones who came here to understand and not just a quick answer.\nI do these especially for time strings:this will work in python 3.xFor zip codes saved as integers:You could also repeat \"0\", prepend it to  and get the rightmost width slice. Quick and dirty little expression."},
{"body": "I was working on my AI pathfinding(which you don't need to understand),\nfor some reason, my On[3] list was expanding when I did this in the shell:\n(After the program messed up.) WHY?The program didn't crash, but that isn't my question.\nA screenshot(Ignore the extra prints, I was trying to narrow down the code that was causing it.) was my Y coordinates.The code in question is at # Find Path\n(Under the bottom section.)\nMy code(Over 200 lines long. :/)I don't use classes or sprites. :P\nThanks for taking a look!Because your  and  are the same object. And this is how it works with same objects:"},
{"body": "When debugging in PHP, I frequently find it useful to simply stick a  in my code to show me what a variable is, what its value is, and the same for anything that it contains.What is a good Python equivalent for this?To display a value nicely, you can use the  module. The easiest way to dump all variables with it is to doIf you are running in CGI, a useful debugging feature is the  module, which displays the value of local variables as part of the traceback.I think the best equivalent to PHP's  is combine  with :The closest thing to 's  is  with the  function in the built-in  module:PHP's  usually shows a serialized version of the object that can be exec()'d to re-create the object.    The closest thing to that in Python is \"For many types, this function makes an attempt to return a string that would yield an object with the same value when passed to eval() [...]\"So I have taken the answers from this question and  and came up below.  I suspect this is not pythonic enough for most people, but I really wanted something that let me get a deep representation of the values some unknown variable has.  I would appreciate any suggestions about how I can improve this or achieve the same behavior easier.Here is the usageand the results.I wrote a very light-weight alternative to PHP's var_dump for using in Python and made it open source later.GitHub: You can simply install it using :For your own classes, just def a  methodOld topic, but worth a try.Here is a simple and efficient var_dump function:Sample output:I don't have PHP experience, but I have an understanding of the Python standard library.For your purposes, Python has several methods: module;Object serialization module which is called . You may write your own wrapper of the pickle module.If your using  for testing, Python has its own  and  modules. It's very simple and fast for design.I use self-written Printer class, but dir() is also good for outputting the instance fields/values.The sample of usage:"},
{"body": "There seem to be many ways to define  in Python. Is there a consensus opinion on Stack\u00a0Overflow?I don't really see the need, as a module with functions (and not a class) would serve well as a singleton. All its variables would be bound to the module, which could not be instantiated repeatedly anyway. If you do wish to use a class, there is no way of creating private classes or private constructors in Python, so you can't protect against multiple instantiations, other than just via convention in use of your API. I would still just put methods in a module, and consider the module as the singleton.Here's my own implementation of singletons. All you have to do is decorate the class; to get the singleton, you then have to use the  method. Here's an example:And here's the code:You can override the  method like this: A slightly different approach to implement the singleton in Python is the  by Alex Martelli (Google employee and Python genius).So instead of forcing all instances to have the same identity, they share state.The module approach works well. If I absolutely need a singleton I prefer the Metaclass approach.See this implementation from , implementing the singleton pattern with a decorator:As the  says, the most idiomatic way is to just .With that in mind, here's a proof of concept:See the  for more details on .Example:Notes:The one time I wrote a singleton in Python I used a class where all the member functions had the classmethod decorator.I'm very unsure about this, but my project uses 'convention singletons' (not enforced singletons9, that is, if I have a class called DataController, I define this in the same module:It is not elegant, since it's a full six lines. But all my singletons use this pattern, and it's at least very explicit (which is pythonic).Creating a singleton decorator (aka an annotation) is an elegant way if you want to decorate (annotate) classes going forward. Then you just put @singleton before your class definition. The  does cover this:I would probably rewrite it to look more like this:It should be relatively clean to extend this:There are also some interesting articles on the Google Testing blog, discussing why singleton are/may be bad and are an anti-pattern:Here is an example from Peter Norvig's Python IAQ  (You should use search feature of your browser to find this question, there is no direct link, sorry)Also Bruce Eckel has another example in his book  (again there is no direct link to the code)I think that  a class or an instance to be a singleton is overkill. Personally, I like to define a normal instantiable class, a semi-private reference, and a simple factory function.Or if there is no issue with instantiating when the module is first imported:That way you can write tests against fresh instances without side effects, and there is no need for sprinkling the module with global statements, and if needed you can derive variants in the future. courtesy of ActiveState.It looks like the trick is to put the class that's supposed to only have one instance inside of another class.My simple solution which is based on the default value of function parameters.Being relatively new to Python I'm not sure what the most common idiom is, but the simplest thing I can think of is just using a module instead of a class. What would have been instance methods on your class become just functions in the module and any data just becomes variables in the module instead of members of the class. I suspect this is the pythonic approach to solving the type of problem that people use singletons for.If you really want a singleton class, there's a reasonable implementation described on the  for \"Python singleton\", specifically:That seems to do the trick.OK, singleton could be good or evil, I know. This is my implementation, and I simply extend a classic approach to introduce a cache inside and produce many instances of a different type or, many instances of same type, but with different arguments.I called it Singleton_group, because it groups similar instances together and prevent that an object of the same class, with same arguments, could be created:Every object carries the singleton cache... This could be evil, but it works great for some :)I completely agree with staale and I leave here a sample of creating a singleton half brother: will report now as being of the same class as singleton even if it does not look like it. So singletons using complicated classes end up depending on we don't mess much with them.Being so, we can have the same effect and use simpler things like a variable or a module. Still, if we want use classes for clarity and because , so we already have the object (not and instance, but it will do just like).There we have a nice assertion error if we try to create an instance, and we can store on derivations static members and make changes to them at runtime (I love Python). This object is as good as other about half brothers (you still can create them if you wish), however it will tend to run faster due to simplicity.In cases where you don't want the metaclass-based solution above, and you don't like the simple function decorator-based approach (e.g. because in that case static methods on the singleton class won't work), this compromise works:"},
{"body": "In a comment on the , someone said they weren't sure what functools.wraps was doing.  So I'm asking this question so that there will be a record of it on StackOverflow for future reference: what does functools.wraps do, exactly?When you use a decorator, you're replacing one function with another.  In other words, if you have a decoratorthen when you sayit's exactly the same as sayingand your function f is replaced with the function with_logging.  Unfortunately, this means that if you then sayit will print  because that's the name of your new function.  In fact, if you look at the docstring for f, it will be blank because with_logging has no docstring, and so the docstring you wrote won't be there anymore.  Also, if you look at the pydoc result for that function, it won't be listed as taking one argument ; instead it'll be listed as taking  and  because that's what with_logging takes.If using a decorator always meant losing this information about a function, it would be a serious problem.  That's why we have .  This takes a function used in a decorator and adds the functionality of copying over the function name, docstring, arguments list, etc.  And since  is itself a decorator, the following code does the correct thing:I very often use classes, rather than functions, for my decorators.  I was having some trouble with this because an object won't have all the same attributes that are expected of a function.  For example, an object won't have the attribute .  I had a specific issue with this that was pretty hard to trace where Django was reporting the error \"object has no attribute ''\".  Unfortunately, for class-style decorators, I don't believe that @wrap will do the job.  I have instead created a base decorator class like so:This class proxies all the attribute calls over to the function that is being decorated.  So, you can now create a simple decorator that checks that 2 arguments are specified like so:"},
{"body": "I have scripts calling other script files but I need to get the filepath of the file that is currently running within the process. For example, let's say I have three files. Using :How can I get the file name and path of , , without having to pass that information as arguments from ?(Executing  returns the original starting script's filepath not the current file's.)p1.py:p2.py:as others have said. You may also want to use:I think this is cleaner:and gets the same information as:Where [0] is the current frame in the stack (top of stack) and [1] is for the file name, increase to go backwards in the stack i.e.would be the file name of the script that called the current frame. Also, using [-1] will get you to the bottom of the stack, the original calling script.Here is an experiment based on the answers in this thread - with Python 2.7.10 on Windows., ie -Here's to these being added to  as functions! Credit to @Usagi and @pablogBased on the following three files, and running script1.py from its folder with  (also tried execfiles with absolute paths and calling from a separate folder). C:\\testpath\\script1.py: \nC:\\testpath\\script2.py: \nC:\\testpath\\lib\\script3.py:The suggestions marked as best are all true if your script consists of only one file. If you want to find out the name of the executable (i.e. the root file passed to the python interpreter for the current program) from a file that may be imported as a module, you need to do this (let's assume this is in a file named ):Because the last thing () on the stack is the first thing that went into it (stacks are LIFO/FILO data structures).Then in file  if you  it'll print , rather than , which would be the value of all of these:this will give us the filename only. i.e. if abspath of file is c:\\abcd\\abc.py then 2nd line will print abc.pyIt's not entirely clear what you mean by \"the filepath of the file that is currently running within the process\".\n usually contains the location of the script that was invoked by the Python interpreter.\nCheck the  for more details.As @Tim and @Pat Notz have pointed out, the __file__ attribute provides access toThe  attribute works for both the file containing the main execution code as well as imported modules.See I have a script that must work under windows environment.\nThis code snipped is what I've finished with:it's quite a hacky decision. But it requires no external libraries and it's the most important thing in my case.this would print the path of the currently executing scriptI think it's just    Sounds like you may also want to checkout the .No need for inspect or any other library.This worked for me when I had to import a script (from a different directory then the executed script), that used a configuration file residing in the same folder as the imported script.You can use This should work:I used the approach with __file__\n\nbut there is a little trick, it returns the .py file \nwhen the code is run the first time, \nnext runs give the name of *.pyc file\nso I stayed with:\n\nor\n  if you want just the filename without  or  you can try this will print testscript\nyou can generate whatever you want by changing the index inside []I wrote a function which take into account eclipse  and .\nIt return the folder of the first script you launch. You can optionally specify the  var, but the main thing is that you don't have to share this variable across all your .Maybe you can handle others stack particular cases I didn't see, but for me it's ok.To get directory of executing script  "},
{"body": "How can I write a / block that catches all exceptions?You can but you shouldn't:However, this will also catch exceptions like  and you usually don't want that, do you? Unless you re-raise the exception right away - see the following example :Apart from a bare  clause (which as others have said you shouldn't use), you can simply catch :You would normally only ever consider doing this at the outermost level of your code if for example you wanted to handle any otherwise uncaught exceptions before terminating.The advantage of  over the bare  is that there are a few exceptions that it wont catch, most obviously  and : if you caught and swallowed those then you could make it hard for anyone to exit your script.You can do this to handle general exceptionsVery simple example, similar to the one found here:If you're attempting to catch ALL exceptions, then put all your code within the \"try:\" statement, in place of 'print \"Performing an action which may throw an exception.\"'.In the above example, you'd see output in this order:1) Performing an action which may throw an exception.2) Finally is called directly after executing the try statement whether an exception is thrown or not.3) \"An exception was thrown!\" or \"Everything looks great!\" depending on whether an exception was thrown.Hope this helps!To catch all possible exceptions, catch . It's on top of the Exception hierarchy:Python 3:\nPython 2.7:\nBut as other people mentioned, you should usually not do this, unless you have a very good reason.It is worth mentioning this is not proper Python coding. This will catch also many errors you might not want to catch.I've just found out this little trick for testing if exception names in Python 2.7 . Sometimes i have handled specific exceptions in the code, so i needed a test to see if that name is within a list of handled exceptions."},
{"body": "I have seen many projects using  module instead of  module from the Standard Library. Also, there are many different  modules. Why would use these alternatives, instead of the one in the Standard Library?  , added to the stdlib. But since  was added in 2.6,  has the advantage of working on more Python versions (2.4+).  is also updated more frequently than Python, so if you need (or want) the latest version, it's best to use  itself, if possible.A good practice, in my opinion, is to use one or the other as a fallback.I have to disagree with the other answers: the built in  library (in Python 2.7) is not necessarily slower than . It also doesn't have .Here is a simple benchmark:And the results on my system (Python 2.7.4, Linux 64-bit):For dumping,  is faster than .\nFor loading,  is faster.Since I am currently building a web service,  is more important\u2014and using a standard library is always preferred.Also,  was not updated in the past 4 years, so I wouldn't touch it.I've been benchmarking json, simplejson and cjson.:All of these answers aren't very helpful because they are . After doing some research of my own I found that  is indeed faster than the builtin,  you keep it updated to the latest version. wanted to install 2.3.2 on ubuntu 12.04, but after finding out the latest  version is actually 3.3.0, so I updated it and reran the time tests. The above statements are in python-2.7.3 and simplejson 3.3.0 (with c speedups)\nAnd to make sure my answer also isn't time sensitive, you should  to check since it varies so much between versions; there's no easy answer that isn't time sensitive. I recently came across a library called  that is performing ~3x faster than  with some basic tests. An API incompatibility I found, with Python 2.7 vs simplejson 3.3.1 is in whether output produces str or unicode objects.\ne.g.vsIf the preference is to use simplejson, then this can be addressed by coercing the argument string to unicode, as in:The coercion does require knowing the original charset, for example:This is the won't fix The builtin  module got included in Python 2.6.  Any projects that support versions of Python < 2.6 need to have a fallback.  In many cases, that fallback is .Another reason projects use simplejson is that the builtin json did not originally include its C speedups, so the performance difference was noticeable.Here's (a now outdated) comparison of Python json libraries: ()Regardless of the results in this comparison you should use the standard library json if you are on Python 2.6.  And.. might as well just use simplejson otherwise.simplejson module is simply 1,5 times faster than json (On my computer, with simplejson 2.1.1 and Python 2.7 x86).If you want, you can try the benchmark: \nOn my PC simplejson is faster than cPickle. I would like to know also your benchmarks!Probably, as said Coady, the difference between simplejson and json is that simplejson includes _speedups.c.\nSo, why don't python developers use simplejson?Some values are serialized differently between simplejson and json.Notably, instances of  are serialized as arrays by  but as objects by . You can override this behaviour by passing  to , but by default the behaviours do not match.I came across this question as I was looking to install simplejson for Python 2.6. I needed to use the 'object_pairs_hook' of json.load() in order to load a json file as an OrderedDict. Being familiar with more recent versions of Python I didn't realize that the json module for Python 2.6 doesn't include the 'object_pairs_hook' so I had to install simplejson for this purpose. From personal experience this is why i use simplejson as opposed to the standard json module.In python3, if you a string of ,  with  you have to  the content before you can load it.   takes care of this so you can just do ."},
{"body": "I have some data either in list contains lists, or list contains tuples.And I want to sort by the 2nd element in the subset. Meaning, sorting by 2,5,8 where 2 is from (1,2,3), 5 is from (4,5,6). What is the common way to do this? Should I store tuples or lists in my list? Since tuples are more inflexible.or: is the one I'd use.  For completeness, here's the DSU (decorate-sort-undecorate) pattern with list comprehensions:Or, more tersely:As noted in the , this has been unnecessary since Python 2.4, when key functions became available.I just want to add to Stephen's answer if you want to sort the array from high to low, another way other than in the comments above is just to add this to the line: and the result will be as follows:For sorting by multiple criteria, namely for instance by the second and third elements in a tuple, letand so define a lambda that returns a tuple that describes priority, for instanceIn more general case (without using lambda) you can use defined function:and then use key to sort it by second parameter: is somewhat faster than , but the increase is relatively modest (around 10 to 25 percent).(IPython session)In order to sort a list of tuples , for  in descending order and  in alphabetical order:I use this method:and it gives me the result:@Stephen 's answer is to the point! Here is an example for better visualization, =) is a function that will be called to transform the collection's items for comparison.. like  method in Java. The parameter passed to key must be something that is callable. Here, the use of  creates an anonymous function (which is a callable).\nThe syntax of lambda is the word lambda followed by a iterable name then a single block of code. Below example, we are sorting a list of tuple that holds the info abt time of certain event and actor name. We are sorting this list by time of event occurrence - which is the 0th element of a tuple.Note -   sorts the items of s in placeSorting a tuple is quite simple:"},
{"body": "How do I learn where the source file for a given Python module is installed? Is the method different on Windows than on Linux?I'm trying to look for the source of the  module in particular, but I'm interested in a more general answer as well.For a pure python module you can find the source by looking at .\nThe datetime module, however, is written in C, and therefore  points to a .so file (there is no  on Windows), and therefore, you can't see the source.If you download a python source tarball and extract it, the modules' code can be found in the  subdirectory.For example, if you want to find the datetime code for python 2.6, you can look atYou can also find the latest Mercurial version on the web at \nRunning  from the command line should tell you what is being imported and from where.  This works for me on Windows and Mac OS X.I'm not sure what those bad mtime's are on my install!I realize this answer is 4 years late, but the existing answers are misleading people.The right way to do this is never , or trying to walk through  and search for yourself, etc. (unless you need to be backward compatible beyond 2.1).It's the  module\u2014in particular,  or .Unless you want to learn and implement the rules (which are documented, but painful, for CPython 2.x, and not documented at all for other implementations, or 3.x) for mapping  to  files; dealing with .zip archives, eggs, and module packages; trying different ways to get the path to / files that don't support ; figuring out what Jython/IronPython/PyPy do; etc. In which case, go for it.Meanwhile, every Python version's source from 2.0+ is available online at  (e.g.,  or ). So, once you discover that  is a  or  file like , you can look it up inside the Modules directory. Strictly speaking, there's no way to be sure of which file defines which module, but nearly all of them are either  or , so it shouldn't be hard to guess that  is what you want.The  list contains the list of directories which will be searched for modules at runtime: is a builtin module, so there is no (Python) source file.For modules coming from  (or ) files, you can use , e.g.from the standard library try If you're using pip to install your modules, just  the location is returned.New in Python 3.2, you can now use e.g.  from the dis module:\nIn the python interpreter you could import the particular module and then type help(module). This gives details such as Name, File, Module Docs, Description et al.Ex:et alCheck out this  to cd to the directory containing the source for the indicated Python module:Here's a one-liner to get the filename for a module, suitable for shell aliasing:Set up as an alias:To use:On windows you can find the location of the python module as shown below:i.e find rest_framework module\nNot all python modules are written in python. Datetime happens to be one of them that is not, and (on linux) is datetime.so.You would have to download the source code to the python standard library to get at it.For those who prefer a GUI solution: if you're using a gui such as Spyder (part of the Anaconda installation) you can just right-click the module name (such as \"csv\" in \"import csv\") and select \"go to definition\" - this will open the file, but also on the top you can see the exact file location (\"C:....csv.py\") On Ubuntu 12.04, for example numpy package for python2, can be found at:Of course, this is not generic answer"},
{"body": "I am looking for minimum and maximum values for integers in python. For eg., in Java, we have  and . Is there something like this in python?In Python 2, there's :And you can calculate the minimum value with  as seen . Of course Python just switches from plain to long integers once you exceed this value.In Python 3 this value has no significance at all for integers. (However, a closely related value is still available as , in case you're actually looking for the machine's .) constant has been removed from Python 3.0 onward, instead use Refer : In Python integers will automatically switch from a fixed-size  representation into a variable width  representation once you pass the value , which is either 2 - 1 or 2 - 1 depending on your platform. Notice the  that gets appended here:From the :Python tries very hard to pretend its integers are mathematical integers and are unbounded. It can, for instance, calculate a  with ease:If you just need a number that's bigger than all others, you can usein similar fashion, a number smaller than all others:This works in both python 2 and 3.I came here from google and was looking for python 3.  In that case its, "},
{"body": "What would be the quickest way to construct a python binding to a C or C++ library?(using windows if this matters)You should have a look at , here is the short introdution taken from their website:I like  a lot,  always tended to give me . Also ctypes has the advantage that you don't need to satisfy any compile time dependency on python, and your binding will work on any python that has ctypes, not just the one it was compiled against.Suppose you have a simple C++ example class you want to talk to in a file called foo.cpp:Since ctypes can only talk to C functions, you need to provide those declaring them as extern \"C\"Next you have to compile this to a shared libraryAnd finally you have to write your python wrapper (e.g. in fooWrapper.py)Once you have that you can call it likeThe quickest way to do this is using .Example from SWIG :Interface file:Building a Python module on Unix:Usage:Note that you have to have python-dev. Also in some systems python header files will be in /usr/include/python2.7 based on the way you have installed it.From the tutorial:I started my journey in the python <-> C++ binding from this page, with the objective of linking high level data types (multidimensional STL vectors with python lists) :-)Having tried the solutions based on both  and  (and not being a software engineer) I have found them complex when high level datatypes binding is required, while I have found  much more simple for such cases.\nThis example uses therefore SWIG and it has been tested in Linux (but swig is available and is widely used in Windows too).The objective is to make available to python a C++ function that takes a matrix in form of a 2D STL vector and returns an average of each row (as a 1D STL vector).The code in C++ (\"code.cpp\") is as follow:The equivalent header (\"code.h\") is:We first compile the C++ code to create an object file:We then define a  (\"code.i\") for our C++ functions.Using swig, we generate a C++ interface source code from the swig interface definition file..We finally compile the generated C++ interface source file and link everything together to generate a shared library that is directly importable by python (the \"_\" matters):We can now use the function in python scripts:Check out  or .  They're python-like languages for interfacing between C/C++ and python.I\u2019ve never used it but I\u2019ve heard good things about . If you\u2019re trying to use it with C++, be sure to evade name mangling via .  basically says: first prototype everything in Python. Then when you need to speed a part up, use SWIG and translate this part to C.I think cffi for python can be an option.There is also , which is like a lightweight version of  and compatible with all modern C++ compilers :  The question is how to call a C function from Python, if I understood correctly. Then the best bet are Ctypes (BTW portable across all variants of Python).For a detailed guide you may want to refer to .First you should decide what is your particular purpose. The official Python documentation on  was mentioned above, I can add a good . The use cases can be divided into 3 categories:In order to give some broader perspective for other interested and since your initial question is a bit vague (\"to a C or C++ library\") I think this information might be interesting to you. On the link above you can read on disadvantages of using binary extensions and its alternatives. Apart from the other answers suggested, if you want an accelerator module, you can try . It works \"by generating optimized machine code using the LLVM compiler infrastructure at import time, runtime, or statically (using the included pycc tool)\". One of the official Python documents contains details on .\nEven without the use of , it\u2019s quite straightforward and works perfectly well on Windows.Cython is definitely the way to go, unless you anticipate writing Java wrappers, in which case SWIG may be preferable.  I recommend using the  command line utility, it makes the process of using Cython extremely easy.  If you need to pass structured data to C++, take a look at Google's protobuf library, it's very convenient.  Here is a minimal examples I made that uses both tools: Hope it can be a useful starting point.  "},
{"body": "I haven't been able to find an understandable explanation of how to actually use Python's  function.  What I'm trying to do is this:I've reviewed , and , but I've had trouble trying to apply them beyond a simple list of numbers. So, how do I use of ?  Is there another technique I should be using?  Pointers to good \"prerequisite\" reading would also be appreciated.As Sebastjan said, The part I didn't get is that in the example construction is the current grouping key, and  is an iterator that you can use to iterate over the group defined by that grouping key. In other words, the  iterator itself returns iterators.Here's an example of that, using clearer variable names:This will give you the output:In this example,  is a list of tuples where the first item in each tuple is the group the second item belongs to. The  function takes two arguments: (1) the data to group and (2) the function to group it with. Here,  tells  to use the first item in each tuple as the grouping key.In the above  statement,  returns three (key, group iterator) pairs - once for each unique key. You can use the returned iterator to iterate over each individual item in that group.Here's a slightly different example with the same data, using a list comprehension:This will give you the output:Can you show us your code?The example on the Python docs is quite straightforward:So in your case, data is a list of nodes, keyfunc is where the logic of your criteria function goes and then  groups the data.You must be careful to  by the criteria before you call  or it won't work.  method actually just iterates through a list and whenever the key changes it creates a new group.A neato trick with groupby is to run length encoding in one line:will give you a list of 2-tuples where the first element is the char and the 2nd is the number of repetitions.Another example:results inNote that igroup is an iterator (a sub-iterator as the documentation calls it).This is useful for chunking a generator:Another example of groupby - when the keys are not sorted.  In the following example, items in xx are grouped by values in yy.  In this case, one set of zeros is output first, followed by a set of ones, followed again by a set of zeros.Produces:WARNING:The syntax list(groupby(...)) won't work the way that you intend. It seems to destroy the internal iterator objects, so usingwill produce:Instead, of list(groupby(...)), try [(k, list(g)) for k,g in groupby(...)], or if you use that syntax often,and get access to the groupby functionality while avoiding those pesky (for small data) iterators all together.I would like to give another example where groupby without sort is not working. Adapted from example by James Sulakoutput isthere are two groups with vehicule, whereas one could expect only one group@CaptSolo, I tried your example, but it didn't work.Output:As you can see, there are two o's and two e's, but they got into separate groups. That's when I realized you need to sort the list passed to the groupby function. So, the correct usage would be:Output:Just remembering, if the list is not sorted, the groupby function !You can use groupby to group things to iterate over. You give groupby an iterable, and a optional  function/callable by which to check the items as they come out of the iterable, and it returns an iterator that gives a two-tuple of the result of the key callable and the actual items in another iterable. From the help:Here's an example of groupby using a coroutine to group by a count, it uses a key callable (in this case, ) to just spit out the count for however many iterations and a grouped sub-iterator of elements:prints"},
{"body": "How to select rows from a DataFrame based on values in some column in pandas?\nIn SQL I would use: To select rows whose column value equals a scalar, , use :To select rows whose column value is in an iterable, , use :Combine multiple conditions with : To select rows whose column value  , use : returns a boolean Series, so to select rows whose value is  in , negate the boolean Series using :For example,yieldsIf you have multiple values you want to include, put them in a\nlist (or more generally, any iterable) and use :yieldsNote, however, that if you wish to do this many times, it is more efficient to\nmake an index first, and then use :yieldsor, to include multiple values from the index use :yieldsThe pandas equivalent to isMultiple conditions:orIn the above code it is the line  that gives the rows based on the column value,  in this case.Multiple conditions are also possible:But at that point I would recommend using the query function, since it's less verbose and yields the same result:I find the syntax of the previous answers to be redundant and difficult to remember. Pandas introduced the  method in v0.13 and I much prefer it. For your question, you could do Reproduced from You can also access variables in the environment by prepending an .Here is a simple example  I just tried editing this, but I wasn't logged in, so I'm not sure where my edit went. I was trying to incorporate multiple selection. So I think a better answer is:For a single value, the most straightforward (human readable) is probably:For lists of values you can also use:For example,yieldsIf you have multiple criteria you want to select against, you can put them in a list and use 'isin':yieldsNote, however, that if you wish to do this many times, it is more efficient to make A the index first, and then use df.loc:yieldsIf you came here looking to select rows from a dataframe by including those whose column's value is NOT any of a list of values, here's how to flip around unutbu's answer for a list of values above:(To not include a single value, of course, you just use the regular not equals operator, .)Example:gives usTo subset to just those rows that AREN'T  or  in column :yieldsTo append to this famous question (though a bit too late): You can also do  to make a new data frame with specified column having a particular value. E.g.Run this gives:"},
{"body": "...do something here... should be :What's the easiest way to do this?The  method of a string (either ASCII or Unicode is fine) does this:However, look out for strings with embedded apostrophes, as noted in the docs.the 'title' method can't work well, Try string.capwords, From the :Just because this sort of thing is fun for me, here are two more solutions.Split into words, initial-cap each word from the split groups, and rejoin.  This will change the white space separating the words into a single white space, no matter what it was.EDIT: I don't remember what I was thinking back when I wrote the above code, but there is no need to build an explicit list; we can use a generator expression to do it in lazy fashion.  So here is a better solution:Use a regular expression to match the beginning of the string, or white space separating words, plus a single non-whitespace character; use parentheses to mark \"match groups\".  Write a function that takes a match object, and returns the white space match group unchanged and the non-whitespace character match group in upper case.  Then use  to replace the patterns.  This one does not have the punctuation problems of the first solution, nor does it redo the white space like my first solution.  This one produces the best result.I'm glad I researched this answer.  I had no idea that  could take a function!  You can do nontrivial processing inside  to produce the final result!Copy-paste-ready version of @jibberia anwser:Why do you complicate your life with joins and for loops when the solution is simple and safe??Just do this:If str.title() doesn't work for you, do the capitalization yourself.One-liner:Clear example:Capitalize words...As Mark pointed out you should use :However, if would like to make the first letter uppercase , you could use this:or using a variable:The suggested method str.title() does not work in all cases.\nFor example:instead of . I think, it is better to do something like this:The simplest solution is to split the sentence into words and capitalize the first letter then join it back together.If you don't want to split the input string into words first, and using fancy generators:or without importing itertoolsor you can use regular expressions, from  Now, these are some  that were posted, and inputs for which they don't work as expected if we are using the definition of a word being the start of the sentence or anything after a blank space:using ' ' for the split will fix the second output, but capwords() still won't work for the firstBe careful with multiple blank spacesAn empty string will raise an Error if you access [1:], therefore I would use:to uppercase the first letter only.I really like this answer:Copy-paste-ready version of @jibberia anwser:But some of the lines that I was sending split off some blank '' characters that caused errors when trying to do s[1:].  There is probably a better way to do this, but I had to add in a if len(s)>0, as in"},
{"body": "Inspired by the question series 'Hidden features of ...', I am curious to hear about your favorite Django tips or lesser known but useful features you know of.I'm just going to start with a tip from myself :)Don't hardcode path's in your settings.py if you want to run your project in different locations. Use the following code in settings.py if your templates and static files are located within the Django project directory:Credits: I got this tip from the screencast ''.Install  and  and then issue the following command to get a really nice looking Django model visualization:Use   decorator instead of .Edited to point out that returning an HttpResponse (such as a redirect) will short circuit the decorator and work just as you expect.There's a set of custom tags I use all over my site's templates. Looking for a way to autoload it (DRY, remember?), I found the following:If you put this in a module that's loaded by default (your main urlconf for instance), you'll have the tags and filters from your custom tag module available in any template, without using .The argument passed to  can be any module path; your custom tag module doesn't have to live in a specific application. For example, it can also be a module in your project's root directory (eg. ). + Python = life saver if you are working on multiple Django projects and there is a possibility that they all don't depend on the same version of Django/an application.Don't hard-code your URLs! Use  instead, and the  function to get the URL itself.When you define your URL mappings, give names to your URLs. Make sure the name is unique per URL.I usually have a consistent format \"project-appplication-view\", e.g. \"cbx-forum-thread\" for a thread view. (shamelessly stealing ):This name can be used in templates with the .Use . For example, it allows to view all SQL queries performed while rendering view and you can also view stacktrace for any of them.Don't write your own login pages.  If you're using django.contrib.auth.The real, dirty secret is that if you're also using django.contrib.admin, and django.template.loaders.app_directories.load_template_source is in your template loaders,  Say you have a different user model and you want to include\nthat in every response. Instead of doing this:Context processes give you the ability to pass any variable to your\ntemplates. I typically put mine in :In your  add the following line to your Now every time a request is made it includes the  key automatically.I wrote a blog post about this a few months ago so I'm just going to cut and paste:Out of the box Django gives you several signals that are\nincredibly useful. You have the ability to do things pre and\npost save, init, delete, or even when a request is being\nprocessed. So lets get away from the concepts and\ndemonstrate how these are used. Say we\u2019ve got a blogSo somehow you want to notify one of the many blog-pinging\nservices we\u2019ve made a new post, rebuild the most recent\nposts cache, and tweet about it. Well with signals you have\nthe ability to do all of this without having to add any\nmethods to the Post class.There we go, by defining that function and using the\npost_init signal to connect the function to the Post model\nand execute it after it has been saved.When I was starting out, I didn't know that there was a , make sure you know of its existence!!Use  to jump into your code at any level and debug using the power of IPython.  Once you have installed IPython just put this code in wherever you want to debug: Then, refresh the page, go to your runserver window and you will be in an interactive IPython window.I have a snippet set up in TextMate so I just type ipshell and hit tab.  I couldn't live without it.Run a development SMTP server that will just output whatever is sent to it (if you don't want to actually install SMTP on your dev server.)command line:From the :If you use the Bash shell, consider installing the Django bash completion script, which lives in  in the Django distribution. It enables tab-completion of  and  commands, so you can, for instance...The  facilty which comes with  is truly awesome.  It creates an enhanced debug page that, amongst other things, uses the Werkzeug debugger to create interactive debugging consoles for each point in the stack (see screenshot).  It also provides a very useful convenience debugging method  for displaying information about an object/frame.To install, you can use pip:Then add  to your  tuple in  and start the development server with the new extension:This will change the way you debug.I like to use the Python debugger pdb to debug Django projects.This is a helpful link for learning how to use it: When trying to exchange data between Django and another application,  is a good friend. Use it to receive and custom-process, say, XML data.Documentation:\nAdd  in your view code to dump debug information.Use  alongside Django.If you find the Django template language extremely restricting (like me!) then you don't have to be stuck with it. Django is flexible, and the template language is loosely coupled to the rest of the system, so just plug-in another template language and use it to render your http responses!I use , it's almost like a powered-up version of the django template language, it uses the same syntax, and allows you to use expressions in if statements! no more making a custom if-tags such as ! you can simply say , or .But that's not all; it has many more features to ease template creation, that I can't go though all of them in here.This adds to the reply above about .The URL names can also be effectively used within templates. For example, for a given URL pattern:you can have the following in templates:Since Django \"views\" only need to be callables that return an HttpResponse, you can easily create class-based views like those in Ruby on Rails and other frameworks.There are several ways to create class-based views, here's my favorite:You can add all sorts of other stuff like conditional request handling and authorization in your base view.Once you've got your views setup your urls.py will look something like this:Instead of using  to bind your context to a template and render it (which is what the Django docs usually show) use the generic view . It does the same thing that  does but it also automatically adds RequestContext to the template context, implicitly allowing context processors to be used. You can do this manually using , but why bother? It's just another step to remember and another LOC. Besides making use of context processors, having RequestContext in your template allows you to do things like:which is very useful. In fact, +1 on generic views in general. The Django docs mostly show them as shortcuts for not even having a views.py file for simple apps, but you can also use them inside your own view functions:I don't have enough reputation to reply to the comment in question, but it's important to note that if you're going to use , it does NOT support the '-' character in template block names, while Django does. This caused me a lot of problems and wasted time trying to track down the very obscure error message it generated.The  is very useful when starting to design your website.  Once imported, you can add this to generate sample text: does allow you to retrieve a model without importing it.James shows how handy it can be: .Everybody knows there is a development server you can run with \"manage.py runserver\", but did you know that there is a development view for serving static files (CSS / JS / IMG) as well ?Newcomers are always puzzled because Django doesn't come with any way to serve static files. This is because the dev team think it is the job for a real life Web server.But when developing, you may not want to set up Apache + mod_wisgi, it's heavy. Then you can just add the following to urls.py:Your CSS / JS / IMG will be available at www.yoursite.com/site_media/.Of course, don't use it in a production environment.I learned this one from the documentation for the  app. You can use the \"as\" keyword in template tags to use the results of the call elsewhere in your template.For example:This is mentioned in passing in the Django templatetag documentation, but in reference to loops only. They don't call out that you can use this elsewhere (anywhere?) as well. -- It provides all the logic & template variables for pagination (one of those I've-written-that-a-thousand-times-now drudgeries).   allows for any logic you need.  This gem has saved me many hours of debugging off-by-one errors in my \"Search Results\" pages and makes the view code cleaner in the process. is a nice environment to code and especially debug, with built-in support for Django.Use  to create Django models that use an XML REST API backend (instead of a SQL one).  This is very useful especially when modelling third party APIs - you get all the same QuerySet syntax that you're used to.  You can install it from PyPI.XML from an API:And now in python:It can also handle relationships and collections.  We use it every day in heavily used production code, so even though it's beta it's very usable.  It also has a good set of stubs that you can use in your tests.(Disclaimer: while I'm not the author of this library, I am now a committer, having made a few minor commits)Use database migrations. Use ."},
{"body": "This is what I normally do in order to ascertain that the input is a / - but not a . Because many times I stumbled upon bugs where a function passes a  object by mistake, and the target function does  assuming that  is actually a  or .My question is: is there a better way of achieving this?I thinkIs actually what you want, otherwise you'll miss out on a lot of things which act like lists, but aren't subclasses of  or .Remember that in Python we want to use \"duck typing\".  So, anything that acts like a list can be treated as a list.  So, don't check for the type of a list, just see if it acts like a list.But strings act like a list too, and often that is not what we want.  There are times when it is even a problem!  So, check explicitly for a string, but then use duck typing.Here is a function I wrote for fun.  It is a special version of  that prints any sequence in angle brackets ('<', '>').This is clean and elegant, overall.  But what's that  check doing there?  That's kind of a hack.  But it is essential.This function calls itself recursively on anything that acts like a list.  If we didn't handle the string specially, then it would be treated like a list, and split up one character at a time.  But then the recursive call would try to treat each character as a list -- and it would work!  Even a one-character string works as a list!  The function would keep on calling itself recursively until stack overflow.Functions like this one, that depend on each recursive call breaking down the work to be done, have to special-case strings--because you can't break down a string below the level of a one-character string, and even a one-character string acts like a list.Note: the / is the cleanest way to express our intentions.  But if this code were somehow time-critical, we might want to replace it with some sort of test to see if  is a sequence.  Rather than testing the type, we should probably test behaviors.  If it has a  method, it's a string, so don't consider it a sequence; otherwise, if it is indexable or iterable, it's a sequence:EDIT: I originally wrote the above with a check for  but I noticed that in the  module documentation, the interesting method is ; this makes sense, that's how you index an object.  That seems more fundamental than  so I changed the above.Python with PHP flavor:For Python 3, note per the :Generally speaking, the fact that a function which iterates over an object works on strings as well as tuples and lists is more feature than bug.  You certainly  use  or duck typing to check an argument, but why should you?That sounds like a rhetorical question, but it isn't.  The answer to \"why should I check the argument's type?\" is probably going to suggest a solution to the real problem, not the perceived problem.  Why is it a bug when a string is passed to the function?  Also:  if it's a bug when a string is passed to this function, is it also a bug if some other non-list/tuple iterable is passed to it?  Why, or why not?I think that the most common answer to the question is likely to be that developers who write  are expecting the function to behave as though they'd written .  There are probably circumstances where it makes more sense to protect developers from themselves than it does to support the use case of iterating across the characters in a string.  But I'd think long and hard about it first.The  object doesn't have an  attributeso you can do a checkand this will also raise a nice  for any other non-iterable object too.\nAs Tim mentions in the comments, this will only work in python 2.x, not 3.xThis is not intended to directly answer the OP, but I wanted to share some related ideas.I was very interested in @steveha answer above, which seemed to give an example where duck typing seems to break. On second thought, however, his example suggests that duck typing is hard to conform to, but it does  suggest that  deserves any special handling.After all, a non- type (e.g., a user-defined type that maintains some complicated recursive structures) may cause @steveha  function to cause an infinite recursion. While this is admittedly rather unlikely, we can't ignore this possibility. Therefore, rather than special-casing  in , we should clarify what we want  to do when an infinite recursion results.It may seem that one reasonable approach is to simply break the recursion in  the moment . This would, in fact, completely solve the problem with , without any .However, a really complicated recursive structure may cause an infinite loop where  never happens. Therefore, while the above check is useful, it's not sufficient. We need something like a hard limit on the recursion depth.My point is that if you plan to handle arbitrary argument types, handling  via duck typing is far, far easier than handling the more general types you may (theoretically) encounter. So if you feel the need to exclude  instances, you should instead demand that the argument is an instance of one of the few types that you explicitly specify.I do this in my testcases.Untested on generators, I think you are left at the next 'yield' if passed in a generator, which may screw things up downstream.  But then again, this is a 'unittest'I find such a function named . And I have verified that it meets your needs. Just do thisI tend to do this (if I really, really had to):If you have pandas already available you can just do this:This is what I do to ensure a list."},
{"body": "Does  in the Python time module return the system's time or the time in UTC?The  function returns the number of seconds since the epoch as seconds in UTC.Here is some sample output I ran on my computer, converting it to a string as well.The  variable is the time returned in seconds.  I then converted it to a string using the  library making it a string that is human readable. This is for the  that can be used in your text files. (The title of the question was different in the past, so the introduction to this answer was changed to clarify how it could be interpreted a the time. [updated 2016-01-14])You can get the timestamp as a string using the  or  of the :The  differs from  as expected -- otherwise they work the same way:You can render the timestamp to the string explicitly:Or you can be even more explicit to format the timestamp the way you like:If you want the ISO format, use the  method of the object:You can use these in variables for calculations and printing without conversions.Based on the answer from #squiguy, to get a true timestamp I would type cast it from float. At least that's the concept.The answer could be neither or both.On how to get timestamps from UTC time in various Python versions, see  There is no such thing as an \"epoch\" in a specific timezone. The epoch is well-defined as a specific moment in time, so if you change the timezone, the time itself changes as well. Specifically, this time is . So  returns the number of seconds since the epoch."},
{"body": "I want to import a function from another file in the same directory.Sometimes it works for me with  but sometimes I get a:Sometimes it works with , but sometimes I also get a:I don't understand the logic here, and I couldn't find any explanation. This looks completely random.Could someone explain to me what's the logic behind all this?It's quite common to have a layout like this......with a  like this......a  like this......and a  like this......which works fine when you run  or , but fails with , due to the relative import...The way you're supposed to run it is......but it's somewhat verbose, and doesn't mix well with a shebang line like .The simplest fix for this case, assuming the name  is globally unique, would be to avoid using relative imports, and just use......although, if it's not unique, or your package structure is more complex, you'll need to include the directory containing your package directory in , and do it like this......or if you want it to work \"out of the box\", you can frob the  in code first with this...It's kind of a pain, but there's a clue as to why in  written by a certain Guido van Rossum...Whether running scripts inside a package is an antipattern or not is subjective, but personally I find it really useful in a package I have which contains some custom wxPython widgets, so I can run the script for any of the source files to display a  containing only that widget for testing purposes.Once  conflicted with :and to address the issue,  introduced the top level variable :(emphasis mine)In case  is ,  returns empty string. This is why there's empty string literal in the error description:The relevant part of the CPython's :CPython raises this exception if it was unable to find  (the name of the package) in  (accessible as ). Since  is , it's now clear that .  The patch from the  has added , which will be executed  the code above:If  (same as above) is empty string, the error message will be . However, you will only see this in Python 3.6 or newer.Consider a directory (which is a Python ):\nAll of the files in  begin with the same 2 lines of code:I'm including these two lines  to make the order of operations obvious. We can ignore them completely, since they don't affect the execution. and  contain only those two lines (i.e., they are effectively empty). additionally attempts to import  via relative import:We're well aware that  will fail. However, we can run the module with  that will : does all the importing stuff for you and automatically sets , but you can do that yourself in theUnfortunately, setting  alone is not enough. You're going to need to import at least  preceding packages in the module hierarchy, where  is the number of parent directories (relative to the directory of the script) that will be searched for the module being imported.Thus,I'll borrow files from the  and add some more subpackages:This time  will import  from the  package using the following relative importWe'll need to precede that line with the boilerplate code, to make it work.It allows us to execute  by filename:A more general solution wrapped in a function can be found . Example usage:The steps are -For instance, the directory structure may be as followswhere  isThe rest of the files were borrowed from the .Installation will allow you to import the package regardless of your working directory (assuming there'll be no naming issues).We can modify  to use this advantage (step 1):Change your working directory to  and run  ( installs the package in ) (step 2):Let's verify that it's now possible to run  as a script:Frankly, the installation is not necessary - you could add some boilerplate code to your script to make absolute imports work.I'm going to borrow files from  and change : runs without problems:I feel that I should warn you: try not to do this,  if your project has a complex structure.As a side note,  recommends the use of absolute imports, but states that in some scenarios explicit relative imports are acceptable:I ran into this issue.  A Hack work around is using a try/except block like this:if both packages are in your import path (sys.path), and the module/class you want is in example/example.py, then to access the class without relative import try:"},
{"body": "I'm trying to convert a list to a tuple. When I google it, I find a lot of answers similar to:But if I do that I get this error message:How can I fix this problem?It should work fine. Don't use ,  or other special names as a variable name. It's probably what's causing your problem.Expanding on eumiro's comment, normally  will convert a list  into a tuple:However, if you've redefined  to be a tuple rather than the  :then you get a TypeError since the tuple itself is not callable:You can recover the original definition for  by quitting and restarting your interpreter, or (thanks to @glglgl):You might have done something like this:Here's the problem... Since you have used a  variable to hold a  earlier... So, now  is an  of type  now...It is no more a  and hence, it is no more . use any built-in types as your variable name... You do have any other name to use. Use any arbitrary name for your variable instead...to convert list to tuple,I find many answers up to date and properly answered but will add something new to stack of answers.In python there are infinite ways to do this, \nhere are some instances\nNormal way Remember tuple is immutable ,used for storing something valuable.\nFor example password,key or hashes are stored in tuples or dictionaries.\nIf knife is needed why to use sword to cut apples.\nUse it wisely,will also make your program efficient. "},
{"body": "Is there any benefit in using compile for regular expressions in Python?vsI've had a lot of experience running a compiled regex 1000s of times versus compiling on-the-fly, and have not noticed any perceivable difference.  Obviously, this is anecdotal, and certainly not a great argument  compiling, but I've found the difference to be negligible.EDIT:\nAfter a quick glance at the actual Python 2.5 library code, I see that Python internally compiles AND CACHES regexes whenever you use them anyway (including calls to ), so you're really only changing WHEN the regex gets compiled, and shouldn't be saving much time at all - only the time it takes to check the cache (a key lookup on an internal  type).From module re.py (comments are mine):I still often pre-compile regular expressions, but only to bind them to a nice, reusable name, not for any expected performance gain.For me, the biggest benefit to  isn't any kind of premature optimization (which is the , ). It's being able to separate definition of the regex from its use.Even a simple expression such as  (integer in base 10 without leading zeros) can be complex enough that you'd rather not have to retype it, check if you made any typos, and later have to recheck if there are typos when you start debugging. Plus, it's nicer to use a variable name such as num or num_b10 than .It's certainly possible to store strings and pass them to re.match; however, that's  readable:Versus compiling:Though it is fairly close, the last line of the second feels more natural and simpler when used repeatedly.FWIW:so, if you're going to be using  the  regex a lot, it may be worth it to do  (especially for more complex regexes).The standard arguments against premature optimization apply, but I don't think you really lose much clarity/straightforwardness by using  if you suspect that your regexps may become a performance bottleneck.Here's a simple test case:with re.compile:    So, it would seem to compiling is faster with this simple case, .I just tried this myself. For the simple case of parsing a number out of a string and summing it, using a compiled regular expression object is about twice as fast as using the  methods.As others have pointed out, the  methods (including ) look up the regular expression string in a cache of previously compiled expressions. Therefore, in the normal case, the extra cost of using the  methods is simply the cost of the cache lookup.However, examination of the , shows the cache is limited to 100 expressions. This begs the question, how painful is it to overflow the cache? The code contains an internal interface to the regular expression compiler, . If we call it, we bypass the cache. It turns out to be about two orders of magnitude slower for a basic regular expression, such as .Here's my test:And here is the output on my machine:The 'reallyCompiled' methods use the internal interface, which bypasses the cache. Note the one that compiles on each loop iteration is only iterated 10,000 times, not one million.I agree with Honest Abe that the  in the given examples are different.  They are not a one-to-one comparisons and thus, outcomes are vary.  To simplify my reply, I use A, B, C, D for those functions in question.  Oh yes, we are dealing with 4 functions in  instead of 3.Running this piece of code:is same as running this code:Because, when looked into the source , (A + B) means:and (C) is actually:So, (C) is not the same as (B).  In fact, (C) calls (B) after calling (D) which is also called by (A).  In other words, .  Therefore, comparing (A + B) inside a loop has same result as (C) inside a loop.  George's  proved this for us.Everyone's interest is, how to get the result of 2.323 seconds.  In order to make sure  only get called once, we need to store the compiled regex object in memory.  If we are using a class, we could store the object and reuse when every time our function get called.If we are not using class (which is my request today), then I have no comment.  I'm still learning to use global variable in Python, and I know global variable is a bad thing.One more point, I believe that using  approach has an upper hand.  Here are some facts as I observed (please correct me if I'm wrong):Here are the only cases that (A + B) is better than (C):Case that (C) is good enough:Just a recap, here are the A B C:Thanks for reading.In general, I find it is easier to use flags (at least easier to remember how), like  when compiling patterns than to use flags inline.vs Interestingly, compiling does prove more efficient for me (Python 2.5.2 on Win XP):Running the above code once as is, and once with the two  lines commented the other way around, the compiled regex is twice as fastI ran this test before stumbling upon the discussion here.  However, having run it I thought I'd at least post my results.I stole and bastardized the example in Jeff Friedl's \"Mastering Regular Expressions\".  This is on a macbook running OSX 10.6 (2Ghz intel core 2 duo, 4GB ram).  Python version is 2.6.1.Using the given examples:The  method in the example above is not the same as the one used below: returns a , which means  is a regex object.The regex object has its own  method with the optional  and  parameters:  The regex object's , , and  methods also support these parameters. does not support them as you can see,\nnor does its , , and  counterparts.A  has attributes that complement these parameters:A  has two unique, possibly useful, attributes:And finally, a  has this attribute:There is one addition perk of using re.compile(), in the form of adding comments to my regex patterns using re.VERBOSEAlthough this does not affect the speed of running your code, I like to do it this way as it is part of my commenting habit. I throughly dislike spending time trying to remember the logic that went behind my code 2 months down the line when I want to make modifications.This is a good question. You often see people use re.compile without reason. It lessens readability. But sure there are lots of times when pre-compiling the expression is called for. Like when you use it repeated times in a loop or some such.It's like everything about programming (everything in life actually). Apply common sense.Performance difference aside, using re.compile and using the compiled regular expression object to do match (whatever regular expression related operations) makes the semantics clearer to Python run-time.I had some painful experience of debugging some simple code:and later I'd use compare in where  is supposed to be a variable containing regular expression string,  is a variable containing string.I had trouble that  did not match some expected string!But if I used the re.compile form:then in Python would have complained that \"string does not have attribute of match\", as by positional argument mapping in ,  is used as regular expression!, when I actually meantIn my case, using re.compile is more explicit of the purpose of regular expression, when it's value is hidden to naked eyes, thus I could get more help from Python run-time checking. So the moral of my lesson is that when the regular expression is not just literal string, then I should use re.compile to let Python to help me to assert my assumption.This answer might be arriving late but is an interesting find. Using compile can really save you time if you are planning on using the regex multiple times (this is also mentioned in the docs). Below you can see that using a compiled regex is the fastest when the match method is directly called on it. passing a compiled regex to re.match makes it even slower and passing re.match with the patter string is somewhere in the middle. Mostly, there is little difference whether you use  or not.  Internally, all of the functions are implemented in terms of a compile step:If you use re.compile() you by-pass a little of overhead for the extra indirection and for the overhead of the caching logic:In addition to the small speed benefit from using , people also like the readability that comes from naming potentially complex pattern specifications and separating them from the business logic where there are applied:Note, one other respondent incorrectly believed that  files stored compiled patterns directly; however, in reality they are rebuilt each time when the PYC is loaded:The above disassembly comes from the PYC file for a  containing:Regular Expressions are compiled before being used when using the second version.  If you are going to executing it many times it is definatly better to compile it first.  If not compiling every time you match for one off's is fine.(months later) it's easy to add your own cache around re.match,\nor anything else for that matter --A wibni, wouldn't it be nice if: cachehint( size= ), cacheinfo() -> size, hits, nclear ...i'd like to motivate that pre-compiling is both conceptually and 'literately' (as in 'literate programming') advantageous. have a look at this code snippet:in your application, you'd write:this is about as simple in terms of functionality as it can get. because this is example is so short, i conflated the way to get  all in one line. the disadvantage of this code is that it occupies a little memory for whatever the lifetime of the  library object is; the advantage is that when doing a foobar search, you'll get away with two function calls and two class dictionary lookups. how many regexes are cached by  and the overhead of that cache are irrelevant here. compare this with the more usual style, below:In the application:I readily admit that my style is highly unusual for python, maybe even debatable. however, in the example that more closely matches how python is mostly used, in order to do a single match, we must instantiate an object, do three instance dictionary lookups, and perform three function calls; additionally, we might get into  caching troubles when using more than 100 regexes. also, the regular expression gets hidden inside the method body, which most of the time is not such a good idea. be it said that every subset of measures---targeted, aliased import statements; aliased methods where applicable; reduction of function calls and object dictionary lookups---can help reduce computational and conceptual complexity. The votes on the accepted answer leads to the assumption that what @Triptych says is true for all cases. This is not necessarily true. One big difference is when you have to decide whether to accept a regex string or a compiled regex object as a parameter to a function:It is always better to compile your regexs in case you need to reuse them. Note the example in the timeit above simulates creation of a compiled regex object once at import time versus \"on-the-fly\" when required for a match.I really respect all the above answers. From my opinion\nYes! For sure it is worth to use re.compile instead of compiling the regex, again and again, every time. My understanding is that those two examples are effectively equivalent. The only difference is that in the first, you can reuse the compiled regular expression elsewhere without causing it to be compiled again.Here's a reference for you: "},
{"body": "Which Python library can I use to extract filenames from paths, no matter what the operating system or path format could be?For example, I'd like all of these paths to return me :Using  or  as others suggest won't work in all cases: if you're running the script on Linux and attempt to process a classic windows-style path, it will fail.Windows paths can use either backslash or forward slash as path separator. Therefore, the  module (which is equivalent to os.path when running on windows) will work for all paths on all platforms.Of course, if the file ends with a slash, the basename will be empty, so make your own function to deal with it:Verification:Actually, there's a  that returns exactly what you want\nis the function you are looking forAssume p is the input string, tail is what you want.See  for detailIn your example you will also need to strip slash from right the right side to return :Second level:update: I think  has provided the right answer. My code will not work with windows-like paths on unix systems and vice versus with unix-like paths on windows system.this will return : This is working for linux and windows as well with standard libraryResults:I have never seen double-backslashed paths, are they existing? The built-in feature of python module  fails for those. All others work, also the caveat given by you with :The Windows separator can be in a Unix filename or Windows Path.  The Unix separator can only exist in the Unix path.  The presence of a Unix separator indicates a non-Windows path.The following will strip (cut trailing separator) by the OS specific separator, then split and return the rightmost value.   It's ugly, but simple based on the assumption above.   If the assumption is incorrect, please update and I will update this response to match the more accurate conditions.sample code:Maybe just my all in one solution without important some new(regard the tempfile for creating temporary files :D )Getting the values of  will be a string like this: \nSo I can replace the  with a space  and then call . That will return a list and I get the\nlast element of the list with No need to get any module imported. best regards4k3nd0Here's a regex-only solution, which seems to work with any OS path on any OS.No other module is needed, and no preprocessing is needed either :The regex can be tested ."},
{"body": "How do I get my python program to sleep for 50 milliseconds?Note that if you rely on sleep taking  50ms, you won't get that. It will just be about it."},
{"body": "I understand that pandas is designed to load fully populated  but I need to .\nWhat is the best way to do this ?I successfully created an empty DataFrame with :Then I can add a new row and fill a field with :It works but seems very odd :-/ (it fails for adding string value)How can I add a new row to my DataFrame (with different columns type) ?Example at @Nasser's answer:You could use  or . For details and examples, see .You could create a list of dictionaries, where each dictionary corresponds to an input data row. Once the list is complete, then create a data frame. This is a much faster approach. I had a similar problem where if I created a data frame for each row and appended it to the main data frame it took 30 mins. On the other hand, if I used the below methodology, it was successful within seconds.If you know the number of entries ex ante, you should preallocate the space by also providing the index (taking the data example from a different answer):And - as from the comments - with a size of 6000, the speed difference becomes even larger:For efficient appending see  and .Add rows through  on  key index data. e.g. :Or:You can append a single row as a dictionary using the  option.This is not an answer to the OP question but a toy example to illustrate the answer of @ShikharDua above which I found very useful. While this fragment is trivial, in the actual data I had 1,000s of rows, and many columns, and I wished to be able to group by different columns and then perform the stats below for more than one taget column. So having a reliable method for building the data frame one row at a time was a great convenience. Thank you @ShikharDua ! Create a  and add to .\npass list of  and corresponding  names to create a  (data_frame)Another way to do it (probably not very performant):You can also enhance the DataFrame class like this:"},
{"body": "What is the proper way to use  in Python when it comes to default values? returns a dictionary, but what is the best way to set default values, or is there one?  Should I just access it as a dictionary?  Use get function?  A simple question, but one that I can't find good resources on.  People do it different ways in code that I've seen and it's hard to know what to use.You can pass a default value to  for keys that are not in the dictionary:However, if you plan on using a particular argument with a particular default value, why not use named arguments in the first place?While most answers are saying that, e.g.,is \"the same as\"this is not true.  In the latter case,  can be called as , while the former case accepts named arguments  -- no positional calls.  Often you want to allow the caller maximum flexibility and therefore the second form, as most answers assert, is preferable: but that is not always the case.  When you accept many optional parameters of which typically only a few are passed, it may be an excellent idea (avoiding accidents and unreadable code at your call sites!) to force the use of named arguments --  is an example.  The first form is how you implement that in Python 2.The idiom is so important that in Python 3 it now has special supporting syntax: every argument after a single  in the  signature is keyword-only, that is, cannot be passed as a positional argument, but only as a named one. So in Python 3 you could code the above as:Indeed, in Python 3 you can even have keyword-only arguments that  optional (ones without a default value).However, Python 2 still has long years of productive life ahead, so it's better to  forget the techniques and idioms that let you implement in Python 2 important design ideas that are directly supported in the language in Python 3!I suggest something like thisAnd then use the values any way you want adds the contents of  to  overwriting any duplicate keys.You'd doorIf you use , then you can check if there are any spurious values sent, and take the appropriate action (if any).Using **kwargs and default values is easy.  Sometimes, however, you shouldn't be using **kwargs in the first place.In this case, we're not really making best use of **kwargs.The above is a \"why bother?\" declaration.  It is the same asWhen you're using **kwargs, you mean that a keyword is not just optional, but conditional.  There are more complex rules than simple default values.When you're using **kwargs, you usually mean something more like the following, where simple defaults don't apply.Since  is used when the number of arguments is unknown, why not doing this?Here's another approach:You could do something like thisI think the proper way to use **kwargs in Python when it comes to default values is to use the dictionary method setdefault, as given below:In this way, if a user passes 'val' or 'val2' in the keyword args, they will be used; otherwise, the default values that have been set will be used.Following up on  suggestion of using :This variant is useful when the class is expected to have all of the items in our  list.If you want to combine this with *args you have to keep *args and **kwargs at the end of the definition.So:@AbhinavGupta and @Steef suggested using , which I found very helpful for processing large argument lists:What if we want to check that the user hasn't passed any spurious/unsupported arguments? @VinaySajip pointed out that  can be used to iteratively process the list of arguments. Then, any leftover arguments are spurious. Nice.Here's another possible way to do this, which keeps the simple syntax of using : is a  containing the names of arguments that don't occur in the defaults."},
{"body": "How do I have a Python script that can accept user input (assuming this is possible) and how do I make it read in arguments if run from the command line?To read user input you can try  for easily creating a mini-command line interpreter (with help texts and autocompletion) and  for less fancy stuff (just reading a line of text from the user).Command line inputs are in sys.argv. Try this in your script for Python 2:For Python 3:Since print has changed from a keyword in Python 2 into a function call in Python 3. There are two modules for parsing command line options:  and . If you just want to input files to your script, behold the power of .The . is no longer available in Python 3.x.  But  was renamed , so the same functionality exists.The best way to process command line arguments is the Use  to get user input.  If you import the  your users will have line editing and history.Careful not to use the  function, unless you know what you're doing. Unlike ,  will accept any python expression, so it's kinda like If you are running Python <2.7, you need , which as the doc explains will create an interface to the command line arguments that are called when your application is run.However, in Python \u22652.7, optparse has been deprecated, and was replaced with the  as shown above. A quick example from the docs...Use 'raw_input' for input from a console/terminal.if you just want a command line argument like a file name or something e.g. then you can use sys.argv...sys.argv is a list where 0 is the program name, so in the above example sys.argv[1] would be \"file_name.txt\"If you want to have full on command line options use the optparse module.PevThis simple program helps you in understanding how to feed the user input from command line and to show help on passing invalid argument.1) To find the square root of 52) Passing invalid argument other than numberAs of Python  2.7, there is now  for processing command line arguments.If it's a 3.x version then just simply use:For example, you want to input 8:x will equal 8 but it's going to be a string except if you define it otherwise.So you can use the convert command, like:In Python 2:In Python 3:"},
{"body": "I was wondering about the best practices for indicating invalid argument combinations in Python. I've come across a few situations where you have a function like so:The only annoyance with this is that every package has its own, usually slightly differing . I know that in Java there exists  -- is it well understood that everybody will be creating their own s in Python or is there another, preferred method?I would just raise , unless you need a more specific exception..There's really no point in doing  - your custom class is identical in use to , so why not use that?I would inherit from It is sometimes better to create your own exceptions, but inherit from a built-in one, which is as close to what you want as possible.If you need to catch that specific error, it is helpful to have a name.I've mostly just seen the builtin  used in this situation.I'm not sure I agree with inheritance from  -- my interpretation of the documentation is that  is  supposed to be raised by builtins... inheriting from it or raising it yourself seems incorrect.-- "},
{"body": "How do you express an integer as a binary number with Python literals?I was easily able to find the answer for hex:and octal:For reference\u2014 Python possibilities:\nStarting with Python 2.6 you can express binary literals using the prefix  or :You can also use the new  function to get the binary representation of a number:Development version of the documentation: Another way.They're not \"binary\" literals, but rather, \"integer literals\". You can express integer literals with a binary format with a  followed by a  or  followed by a series of zeros and ones, for example:From the Python 3 , these are the ways of providing integer literals in Python:You can have the zeros and ones in a string object which can be manipulated (although you should probably just do bitwise operations on the integer in most cases) - just pass int the string of zeros and ones and the base you are converting from (2):You can optionally have the  or  prefix:If you pass it  as the base, it will assume base 10 if the string doesn't specify with a prefix:You can pass an integer to bin to see the string representation of a binary literal:And you can combine  and  to go back and forth:You can use a format specification as well, if you want to have minimum width with preceding zeros:0 in the start here specifies that the base is 8 (not 10), which is pretty easy to see: If you don't start with a 0, then python assumes the number is base 10.As far as I can tell Python, up through 2.5, only supports hexadecimal & octal literals.  I did find some discussions about adding binary to future versions but nothing definite.I am pretty sure this is one of the things due to change in Python 3.0 with perhaps bin() to go with hex() and oct().EDIT:\nlbrandy's answer is correct in all cases."},
{"body": "I have a list:and want to search for items that contain the string . How can I do that?would check if  exists in the list but it is a part of  and ,  does not exist on its own. So how can I get all items that contain  ?If you only want to check for the presence of  in any string in the list, you could tryIf you really want to get all the items containing , useUse  to get at the elements that have .You can also use a list comprehension.By the way, don't use the word  as a variable name since it is already used for the  type.Just throwing this out there: if you happen to need to match against more than one string, for example  and , you can put combine two list comprehensions as follows:Output:This is quite an old question, but I offer this answer because the previous answers do not cope with items in the list that are not strings (or some kind of iterable object). Such items would cause the entire list comprehension to fail with an exception.To gracefully deal with such items in the list by skipping the non-iterable items, use the following:then, with such a list:you will still get the matching items ()The test for iterable may not be the best. Got it from here: This is the shortest way:From my knowledge, a 'for' statement will always consume time.When the list length is growing up, the execution time will also grow.I think that, searching a substring in a string with 'is' statement is a bit faster.But, I agree that the  statement is more readable."},
{"body": "Is this the cleanest way to write a list to a file, since  doesn't insert newline characters?It seems like there would be a standard way...I'd use a loop:or:If you're keen on a single function call, at least remove the square brackets  so that the strings to be printed get made one at a time (a genexp rather than a listcomp) -- no reason to take up all the memory required to materialize the whole list of strings.What are you going to do with the file?  Does this file exist for humans, or other programs with clear interoperability requirements, or are you just trying to serialize a list to disk for later use by the same python app.  If the second case is it, you should be  the list.To read it back:The best way is:Yet another way.  Serialize to json using  (included as  in python 2.6):If you examine output.txt:This is useful because the syntax is pythonic, it's human readable, and it can be read by other programs in other languages.Using  syntax:This is platform-independent.I thought it would be interesting to explore the benefits of using a genexp, so here's my take.The example in the question uses square brackets to create a temporary list, and so is equivalent to:Which needlessly constructs a temporary list of all the lines that will be written out, this may consume significant amounts of memory depending on the size of your list and how verbose the output of  is.Drop the square brackets (equivalent to removing the wrapping  call above) will instead pass a temporary  to :This generator will create newline-terminated representation of your  objects on-demand (i.e. as they are written out). This is nice for a couple of reasons:This avoids memory issues, such as:(I triggered this error by limiting Python's max. virtual memory to ~100MB with ).Putting memory usage to one side, this method isn't actually any faster than the original:(Python 2.6.2 on Linux)Following is the syntax for  methodSerialize list into text file with comma sepparated valueYou can also use the print function if you're on python3 as follows.Why don't you tryLet avg be the list, then:You can use  or  depending on your requirement.How It Works:\nFirst, open a \ufb01le by using the built-in open function and specifying the name of\nthe \ufb01le and the mode in which we want to open the \ufb01le. The mode can be a\nread mode (\u2019r\u2019), write mode (\u2019w\u2019) or append mode (\u2019a\u2019). We can also specify\nwhether we are reading, writing, or appending in text mode (\u2019t\u2019) or binary\nmode (\u2019b\u2019). There are actually many more modes available and help(open)\nwill give you more details about them. By default, open() considers the \ufb01le to\nbe a \u2019t\u2019ext \ufb01le and opens it in \u2019r\u2019ead mode.\nIn our example, we \ufb01rst open the \ufb01le in write text mode and use the write\nmethod of the \ufb01le object to write to the \ufb01le and then we \ufb01nally close the \ufb01le.\n"},
{"body": "I have a basic dict as follows:When I try to do  I get:What can I do such that my dictionary sample can overcome the error above?Note: Though it may not be relevant, the dictionaries are generated from the retrieval of records out of mongodb where when I print out , the output is .As you are using mongoengine (per comments) and pymongo is a dependency, pymongo has built-in utilities to help with json serialization:\nExample usage (serialization):Example usage (deserialization):Building on other answers, a simple solution based on a specific serializer that just converts  and  objects to strings.As seen, the code just checks to find out if object is of class  or , and then uses  to produce a serialized version of it, according to ISO 8601 format, YYYY-MM-DDTHH:MM:SS (which is easily decoded by JavaScript). If more complex serialized representations are sought, other code could be used instead of str() (see other answers to this question for examples). The code ends by raising an exception, to deal with the case it is called with a non-serializable type.This json_serial function can be used as follows:The details about how the default parameter to json.dumps works can be found in . Convert the date to a stringI have just encountered this problem and my solution is to subclass :In your call do something like:  The  I got from one of the answers above.For others who do not need or want to use the pymongo library for this.. you can achieve datetime JSON conversion easily with this small snippet:Then use it like so:output:\u00a0My quick & dirty JSON dump that eats dates and everything:I have an application with a similar issue; my approach was to JSONize the datetime value as a 6-item list (year, month, day, hour, minutes, seconds); you could go to microseconds as a 7-item list, but I had no need to:produces:Here is my solution:Then you can use it like that:My solution (with less verbosity, I think):Then use  instead of . It will print:I you want, later you can add other special cases to this with a simple twist of the  method. Example:  This Q repeats time and time again - a simple way to patch the json module such that serialization would support datetime.  Than use json serialization as you always do - this time with datetime being serialized as isoformat.Resulting in: '{\"created\": \"2015-08-26T14:21:31.853855\"}'See more details and some words of caution at:\nHere is a simple solution to over come \"datetime not JSON serializable\"\nproblem.  {\"date\": \"2015-12-16T04:48:20.024609\"} You have to supply a custom encoder class with the  parameter of . To quote from the :This uses complex numbers as the example, but you can just as easily create a class to encode dates (except I think JSON is a little fuzzy about dates)The simplest way to do this is to change the part of the dict that is in datetime format to isoformat. That value will effectively be a string in isoformat which json is ok with.Here is my full solution for converting datetime to JSON and back..OutputJSON FileThis has enabled me to import and export strings, ints, floats and datetime objects.\nIt shouldn't be to hard to extend for other types.If you are using the result in a view be sure to return a proper response. According to the API, jsonify does the following:To mimic this behavior with json.dumps you have to add a few extra lines of code.You should also return a dict to fully replicate jsonify's response. So, the entire file will look like thisMy solution ...Ok, now some tests.I got the same error message while writing the serialize decorator inside a Class  with sqlalchemy. So instead of :I  simply borrowed jgbarah's idea of using isoformat() and appended the original value with isoformat(), so that it now looks like:Convert the   to  A quick fix if you want your own formattingGenerally there are several ways to serialize datetimes, like:If you're okay with the last way, the  package handles dates, times and datetimes including timezones.which gives:So all you need to do isand then import from  instead of .The advantage of not storing it as a single string, int or float comes when decoding: if you encounter just a string or especially int or float, you need to know something about the data to know if it's a datetime. As a dict, you can store metadata so it can be decoded automatically, which is what  does for you. It's also easily editable for humans.Disclaimer: it's made by me. Because I had the same problem.I had encountered same problem when externalizing django model object to dump as JSON.\nHere is how you can solve it.Either make both the dates in mysql as well as in python code json as String or both date or datetime.\nIt worked for me as I converted mysql type to String.My solution was just to use EPOCH time (which is a number) since my use case didn't require an end user to read the time in the JSON. It was SO MUCH EASIER to work with epoch time.I may not 100% correct but,\nthis is the simple way to do serialize"},
{"body": "I am learning the ropes in Python. When I try to print an object of class  using the  function, I get an output like this:Is there a way I can set the  (or the ) of a  and its ? For instance, when I call  on a class object, I would like to print its data members in a certain format. How to achieve this in Python?If you are familiar with C++ classes, the above can be achieved for the standard  by adding a  method for the class.The  method is what happens when you print it, and the  method is what happens when you use the  function (or when you look at it with the interactive prompt). If this isn't the most  method, I apologize, because I'm still learning too - but it works.If no  method is given, Python will print the result of  instead. If you define  but not , Python will use what you see above as the , but still use  for printing.As Chris Lutz mentioned, this is defined by the  method in your class.From the documentation of :Given the following class Test:..it will act the following way in the Python shell:If no  method is defined,  (or ) will use the result of  insteadIf no  method is defined then the default is used, which is pretty much equivalent to..You need to use . This is a standard function like .\nFor example:Just to add my two cents to @dbr's answer, following is an example of how to implement this sentence from the official documentation he's cited:Given this class definition:Now, is easy to serialize instance of  class:So, running last piece of code, we'll get:But, as I said in my last comment: more info is just !A generic way that can be applied to any class without specific formatting could be done as follows:And then,producesFor Python 3:If the specific format isn't important (e.g. for debugging) just inherit from the Printable class below.  No need to write code for every object.  Inspired by  answerThere are already a lot of answers in this thread but none of them particularly helped me, I had to work it out myself, so I hope this one is a little more informative.You just have to make sure you have parentheses at the end of your class, e.g:Here's an example of code from a project I was working on:To print my Hydrogen class, I used the following:Please note, this will not work without the parentheses at the end of Hydrogen. They are necessary.Hope this helps, let me know if you have anymore questions. "},
{"body": "I have a Python script which takes as input a list of integers, which I need to work with four integers at a time.  Unfortunately, I don't have control of the input, or I'd have it passed in as a list of four-element tuples.  Currently, I'm iterating over it this way:It looks a lot like \"C-think\", though, which makes me suspect there's a more pythonic way of dealing with this situation.  The list is discarded after iterating, so it needn't be preserved.  Perhaps something like this would be better?Still doesn't quite \"feel\" right, though.  :-/Related question: Modified from the  section of Python's  docs:\nIn pseudocode to keep the example terse.  is new to Python 2.6. In Python 3 use .Simple. Easy. Fast. Works with any sequence:I'm a fan of Another way:Posting this as an answer since I cannot comment...Using map() instead of zip() fixes the padding issue in J.F. Sebastian's answer:Example:I needed a solution that would also work with sets and generators. I couldn't come up with anything very short and pretty, but it's quite readable at least.List:Set:Generator:Similar to other proposals, but not exactly identical, I like doing it this way, because it's simple and easy to read:This way you won't get the last partial chunk. If you want to get  as last chunk, just use  from .Since nobody's mentioned it yet here's a  solution:It works only if your sequence's length is always divisible by the chunk size or you don't care about a trailing chunk if it isn't.Example:Or using  to return an iterator instead of a list:Padding can be fixed using :Using little functions and things really doesn't appeal to me; I prefer to just use slices:If the list is large, the highest-performing way to do this will be to use a generator:In your second method, I would advance to the next group of 4 by doing this:However, I haven't done any performance measurement so I don't know which one might be more efficient.Having said that, I would usually choose the first method. It's not pretty, but that's often a consequence of interfacing with the outside world.Yet another answer, the advantages of which are:1) Easily understandable\n2) Works on any iterable, not just sequences (some of the above answers will choke on filehandles)\n3) Does not load the chunk into memory all at once\n4) Does not make a chunk-long list of references to the same iterator in memory\n5) No padding of fill values at the end of the listThat being said, I haven't timed it so it might be slower than some of the more clever methods, and some of the advantages may be irrelevant given the use case.\nA couple of drawbacks due to the fact the inner and outer loops are pulling values from the same iterator:\n1) continue doesn't work as expected in the outer loop - it just continues on to the next item rather than skipping a chunk. However, this doesn't seem like a problem as there's nothing to test in the outer loop.\n2) break doesn't work as expected in the inner loop - control will wind up in the inner loop again with the next item in the iterator. To skip whole chunks, either wrap the inner iterator (ii above) in a tuple, e.g. , or set a flag and exhaust the iterator.Another approach would be to use the two-argument form of : This can be adapted easily to use padding (this is similar to \u2019s answer):These can even be combined for optional padding:You can use  or  function from  library:These functions also has iterator versions  and , which will be more efficient in this case.You can also peek at .To avoid all conversions to a list  and:Produces:I checked  and it doesn't convert to list or use  so I (think) this will delay resolution of each value until it is actually used.  Sadly none of the available answers (at this time) seemed to offer this variation.Obviously if you need to handle each item in turn nest a for loop over g:My specific interest in this was the need to consume a generator to submit changes in batches of up to 1000 to the gmail API:With NumPy it's simple:output:About solution gave by  :It's clever, but has one disadvantage - always return tuple. How to get string instead?\nOf course you can write , but the temporary tuple is constructed anyway.You can get rid of the temporary tuple by writing own , like this:ThenExample usage:Here is a chunker without imports that supports generators:Example of use:There doesn't seem to be a pretty way to do this.   is a page that has a number of methods, including:If the lists are the same size, you can combine them into lists of 4-tuples with . For example:Here's what the  function produces:If the lists are large, and you don't want to combine them into a bigger list, use , which produces an iterator, rather than a list.The ideal solution for this problem works with iterators (not just sequences). It should also be fast.This is the solution provided by the documentation for itertools:Using ipython's  on my mac book air, I get 47.5 us per loop.However, this really doesn't work for me since the results are padded to be even sized groups. A solution without the padding is slightly more complicated. The most naive solution might be:Simple, but pretty slow: 693 us per loopThe best solution I could come up with uses  for the inner loop:With the same dataset, I get 305 us per loop.Unable to get a pure solution any faster than that, I provide the following solution with an important caveat: If your input data has instances of  in it, you could get wrong answer.I really don't like this answer, but it is significantly faster. 124 us per loopOne-liner, adhoc solution to iterate over a list  in chunks of size  -At first, I designed it to split strings into substrings to parse string containing hex.\nToday I turned it into complex, but still simple generator.It is easy to make  work for you to get an iterable of iterables, without creating any temporary lists:Don't get put off by the nested lambdas, outer lambda runs just once to put  generator and the constant  into the scope of the inner lambda.I use this to send chunks of rows to mysql.I like this. It feels simple and not magical, and supports all iterable types."},
{"body": "Why do I receive a syntax error when printing a string in Python 3?In Python 3,  . This means that you need to include parenthesis now like mentioned below:It looks like you're using Python 3.0, in which  rather than a statement.Because in Python 3,  has been replaced with a , with keyword arguments to replace most of the special syntax of the old print statement. So you have to write it as But if you write this in a programme and some one using Python 2.x tries to run, they will get an error. To avoid this, it is a good practice to import print functionNow you code works on both 2.x & 3.xCheck out below examples also to get familiar with print() function.Source: In Python 3.0,  is a regular function that requires ():In Python 3, it's  , not .It looks like you're using Python 3. In Python 3, print has been changed to a method instead of a statement. Try this:You have to use bracket with print In Python 3, you must do . This is because in Python 3 it has become a function. If you must, you can use your Python 2 code and convert it to Python 3 code using  - it is a great built-in program which comes with Python. For more, see .In Python 2.X  is a keyword as you can see in this . However, in Python 3.X  becomes a function, so the correct way to do it is .\nYou can get the list of keywords for each version by executing the following:2to3 is a Python program that reads Python 2.x source code and applies a series of fixers to transform it into valid Python 3.x codeFurther informations can be found here: "},
{"body": "What would be your preferred way to concatenate strings from a sequence such that between each two consecutive pair a comma is added. That is, how do you map, for instance,  to ? (The cases  and  should be mapped to  and , respectively.)I usually end up using something like , but also feeling somewhat unsatisfied.Edit: I'm both ashamed and happy that the solution is so simple. Obviously I have hardly a clue as to what I'm doing. (I probably needed \"simple\" concatenation in the past and somehow memorised  as a shorthand for .)This won't work if the list contains numbers.As  suggested, if it contains non-string types (such as integers, floats, bools, None) then do:Why the map/lambda magic? Doesn't this work?Edit: @mark-biek points out the case for numbers.\nPerhaps the list comprehension:is more \"pythonic\".Edit2:\nThanks for the suggestions. I'll use the generator rather than the list comprehension in the future.Don't you just want:Obviously it gets more complicated if you need to quote/escape commas etc in the values. In that case I would suggest looking at the csv module in the standard library:@Peter HoffmannUsing generator expressions has the benefit of also producing an iterator but saves importing itertools. Furthermore, list comprehensions are generally preferred to map, thus, I'd expect generator expressions to be preferred to imap.Here is a alternative solution in Python 3.0 which allows non-string list items:NOTE: The space after comma is intentional.@jmanning2k using a list comprehension has the downside of creating a new temporary list. The better solution would be using itertools.imap which returns an iterator will not work for all cases. I'd suggest using the csv module with StringIOfor converting list containing numbers do the following:Unless I'm missing something,  should do what you're asking for.(edit:  and as jmanning2k points out, is safer and quite Pythonic, though the resulting string will be difficult to parse if the elements can contain commas -- at that point, you need the full power of the  module, as Douglas points out in his answer.)Here is an example with listMore Accurate:-Example 2:- may contain any type of variables. This avoid the result ."},
{"body": "I want to install . It should support Python 3, but it requires setuptools, which is available only for Python 2.How can I install pip with Python 3?edit: Manual installation and use of  is not the standard process anymore.Congrats, you  already have  installed. If you do not, read onward.You can usually install the package for  through your package manager if your version of Python is older than 2.7.9 or 3.4, or if your system did not include it for whatever reason.Instructions for some of the more common distros follow.Run the following command from a terminal:Run the following command from a terminal:On CentOS 7, you have to install setup tools first, and then use that to install , as there is no direct package for it.Assuming you installed Python 3.4 , you can install Python 3's setup tools and use it to install .Install using the manual way detailed below.If you want to do it the manual way, the now-recommended method is to install using the  script from .I was able to install pip for python 3 on Ubuntu just by running . Good news!  (released March 2014) ships with Pip. This is the best feature of any Python release. It makes the community's wealth of libraries accessible to everyone. Newbies are no longer excluded by the prohibitive difficulty of setup. In shipping with a package manager, Python joins Ruby, Nodejs, Haskell, Perl, Go--almost every other contemporary language with a majority open-source community. Thank you Python.Of course, that doesn't mean Python packaging is problem solved. The experience remains frustrating. I discuss this at Alas for everyone using an earlier Python. Manual instructions follow.Follow my detailed instructions at   . EssentiallyPer Download , being careful to save it as a  file rather than . Then, run it from the command prompt.You possibly need an administrator command prompt to do this. Follow For me, this installed Pip at . Find  on your computer, then add its folder (eg.  ) to your path (Start / Edit environment variables). Now you should be able to run  from the command line. Try installing a package:There you go (hopefully)! For Ubuntu 12.04 or older, won't work. Instead, use:As per  the current way is:I think that should work for any versionI'm not sure when exactly this was introduced, but it's installed pip3 for me when it didn't already exist.To install packages in Python always follow these steps:Note: This is assuming no alias is set for Through this method, there will be no confusion regarding which python version is receiving the package.If you use several different versions of python try using  Then install a local environment in the current directory by:Then there are now an local pythonenvironment in that folder. Now there should be use \n to list the local installed libraries.use  to install at the local environment.use  to run your python script.if you're using python 3.4+just type:Here is my way to solve this problem at ubuntu 12.04:Then install the python3 from source code:When you finished installing all of them, pip3 will get installed automatically.This is what I did on OS X Mavericks to get this to work.Firstly, have  installedInstall python 3.4Then I get the latest version of distribute:I hope this helps.For python3 try this:The good thing is that It will also detect what version of python you have (even if it's an environment of python in your custom location).\nAfter this you can proceed normally with (for example)source:\nAssuming you are in a highly restricted computer env (such as myself) without root access or ability to install packages...  I had never setup a fresh/standalone/raw/non-root instance of Python+virtualenv before this post.  I had do quite a bit of Googling to make this work.Then... pip, pip, pip!Final tip to newbie Pythoneers: You don't think you need virtualenv when you start, but you will be happy to have it later.  Helps with \"what if\" installation / upgrade scenarios for open source / shared packages.Ref: pip should always be availableBy default, the commands pipX and pipX.Y will be installed on all platforms (where X.Y stands for the version of the Python installation), along with the pip Python package and its dependencies.so if you have python 3.4 installed, you can just: To install pip, securely download .Then run the following:Refer: "},
{"body": "I'm having some brain failure in understanding reading and writing text to a file (Python 2.4).So I type in  into my favorite editor, in file f2.Then:What am I not understanding here? Clearly there is some vital bit of magic (or good sense) that I'm missing. What does one type into text files to get proper conversions?What I'm truly failing to grok here, is what the point of the UTF-8 representation is, if you can't actually get Python to recognize it, when it comes from outside. Maybe I should just JSON dump the string, and use that instead, since that has an asciiable representation! More to the point, is there an ASCII representation of this Unicode object that Python will recognize and decode, when coming in from a file?  If so, how do I get it?In the notationthe \"\\xe1\" represents just one byte. \"\\x\" tells you that \"e1\" is in hexadecimal.\nWhen you writeinto your file you have \"\\xc3\" in it. Those are 4 bytes and in your code you read them all. You can see this when you display them:You can see that the backslash is escaped by a backslash. So you have four bytes in your string: \"\\\", \"x\", \"c\" and \"3\".Edit:As others pointed out in their answers you should just enter the characters in the editor and your editor should then handle the conversion to UTF-8 and save it.If you actually have a string in this format you can use the  codec to decode it into a normal string:The result is a string that is encoded in UTF-8 where the accented character is represented by the two bytes that were written  in the original string. If you want to have a unicode string you have to decode again with UTF-8.To your edit: you don't have UTF-8 in your file. To actually see how it would look like:Compare the content of the file  to the content of the file you saved with your editor.Rather than mess with the encode and decode methods I find it easier to use the open method from the codecs module.Then after calling f's read() function, an encoded Unicode object is returned.If you know the encoding of a file, using the codecs package is going to be much less confusing.See So, I've found a solution for what I'm looking for, which is:There are some unusual codecs that are useful here. This particular reading allows one to take UTF-8 representations from within Python, copy them into an ASCII file, and have them be read in to Unicode. Under the \"string-escape\" decode, the slashes won't be doubled.This allows for the sort of round trip that I was imagining.Now all you need in Python3 is Python3 added the  parameter to its open function. The following information about the open function is gathered from here: So by adding  as a parameter to the open function, the file reading and writing is all done as utf8 (which is also now the default encoding of everything done in Python.)Actually, this worked for me for reading a file with UTF-8 encoding in Python 3.2:To read in an Unicode string and then send to HTML, I did this:Useful for python powered http servers.Well, your favorite text editor does not realize that  are supposed to be character literals, but it interprets them as text. That's why you get the double backslashes in the last line -- it's now a real backslash + , etc. in your file.If you want to read and write encoded files in Python, best use the  module.Pasting text between the terminal and applications is difficult, because you don't know which program will interpret your text using which encoding. You could try the following:Then paste this string into your editor and make sure that it stores it using Latin-1. Under the assumption that the clipboard does not garble the string, the round trip should work.You have stumbled over the general problem with encodings: How can I tell in which encoding a file is?Answer: You can't  the file format provides for this. XML, for example, begins with:This header was carefully chosen so that it can be read no matter the encoding. In your case, there is no such hint, hence neither your editor nor Python has any idea what is going on. Therefore, you must use the  module and use  which provides the missing bit in Python.As for your editor, you must check if it offers some way to set the encoding of a file.The point of UTF-8 is to be able to encode 21-bit characters (Unicode) as an 8-bit data stream (because that's the only thing all computers in the world can handle). But since most OSs predate the Unicode era, they don't have suitable tools to attach the encoding information to files on the hard disk.The next issue is the representation in Python. This is explained perfectly in the . You must understand that your console can only display ASCII. In order to display Unicode or anything >= charcode 128, it must use some means of escaping. In your editor, you must not type the escaped display string but what the string means (in this case, you must enter the umlaut and save the file).That said, you can use the Python function eval() to turn an escaped string into a string:As you can see, the string \"\\xc3\" has been turned into a single character. This is now an 8-bit string, UTF-8 encoded. To get Unicode: asked: I think there are some pieces missing here: the file f2 contains: hex:, for example, reads them all in a separate chars (expected) Is there any way to write to a file in ASCII that would work?Answer: That depends on what you mean. ASCII can't represent characters > 127. So you need some way to say \"the next few characters mean something special\" which is what the sequence \"\\x\" does. It says: The next two characters are the code of a single character. \"\\u\" does the same using four characters to encode Unicode up to 0xFFFF (65535).So you can't directly write Unicode to ASCII (because ASCII simply doesn't contain the same characters). You can write it as string escapes (as in f2); in this case, the file can be represented as ASCII. Or you can write it as UTF-8, in which case, you need an 8-bit safe stream.Your solution using  does work, but you must be aware how much memory you use: Three times the amount of using .Remember that a file is just a sequence of bytes with 8 bits. Neither the bits nor the bytes have a meaning. It's you who says \"65 means 'A'\". Since  should become \"\u00e0\" but the computer has no means to know, you must tell it by specifying the encoding which was used when writing the file.The \\x.. sequence is something that's specific to Python. It's not a universal byte escape sequence. How you actually enter in UTF-8-encoded non-ASCII depends on your OS and/or your editor. . For OS X to enter  with an acute accent you can just hit  + , then , and almost all text editors in OS X support UTF-8.You can also improve the original  function to work with Unicode files by replacing it in place, using the  function. The beauty of this solution is you don't need to change any old code. It's transparent.I was trying to parse  using Python 2.7.9:But I was getting:and it was fixed with just:(Now it can print lik\u00e9 \u00e1 b\u00f6ss.)"},
{"body": "I'm using the Python bindings to run Selenium WebDriver.I know I can grab a webelement like so...And I know I can get the full page source with...But is there anyway to get the \"element source\"?The selenium webdriver docs for Python are basically non-existent and I don't see anything in the code that seems to enable that functionality.Any thoughts on the best way to access the HTML of an element (and its children)?You can read  attribute to get source of the  of the element or  for source  the current element.Python:Java:C#:Ruby:JS:Tested and works with the .There is not really a straight-forward way of getting the html source code of a webelement. You will have to use JS. I am not too sure about python bindings but you can easily do like this in Java. I am sure there must be something similar to  class in Python.Sure we can get all HTML source code with this script below in Selenium Python:If you you want to save it to file:I suggest saving to a file because source code is very very long.In Ruby, using selenium-webdriver (2.32.1), there is a  method that contains the entire page source.Using the attribute method is, in fact, easier and more straight forward.  Using Ruby with the Selenium and PageObject gems, to get the class associated with a certain element, the line would be . The same concept applies if you wanted to get other attributes tied to the element. For example, if I wanted the String of an element, . Looks outdated, but let it be here anyway. The correct way to do it in your case:orBoth are working for me (selenium-server-standalone-2.35.0)Java with Selenium 2.53.0I hope this could help:\nHere is described Java method:But unfortunately it's not available in Python. So you can translate the method names to Python from Java and try another logic using present methods without getting the whole page source...E.g.And in PHPUnit selenium test it's like this:If you are interested in a solution for Remote Control in Python, here is how to get innerHTML:This code really works to get JavaScript from source as well!"},
{"body": "I have a python script that needs to execute an external program, but for some reason fails.If I have the following script:Then it fails with the following error:If I escape the program with quotes:Then it works. However, if I add a parameter, it stops working again:What is the right way to execute a program and wait for it to complete? I do not need to read output from it, as it is a visual program that does a job and then just exits, but I need to wait for it to complete.Also note, moving the program to a non-spaced path is not an option either. This does not work either:Note the swapped single/double quotes.with or without a parameter to notepad here, it fails with the error message will avoid problems with having to deal with quoting conventions of various shells. It accepts a list, rather than a string, so arguments are more easily delimited. i.e.Here's a different way of doing it.If you're using windows the following acts like double-clicking the file in Explorer, or giving the file name as an argument to the DOS \"start\" command:  the file is opened with whatever application (if any) its extension is associated.Example:This will open textfile.txt with notepad if notepad is associted with .txt files.The outermost quotes are consumed by Python itself, and the Windows shell doesn't see it. As mentioned above, Windows only understands double-quotes. \nPython will convert forward-slashed to backslashes on Windows, so you can useThe ' is consumed by Python, which then passes \"C://Temp/a b c/Notepad.exe\" (as a Windows path, no double-backslashes needed) to CMD.EXEAt least in Windows 7 and Python 3.1,  in Windows wants the command line  if there are spaces in path to the command. For example:A real-world example that was stumping me was cloning a drive in VirtualBox. The  solution above didn't work because of some access rights issue, but when I double-quoted the command,  became happy:I suspect it's the same problem as when you use shortcuts in Windows... Try this:"},
{"body": "How do I get the path of a the Python script I am running in? I was doing , however on Mac I only get the filename - not the full path as I do on Windows.No matter where my application is launched from, I want to open files that are relative to my script file(s). will give you the path of the current file, resolving any symlinks in the path. This works fine on my mac.7.2 of Dive Into Python: .The accepted solution for this will not work if you are planning to compile your scripts using py2exe.  If you're planning to do so, this is the functional equivalent:Py2exe does not provide an  variable.  For reference:  If you have even the relative pathname (in this case it appears to be ) you can open files relative to your script file(s). I use Perl, but the same general solution can apply: I split the directory into an array of folders, then  off the last element (the script), then  (or for you, ) on whatever I want, and then join them together again, and BAM! I have a working pathname that points to exactly where I expect it to point, relative or absolute.Of course, there are better solutions, as posted. I just kind of like mine."},
{"body": "Can I run the python interpreter without generating the compiled .pyc files?From :Update 2010-11-27: Python 3.2 addresses the issue of cluttering source folders with  files by introducing a special  subfolder, see .There actually IS a way to do it in Python 2.3+, but it's a bit esoteric.  I don't know if you realize this, but you can do the following:According to the  library:Thus, all you have to do is zip the files up, add the zipfile to your sys.path and then import them.If you're building this for UNIX, you might also consider packaging your script using this recipe:  , but note that you might have to tweak this if you plan on using stdin or reading anything from sys.args (it CAN be done without too much trouble).In my experience performance doesn't suffer too much because of this, but you should think twice before importing any very large modules this way.In 2.5, theres no way to suppress it, other than measures like not giving users write access to the directory.In python 2.6 and 3.0 however, there may be a setting in the sys module called \"dont_write_bytecode\" that can be set to suppress this.  This can also be set by passing the \"-B\" option, or setting the environment variable \"PYTHONDONTWRITEBYTECODE\"I have several test cases in a test suite and before I was running the test suite in the Mac Terminal like this: Running the command this way my directory was being populated with .pyc files. I tried the below stated method and it solved the issue:This method works if you are importing test cases into the test suite and running the suite on the command line. As far as I know python will compile all modules you \"import\". However python will NOT compile a python script run using: \"python script.py\" (it will however compile any modules that the script imports).The real questions is why you don't want python to compile the modules? You could probably automate a way of cleaning these up if they are getting in the way.You could make the directories that your modules exist in read-only for the user that the Python interpreter is running as.I don't think there's a more elegant option.  appears to have been an attempt to introduce a simple option for this, but it appears to have been abandoned.I imagine there's probably some other problem you're trying to solve, for which disabling .py[co] would appear to be a workaround, but it'll probably be better to attack whatever this original problem is instead."},
{"body": "How can I get the current time in milliseconds in Python? For what I needed, here's what I did, based on @samplebias' comment above:Quick'n'easy. Thanks all, sorry for the brain fart.For reuse:Then:time.time() may only give resolution to the second, the preferred approach for milliseconds is datetimeanother solution is the function you can embed into your own utils.pyIf you want a simple method in your code that returns the milliseconds with datetime:Microseconds is 1/1000000 seconds, milliseconds is 1/1000 seconds sodt.microseconds/1000.0 should be dt.microseconds/1000000.0"},
{"body": "Is there a way to dump a NumPy array into a CSV file? I have a 2D NumPy array and need to dump it in human-readable format. saves an array to a text file. is a convenient function to do this:The man page has some useful notes:Note. This fuction does not produce multi-line csv files, it saves everything to one line.It's easy and fast with pandasWriting record arrays as CSV files with headers requires a bit more work.This example reads a CSV file with the header on the first line, then writes the same file.Note that this example does not consider strings with commas, which would require quotes.If you want to save your numpy array (e.g. ) to one cell, you could convert it first with .Then save it the normal way to one cell, with \nand the cell in the csv-file will look like this Then you could restore your array like this:\nHere 'a' is the name of numpy array and 'file' is the variable to write in a file."},
{"body": "In Python, without using the  module, is there a way to determine a function's name from within that function?Say I have a module foo with a function bar.  When executing , is there a way for bar to know bar's name?  Or better yet, 's name?Python doesn't have a feature to access the function or its name within the function itself. It has been  but rejected. If you don't want to play with the stack yourself, you should either use  or  depending on context.There are a few ways to get the same result:Note that the  calls are thousands of times slower than the alternatives:You can get the name that it was defined with using , but that may not be the name that the function was called with:Whether that distinction is important to you or not I can't say.I wanted a very similar thing because I wanted to put the function name in a log string that went in a number of places in my code. Probably not the best way to do that, but here's a way to get the name of the current function.I found a wrapper that will write the function nameThis will printI keep this handy utility nearby:Usage:This is actually derived from the other answers to the question.Here's my take:The likely advantage of this version over using inspect.stack() is that it should be thousands of times faster [see Alex Melihoff's post and timings regarding using sys._getframe() versus using inspect.stack() ].In IDE the code outputsI guess  is the best way to do this. Example:Here's a future-proof approach.Combining @CamHart's and @Yuval's suggestions with @RoshOxymoron's  has the benefit of avoiding:So I think this plays nice with future python versions (tested on 2.7.3 and 3.3.2): seems to work too (Python 3.5).Test:Output:Your could use a decorator/wrapper:I do my own approach used for calling super with safety inside multiple inheritance scenario (I put all the code)sample usage:testing it :output:I did what CamHart said:Output:"},
{"body": "What will happen if two modules import each other?To generalize the problem, what about the cyclic imports in Python?There was a really good discussion on this over at  last year. It answers your question pretty thoroughly.If you do  inside  and  inside , it will work fine. By the time anything actually runs, both modules will be fully loaded and will have references to each other.The problem is when instead you do  and . Because now each module requires the other module to already be imported (so that the name we are importing exists) before it can be imported.Cyclic imports terminate, but you need to be careful not to use the cyclically-imported modules during module initialization.Consider the following files:a.py:b.py:If you execute a.py, you'll get the following:On the second import of b.py (in the second ), the Python interpreter does not import  again, because it already exists in the module dict.If you try to access  from  during module initialization, you will get an .Append the following line to :Then, the output is:This is because modules are executed on import and at the time  is accessed, the line  has not be executed yet, which will only happen after .As other answers describe this pattern is acceptable in python:Which will avoid the execution of the import statement when the file is imported by other modules. Only if there is a logical circular dependency, this will fail.Most Circular Imports are not actually logical circular imports but rather raise  errors, because of the way  evaluates top level statements of the entire file when called.:Consider this circular import:From David Beazleys excellent talk , , here is a way to deal with circular imports in python:This tries to import  and if  is raised, because it already is imported, it will pull it from the importcache.I got an example here that struck me! $ python main.pyI completely agree with pythoneer's answer here. But I have stumbled on some code that was flawed with circular imports and caused issues when trying to add unit tests. So to quickly patch it without changing everything you can resolve the issue by doing a dynamic import.Again, this isn't a permanent fix but may help someone that wants to fix an import error without changing too much of the code.Cheers! Ok, I think I have a pretty cool solution.\nLet's say you have file  and file .\nYou have a  or a  in file  that you want to use in module , but you have something else, either a , , or variable from file  that you need in your definition or class in file . \nWhat you can do is, at the bottom of file , after calling the function or class in file  that is needed in file , but before calling the function or class from file  that you need for file , say \nThen, and here is the , in all of the definitions or classes in file  that need the  or  from file  (let's call it ), you say This works because you can import file  without Python executing any of the import statements in file , and thus you elude any circular imports.For example:Voila."},
{"body": "I found some answers online, but I have no experience with regular expressions, which I believe is what is needed here.I have a string that needs to be split by either a ';' or ', '\nThat is, it has to be either a semicolon or a comma followed by a space. Individual commas without trailing spaces should be left untouchedExample string:should be split into a list containing the following:Luckily, Python has this built-in :)Following your comment:Do a  and then a Here's a safe way for any iterable of delimiters, using regular expressions: allows to build the pattern automatically and have the delimiters escaped nicely.Here's this solution as a function for your copy-pasting pleasure:If you're going to split often using the same delimiters, compile your regular expression beforehand like described and use .In response to Jonathan's answer above, this only seems to work for certain delimiters.  For example:By putting the delimiters in square brackets it seems to work more effectively.This is how the regex look like:"},
{"body": "I have a file called , located on . has a subdirectory called , with a file called :I want to import  from . I have tried this:Which resulted:Any ideas how to import  from the subdirectory?The  was the problem, but don't forget to refer to  as , or use:Take a look at the Packages documentation (Section 6.4) here: In short, you need to put a blank file named in the \"lib\" directory.Much later -- in linux, it would look like this:You can try inserting it in :Does your lib directory contain a  file?Python uses  to determine if a directory is a module.Try . For more information read about relative import in .I do this which basically covers all cases (make sure you have  in relative/path/to/your/lib/folder):\n \nYou have in your project folder: You have in another project folder: You want to use  and call foo function which is in it.So you write in app.py:try this:"},
{"body": "I want to find out the following:\ngiven a date ( object), what is the corresponding day of the week.For instance Sunday is the first day, Monday: second day.. and so onAnd then if the input is something like Today's date.\nThe output is maybe  (since its Friday)Use  ():From the documentation:If you'd like to have the date in English:Use  or .A solution whithout imports for dates after 1700/1/1 I solved this for a codechef .This is a solution if the date is a datetime object.Assuming you are given the day, month, and year, you could do:If you have reason to avoid the use of the datetime module, then this function will work.Note: The change from the Julian to the Gregorian calendar is assumed to have occurred in 1582. If this is not true for your calendar of interest then change the line  accordingly.datetime library sometimes gives errors with strptime() so I switched to dateutil library. Here's an example of how you can use it :The output that you get from this is . If you want the output as 'Monday', use the following :This worked for me pretty quickly. I was having problems while using the datetime library because I wanted to store the weekday name instead of weekday number and the format from using the datetime library was causing problems. If you're not having problems with this, great! If you are, you cand efinitely go for this as it has a simpler syntax as well. Hope this helps.To get Sunday as 1 through Saturday as 7, this is the simplest solution to your question:All of them:Output::)"},
{"body": "I have some python code that have inconsistent indentation, there is a lot of mixture of tabs and spaces to make the matter even worse even space indentation is not preserved.The code works as expected but it's difficult to maintain.How can I fix the indentation (like \"html tidy\" but for python) without breaking the code?Use the  script that you find in the  directory of your Python installation:Have a look at that script for detailed usage instructions.If you're using , see .For example, if you simply typeall your tabs will be expanded into spaces.You may want toto make sure that any new lines will not use literal tabs.If you're not using Vim,will replace tabs with spaces, assuming tab stops every 8 characters, in  (with the original going to , just in case).  Replace the 8s with 4s if your tab stops are every 4 spaces instead.I would reach for  to do this:You can apply this to your entire project using recursive flag:Use autopep8 automatically formats Python code to conform to the PEP 8\n nullstyle guide. It uses the pep8 utility to determine what parts of the\n nullcode needs to be formatted. autopep8 is capable of fixing most of the\n nullformatting issues that can be reported by pep8.There is also PythonTidy (since you said you like HTMLTidy)\nIt can be found here: \nIt can do a lot more than just clean up tabs though.  If you like that type of thing it's worth a look.Using vim, it shouldn't be more involved than hitting Esc, and then typing....on the file you want to change.  That will convert all tabs to 4 spaces.  If you have inconsistent spacing as well, then that will be more difficult.If you're lazy (like me), you can also download a trial version of Wingware Python IDE, which has an auto-fix tool for messed up indentation. It works pretty well.\nThe reindent script did not work for me, due to some missing module. Anyways, I found this  command which does the job perfect for me:Try emacs. It has good support for indentation needed in Python. Please check this link cheersOn most UNIX-like systems, you can also run:from the command line, changing the number if you want to replace tabs with a number of spaces other than 4. You can easily write a shell script to do this with a bunch of files at once, retaining the original file names.I have a simple solution for this problem. You can first type \":retab\" and then \":retab!\", then everything would be finetry Idle, and use ALT+X to find indentation"},
{"body": "What is the recommended way of handling settings for local development and the production server? Some of them (like constants, etc) can be changed/accessed in both, but some of them (like paths to static files) need to remain different, and hence should not be overwritten every time the new code is deployed.Currently, I am adding all constants to . But every time I change some constant locally, I have to copy it to the production server and edit the file for production specific changes... :( In :You can override what needed in ; it should stay out of your version control then. But since you mention copying I'm guessing you use none ;) suggests using version control for your settings files and storing the files in a separate directory:The  file contains common settings (such as MEDIA_ROOT or ADMIN), while  and  have site-specific settings:In the base file :In the local development settings file :In the file production settings file :Then when you run django, you add the  option:The authors of the book have also put up  on Github.Instead of , use this layout: is where most of your configuration lives. imports everything from common, and overrides whatever it needs to override:Similarly,  imports everything from  and overrides whatever it needs to override.Finally,  is where you decide which settings to load, and it's also where you store secrets (therefore this file should not be versioned):What I like about this solution is:I use a slightly modified version of the \"if DEBUG\" style of settings that Harper Shelby posted.  Obviously depending on the environment (win/linux/etc.) the code might need to be tweaked a bit.I was in the past using the \"if DEBUG\" but I found that occasionally I needed to do testing with DEUBG set to False.  What I really wanted to distinguish if the environment was production or development, which gave me the freedom to choose the DEBUG level.I'd still consider this way of settings a work in progress. I haven't seen any one way to handling Django settings that covered all the bases and at the same time wasn't a total hassle to setup (I'm not down with the 5x settings files methods).I use a settings_local.py and a settings_production.py. After trying several options I've found that it's easy to waste time with complex solutions when simply having two settings files feels easy and fast.When you use mod_python/mod_wsgi for your Django project you need to point it to your settings file. If you point it to app/settings_local.py on your local server and app/settings_production.py on your production server then life becomes easy. Just edit the appropriate settings file and restart the server (Django development server will restart automatically).The problem with most of these solutions is that you either have your local settings applied  the common ones, or  them.So it's impossible to override things likeat the same time.One solution can be implemented using \"ini\"-style config files with the ConfigParser class. It supports multiple files, lazy string interpolation, default values and a lot of other goodies.\nOnce a number of files have been loaded, more files can be loaded and their values will override the previous ones, if any.You load one or more config files, depending on the machine address, environment variables and even values in previously loaded config files. Then you just use the parsed values to populate the settings. One strategy I have successfully used has been:As an example of something you can achieve with this, you can define a \"subdomain\" value per-env, which is then used in the default settings (as ) to define all the necessary hostnames and cookie things django needs to work.This is as DRY I could get, most (existing) files had just 3 or 4 settings. On top of this I had to manage customer configuration, so an additional set of configuration files (with things like database names, users and passwords, assigned subdomain etc) existed, one or more per customer.One can scale this as low or as high as necessary, you just put in the config file the keys you want to configure per-environment, and once there's need for a new config, put the previous value in the default config, and override it where necessary.This system has proven reliable and works well with version control. It has been used for long time managing two separate clusters of applications (15 or more separate instances of the django site per machine), with more than 50 customers, where the clusters were changing size and members depending on the mood of the sysadmin...I manage my configurations with the help of . It is a drop-in replacement for the default settings. It is simple, yet configurable. And refactoring of your exisitng settings is not required.Here's a small example (file ):That's it.Remember that settings.py is a live code file. Assuming that you don't have DEBUG set on production (which is a best practice), you can do something like:Pretty basic, but you could, in theory, go up to any level of complexity based on just the value of DEBUG - or any other variable or code check you wanted to use.I am also working with Laravel and I like the implementation there. I tried to mimic it and combining it with the solution proposed by T. Stone (look above):Maybe something like this would help you.My solution to that problem is also somewhat of a mix of some solutions already stated here:I then base all my environment-dependent settings on that one:I prefer this to having two separate settings.py files that I need to maintain as I can keep my settings structured in a single file easier than having them spread across several files. Like this, when I update a setting I don't forget to do it for both environments.Of course that every method has its disadvantages and this one is no exception. The problem here is that I can't overwrite the  file whenever I push my changes into production, meaning I can't just copy all files blindly, but that's something I can live with.For most of my projects I use following pattern:(To run manage.py with custom settings file you simply use --settings command option: )I use a variation of what jpartogi mentioned above, that I find a little shorter:Basically on each computer (development or production) I have the appropriate hostname_settings.py file that gets dynamically loaded.There is also Django Classy Settings. I personally am a big fan of it. It's built by one of the most active people on the Django IRC. You would use environment vars to set things.I differentiate it in manage.py and created two separate settings file: local_settings.py and prod_settings.py. In manage.py I check whether the server is local server or production server. If it is a local server it would load up local_settings.py and it is a production server it would load up prod_settings.py. Basically this is how it would look like:I found it to be easier to separate the settings file into two separate file instead of doing lots of ifs inside the settings file.As an alternative to maintain different file if you wiil:\nIf you are using git or any other VCS to push codes from local to server, what you can do is add the settings file to .gitignore.This will allow you to have different content in both places without any problem. SO on server you can configure an independent version of settings.py and any changes made on the local wont reflect on server and vice versa.In addition, it will remove the settings.py file from github also, the big fault, which i have seen many newbies doing.In order to use different  configuration on different environment, create different settings file. And in your deployment script, start the server using  parameter, via which you can use different  on different environment.:TL;DR: The trick is to modify  before you import  in any , this will greatly simplify things.Just thinking about all these intertwining files gives me a headache. \nCombining, importing (sometimes conditionally), overriding, patching of what was already set in case  setting changed later on. \nWhat a nightmare!Through the years I went through all different solutions. They all  work, but are so painful to manage. \nWTF! Do we really need all that hassle? We started with just one  file. \nNow we need a documentation just to correctly combine all these together in a correct order!I hope I finally hit the (my) sweet spot with the solution below.My strategy consists of excellent  used with  style files, \nproviding  defaults for local development, some minimal and short  files that have an \n  the  was set from an  file. This effectively give us a kind of settings injection.The trick here is to modify  before you import .To see the full example go do the repo: A defaults for local development. A secret file, to mostly set required environment variables. \nSet them to empty values if they are not required in local development. \nWe provide defaults here and not in  to fail on any other machine if the're missing from the environment.What happens in here, is loading environment from , then importing common settings \nfrom . After that we can override a few to ease local development.For production we should not expect an environment file, but it's easier to have one if we're testing something. \nBut anyway, lest's provide few defaults inline, so  can respond accordingly. The main point of interest here are  and  overrides, \nthey will be applied to the python  ONLY if they are MISSING from the environment and the file. These will be our production defaults, no need to put them in the environment or file, but they can be overridden if needed. Neat!These are your mostly vanilla django settings, with a few conditionals and lot's of reading them from the environment. \nAlmost everything is in here, keeping all the purposed environments consistent and as similar as possible.The main differences are below (I hope these are self explanatory):The last bit shows the power here.  has a sensible default, \nwhich can be overridden in  and even that that can be overridden by an environment setting! Yay! In effect we have a mixed hierarchy of importance:1 - Create a new folder inside your app and name settings to it.2 - Now create a new .py file in it and inside it writefrom .local import *passfrom .production import *pass3 - Create three new files in the settings folder name local.py and production.py and base.py4 - Inside base.py copy all the content of previous settings.p folder and rename it with something different let say old_settings.py5 - In base.py change your BASE_DIR path to point to your new path of settingOld path-> BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath()))New path -> BASE_DIR = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath()))now in this way the project dir can be structured and can be manageable among production and local development.I found the responses here very helpful. (Has this been more definitively solved? The last response was a year ago.) After considering all the approaches listed, I came up with a solution that I didn't see listed here. My criteria were:I thought switching on the host machine made some sense, but then figured the real issue here is different settings for different , and had an aha moment. I put this code at the  of my settings.py file:This way, the app  to production settings, which means you are explicitly \"whitelisting\" your development environment. It is much safer to forget to set the environment variable locally than if it were the other way around and you forgot to set something in production and let some dev settings be used. When developing locally, either from the shell or in a .bash_profile or wherever:(Or if you're developing on Windows, set via the Control Panel or whatever its called these days... Windows always made it so obscure that you could set environment variables.)With this approach, the dev settings are all in one (standard) place, and simply override the production ones where needed. Any mucking around with development settings should be completely safe to commit to source control with no impact on production. "},
{"body": "I understand how this construct works:But I don't understand why  is used as the keyword here, since it suggests the code in question only runs if the  block does not complete, which is the opposite of what it does! No matter how I think about it, my brain can't progress seamlessly from the  statement to the  block. To me,  or  would make more sense (and I'm trying to train myself to read it as such).I'm wondering how Python coders read this construct in their head (or aloud, if you like). Perhaps I'm missing something that would make such code blocks more easily decipherable?It's a strange construct even to seasoned Python coders. When used in conjunction with for-loops it basically means \"find some item in the iterable, else if none was found do ...\". As in:But anytime you see this construct, a better alternative is to either encapsulate the search in a function:Or use a list comprehension:It is not semantically equivalent to the other two versions, but works good enough in non-performance critical code where it doesn't matter whether you iterate the whole list or not. Others may disagree, but I personally would avoid ever using the for-else or while-else blocks in production code. See also A common construct is to run a loop until something is found and then to break out of the loop. The problem is that if I break out of the loop or the loop ends I need to determine which case happened. One method is to create a flag or store variable that will let me do a second test to see how the loop was exited.For example assume that I need to search through a list and process each item until a flag item is found and then stop processing. If the flag item is missing then an exception needs to be raised.Using the Python ... construct you haveCompare this to a method that does not use this syntactic sugar:In the first case the  is bound tightly to the for loop it works with. In the second the binding is not as strong and errors may be introduced during maintenance.There's an excellent presentation by Raymond Hettinger, titled , in which he briefly addresses the history of the  construct. The relevant section is \"Distinguishing multiple exit points in loops\"  and continuing for about three minutes. Here are the high points:So, if the question is, \"Why don't they change this keyword?\" then  \u2013 at this point, it would be too destructive to existing code to be practical. But if the question you're really asking is why  was reused in the first place, well, apparently it seemed like a good idea at the time.Personally, I like the compromise of commenting  in-line wherever the  could be mistaken, at a glance, as belonging inside the loop. It's reasonably clear and concise. This option gets a brief mention in  at the end of his answer:Because they didn't want to introduce a new keyword to the language. Each one steals an identifier and causes backwards compatibility problems, so it's usually a last resort.I read it something like:If still on the conditions to run the loop, do stuff,  do something else.The easiest way I found to 'get' what the for/else did, and more importantly, when to use it, was to concentrate on where the break statement jumps to. The For/else construct is a single block. The break jumps out of the block, and so jumps 'over' the else clause. If the contents of the else clause simply followed the for clause, it would never be jumped over, and so the equivalent logic would have to be provided by putting it in an if. This has been said before, but not quite in these words, so it may help somebody else. Try running the following code fragment. I'm wholeheartedly in favour of the 'no break' comment for clarity.I think documentation has a great explanation of  , Source: I read it like \"When the  is exhausted completely, and the execution is about to proceed to the next statement after finishing the , the else clause will be executed.\" Thus, when the iteration is broken by , this will not be executed.Since the technical part has been pretty much answered, my comment is just in relation with the  that produce this  keyword.Being Python a very  programming language, the misuse of a keyword is more notorious. The  keyword perfectly describes  part of the flow of a decision tree, \"if you can't do this, (else) do that\". It's  in our own language.Instead, using this keyword with  and  statements creates confusion. The reason, our career as programmers has taught us that the  statement resides within a decision tree; its , a wrapper that  return a path to follow. Meanwhile, loop statements have a figurative explicit goal to reach something. The goal is met after continuous iterations of a process.  . Loops .The issue is that  is a word that clearly define the last option in a condition. The  of the word are both  by Python and Human Language. But the else word in Human Language is never used to indicate the actions someone or something will take after something is completed. It will be used if, in the process of completing it, an issue rises (more like a  statement).At the end, the keyword will remain in Python. It's clear it was mistake, clearer when every programmer tries to come up with a story to understand its usage like some mnemonic device. I'd have loved if they have chosen instead the keyword . I believe that this keyword fits perfectly in that iterative flow, the  after the loop.It resembles that situation that some child has after following every step in assembling a toy: And  what Dad?You could think of it like,\n as in the rest of the stuff, or the other stuff, that wasn't done in the loop.I know this is an old thread, but I am looking into the same question right now,  and I'm not sure anyone has captured the answer to this question in the way I understand it.For me, there are three ways of \"reading\" the  in  or  statements, all of which are equivalent, are:I think the key is that the  is pointless without the 'break', so a  includes:So, essential elements of a  loop are as follows, and you would read them in plainer English as:As the other posters have said, a break is generally raised when you are able to locate what your loop is looking for, so the  becomes \"what to do if target item not located\".You can also use exception handling, breaks, and for loops all together.Simple example with a break being hit.Simple example where there no break, no condition raising a break, and no error are encountered.The  keyword can be confusing here, and as many people have pointed out, something like ,  is more appropriate.In order to understand  logically, compare it with , not , most of python programmers are familiar with the following code:Similarly, think of  as a special kind of :The difference is  implies  and you can not write it out, so it becomes:Yes, I know this comparison can be difficult and tiresome, but it does clarify the confusion.Codes in  statement block will be executed when the  loop was not be broke.From the "},
{"body": "I have a list :For numbers above 45 inclusive, I would like to add 1; and for numbers less than it, 5.I triedBut it gives me a syntax error. How can I achieve an  \u2013  like this in a list comprehension?Do-something if , else do-something else.The reason you're getting this error has to do with how the list comprehension is performed.Keep in mind the following:Is equivalent to:Where the  is in a slightly different format (think switching the subject and verb order in a sentence).Therefore, your code  does this:However, this code  does this (after rearranging the ):And for a reward, here is the comment, I wrote to remember this the first time I did this error:You must put the expression at the beginning of the list comprehension, an if statement at the end filters elements!You can also put the conditional expression in brackets inside the list comprehension:[false,true][condition] is the syntaxYou could move the conditional to:But it's starting to look a little ugly, so you might be better off using a normal loop. Note that I used  instead of  for the list variable to reduce confusion with the number 1 (I think  and  should be avoided as variable names under any circumstances, even in quick-and-dirty example code).I just had a similar problem, and found this question and the answers really useful. Here's the part I was confused about. I'm writing it explicitly because no one actually stated it simply in English: Normally, a loop goes Everyone states the list comprehension part simply as the first answer did, but that's actually not what you do in this case. (I was trying to do it that way)In this case, it's more like this:Like in , the two s with  and  doing two different things. The part  is from a lambda expression:while the other  is another lambda:Whole list comprehension can be regard as combination of  and :"},
{"body": "Given the name of a Python (2.X) package that can be installed with  and , is there any way to find out a list of all the possible versions of it that pip could install? Right now it's trial and error.I'm trying to install a version for a third party library, but the newest version is too new, there were backwards incompatible changes made. So I'd like to somehow have a list of all the versions that pip knows about, so that I can test them.The script at pastebin does work. However it's not very convenient if you're working with multiple environments/hosts because you will have to copy/create it every time.A better all-around solution would be to use , which is available to install with pip. E.g. to see what versions of Django are available:A minor caveat: yolk depends on distribute. This is not a bad thing, but it may be a problem if you need for some reason to stick with (the deprecated) python setuptools. I am not involved in the development of yolk.  Use the  instead and consider submitting a fix, if possible.Without actually having to download or install any additional packages you can use  for specifying a particular version while , and the available versions will be printed:update: I had changed my answer to have nothing after the ==, this however doesn't work for older versions of pip (those of you should probably update pip) add any string after the == that is not likely to be an install candidate:Use , you can see all versions that availableTo not install any package, use one of following solution:or Tested with pip 1.0You don't need a third party package to get this information. pypi provides simple JSON feeds for all packages underHere's some Python code using only the standard library which gets all versions.That code prints (as of Feb 23rd, 2015):You could the yolk3k package instead of yolk. yolk3k is a fork from the original yolk and it supports both python2 and 3.After looking at pip's code for a while, it looks like the code responsible for locating packages can be found in the  class in . Its method  looks up the versions of a , but unfortunately only returns the most recent version. The code below is almost a 1:1 copy of the original function, with the return in line 114 changed to return all versions.The script expects one package name as first and only argument and returns all versions. But hopefully this helps.Sample outputThe code:I came up with dead-simple bash script. Thanks to 's author. - works for packages whose maintainers choose to show all packages\n - should do the trick anyhow (lists all links)Pip 7.1.0 has removed --no-install option from install. I found a method to get all versions of a package without any extra package.I just ran this:e.g.:I didn't have any luck with ,  or  but so I ended up using this (adapted to Python 3 from eric chiang's answer):You can use this short Python3 snippet to grab the list of available versions for a package from . Unlike some other Python solutions posted here, this doesn't break on loose versions like 's  or 's :This works for me on OSX:It returns the list one per line:Or to get the latest version available:Keep in mind  has to be installed (on OSX) to parse the versions. You can install it with you can grep the result of your "},
{"body": "I am  to understand what Python's descriptors are and what they can be useful for. However, I am failing at it. I understand how they work, but here are my doubts. Consider the following code:The descriptor is how Python's  type is implemented. A descriptor simply implements , , etc. and is then added to another class in its definition (as you did above with the Temperature class). For example:Accessing the property you assigned the descriptor to ( in the above example) calls the appropriate descriptor method. in  is the instance of the class (so above,  would receive , while  is the class with the descriptor (so it would be ).You need to use a descriptor class to encapsulate the logic that powers it. That way, if the descriptor is used to cache some expensive operation (for example), it could store the value on itself and not its class.An article about descriptors can be found at EDIT: As jchl pointed out in the comments, if you simply try ,  will be .it gives you extra control over how attributes work.  if you're used to getters and setters in java, for example, then it's python's way of doing that.  one advantage is that it looks to users just like an attribute (there's no change in syntax).  so you can start with an ordinary attribute and then, when you need to do something fancy, switch to a descriptor.an attribute is just a mutable value.  a descriptor lets you execute arbitrary code when reading or setting (or deleting) a value.  so you could imagine using it to map an attribute to a field in a database, for example - a kind of ORM.another use might be refusing to accept a new value by throwing an exception in  - effectively making the \"attribute\" read only.this is pretty subtle (and the reason i am writing a new answer here - i found this question while wondering the same thing and didn't find the existing answer that great).a descriptor is defined on a class, but is typically called from an instance.  when it's called from an instance both  and  are set (and you can work out  from  so it seems kinda pointless).  but when called from a class, only  is set - which is why it's there.this is only needed for  because it's the only one that can be called on a class.  if you set the class value you set the descriptor itself.  similarly for deletion.  which is why the  isn't needed there.well, here's a cool trick using similar classes:(i'm using python 3; for python 2 you need to make sure those divisions are  and ).  that gives:now there are other, arguably better ways to achieve the same effect in python (eg if celsius were a property, which is the same basic mechanism but places all the source inside the Temperature class), but that shows what can be done...Descriptors are objects with any of , , or . These descriptor objects can be used as attributes on other object class definitions. Descriptor objects can be used to programmatically manage the results of a dotted lookup (e.g. ) in a normal expression, an assignment, and even a deletion. Bound methods, , , and  all use these special methods to manage how they are accessed via the dotted lookup.A descriptor is an object with any of the following methods (, , or ), intended to be used via dotted-lookup as if it were a typical attribute of an instance. For an owner-object, , with a  object: is the instance whose class contains the descriptor object's instance.  is the instance of the  (probably just one for the class of the )To define this with code, an object is a descriptor if the set of its attributes intersects with any of the required attributes:A  has a  and/or .\nA  has neither  nor .We can see that  and  are Non-Data-Descriptors:Both only have the  method:Note that all functions are also Non-Data-Descriptors:However,  is a Data-Descriptor:These are important , as they affect the lookup order for a dotted lookup. The consequence of this lookup order is that Non-Data-Descriptors like functions/methods can be .We have learned that descriptors are objects with any of , , or . These descriptor objects can be used as attributes on other object class definitions. Now we will look at how they are used, using your code as an example.Here's your code, followed by your questions and answers to each:Your descriptor ensures you always have a float for this class attribute of , and that you can't use  to delete the attribute:Otherwise, your descriptors ignore the owner-class and instances of the owner, instead, storing state in the descriptor. You could just as easily share state across all instances with a simple class attribute (so long as you always set it as a float to the class and never delete it, or are comfortable with users of your code doing so):This gets you exactly the same behavior as your example (see response to question 3 below), but uses a Pythons builtin (), and would be considered more idiomatic: is the instance of the owner that is calling the descriptor. The owner is the class in which the descriptor object is used to manage access to the data point. See the descriptions of the special methods that define descriptors next to the first paragraph of this answer for more descriptive variable names.Here's a demonstration:You can't delete the attribute:And you can't assign a variable that can't be converted to a float:Otherwise, what you have here is a global state for all instances, that is managed by assigning to any instance.  The expected way that most experienced Python programmers would accomplish this outcome would be to use the  decorator, which makes use of the same descriptors under the hood, but brings the behavior into the implementation of the owner class:Which has the exact same expected behavior of the original piece of code:We've covered the attributes that define descriptors, the difference between data- and non-data-descriptors, builtin objects that use them, and specific questions about use.So again, how would you use the question's example? I hope you wouldn't. I hope you would start with my first suggestion (a simple class attribute) and move on to the second suggestion (the property decorator) if you feel it is necessary. I tried (with minor changes as suggested) the code from Andrew Cooke's answer. (I am running python 2.7).The code:The result:With Python prior to 3, make sure you subclass from object which will make the descriptor work correctly as the  magic does not work for old style classes."},
{"body": "If I do the following:I get:Apparently a cStringIO.StringIO object doesn't quack close enough to a file duck to suit subprocess.Popen.  How do I work around this? documentation:So your example could be written as follows:I figured out this workaround:Is there a better one?I'm a bit surprised nobody suggested creating a pipe, which is in my opinion the far simplest way to pass a string to stdin of a subprocess:\"Apparently a cStringIO.StringIO object doesn't quack close enough to a file duck to suit subprocess.Popen\":-)I'm afraid not.  The pipe is a low-level OS concept, so it absolutely requires a file object that is represented by an OS-level file descriptor.  Your workaround is the right one.I am using python3 and found out that you need to encode your string before you can pass it into stdin:Beware that may give you trouble ifis too big, because apparently the parent process will buffer it  forking the child subprocess, meaning it needs \"twice as much\" used memory at that point (at least according to the \"under the hood\" explanation and linked documentation found ). In my particular case,was a generator that was first fully expanded and only then written to so the parent process was huge right before the child was spawned, \nand no memory was left to fork it:There's a beatiful solution if you're using Python 3.4 or better. Use the  argument instead of the  argument, which accepts a bytes argument:"},
{"body": "I wonder if there is a direct way to import the contents of a csv file into a record array, much in the way that R's , , and  family imports data to R's data frame? Or is the best way to use  and then apply something like ?You can use Numpy's  method to do so, by setting the  kwarg to a comma.More information on the function can be found at its respective .I would recommend the  function from the  library:This gives a pandas  - allowing .I would also recommend . However, since the question asks for a , as opposed to a normal array, the  parameter needs to be added to the  call:Given an input file, :gives an array:and gives a record array:This has the advantage that file with .You can also try  which can guess data types and return a properly formatted record array.I timed the versus on 4.6 million rows with about 70 columns and found that the numpy path took 2 min 16s and the csv-list comprehension method took 13s.I would recommend the csv-list comprehension method as it is most likely relies on pre-compiled libraries and not the interpreter as much as numpy. i suspect the pandas method would have similar interpreter overhead."},
{"body": "I have a list of dictionaries like this:and I want to turn this into a pandas  like this:Ultimately, the goal is to write this to a text file and this seems like the best solution I could find.  How can I turn the list of dictionaries into a panda DataFrame as shown above?Supposing  is your list of dicts, simply:in pandas 16.2, I had to do  to get this to work. "},
{"body": "Let's say I have a class that has a member called data which is a list.  I want to be able to initialize the class with, for example, a filename (which contains data to initialize the list) or with an actual list.What's your technique for doing this?Do you just check the type by looking at ?Is there some trick I might be missing?I'm used to C++ where overloading by argument type is easy. A much neater way to get 'alternate constructors' is to use classmethods. For instance:The reason it's neater is that there is no doubt about what type is expected, and you aren't forced to guess at what the caller intended for you to do with the datatype it gave you. The problem with  is that there is no way for the caller to tell you, for instance, that even though the type is not a basestring, you should treat it as a string (and not another sequence.) And perhaps the caller would like to use the same type for different purposes, sometimes as a single item, and sometimes as a sequence of items. Being explicit takes all doubt away and leads to more robust and clearer code.Excellent question. I've tackled this problem as well, and while I agree that \"factories\" (class-method constructors) are a good method, I would like to suggest another, which I've also found very useful:Here's a sample (this is a  method and not a constructor, but the idea is the same):The key idea is here is using Python's excellent support for named arguments to implement this. Now, if I want to read the data from a file, I say:And to read it from a string, I say:This way the user has just a single method to call. Handling it inside, as you saw, is not overly complexQuick and dirty fixThen you can call it withA better way would be to use isinstance and type conversion. If I'm understanding you right, you want this:You should use isinstanceYou probably want the  builtin function:My preferred solution is:Then invoke it with either  or .Hope that helps. Happy Coding!OK, great. I just tossed together this example with a tuple, not a filename, but that's easy. Thanks all.a = [1,2]b = (2,3)c = MyData(a)d = MyData(b)c.GetData()d.GetData()[1, 2][2, 3]Why don't you go even more pythonic?\n"},
{"body": "I want to create string using integer appended to it, in a for loop.  Like this:But it returns an error:What's the best way to concatenate the String and Integer?The method used in this answer (backticks) is deprecated in later versions of Python 2, and removed in Python 3. Use the  function instead.You can use :It will print .To get  you can use this as @YOU suggestedYou can use :It will print .To get  you can use this as @YOU suggestedTo get , you could do likeThe  function converts the integer into a string.What you did () is And  is a TypeError since you can't add an integer to a string (unlike JavaScript).Look at the documentation for , it is very powerful.You can use a generator to do this ! If we want output like  then we can use  and  method of string. If we want List of string values then use  method. Use  for Python 2.xUSe  for Python 3.xI did something else.\nI wanted to replace a word, in lists off lists, that contained phrases.\nI wanted to replace that sttring / word with a new word that will be a join between string and number, and that number / digit will indicate the position of the phrase / sublist / lists of lists. That is, I replaced a string with a string and an incremental number that follow it. Concatenation of a string and integer is simmple:\njust use"},
{"body": "Is there a way to expand a Python tuple into a function - as actual parameters?For example, here  does the magic:I know one could define  as , but of course there may be legacy code.\nThanks does  what you request.Side issue:  use as your identifiers builtin type names such as , , , , and so forth -- it's horrible practice and it  come back and byte you when you least expect it,\nso just get into the habit of actively  hiding builtin names with your own identifiers.Note that you can also expand part of argument list:Take a look at  section 4.7.3 and 4.7.4.\nIt talks about passing tuples os arguments.I would also consider using named parameters (and passing a dictionary) over a tuple and passing a sequence. I find the use of positional arguments to be a bad practice when the positions are not intuitive or there are multiple parameters. This is the functional programming method. It lifts the tuple expansion feature out of syntax sugar:Example usage: redefiniton of  saves a lot of  calls in the long run."},
{"body": "How can I delete the contents of a local folder in Python?The current project is for Windows but I would like to see *nix also.Updated to only delete files and to used the os.path.join() method suggested in the comments. If you also want to remove subdirectories, uncomment the elif statement.Try the shutil moduleYou can simply do this :You can of corse use an other filter in you path, for exemple : /YOU/PATH/*.txt for removing all text files in a directory.Expanding on mhawke's answer this is what I've implemented. It removes all the content of a folder but not the folder itself. Tested on Linux with files, folders and symbolic links, should work on Windows as well.Using  and recreating the folder could work, but I have run into errors when deleting and immediately recreating folders on network drives.The proposed solution using walk does not work as it uses  to remove folders and then may attempt to use  on the files that were previously in those folders.  This causes an error.The posted  solution will also attempt to delete non-empty folders, causing errors.I suggest you use:As a oneliner:A more robust solution accounting for files and directories as well would be (2.7):You might be better off using  for this. doesn't distinguish files from directories and you will quickly get into trouble trying to unlink these. There is a good example of using  to recursively remove a directory , and hints on how to adapt it to your circumstances.Here's a long and ugly, but reliable and efficient solution.It resolves a few problems which are not addressed by the other answerers:Here's the code (the only useful function is ):I konw it's an old thread but I have found something interesting from the official site of python. Just for sharing another idea for removing of all contents in a directory. Because I have some problems of authorization when using shutil.rmtree() and I don't want to remove the directory and recreate it. The address original is . Hope that could help someone.Yet Another Solution:An earlier comment also mentions using os.scandir in Python 3.5+.  For example:I used to solve the problem this way:This is the only answer so far, which:Code:As many other answers, this does not try to adjust permissions to enable removal of files/directories.I resolved the error issue with rmtree makedirs by adding time.sleep() between.Answer for a limited, specific situation:\nassuming you want to delete the files while maintainig the subfolders tree, you could use a recursive algorithm:Maybe slightly off-topic, but I think many would find it usefulThis should do the trick just using the OS module to list and then remove!Worked for me, any problems let me know!    "},
{"body": "A (long) while ago I wrote a web-spider that I multithreaded to enable concurrent requests to occur at the same time.  That was in my Python youth, in the days before I knew about the  and the associated woes it creates for multithreaded code (IE, most of the time stuff just ends up serialized!)...I'd like to rework this code to make it more robust and perform better.  There are basically two ways I could do this: I could use the new  in 2.6+ or I could go for a reactor / event-based model of some sort.  I would rather do the later since it's far simpler and less error-prone.So the question relates to what framework would be best suited to my needs.  The following is a list of the options I know about so far:Is there anything I have missed at all?  Surely there must be a library out there that fits the sweet-spot of a simplified async networking library![edit: big thanks to  for his pointer to .  If you scroll to the bottom you will see there is a really nice list of projects that aim to tackle this task in one way or another.  It seems actually that things have indeed moved on since the inception of Twisted: people now seem to favour a  based solution rather than a traditional reactor / callback oriented one.  The benefits of this approach are clearer more direct code: I've certainly found in the past, especially when working with  in C++ that callback based code can lead to designs that can be hard-to-follow and are relatively obscure to the untrained eye.  Using co-routines allows you to write code that looks a little more synchronous at least.  I guess now my task is to work out which one of these many libraries I like the look of and give it a go!  Glad I asked now...][edit: perhaps of interest to anyone who followed or stumbled on this this question or cares about this topic in any sense: I found a really great writeup of the current state of  for this job]I liked the  Python module which relies on either Stackless Python microthreads or Greenlets for light-weight threading. All blocking network I/O is transparently made asynchronous through a single  loop, so it should be nearly as efficient as an real asynchronous server.I suppose it's similar to Eventlet in this way.The downside is that its API is quite different from Python's / modules; you need to rewrite a fair bit of your application (or write a compatibility shim layer) It seems that there's also , which is similar, but uses Python 2.5's  for its coroutines, instead of Greenlets. This makes it more portable than concurrence and other alternatives. Network I/O is done directly with epoll/kqueue/iocp.Twisted is complex, you're right about that. Twisted is  bloated. If you take a look here:  you'll find an organized, comprehensive, and very well tested suite of  protocols of the internet, as well as helper code to write and deploy very sophisticated network applications. I wouldn't confuse bloat with comprehensiveness.It's well known that the Twisted documentation isn't the most user-friendly from first glance, and I believe this turns away an unfortunate number of people. But Twisted is amazing (IMHO) if you put in the time.  I did and it proved to be worth it, and I'd recommend to others to try the same. is .API-wise it follows the same conventions as the standard library (in particular, threading and multiprocessing modules) where it makes sense. So you have familiar things like  and  to work with.It only supports  ( ) as reactor implementation but takes full advantage of it, featuring a fast WSGI server based on libevent-http and resolving DNS queries through libevent-dns as opposed to using a thread pool like most other libraries do. ( since 1.0 c-ares is used to make async DNS queries; threadpool is also an option.)Like eventlet, it makes the callbacks and Deferreds unnecessary by using .Check out the examples: , .A really  of such frameworks was compiled by Nicholas Pi\u00ebl on his blog: it's well worth a read!None of these solutions will avoid that fact that the GIL prevents CPU parallelism - they are just better ways of getting IO parallelism that you already have with threads.  If you think you can do better IO, by all means pursue one of these, but if your bottleneck is in processing the results nothing here will help except for the multiprocessing module.  I wouldn't go as far as to call Twisted bloated, but it is difficult to wrap your head around. I avoided really settling in an learn for quite a while as I always wanted something a little easier for 'small tasks'. However, now that I have worked with it some more I have to say having all the batteries included is VERY nice.All the other async libraries I've worked with end being way less mature than they even appear. Twisted's event loop is solid. I'm not quite sure how to solve the steep Twisted learning curve. It might help if someone would fork it and clean a few things up, like removing all the backwards compatability cruft and the dead projects. But that's the nature of mature software I guess. hasn't been mentioned yet. Its concurrency model is based on wiring together components with message passing between inboxes and outboxes. 's a brief overview.I've started to use twisted for some things.  The beauty of it almost is because it's \"bloated.\"  There are connectors for just about any of the main protocols out there.  You can have a jabber bot that will take commands and post to an irc server, email them to someone, run a command, read from an NNTP server, and monitor a web page for changes.  The bad news is it can do all of that and can make things overly complex for simple tasks like the OP explained.  The advantage of python though is you only include what you need.  So while the download may be 20mb, you may only include 2mb of libraries (which is still a lot).  My biggest complaint with twisted is although they include examples, anything beyond a basic tcp server you're on your own.  While not a python solution, I've seen node.js gain a lot more traction as of late.  In fact I've considered looking into it for smaller projects but I just cringe when I hear javascript :)There is a good book on the subject: \"Twisted Network Programming Essentials\", by Abe Fettig.  The examples show how to write very Pythonic code, and to me personally, do not strike me as  based on a bloated framework.  Look at the solutions in the book, if they aren't clean, then I don't know what clean means. My only enigma is the same I have with other frameworks, like Ruby. I worry, does it scale up?  I would hate to commit a client to  a framework that is going to have scalability problems. is a tiny asynchronous socket framework that uses pyev. Its very fast, primarily because of pyev. It attempts to provide a similiar interface as twisted with some slight changes.Also try . It's coroutine-based (so it's similar to Concurrence, Eventlet and gevent). It implements drop-in non-blocking replacements for socket.socket, socket.gethostbyname (etc.), ssl.SSLSocket, time.sleep and select.select. It's fast. It needs Stackless Python and libevent. It contains a mandatory Python extension written in C (Pyrex/Cython).I Confirm the goodness of . It can use libev (the newer, cleaner and better performance version of libevent). Some times ago it doesn't has as much support as libevent has, but now the development process go further and is very useful.If you just want a Simplified, lightweight HTTP Request Library then I find  really good   You are welcome to have a look at PyWorks, which takes a quite different approach. It lets object instances run in their own thread and makes function call's to that object async. Just let a class inherit from Task instead of object and it is async, all methods calls are Proxies. Return values (if you need them) are Future proxies.PyWorks can be found on "},
{"body": "When I print a numpy array, I get a truncated representation, but I want the full array. Is there any way to do this?To clarify on Reed's replyNote that the reply as given above works with an initial 'from numpy import *', which is not advisable. \nThis also works for meFor full documentation, see . I suggest using  instead of  which is suggested by others. They both work for your purpose, but by setting the threshold to \"infinity\" it is obvious to everybody reading your code what you mean. Having a threshold of \"not a number\" seems a little vague to me.This sounds like you're using numpy.If that's the case, you can add:That will disable the corner printing.  For more information, see this .Here is a one-off way to do this, which is useful if you don't want to change your default settings:The previous answers are the correct ones, but as a weeker alternative you can transform into a list:Using a context manager as  sugggested is a good option for a one time usage:or if you need a string:Default output format is:and it can be configured with further arguments.Tested on Python 2.7.12, numpy 1.11.1.For these who like to import as np:Will also work "},
{"body": "I have a Python pandas DataFrame :I can filter the rows whose stock id is  like this: and I want to get all the rows of some stocks together, such as . That means I want a syntax like this: Since pandas not accept above command, how to achieve the target? Use the  method.  . is ideal if you have a list of exact matches, but if you have a list of partial matches or substrings to look for, you can filter using the  method and regular expressions.For example, if we want to return a DataFrame where all of the stock IDs which begin with  and then are followed by any three digits:Suppose now we have a list of strings which we want the values in  to end with, e.g.We can join these strings with the regex 'or' character  and pass the string to  to filter the DataFrame: Finally,  can ignore case (by setting ), allowing you to be more general when specifying the strings you want to match.For example,would match , , , and so on.you can also use ranges by using:You can also directly  your DataFrame for this information.Or similarly search for ranges:Given a dataframe like this:There are multiple ways of selecting or slicing the data.The most obvious is the  feature. You can create a mask that gives you a series of / statements, which can be applied to a dataframe like this:Masking is the ad-hoc solution to the problem, but does not always perform well in terms of speed and memory.By setting the index to the  column, we can use the pandas builtin slicing object This is the fast way of doing it, even if the indexing can take a little while, it saves time if you want to do multiple queries like this.This can also be done by merging dataframes. This would fit more for a scenario where you have a lot more data than in these examples.All the above methods work even if there are multiple rows with the same You can use , i.e.:"},
{"body": "I want to get, given a character, its  value.For example, for the character , I want to get , and vice versa.Use  and :The question has been answered but I think this reference is a good thing to keep note of. ord and chr"},
{"body": "I am trying to fix how python plots my data.Say and Then I would do:and the x axis' ticks are plotted in intervals of 5. Is there a way to make it show intervals of 1?You could explicitly set where you want to tick marks with :For example,( was used rather than Python's  function just in case  and  are floats instead of ints.) The  (or ) function will automatically set default  and  limits. If you wish to keep those limits, and just change the stepsize of the tick marks, then you could use  to discover what limits Matplotlib has already set. The default tick formatter should do a decent job rounding the tick values to a sensible number of significant digits. However, if you wish to have more control over the format, you can define your own formatter. For example,Here's a runnable example:Another approach is to set the axis locator:There are several different types of locator depending upon your needs.I like this solution (from the ):This solution give you explicit control of the tick spacing via the number given to , allows automatic limit determination, and is easy to read later.In case anyone is interested in a general one-liner, simply get the current ticks and use it to set the new ticks by sampling every other tick.This is a bit hacky, but by far the cleanest/easiest to understand example that I've found to do this. It's from an answer on SO here:Then you can loop over the labels setting them to visible or not depending on the density you want.edit: note that sometimes matplotlib sets labels == , so it might look like a label is not present, when in fact it is and just isn't displaying anything. To make sure you're looping through actual visible labels, you could try:This is an old topic, but I stumble over this every now and then and made this function. It's very convenient:One caveat of controlling the ticks like this is that one does no longer enjoy the interactive automagic updating of max scale after an added line. Then doand run the resadjust function again.I developed an inelegant solution. Consider that we have the X axis and also a list of labels for each point in X."},
{"body": "I want to create an empty list (or whatever is the best way) that can hold 10 elements.After that I want to assign values in that list, for example this is supposed to display 0 to 9:But when I run this code, it generates an error or in another case it just displays  (empty). Can someone explain why?You cannot assign to a list like . You need to use append. .(You could use the assignment notation if you were using a dictionary).Creating an empty list:range(x) creates a list from [0, 1, 2, ... x-1]Using a function to create a list:List comprehension (Using the squares because for range you don't need to do all this, you can just return  ):Try this instead:The above will create a list of size 10, where each position is initialized to . After that, you can add elements to it:Admittedly, that's not the Pythonic way to do things. Better do this:Or , use list comprehensions like this:You can  to the list, e.g.: . What you are currently trying to do is access an element () that does not exist.varunl's currently accepted answerWorks well for non-reference types like numbers. Unfortunately if you want to create a list-of-lists you will run into referencing errors. Example in Python 2.7.6:As you can see, each element is pointing to the same list object. To get around this, you can create a method that will initialize each position to a different object reference.There is likely a default, built-in python way of doing this (instead of writing a function), but I'm not sure what it is. Would be happy to be corrected! There are two \"quick\" methods:It appears that  is faster:But if you are ok with a range (e.g. ), then  might be fastest:All lists can hold as many elements as you like, subject only to the limit of available memory. The only \"size\" of a list that matters is the number of elements  in it. is not valid syntax; based on your description of what you're seeing, I assume you meant  and then . For that to run, you must have previously defined a global  to pass into the function.Calling  does not modify the list you pass in, as written. Your code says \" is a name for whatever thing was passed in to the function; ok, now the first thing we'll do is forget about that thing completely, and let  start referring instead to a newly created . Now we'll modify that \". This has no effect on the value you passed in.There is no reason to pass in a value here. (There is no real reason to create a function, either, but that's beside the point.) You want to \"create\" something, so that is the output of your function. No information is required to create the thing you describe, so don't pass any information in. To get information out,  it.That would give you something like:The next problem you will note is that your list will actually have only 9 elements, because the end point is skipped by the  function. (As side notes,  works just as well as , the semicolon is unnecessary,  is a poor name for the variable, and only one parameter is needed for  if you're starting from .) So then you end up withHowever, this is still missing the mark;  is not some magical keyword that's part of the language the way  and  are, but instead it's a function. And guess what that function returns? That's right - a list of those integers. So the entire function collapses to and now you see why we don't need to write a function ourselves at all;  is already the function we're looking for. Although, again, there is no need or reason to \"pre-size\" the list."},
{"body": "Is there a way to have a  in order to make the following code work? needs to be built ad-hoc, depending on  and  elements.I could use:but then I wouldn't be able to use:Yes like this:The parameter to the defaultdict constructor is the function which will be called for building new elements. So let's use a lambda !Since Python 2.7, there's an :Some bonus featuresFor more information see  and I find it slightly more elegant to use :Of course, this is the same as a lambda.Others have answered correctly your question of how to get the following to work:An alternative would be to use tuples for keys:The nice thing about this approach is that it is simple and can be easily expanded.  If you need a mapping three levels deep, just use a three item tuple for the key."},
{"body": "The below code will not join, when debugged the command does not store the whole path but just the last entry.When I test this it only stores the  part of the code.The latter strings shouldn't start with a slash. If they start with a slash, then they're considered an \"absolute path\" and everything before them is discarded.Quoting the :Note on Windows, the behaviour in relation to drive letters, which seems to have changed compared to earlier Python versions:The idea of  is to make your program cross-platform (linux/win32/etc).Even one slash ruins it.So it only makes sense when being used with some kind of reference point like\n or . can be used in conjunction with  to create an absolute rather than relative path.do not use forward slashes at the beginning of path components, except when refering to the root dir:see also: It's because your  begins with a  and thus is assumed to be relative to the root directory. Remove the leading .To help understand why this surprising behavior isn't  terrible, consider an application which accepts a config file name as an argument:If the application is executed with:The config file  will be used.But consider what happens if the application is called with:Then   use the config file at  (and not  or similar).It may not be great, but I believe this is the motivation for the absolute path behaviour.To make your function more portable, use it as such:orTry with  onlydo it like this, without too the extra slashesNote that a similar issue can bite you if you use  to include an extension that already includes a dot, which is what happens automatically when you use . In this example:Even though  might be  you end up with a folder named \"foobar\" rather than a file called \"foobar.jpg\". To prevent this you need to append the extension separately:"},
{"body": "I am new to Django and pretty new to Ajax. I am working on a project where I need to integrate the two. I believe that I understand the principles behind them both, but have not found a good explanation of the two together. Could someone give me a quick explanation of how the codebase must change with the two of them integrating together?For example, can I still use the  with Ajax, or do my responses have to change with the use of Ajax? If so, could you please provide an example of how the responses to the requests must change? If it makes any difference, the data I am returning is JSON. Even though this isn't entirely in the SO spirit, I love this question, because I had the same trouble when I started so I'll give you a quick guide. Obviously you don't understand the principles behind them (don't take it as an offense, but if you did you wouldn't be asking). Django is server-side. It means, say a client goes to url you have a function inside views that renders what he sees and returns a response in html. let's break it up into examples:views.pyindex.html:urls.pyThat's an example of the simplest of usages. Going to  means a request to the hello function, going to  will return the  and replace all the variables as asked (you probably know all this by now).Now let's talk about AJAX. AJAX calls are client-side code that does asynchronous requests. That sounds complicated, but it simply means it does a request for you in the background and then handles the response. So when you do an AJAX call for some url, you get the same data you would get as a user going to that place. For example, an ajax call to  will return the same thing it would as if you visited it. Only this time, you have it inside a js function and you can deal with it however you'd like. Let's look at a simple use case:The general process is this:Now what would happen here? You would get an alert with 'hello world' in it. What happens if you do an ajax call to home? Same thing, you'll get an alert stating .In other words - there's nothing new about AJAX calls. They are just a way for you to let the user get data and information without leaving the page, and it makes for a smooth and very neat design of your website. A few guidelines you should take note of:That's everything that comes to my head. It's a vast subject, but yeah, there's probably not enough examples out there. Just work your way there, slowly, you'll get it eventually.Further from yuvi's excellent answer, I would like to add a small specific example on how to deal with this within Django (beyond any js that will be used). The example uses  and assumes an Author model.Source: Simple and Nice. You don't have to change your views. Bjax handles all your links. Check this out:\nUsage:Finally, include this in the HEAD of your html:For more settings, checkout demo here: \nI have tried to use  in my project, but had ended up with the following error message:That is because the CreateView will return a  instead of returning a HttpResponse for you to send JSON result to the browser. So I have made some changes to the . If the request is an ajax request, it will not call the  method, just call the  directly."},
{"body": "When should I use a dictionary, list or set?Are there scenarios that are more suited for each data type?A  keeps order,  and  don't: when you care about order, therefore, you must use  (if your choice of containers is limited to these three, of course;-). associates with each key a value, while  and  just contain values: very different use cases, obviously. requires items to be hashable,  doesn't: if you have non-hashable items, therefore, you cannot use  and must instead use . forbids duplicates,  does not: also a crucial distinction.  (A \"multiset\", which maps duplicates into a different count for items present more than once, can be found in  -- you could build one as a , if for some weird reason you couldn't import , or, in pre-2.7 Python as a , using the items as keys and the associated value as the count).Checking for membership of a value in a  (or , for keys) is blazingly fast (taking about a constant, short time), while in a list it takes time proportional to the list's length in the average and worst cases.  So, if you have hashable items, don't care either way about order or duplicates, and want speedy membership checking,  is better than .When you want an unordered collection of unique elements, use a . (For example, when you want the set of all the words used in a document).When you want to collect an immutable ordered list of elements, use a . (For example, when you want a (name, phone_number) pair that you wish to use as an element in a set, you would need a tuple rather than a list since sets require elements be immutable).When you want to collect a mutable ordered list of elements, use a . (For example, when you want to append new phone numbers to a list: [number1, number2, ...]).When you want a mapping from keys to values, use a . (For example, when you want a telephone book which maps names to phone numbers: ). Note the keys in a dict are unordered. (If you iterate through a dict (telephone book), the keys (names) may show up in any order).Although this doesn't cover s, it is a good explanation of s and s:"},
{"body": "I am using the datetime Python module.  I am looking to calculate the date 6 months from the current date. Could someone give me a little help doing this?The reason I want to generate a date 6 months from the current date is to produce a Review Date.  If the user enters data into the system it will have a review date of 6 months from the date they entered the data.  I found this solution to be good.  (This uses the )The advantage of this approach is that it takes care of issues with 28, 30, 31 days etc. This becomes very useful in handling business rules and scenarios (say invoice generation etc.)Well, that depends what you mean by 6 months from the current date.What do you mean by '6 months'. Is 2009-02-13 + 6 months == 2009-08-13 or is it 2009-02-13 + 6*30 days?More info about This solution works correctly for December, which most of the answers on this page do not.\nYou need to first shift the months from base 1 (ie Jan = 1) to base 0 (ie Jan = 0) before using modulus ( % ) or integer division ( // ), otherwise November (11) plus 1 month gives you 12, which when finding the remainder ( 12 % 12 ) gives 0.(And dont suggest \"(month % 12) + 1\" or Oct + 1 = december!)However ... This doesnt account for problem like Jan 31 + one month. So we go back to the OP - what do you mean by adding a month? One soln is to backtrack until you get to a valid day, given that most people would presume the last day of jan, plus one month, equals the last day of Feb.\nThis will work on negative numbers of months too.\nProof: has implementation of such functionality. But be aware, that this will be , as others pointed already.There's no direct way to do it with Python's datetime.Check out the relativedelta type at . It allows you to specify a  time delta in months.I know this was for 6 months, however the answer shows in google for \"adding months in python\" if you are adding one month:this would count the days in the current month and add them to the current date, using 365/12 would ad 1/12 of a year can causes issues for short / long months if your iterating over the date.Just use the  method to extract the months, add your months and build a new dateobject. If there is a already existing method for this I do not know it.The API is a bit clumsy, but works as an example. Will also obviously not work on corner-cases like 2008-01-31 + 1 month. :)So, here is an example of the  which I found useful for iterating through the past year, skipping a month each time to the present date:As with the other answers, you have to figure out what you actually mean by \"6 months from now.\"  If you mean \"today's day of the month in the month six years in the future\" then this would do:For beginning of month to month calculation:The QDate class of PyQt4 has an addmonths function.I have a better way to solve the 'February 31st' problem:I think that it also works with negative numbers (to subtract months), but I haven't tested this very much.Modified the AddMonths() for use in Zope and handling invalid day numbers:How about this? Not using another library () or ?\nbuilding on 's answer I did this and I believe it works:I tried using , but because it is counting the days,  or  does not always translate to 6 months, but rather 182 days. e.g.I believe that we usually assume that 6 month's from a certain day will land on the same day of the month but 6 months later (i.e.  --> , Not )I hope you find this helpful.I solved this problem like this:Modified Johannes Wei's answer in the case 1new_month = 121.  This works perfectly for me.  The months could be positive or negative.Yet another solution - hope someone will like it:This solution doesn't work for days 29,30,31 for all cases, so more robust solution is needed (which is not so nice anymore :) ):From , see . Code example follows. More details: , and apparent .The above code generates the following from a MacOSX machine:given that your datetime variable is called date:Use the python datetime module to add a timedelta of six months to datetime.today() .You will of course have to solve the issue raised by Johannes Wei\u00df-- what  you mean by 6 months?This is what I came up with. It moves the correct number of months and years but ignores days (which was what I needed in my situation).I use this function to change year and month but keep day:You should write:I think it would be safer to do something like this instead of manually adding days:my modification to tony diep's answer, possibly marginally more elegant:adds months according to a business needs interpretationRework of an earlier answer by user417751. Maybe not so pythonic way, but it takes care of different month lengths and leap years. In this case 31 January 2012 + 1 month = 29 February 2012. In this function, n can be positive or negative.Here's a example which allows the user to decide how to return a date where the day is greater than the number of days in the month."},
{"body": "What's your preferred way of getting current system status (current CPU, RAM, free disk space, etc.) in Python? Bonus points for *nix and Windows platforms.There seems to be a few possible ways of extracting that from my search:It's not that those methods are bad but is there already a well-supported, multi-platform way of doing the same thing? will give you some system information (CPU / Memory usage) on a variety of platforms:Use the .  For me on Ubuntu, pip installed 0.4.3.  You can check your version of psutil by doing in Python.To get some memory and CPU stats:I also like to do:which gives the current memory use of your Python script.There are some more in-depth examples on the  and .For Ubuntu 16 and 14, installing from pip gave me version 4.3.0, which doesn't have the phymem_usage() method. To get 0.5.0, , then do Here's something I put together a while ago, it's windows only but may help you get part of what you need done.Derived from:\n\"for sys available mem\"\n\"individual process information and python script examples\"\nNOTE: the WMI interface/process is also available for performing similar tasks\n        I'm not using it here because the current method covers my needs, but if someday it's needed to extend or improve this, then may want to investigate the WMI tools a vailable.WMI for python:The code:\"... current system status (current CPU, RAM, free disk space, etc.)\"  And \"*nix and Windows platforms\" can be a difficult combination to achieve.The operating systems are fundamentally different in the way they manage these resources.  Indeed, they differ in core concepts like defining what counts as system and what counts as application time.\"Free disk space\"?  What counts as \"disk space?\"  All partitions of all devices?  What about foreign partitions in a multi-boot environment?I don't think there's a clear enough consensus between Windows and *nix that makes this possible.  Indeed, there may not even be any consensus between the various operating systems called Windows.  Is there a single Windows API that works for both XP and Vista?Below codes, without external libraries worked for me. I tested at Python 2.7.9You can use psutil or psmem with subprocess\nexample code Reference  One-liner for the RAM usage with only stdlib dependency:I don't believe that there is a well-supported multi-platform library available. Remember that Python itself is written in C so any library is simply going to make a smart decision about which OS-specific code snippet to run, as you suggested above. "},
{"body": "Is there a way of reading one single character from the user input? For instance, they press one key at the terminal and it is returned (sort of like ). I know there's a function in Windows for it, but I'd like something that is cross-platform.Here's a link to a site that says how you can read a single character in Windows, Linux and OSX: will basically read 1 byte from STDIN.If you must use the method which does not wait for the  you can use this code as suggested in previous answer:( )The ActiveState  quoted verbatim in two answers is over-engineered. It can be boiled down to this:Also worth trying is the  library, which is in part based on the ActiveState recipe mentioned in other answers.Installation:Usage:Tested on Windows and Linux with Python 2.7.On Windows, only keys which map to letters or ASCII control codes are supported (, , , , +). On GNU/Linux (depending on exact terminal, perhaps?) you also get , , , , ,  and  keys... but then, there's issues separating these special keys from an .Caveat: Like with most (all?) answers in here, signal keys like +, + and + are caught and returned (as ,  and  respectively); your program can be come difficult to abort.An alternative method:From .I think it gets extremely clunky at this point, and debugging on the different platforms is a big mess.You'd be better off using something like pyglet, pygame, cocos2d - if you are doing something more elaborate than this and will need visuals, OR  if you are going to work with the terminal.Curses is standard: This code, based off , will correctly raise KeyboardInterrupt and EOFError if + or + are pressed.Should work on Windows and Linux. An OS X version is available from the original source.The (currently) top-ranked answer (with the ActiveState code) is overly complicated. I don't see a reason to use classes when a mere function should suffice. Below are two implementations that accomplish the same thing but with more readable code. I missed one advantage of the ActiveState code. If you plan to read characters multiple times, that code avoids the (negligible) cost of repeating the Windows import and the ImportError exception handling on Unix-like systems. While you probably should be more concerned about code readability than that negligible optimization, here is an alternative (it is similar to Louis's answer, but getChar() is self-contained) that functions the same as the ActiveState code and is more readable:This might be a use case for a context manager.  Leaving aside allowances for Windows OS, here's my suggestion:The answers  were informative, however I also wanted a way to get key presses asynchronously and fire off key presses in separate events, all in a thread-safe, cross-platform way. PyGame was also too bloated for me. So I made the following (in Python 2.7 but I suspect it's easily portable), which I figured I'd share here in case it was useful for anyone else. I stored this in a file named keyPress.py.The idea is that you can either simply call , which will read a key from the keyboard, then return it.If you want something more than that, I made a  object. You can create one via something like .Then there are three things you can do: takes in any function that takes in one parameter. Then every time a key is pressed, this function will be called with that key's string as it's input. These are ran in a separate thread, so you can block all you want in them and it won't mess up the functionality of the KeyCapturer nor delay the other events. returns a key in the same blocking way as before. It is now needed here because the keys are being captured via the  object now, so  would conflict with that behavior and both of them would miss some keys since only one key can be captured at a time. Also, say the user presses 'a', then 'b', you call , the the user presses 'c'. That  call  will immediately return 'a', then if you call it again it will return 'b', then 'c'. If you call it again it will block until another key is pressed. This ensures that you don't miss any keys, in a blocking way if desired. So in this way it's a little different than  from beforeIf you want the behavior of  back,  is like , except that it only returns keys pressed  the call to . So in the above example,  would block until the user presses 'c', and then if you call it again it will block until another key is pressed. is a little different. It's designed for something that does a lot of processing, then occasionally comes back and checks which keys were pressed. Thus  returns a list of all the keys pressed since the last call to , in order from oldest key pressed to most recent key pressed. It also doesn't block, meaning that if no keys have been pressed since the last call to , an empty   will be returned.To actually start capturing keys, you need to call  with your  object made above.  is non-blocking, and simply starts one thread that just records the key presses, and another thread to process those key presses. There are two threads to ensure that the thread that records key presses doesn't miss any keys.If you want to stop capturing keys, you can call  and it will stop capturing keys. However, since capturing a key is a blocking operation, the thread capturing keys might capture one more key after calling .To prevent this, you can pass in an optional parameter(s) into  of a function that just does something like checks if a key equals 'c' and then exits. It's important that this function does very little before, for example, a sleep here will cause us to miss keys.However, if  is called in this function, key captures will be stopped immediately, without trying to capture any more, and that all  calls will be returned immediately, with None if no keys have been pressed yet.Also, since  and  store all the previous keys pressed (until you retrieve them), you can call  and  to forget the keys previously pressed.Note that ,  and events are independent, so if a key is pressed:\n1. One call to  that is waiting, with lossy on, will return\n   that key. The other waiting calls (if any) will continue waiting.\n2. That key will be stored in the queue of get keys, so that  with lossy off will return the oldest key pressed not returned by  yet.\n3. All events will be fired with that key as their input\n4. That key will be stored in the list of  keys, where that lis twill be returned and set to empty list on the next call to If all this is too much, here is an example use case:It is working well for me from the simple test I made, but I will happily take others feedback as well if there is something I missed.I posted this  as well.This is NON-BLOCKING, reads a key and and stores it in keypress.key. in your programmTry this with pygame:Try using this: \nIt's non-blocking (that means that you can have a while loop and detect a key press without stopping it) and cross-platform.An example to use this:Or you could use the . But this would block the while loopThe build-in raw_input should help. The  package in python can be used to enter \"raw\" mode for character input from the terminal with just a few statements.  Curses' main use is to take over the screen for output, which may not be what you want. This code snippet uses  statements instead, which are usable, but you must be aware of how curses changes line endings attached to output. My solution for python3, not depending on any pip packages."},
{"body": "What would be the most elegant and efficient way of finding/returning the first list item that matches a certain criterion?For example, if I have a list of objects and I would like to get the first object of those with attribute . I could of course use list comprehension, but that would incur O(n) and if n is large, it's wasteful. I could also use a loop with  once the criterion was met, but I thought there could be a more pythonic/elegant solution.If you don't have any other indexes or sorted information for your objects, then you will have to iterate until such an object is found:This is however faster than a complete list comprehension. Compare these two:The first one needs 5.75ms, the second one 58.3\u00b5s (100 times faster because the loop 100 times shorter).it'll return the object if found else it'll return \"not found\""},
{"body": "Does anyone know how the built in dictionary type for python is implemented?  My understanding is that it is some sort of hash table, but I haven't been able to find any sort of definitive answer.Here is everything about Python dicts that I was able to put together (probably more than anyone would like to know; but the answer is comprehensive). NOTE: I did the research on Python Dict implementation in response to my own  about how multiple entries in a dict can have same hash values. I posted a slightly edited version of the response here because all the research is very relevant for this question as well.Python Dictionaries use  () , a.k.a  should, as noted in Wikipedia, not be confused with its opposite ! (which we see in the accepted answer).Open addressing means that the dict uses array slots, and when an object's primary position is taken in the dict, the object's spot is sought at a different index in the same array, using a \"perturbation\" scheme, where the object's hash value plays part.It is a hash table.  You can read about it some in the .  Otherwise, the code is well-written and should be easy to understand.For a long time, it worked like this. Python would preallocate 8 empty rows and use the hash to determine where to stick the key-value pair. For example, if the hash for the key ended in 001, it would stick it in the 1 index (like the example below.) Each row takes up 24 bytes on a 64 bit architecture, 12 on a 32 bit. (Note that the column headers are just labels - they don't actually exist in memory.)If the hash ended the same as a preexisting key's hash, this is a collision, and then it would stick the key-value pair in a different location.After 5 key-values are stored, when adding another key-value pair, the probability of hash collisions is too large, so the dictionary is doubled in size. In a 64 bit process, before the resize, we have 72 bytes empty, and after, we are wasting 240 bytes due to the 10 empty rows.This takes a lot of space, but the lookup time is fairly constant. The key comparison algorithm is to compute the hash, go to the expected location, compare the key's id - if they're the same object, they're equal. If not then compare the hash values, if they are  the same, they're not equal. Else, then we finally compare keys for equality, and if they are equal, return the value. The final comparison for equality can be quite slow, but the earlier checks usually shortcut the final comparison, making the lookups very quick.(Collisions slow things down, and an attacker could theoretically use hash collisions to perform a denial of service attack, so we randomized the hash function such that it computes a different hash for each new Python process.)The wasted space described above has led us to modify the implementation of dictionaries, with an exciting new (if unofficial) feature that dictionaries are now ordered (by insertion).We start, instead, by preallocating an array for the index of the insertion.Since our first key-value pair goes in the second slot, we index like this:And our table just gets populated by insertion order:So when we do a lookup for a key, we use the hash to check the position we expect (in this case, we go straight to index 1 of the array), then go to that index in the hash-table (e.g. index 0), check that the keys are equal (using the same algorithm described earlier), and if so, return the value.We retain constant lookup time, with minor speed losses in some cases and gains in others, with the upside that we save quite a lot of space over the pre-existing implementation. The only space wasted are the null bytes in the index array.Raymond Hettinger introduced this to  in December of 2012. It finally got into CPython in . Ordering by insertion is still considered an implementation detail to allow other implementations of Python a chance to catch up.Another optimization to save space is an implementation that shares keys. Thus, instead of having redundant dictionaries that take up all of that space, we have dictionaries that reuse the shared keys and keys' hashes. You can think of it like this:For a 64 bit machine, this could save up to 16 bytes per key per extra dictionary.These shared-key dicts are intended to be used for custom objects' . To get this behavior, I believe you need to finish populating your  before you instantiate your next object (). This means you should assign all your attributes in the  or , else you might not get your space savings.However, if you know all of your attributes at the time your  is executed, you could also provide  for your object, and guarantee that  is not created at all (if not available in parents), or even allow  but guarantee that your foreseen attributes are stored in slots anyways. For more on , ."},
{"body": "I'm launching a subprocess with the following command:However, when I try to kill using:or The command keeps running in the background, so I was wondering how can I actually terminate the process. Note that when I run the command with:It does terminate successfully when issuing the .Use a  so as to enable sending a signal to all the process in the groups. For that, you should attach a  to the parent process of the spawned/child processes, which is a shell in your case. This will make it the group leader of the processes. So now, when a signal is sent to the process group leader, it's transmitted to all of the child processes of this group.Here's the code: ends up killing the shell process and  is still running.I found a convenient fix this by:This will cause cmd to inherit the shell process, instead of having the shell launch a child process, which does not get killed.   will be the id of your cmd process then. should work.I don't know what effect this will have on your pipe though.If you can use , then this works perfectly:I could do it using it killed the  and the program that i gave the command for.(On Windows)As Sai said, the shell is the child, so signals are intercepted by it -- best way I've found is to use shell=False and use shlex to split the command line:Then p.kill() and p.terminate() should work how you expect.When  the shell is the child process, and the commands are its children. So any  or  will kill the shell but not its child processes, and I don't remember a good way to do it.\n The best way I can think of is to use , otherwise when you kill the parent shell process, it will leave a defunct shell process."},
{"body": "I want to run a Python script from another Python script. I want to pass variables like I would using the command line.For example, I would run my first script that would iterate through a list of values (0,1,2,3) and pass those to the 2nd script  then  , etc.I found SO  which is a similar question but ars's answer calls a function, where as I want to run the whole script not just a function, and balpha's answer calls the script but with no args. I altered this to something like the below as a test: But it is not accepting variables properly.  When I print out the  in script2.py it is the original command call to first script \"['C:\\script1.py'].I don't really want to change the original script (i.e. script2.py in my example) since I don't own it.I figure there must be a way to do this, I am just confused how you do it.  Try using : is different because it is designed to run a sequence of Python statements in the  execution context. That's why  didn't change for you.This is inherently the wrong thing to do. If you are running a Python script from another Python script, you should communicate through Python instead of through the OS:In an ideal world, you will be able to call a function inside  directly:If necessary, you can hack . There's a neat way of doing this using a context manager to ensure that you don't make any permanent changes.I think this is preferable to passing all your data to the OS and back; that's just silly.Ideally, the Python script you want to run will be set up with code like this near the end:In other words,  the module is called from the command line, it parses the command line options and then calls another function, , to do the actual work. (The actual arguments will vary, and the parsing may be more involved.)If you want to call such a script from another Python script, however, you can simply  it and call  directly, rather than going through the operating system. will work, but it is the roundabout (read \"slow\") way to do it, as you are starting a whole new Python interpreter process each time for no raisin.SubProcess module:\nWith this, you can also redirect stdin, stdout, and stderr.I think the good practice may be something like this; according to documentation \nThe subprocess module allows you to spawn new processes, connect to their input/output/error pipes, and obtain their return codes. This module intends to replace several older modules and functions:Use communicate() rather than .stdin.write, .stdout.read or .stderr.read to avoid deadlocks due to any of the other OS pipe buffers filling up and blocking the child process.\nIf os.system isn't powerful enough for you, there's ."},
{"body": "Many Python programmers are probably unaware that the syntax of  loops and  loops includes an optional  clause:The body of the  clause is a good place for certain kinds of clean-up actions, and is executed on normal termination of the loop: I.e., exiting the loop with  or  skips the  clause; exiting after a  executes it. I know this only because I just  (yet again), because  I can never remember  the  clause is executed.Always? On \"failure\" of the loop, as the name suggests? On regular termination? Even if the loop is exited with ? I can never be entirely sure without looking it up.I blame my persisting uncertainty on the choice of keyword: I find  incredibly unmnemonic for this semantics. My question is not \"why is this keyword used for this purpose\" (which I would probably vote to close, though only after reading the answers and comments), but I'm sure there was a fair amount of discussion about this, and I can imagine that the choice was made for consistency with the  statement's  clause (which I also have to look up), and with the goal of not adding to the list of Python's reserved words. Perhaps the reasons for choosing  will clarify its function and make it more memorable, but I'm after connecting name to function, not after historical explanation per se.The answers to , which my question was briefly closed as a duplicate of, contain a lot of interesting back story. My question has a different focus (how to connect the specific semantics of  with the keyword choice), but I feel there should be a link to this question somewhere.(This is inspired by @Mark Tolonen's answer.)An  statement runs its  clause if its condition evaluates to false.\nIdentically, a  loop runs the else clause if its condition evaluates to false.This rule matches the behavior you described: loops behave the same way. Just consider the condition as true if the iterator has more elements, or false otherwise.Better to think of it this way: The  block will  be executed if everything goes  in the preceding  block such that it reaches exhaustion.  in this context will mean no , no , no . Any statement that hijacks control from  will cause the  block to be bypassed.A common use case is found when searching for an item in an , for which the search is either called off when the item is found or a  flag is raised/printed via the following  block:A  does not hijack control from , so control will proceed to the  after the  is exhausted.When does an  execute an ? When its condition is false. It is exactly the same for the /.  So you can think of / as just an  that keeps running its true condition until it evaluates false.  A  doesn't change that.  It just jumps of of the containing loop with no evaluation.  The  is only executed if  the / condition is false.The  is similar, except its false condition is exhausting its iterator.  and  don't execute .  That isn't their function.  The  exits the containing loop.  The  goes back to the top of the containing loop, where the loop condition is evaluated.  It is the act of evaluating / to false (or  has no more items) that executes  and no other way.This is what it essentially means:It's a nicer way of writing of this common pattern:The  clause will not be executed if there is a  because  leaves the function, as it is meant to. The only exception to that which you may be thinking of is , whose purpose is to be sure that it is always executed. has nothing special to do with this matter. It causes the current iteration of the loop to end which may happen to end the entire loop, and clearly in that case the loop wasn't ended by a . is similar:If you think of your loops as a structure similar to this (somewhat pseudo-code):it might make a little bit more sense. A loop is essentially just an  statement that is repeated until the condition is . And this is the important point. The loop checks its condition and sees that it's , thus executes the  (just like a normal ) and then the loop is done.So notice that the  . That means that if you exit the body of the loop in the middle of execution with for example a  or a , since the condition is not checked again, the  case won't be executed.A  on the other hand stops the current execution and then jumps back to check the condition of the loop again, which is why the  can be reached in this scenario.My gotcha moment with the loop's  clause was when I was watching a talk by , who told a story about how he thought it should have been called . Take a look at the following code, what do you think it would do?What would you guess it does? Well, the part that says  would only be executed if a  statement wasn't hit in the loop.Usually I tend to think of a loop structure like this:To be a lot like a variable number of  statements:In this case the  statement on the for loop works exactly like the  statement on the chain of s, it only executes if none of the conditions before it evaluate to True. (or break execution with  or an exception)  If my loop does not fit this specification usually I choose to opt out of using  for the exact reason you posted this question: it is non-intuitive.In  (TDD), when using the  paradigm, you treat loops as a generalization of conditional statements.This approach combines well with this syntax, if you consider only simple  (no ) statements:generalizes to:nicely.In other languages, TDD steps from a single case to cases with collections require more refactoring.Here is an example from :In the linked article at 8thlight blog, the Word Wrap kata is considered: adding line breaks to strings (the  variable in the snippets below) to make them fit a given width (the  variable in the snippets below). At one point the implementation looks as follows (Java):and the next test, that currently fails is:So we have code that works conditionally: when a particular condition is met, a line break is added. We want to improve the code to handle multiple line breaks. The solution presented in the article proposes to apply the  transformation, however the author makes a comment that:which forces to do more changes to the code in the context of one failing test:In TDD we want to write as less code as possible to make tests pass. Thanks to Python's syntax the following transformation is possible:from:to:Others have already explained the mechanics of , and the  has the authoritative definition (see  and ), but here is my personal mnemonic, FWIW. I guess the key for me has been to break this down into two parts: one for understanding the meaning of the  in relation to the loop conditional, and one for understanding loop control.I find it's easiest to start by understanding :The  mnemonic is basically the same:In both cases, the  part is only reached once there are no more items to process, and the last item has been processed in a regular manner (i.e. no  or ). A  just goes back and sees if there are any more items. My mnemonic for these rules applies to both  and :\u2013 with \"loop back to start\" meaning, obviously, the start of the loop where we check whether there are any more items in the iterable, so as far as the  is concerned,  really plays no role at all.The way I see it,  fires when you iterate past the end of the loop.If you  or  or  you don't iterate past the end of loop, you stop immeadiately, and thus the  block won't run. If you  you still iterate past the end of loop, since continue just skips to the next iteration. It doesn't stop the loop. Think of the  clause as being part of the loop construct;  breaks out of the loop construct entirely, and thus skips the  clause.But really, my mental mapping is simply that it's the 'structured' version of the pattern C/C++ pattern:So when I encounter  or write it myself, rather than understand it , I mentally translate it into the above understanding of the pattern and then work out which parts of the python syntax map to which parts of the pattern.(I put 'structured' in scare quotes because the difference is not whether the code is structured or unstructured, but merely whether there are keywords and grammar dedicated to the particular structure)The way I think about it, the key is to consider the meaning of  rather than .The other keywords you mention break out of the loop (exit abnormally) whilst  does not, it just skips the remainder of the code block inside the loop. The fact that it can precede loop termination is incidental: the termination is actually done in the normal way by evaluation of the loop conditional expression.Then you just need to remember that the  clause is executed after normal loop termination.Decision structures evaluate multiple expressions which produce TRUE or FALSE as outcome. You need to determine which action to take and which statements to execute if outcome is TRUE or FALSE otherwise.In the above code, it get breaks when i equal to 6..otherwise it print welcome for 5 times.!Thanks,\n"},
{"body": "Which tar do I need to download off this site?I've tried the fortrans, but I keep getting this error (after setting the environment variable obviously).The  used to provide build and installation instructions, but the instructions there now rely on OS binary distributions. To build SciPy (and NumPy) on operating systems without precompiled packages of the required libraries, you must build and then statically link to the Fortran libraries  and :Execute only one of the five g77/gfortran/ifort commands. I have commented out all, but the gfortran which I use. The subsequent LAPACK installation requires a  compiler, and since both installs should use the same Fortran compiler, g77 should not be used for BLAS.Next, you'll need to install the LAPACK stuff. The SciPy webpage's instructions helped me here as well, but I had to modify them to suit my environment:Update on 3-Sep-2015:\nVerified some comments today (thanks to all): Before running  edit the  file and add  option to  and  settings. If you are on a 64bit architecture or want to compile for one, also add . It is important that BLAS and LAPACK are compiled with these options set to the same values. If you forget the  SciPy will actually give you an error about missing symbols and will recommend this switch. The specific section of  looks like this in my setup:On old machines (e.g. RedHat 5), gfortran might be installed in an older version (e.g. 4.1.2) and does not understand option . Simply remove it from the  file in such cases.The lapack test target of the Makefile fails in my setup because it cannot find the blas libraries. If you are thorough you can temporarily move the blas library to the specified location to test the lapack. I'm a lazy person, so I trust the devs to have it working and verify only in SciPy.If you need to use the latest versions of SciPy rather than the packaged version, without going through the hassle of building BLAS and LAPACK, you can follow the below procedure.Install linear algebra libraries from repository (for Ubuntu),Then install SciPy, (after downloading the SciPy source):  orAs the case may be.On Fedora, this works: Remember to install '' and '' in addition to 'blas' and 'lapack' otherwise you'll get the error you mentioned or the \"numpy.distutils.system_info.\" error. I guess you are talking about installation in Ubuntu. Just use: That should take care of the BLAS libraries compiling as well. Else, compiling the BLAS libraries is very difficult.For Windows users there is a nice binary package by Chris (warning: it's a pretty large download, 191\u00a0MB):Following the instructions given by 'cfi' works for me, although there are a few pieces he left out that you might need:   1) Your lapack directory, after unzipping, may be called lapack-X-Y (some version number), so you can just rename that to LAPACK.  2) In that directory, you may need to do:Try using "},
{"body": "I got a lot of errors with the message :after changed from python-psycopg to python-psycopg2 as Django project's database engine.The code remains the same, just dont know where those errors are from.This is what postgres does when a query produces an error and you try to run another query without first rolling back the transaction.  To fix it, you'll want to figure out where in the code that bad query is being executed.  It might be helpful to use the  and  options in your postgresql server.To get rid of the error,  after you've fixed your code:You can use try-except to prevent the error from occurring:Refer : So, I ran into this same issue. The problem I was having here was that my database wasn't properly synced. Simple problems always seem to cause the most angst...To sync your django db, from within your app directory, within terminal, type:Edit: Note that if you are using django-south, running the '$ python manage.py migrate' command may also resolve this issue.Happy coding!In my experience, these errors happen this way:There nothing wrong with the second query, but since the real error was caught, the second query is the one that raises the (much less informative) error.edit: this only happens if the  clause catches  (or any other low level database exception), If you catch something like  this error will not come up, because  does not corrupt the transaction.The lesson here is don't do try/except/pass.I think the pattern priestc mentions is more likely to be the usual cause of this issue when using PostgreSQL.However I feel there are valid uses for the pattern and I don't think this issue should be a reason to always avoid it. For example:If you do feel OK with this pattern, but want to avoid explicit transaction handling code all over the place then you might want to look into turning on autocommit mode (PostgreSQL 8.2+): I am unsure if there are important performance considerations (or of any other type).If you get this while in interactive shell and need a quick fix, do this:originally seen in I've got the silimar problem. The solution was to migrate db ( or  if you use south).I encountered a similar behavior while running a malfunctioned transaction on the  terminal. Nothing went through after this, as the  is in a state of . However, just as a quick fix, if you can afford to avoid . Following did the trick for me:I just had this error too but it was masking another more relevant error message where the code was trying to store a 125 characters string in a 100 characters column:I had to debug through the code for the above message to show up, otherwise it displaysIn response to @priestc and @Sebastian, what if you do something like this?I just tried this code and it seems to work, failing silently without having to care about any possible errors, and working when the query is good.I believe @AnujGupta's answer is correct. However the rollback can itself raise an exception which you should catch and handle:If you find you're rewriting this code in various  locations, you can extract-method:Finally, you can prettify it using a decorator that protects methods which use :Even if you implement the decorator above, it's still convenient to keep  as an extracted method in case you need to use it manually for cases where specific handling is required, and the generic decorator handling isn't enough.This is very strange behavior for me. I'm surprised that no one thought of savepoints. In my code failing query was expected behavior:I have changed code this way to use savepoints:i got the same error. first check your models. actually it occurs due to some field or length mismatch in your models. for finding exact error use \"try except\" to find the exact error and its location. in \"except:\" use print repr(format_exc())\neg:\ntry:\n # your code\nexcept:\n print repr(format_exc())imp: please import-  from traceback import format_exc\nit helps to get your exact error.you could disable transaction via \"set_isolation_level(0)\""},
{"body": "I have a list with 15 numbers in, and I need to write some code that produces all 32,768 combinations of those numbers. I've found  (by Googling) that apparently does what I'm looking for, but I found the code fairly opaque and am wary of using it. Plus I have a feeling there must be a more elegant solution.The only thing that occurs to me would be to just loop through the decimal integers 1\u201332768 and convert those to binary, and use the binary representation as a filter to pick out the appropriate numbers. Does anyone know of a better way? Using , maybe?Have a look at :Since 2.6, batteries are included! missed one aspect: the OP asked for ALL combinations... not just combinations of length \"r\".So you'd either have to loop through all lengths \"L\":Or -- if you want to get snazzy (or bend the brain of whoever reads your code after you) -- you can generate the chain of \"combinations()\" generators, and iterate through that:Here's a lazy one-liner, also using itertools:Main idea behind this answer: there are 2^N combinations -- same as the number of binary strings of length N. For each binary string, you pick all elements corresponding to a \"1\".Things to consider:I agree with Dan H that Ben indeed asked for  combinations.  does not give all combinations.Another issue is, if the input iterable is big, it is perhaps better to return a generator instead of everything in a list:Here is one using recursion:This one-liner gives you all the combinations (between  and  items if the original list/set contains  distinct elements) and uses the native method :The output will be:Try it online:In comments under the highly upvoted  by @Dan H, mention is made of the  recipe in the \u2014including one by . , so far no one has posted it as an answer. Since it's probably one of the better if not the best approach to the problem\u2014and given a  from another commenter, it's shown below. The function produces  unique combinations of the list elements of  length possible.: If the, subtly different, goal is to obtain only combinations of unique elements, change the line  to  to eliminate any duplicate elements. Regardless, the fact that the  is ultimately turned into a  means it will work with generators (unlike several of the other answers).Output:Here is yet another solution (one-liner), involving using the  function, but here we use a double list comprehension (as opposed to a for loop or sum):Demo:Below is a \"standard recursive answer\", similar to the other similar answer  . (We don't realistically have to worry about running out of stack space since there's no way we could process all N! permutations.)It visits every element in turn, and either takes it or leaves it (we can directly see the 2^N cardinality from this algorithm).Demo:Using list comprehension:Output would be:I thought I would add this function for those seeking an answer without importing itertools or any other extra libraries.Simple Yield Generator Usage:Output from Usage example above:This code employs a simple algorithm with nested lists...This is my implementation"},
{"body": "Consider three functions:They all appear to return None. Are there any differences between how the returned value of these functions behave?  Are there any reasons to prefer one versus the other?On the actual behavior, there is no difference. They all return  and that's it. However, there is a time and  place for all of these.\nThe following instructions are basically how the different methods should be used (or atleast how I was taught they should be used), but they are not absolute rules so you can mix them up if you feel necessary to.Using .This tells that the function is indeed meant to return a value for later use, and in this case it returns . This value  can then be used elsewhere.  is never used if there are no other possible return values from the function.In the following example, we return person's mother if the person given is a human. If it's not a human, we return  since the \"person\" doesn't have a mother (let's suppose it's not an animal or so).Using .This is used for the same reason as  in loops. The return value doesn't matter and you only want to exit the whole function. It's extremely useful in some places, even tho you don't need it that often.We got 15 prisoners and we know one of them has a knife. We loop through each prisoner one by one to check if they have a knife. If we hit the person with a knife, we can just exit the function cause we know there's only one knife and no reason the check rest of the prisoners. If we don't find the prisoner with a knife, we raise an the alert. This could be done in many different ways and using  is probably not even the best way, but it's just an example to show how to use  for exiting a function.Note: You should never do , since the return value is not meant to be caught.Using no return at all.This will also return , but that value is not meant to be used or caught. It simply means that the function ended successfully. It's basically the same as  in  functions in languages such as C++ or Java.In the following example, we set person's mother's name, and then the function exits after completing successfully.Note: You should never do , since the return value is not meant to be caught.Yes, they are all the same. We can review the interpreted machine code to confirm that that they're all doing the exact same thing.They each return the same singleton  -- There is no functional difference. I think that it is reasonably idiomatic to leave off the  statement unless you need it to break out of the function early (in which case a bare  is more common), or return something other than .  It also makes sense and seems to be idiomatic to write  when it is in a function that has another path that returns something other than .  Writing  out explicitly is a visual cue to the reader that there's another branch which returns something more interesting (and that calling code will probably need to handle both types of return values).Often in Python, functions which return  are used like  functions in C -- Their purpose is generally to operate on the input arguments  (unless you're using global data ()).  Returning  usually makes it more explicit that the arguments were mutated.  This makes it a little more clear why it makes sense to leave off the  statement from a \"language conventions\" standpoint.That said, if you're working in a code base that already has pre-set conventions around these things, I'd definitely follow suit to help the code base stay uniform..."},
{"body": "I think I understand , but every time I look for examples for what is weak typing I end up finding examples of programming languages that simply coerce/convert types automatically.For instance, in this article named  says that Python is strongly typed because you get an exception if you try to:However, such thing is possible in Java and in C#, and we do not consider them weakly typed just for that.In this another article named  the author says that Perl is weakly typed simply because I can concatenate a string to a number and viceversa without any explicit conversion.So the same example makes Perl weakly typed, but not Java and C#?.Gee, this is confusing The authors seem to imply that a language that prevents the application of certain operations on values of different types is strongly typed and the contrary means weakly typed.Therefore, at some point I have felt prompted to believe that if a language provides a lot of automatic conversions or coercion between types (as perl) may end up being considered weakly typed, whereas other languages that provide only a few conversions may end up being considered strongly typed. I am inclined to believe, though, that I must be wrong in this interepretation, I just do not know why or how to explain it.So, my questions are: UPDATE:  Thanks for the great question!It means \"this language uses a type system that I find distasteful\". A \"strongly typed\" language by contrast is a language with a type system that I find pleasant.The terms are essentially meaningless and you should avoid them.  lists  for \"strongly typed\", several of which are contradictory. This indicates that the odds of confusion being created are high in any conversation involving the term \"strongly typed\" or \"weakly typed\". All that you can really say with any certainty is that a \"strongly typed\" language under discussion has some additional restriction in the type system, either at runtime or compile time, that a \"weakly typed\" language under discussion lacks. What that restriction might be cannot be determined without further context.Instead of using \"strongly typed\" and \"weakly typed\", you should describe in detail what kind of type safety you mean. For example, C# is a  language and a  language and a  language, . C# allows all three of those forms of \"strong\" typing to be violated.  The cast operator violates static typing; it says to the compiler \"I know more about the runtime type of this expression than you do\". If the developer is wrong, then the runtime will throw an exception in order to protect type safety. If the developer wishes to break type safety or memory safety, they can do so by turning off the type safety system by making an \"unsafe\" block. In an unsafe block you can use pointer magic to treat an int as a float (violating type safety) or to write to memory you do not own. (Violating memory safety.)C# imposes type restrictions that are checked at both compile-time and at runtime, thereby making it a \"strongly typed\" language compared to languages that do less compile-time checking or less runtime checking. C# also allows you to in special circumstances do an end-run around those restrictions, making it a \"weakly typed\" language compared with languages which do not allow you to do such an end-run. Which is it really?  It is impossible to say; it depends on the point of view of the speaker and their attitude towards the various language features. As others have noted, the terms \"strongly typed\" and \"weakly typed\" have so many different meanings that there's no single answer to your question.  However, since you specifically mentioned Perl in your question, let me try to explain in what sense Perl is weakly typed.The point is that, in Perl, there is no such thing as an \"integer variable\", a \"float variable\", a \"string variable\" or a \"boolean variable\".  In fact, as far as the user can (usually) tell, there aren't even integer, float, string or boolean : all you have are \"scalars\", which are all of these things at the same time.  So you can, for example, write:Of course, as you correctly note, all of this can be seen as just type coercion.  But the point is that, in Perl, types are  coerced.  In fact, it's quite hard for a user to tell what the internal \"type\" of a variable might be: at line 2 in my example above, asking whether the value of  is the string  or the number  is pretty much meaningless, since, as far as Perl is concerned, .  Indeed, it's even possible for a Perl scalar to internally have  a string and a numeric value at the same time, as is e.g. the case for  after line 2 above.The flip side of all this is that, since Perl variables are untyped (or, rather, don't expose their internal type to the user), operators cannot be overloaded to do different things for different types of arguments; you can't just say \"this operator will do X for numbers and Y for strings\", because the operator can't (won't) tell which kind of values its arguments are.Thus, for example, Perl has and needs both a numeric addition operator () and a string concatenation operator (): as you saw above, it's perfectly fine to add strings () or to concatenate numbers ().  Similarly, the numeric comparison operators , , , , ,  and  compare the numeric values of their arguments, while the string comparison operators , , , , ,  and  compare them lexicographically as strings.  So , but  (but , while ).  (Mind you, certain  languages, like JavaScript, try to accommodate Perl-like weak typing while  doing operator overloading.  This often leads to ugliness, like the loss of associativity for .)(The fly in the ointment here is that, for historical reasons, Perl 5 does have a few corner cases, like the bitwise logical operators, whose behavior depends on the internal representation of their arguments.  Those are generally considered an annoying design flaw, since the internal representation can change for surprising reasons, and so predicting just what those operators do in a given situation can be tricky.)All that said, one could argue that Perl  have strong types; they're just not the kind of types you might expect.  Specifically, in addition to the \"scalar\" type discussed above, Perl also has two structured types: \"array\" and \"hash\".  Those are  distinct from scalars, to the point where Perl variables have different  indicating their type ( for scalars,  for arrays,  for hashes).  There  coercion rules between these types, so you  write e.g. , but many of them are quite lossy: for example,  assigns the  of the array   to , not its contents.  (Also, there are a few other strange types, like typeglobs and I/O handles, that you don't often see exposed.)Also, a slight chink in this nice design is the existence of reference types, which are a special kind of scalars (and which  be distinguished from normal scalars, using the  operator).  It's possible to use references as normal scalars, but their string/numeric values are not particularly useful, and they tend to lose their special reference-ness if you modify them using normal scalar operations.  Also, any Perl variable can be ed to a class, turning it into an object of that class; the OO class system in Perl is somewhat orthogonal to the primitive type (or typelessness) system described above, although it's also \"weak\" in the sense of following the  paradigm.  The general opinion is that, if you find yourself checking the class of an object in Perl, you're doing something wrong. Actually, the sigil denotes the type of the value being accessed, so that e.g. the first scalar in the array  is denoted .  See  for more details. Objects in Perl are (normally) accessed through references to them, but what actually gets ed is the (possibly anonymous) variable the reference points to.  However, the blessing is indeed a property of the variable,  of its value, so e.g. that assigning the actual blessed variable to another one just gives you a shallow, unblessed copy of it.  See  for more details.In addition to what Eric has said, consider the following C code:In contrast to languages such as Python, C#, Java or whatnot, the above is weakly typed because we  type information. Eric correctly pointed out that in C# we can circumvent the compiler by casting, effectively telling it \u201cI know more about the type of this variable than you\u201d.But even then, the runtime will still check the type! If the cast is invalid, the runtime system will catch it and throw an exception.With type erasure, this doesn\u2019t happen \u2013 type information is thrown away. A cast to  in C does exactly that. In this regard, the above is fundamentally different from a C# method declaration such as .(Technically, C# also allows type erasure through unsafe code or marshalling.) is as weakly typed as it gets. Everything else is just a matter of static vs. dynamic type checking, i.e. of the time  a type is checked.A perfect example comes from :Generally strong typing implies that the programming language places severe restrictions on the intermixing that is permitted to occur.Notice that a weak typing language can intermix different types without errors. A strong type language requires the input types to be the expected types. In a strong type language a type can be converted ( converts an integer to a string) or cast (). This all depends on the interpretation of typing.I would like to contribute to the discussion with my own research on the subject, as others comment and contribute I have been reading their answers and following their references and I have found interesting information. As suggested, it is probable that most of this would be better discussed in the Programmers forum, since it appears to be more theoretical than practical.From a theoretical standpoint, I think the article by Luca Cardelli and Peter Wegner named  has one of the best arguments I have read.This statement seems to suggest that weakly typing would let us access the inner structure of a type and manipulate it as if it was something else (another type). Perhaps what we could do with unsafe code (mentioned by Eric) or with c type-erased pointers mentioned by Konrad.The article continues...As such, strong typing means the absence of type errors, I can only assume that weak typing means the contrary: the likely presence of type errors. At runtime or compile time? Seems irrelevant here.Funny thing, as per this definition, a language with powerful type coercions like Perl would be considered strongly typed, because the system is not failing, but it is dealing with the types by coercing them into appropriate and well defined equivalences.On the other hand, could I say than the allowance of  and  (in Java) and ,  (in C#) would indicate a level of weakly typing, at least at compile time? Eric's answer seems to agree with this.In a second article named  provided in one of the references provided in one of the answers in this question, Luca Cardelli delves into the concept of type violations:As such, type coercions like those provided by operators could be considered type violations, but unless they break the consistency of the type system, we might say that they do not lead to a weakly typed system.Based on this neither Python, Perl, Java or C# are weakly typed.Cardelli mentions two type vilations that I very well consider cases of truly weak typing:This kind of things possible in languages like C (mentioned by Konrad) or through unsafe code in .Net (mentioned by Eric) would truly imply weakly typing.I believe the best answer so far is Eric's, because the definition of this concepts is very theoretical, and when it comes to a particular language, the interpretations of all these concepts may lead to different debatable conclusions.Weak typing does indeed mean that a high percentage of types can be implicitly coerced, attempting to guess what the coder intended.Strong typing means that types are not coerced, or at least coerced less.Static typing means your variables' types are determined at compile time.Many people have recently been confusing \"manifestly typed\" with \"strongly typed\".  \"Manifestly typed\" means that you declare your variables' types explicitly.Python is mostly strongly typed, though you can use almost anything in a boolean context, and booleans can be used in an integer context, and you can use an integer in a float context.  It is not manifestly typed, because you don't need to declare your types (except for Cython, which isn't entirely python, albeit interesting).  It is also not statically typed.C and C++ are manifestly typed, statically typed, and somewhat strongly typed, because you declare your types, types are determined at compile time, and you can mix integers and pointers, or integers and doubles, or even cast a pointer to one type into a pointer to another type.Haskell is an interesting example, because it is not manifestly typed, but it's also statically and strongly typed.The strong <=> weak typing is not only about the continuum on how much or how little of the values are coerced automatically by the language for one datatype to another, but how strongly or weakly the actual  are typed. In Python and Java, and mostly in C#, the values have their types set in stone. In Perl, not so much - there are really only a handful of different valuetypes to store in a variable. Let's open the cases one by one.In Python example ,  operator calls the  for type  giving it the string  as an argument - however, this results in NotImplemented:Next, the interpreter tries the  of str:As it fails, the  operator fails with the  the result . As such, the exception does not say much about strong typing, but the fact that the operator   its arguments automatically to the same type, is a pointer to the fact that Python is not the most weakly typed language in the continuum.On the other hand, in Python   implemented:That is, The fact that the operation is different requires some strong typing - however the opposite of  coercing the values to numbers before multiplying still would not necessarily make the values weakly typed.The Java example,  works only because as a fact of convenience, the operator  is overloaded for strings. The Java  operator replaces the sequence with creating a  (see ):This is rather an example of very static typing, without no actual coercion -  has a method  that is specifically used here. The documentation says the following:Where  then Thus this is a case of absolutely no coercion by the language - delegating every concern to the objects itself.According to the , operator  is not even overloaded for the  class - akin to Java, this is just convenience generated by the compiler, thanks to both static and strong typing.As the  explains,Perl however does not have a separate data type for numbers, booleans, strings, nulls, s, references to other objects etc - it just has one type for these all, the scalar type; 0 is a scalar value as much as is \"0\". A scalar  that was set as a string can really change into a number, and from there on behave differently from \"just a string\", . The scalar can hold anything in Perl, it is as much the object as it exists in the system. whereas in Python the names just refers to the objects, in Perl the scalar values in the names are changeable objects. Furthermore, the Object Oriented Type system is glued on top of this: there are just 3 datatypes in perl - scalars, lists and hashes. A user defined object in Perl is a reference (that is a pointer to any of the 3 previous) ed to a package - you can take any such value and bless it to any class at any instant you want. Perl even allows you to change the classes of values at whim - this is not possible in Python where to create a value of some class you need to explicitly construct the value belonging to that class with  or similar. In Python you cannot really change the essence of the object after the creation, in Perl you can do much anything:thus the type identity is weakly bound to the variable, and it can be changed through any reference on the fly. In fact, if you do does not have the class identity, even though  will still give the blessed reference.There are much more about weak typing to Perl than just automatic coercions, and it is more about that the types of the values themselves are not set into stone, unlike the Python which is dynamically yet very strongly typed language. That python gives  on  is an indication that the language is strongly typed, even though the contrary one of doing something useful, as in Java or C# does not preclude them being strongly typed languages.I like , but to address the question - strongly typed languages typically have explicit knowledge of the types of variables at each point of the program. Weakly typed languages do not, so they can attempt to perform an operation that may not be possible for a particular type.\nIt think the easiest way to see this is in a function.\nC++:The variable  is known to be of type string and any incompatible operation will be caught at compile time.Python:The variable  could be anything and we can have code that calls an invalid method, which will only get caught at runtime.As many others have expressed, the entire notion of \"strong\" vs \"weak\" typing is problematic.As a archetype, Smalltalk is very strongly typed -- it will  raise an exception if an operation between two objects is incompatible. However, I suspect few on this list would call Smalltalk a strongly-typed language, because it is I find the notion of \"static\" versus \"dynamic\" typing more useful than \"strong\" versus \"weak.\" A statically-typed language has all the types figured out at compile-time, and the programmer has to explicitly declare if otherwise.Contrast with a dynamically-typed language, where typing is performed at run-time. This is typically a requirement for polymorphic languages, so that decisions about whether an operation between two objects is legal does not have to be decided by the programmer in advance.In polymorphic, dynamically-typed languages (like Smalltalk and Ruby), it's more useful to think of a \"type\" as a \"conformance to protocol.\" If an object obeys a protocol the same way another object does -- even if the two objects do not share any inheritance or mixins or other voodoo -- they are considered the same \"type\" by the run-time system. More correctly, an object in such systems is autonomous, and can decide if it makes sense to respond to any particular message referring to any particular argument.Want an object that can make some meaningful response to the message \"+\" with an object argument that describes the colour blue? You can do that in dynamically-typed languages, but it is a pain in statically-typed languages."},
{"body": "I've come across situations where a current version of a package seems not to be working and requires reinstallation. But  won't touch a package that is already up-to-date. I see how to force a reinstallation by first uninstalling (with ) and then installing, but is there a way to simply force an \"update\" to a nominally current version in a single step?When upgrading, reinstall all packages even if they are already up-to-date.Ignore the installed packages (reinstalling instead).You might want to have all three options:  and  ensures reinstallation, while  avoids reinstalling dependencies.Otherwise you might run into the problem that pip starts to recompile Numpy or other large packages.doesn't appear to force reinstall using python2.7 with pip-1.5I've had to useIf you want to reinstall packages specified in a requirements.txt file, without upgrading, so just reinstall the specific versions specified in the requirements.txt file:"},
{"body": "I'm trying to get the name of the Python script that is currently running.For example, I have a script called  and I would like to do something like this inside it:and get: .Use .  If you want to omit the directory part (which might be present), you can use ,.This will print  for ,  for , etc. It's the first argument to . (Note that after py2exe it would be .)Note that  will give the file where this code resides, which can be imported and different from the main file being interpreted. To get the main file, the special  module can be used:Note that  works in Python 2.7 but not in 3.2, so use the import-as syntax as above to make it portable.The Above answers are good . But I found this method more efficient using above results.\nThis results in actual script file name not a path.For completeness' sake, I thought it would be worthwhile summarizing the various possible outcomes and supplying references for the exact behaviour of each: may be invoked of any of the above in order to extract the actual file name.Try this:The first argument in sys will be the current file name so this will work"},
{"body": "I want to perform my own complex operations on financial data in dataframes in a sequential manner.For example I am using the following MSFT CSV file taken from :I then do the following:Is that the most efficient way? Given the focus on speed in pandas, I would assume there must be some special function to iterate through the  values in a manner that one also retrieves the index (possibly through a generator to be memory efficient)?  unfortunately only iterates column by column.The newest versions of pandas now include a built-in function for iterating over rows. Or, if you want it faster use But, unutbu's suggestion to use numpy functions to avoid iterating over rows will produce the fastest code. Pandas is based on NumPy arrays.\nThe key to speed with NumPy arrays is to perform your operations on the whole array at once, never row-by-row or item-by-item.For example, if  is a 1-d array, and you want the day-over-day percent change,This computes the entire array of percent changes as one statement, instead of So try to avoid the Python loop  entirely, and\nthink about how to perform your calculations with operations on the entire array (or dataframe) as a whole, rather than row-by-row.You can loop through the rows by transposing and then calling iteritems:I am not certain about efficiency in that case. To get the best possible performance in an iterative algorithm, you might want to explore writing it in , so you could do something like:I would recommend writing the algorithm in pure Python first, make sure it works and see how fast it is-- if it's not fast enough, convert things to Cython like this with minimal work to get something that's about as fast as hand-coded C/C++.Like what has been mentioned before, pandas object is most efficient when process the whole array at once. However for those who really need to loop through a pandas DataFrame to perform something, like me, I found at least three ways to do it. I have done a short test to see which one of the three is the least time consuming.Result:This is probably not the best way to measure the time consumption but it's quick for me.Here are some pros and cons IMHO:I checked out  after noticing  answer, but found that it yields (index, Series) tuples. Not sure which would work best for you, but I ended up using the  method for my problem, which yields (index, row_value1...) tuples.There's also , which iterates through (column, series) tuples.Just as a small addition, you can also do an apply if you have a complex function that you apply to a single column:Another suggestion would be to combine groupby with vectorized calculations if subsets of the rows shared characteristics which allowed you to do so. "},
{"body": "I want to use  in . I googled this problem but couldn't find proper solutions.I find   may be a good choice. But I couldn't pass some arguments to it.Using  may be a proper solution for now. I want to know whether there's a better solution or not.Is there a way to use PhantomJS in Python?The easiest way to use PhantomJS in python is via Selenium. The simplest installation method isAfter installation, you may use phantom as simple as:If your system path environment variable isn't set correctly, you'll need to specify the exact path as an argument to .  Replace this:... with the following:References:PhantomJS recently  altogether. However, PhantomJS now embeds .A new project has since stepped up to fill the void: . You probably want to use that instead:Now since the GhostDriver comes bundled with the PhantomJS, it has become even more convenient to use it through Selenium.I tried the Node installation of PhantomJS, as suggested by Pykler, but in practice I found it to be slower than the standalone installation of PhantomJS. I guess standalone installation didn't provided these features earlier, but as of v1.9, it very much does so. Now you can use like thisHere's how I test javascript using PhantomJS and Django::::this is what I do, python3.3.  I was processing huge lists of sites, so failing on the timeout was vital for the job to run through the entire list.If using Anaconda, install with:in your script:works perfectly.  In case you are using , you can easily automate the installation processes that Pykler describes using the  recipe.That part installs node.js as binary (at least on my system) and then uses npm to install PhantomJS. Finally it creates an entry point , which you can call the PhantomJS webdriver with. (To install Selenium, you need to specify it in your egg requirements or in the Buildout configuration.)The  is great but the Node requirement is outdated. The comments in that answer suggest the simpler answer, which I've put here to save others time:"},
{"body": "I have such db model:And when new instance is added:I've an issue: all records in database have the same value in date field - the date of the first payment. After server restart - one record have new date and others have the same as the first. It's look like some data cache is used but I can't find where.database: mysql 5.1.25django v1.1.1it looks like  is being evaluated when the model is defined, and not each time you add a record.Django has a feature to accomplish what you are trying to do already:orThe difference between the second example and what you currently have is the lack of parentheses. By passing  without the parentheses, you are passing the actual function, which will be called each time a record is added. If you pass it , then you are just evaluating the function and passing it the return value.More information is available at Django's Instead of using  you should be really using From the  on the django model default field:Therefore following should work:David had the right answer. The parenthesis () makes it so that the  timezone.now() is called every time the model is evaluated. If you remove the () from timezone.now() (or datetime.now(), if using the naive datetime object) to make it just this:Then it will work as you expect:\n    New objects will receive the current date when they are created, but the date won't be overridden every time you do manage.py makemigrations/migrate.I just encountered this. Much thanks to David.The  is evaluated when the class is created, not when new record is being added to the database.To achieve what you want define this field as:This way the  field will be set to current date for each new record.The answer to this one is actually wrong.Auto filling in the value (auto_now/auto_now_add isn't the same as default).  The default value will actually be what the user sees if its a brand new object.  What I typically do is:    Make sure, if your trying to represent this in an Admin page, that you list it as 'read_only' and reference the field nameAgain, I do this since my default value isn't typically editable, and Admin pages ignore non-editables unless specified otherwise.  There is certainly a difference however between setting a default value and implementing the auto_add which is key here.  Test it out! is being evaluated once, when your class is instantiated. Try removing the parenthesis so that the function  is returned and THEN evaluated. I had the same issue with setting default values for my s and wrote up my solution .From the Python language reference, under :Fortunately, Django has a way to do what you want, if you use the  argument for the :See the Django docs for ."},
{"body": "I'd like to grab daily sunrise/sunset times from a web site. Is it possible to scrape web content with Python? what are the modules used? Is there any tutorial available?Use urllib2 in combination with the brilliant  library:I'd really recommend Scrapy, for reasons being elaborated in .Quote from the answer:I collected together scripts from my web scraping work into .Example script for your case:Output:I would strongly suggest checking out . It uses jquery-like (aka css-like) syntax which makes things really easy for those coming from that background.For your case, it would be something like:Output:You can use  to make the HTTP requests, and then you'll have web content.You can get it like this: is a python HTML parser that is supposed to be good for screen scraping.In particular,  is their tutorial on parsing an HTML document.Good luck!I use a combination of  (finding urls - py2) and  (downloading images - py2+3). The scrapemark.py has 500 lines of code, but uses regular expressions, so it may be not so fast, did not test.Example for scraping your website:\nUsage:Result:I just saw  in .Scrapy open source framework will help to web scrap in python.This open source and collaborative framework for extracting the data you need from websites. Web scraping is closely related to web indexing, which indexes information on the web using a bot or web crawler and is a universal technique adopted by most search engines.I know I have come late to party but I have a nice suggestion for you.Using  is already been suggested I would rather prefer using  to scrape data inside HTMLHere is a simple web crawler, i used BeautifulSoup and we will search for all the links(anchors) who's class name is _3NFO0d. I used Flipkar.com, it is an online retailing store.If we think of getting all the links to the Mobile names then it is easier to use this:"},
{"body": "I'm trying to plot a figure without tickmarks or numbers on either of the axes (I use axes in the traditional sense, not the matplotlib nomenclature!). An issue I have come across is where matplotlib adjusts the x(y)ticklabels by subtracting a value N, then adds N at the end of the axis.This may be vague, but the following simplified example highlights the issue, with '6.18' being the offending value of N:The three things I would like to know are:Instead of hiding each element, you can hide the whole axis:Or, you can set the ticks to an empty list:In this second option, you can still use  and  to add labels to the axes.If you want to hide just the axis text keeping the grid lines:Doing  or  will also hide the grid lines.Somewhat of an old thread but, this seems to be a faster method using the latest version of matplotlib:set the major formatter for the x-axisIf you are like me and don't always retrieve the axes,  when plotting the figure, then a simple solution would be to do I was not actually able to render an image without borders or axis data based on any of the code snippets here (even the one accepted at the answer). After digging through some API documentation, I landed on this code to render my imageI used the tick_params call to basically shut down any extra information that might be rendered and I have a perfect graph in my output file :). Hope this helps!"},
{"body": "I'm starting to learn Python and I've come across generator functions, those that have a yield statement in them.  I want to know what types of problems that these functions are really good at solving.Generators give you lazy evaluation. You use them by iterating over them, either explicitly with 'for' or implicitly by passing it to any function or construct that iterates. You can think of generators as returning multiple items, as if they return a list, but instead of returning them all at once they return them one-by-one, and the generator function is paused until the next item is requested.Generators are good for calculating large sets of results (in particular calculations involving loops themselves) where you don't know if you are going to need all results, or where you don't want to allocate the memory for all results at the same time. Or for situations where the generator uses  generator, or consumes some other resource, and it's more convenient if that happened as late as possible.Another use for generators (that is really the same) is to replace callbacks with iteration. In some situations you want a function to do a lot of work and occasionally report back to the caller. Traditionally you'd use a callback function for this. You pass this callback to the work-function and it would periodically call this callback. The generator approach is that the work-function (now a generator) knows nothing about the callback, and merely yields whenever it wants to report something. The caller, instead of writing a separate callback and passing that to the work-function, does all the reporting work in a little 'for' loop around the generator.For example, say you wrote a 'filesystem search' program. You could perform the search in its entirety, collect the results and then display them one at a time. All of the results would have to be collected before you showed the first, and all of the results would be in memory at the same time. Or you could display the results while you find them, which would be more memory efficient and much friendlier towards the user. The latter could be done by passing the result-printing function to the filesystem-search function, or it could be done by just making the search function a generator and iterating over the result.If you want to see an example of the latter two approaches, see os.path.walk() (the old filesystem-walking function with callback) and os.walk() (the new filesystem-walking generator.) Of course, if you really wanted to collect all results in a list, the generator approach is trivial to convert to the big-list approach:One of the reasons to use generator is to make the solution clearer for some kind of solutions.The other is to treat results one at a time, avoiding building huge lists of results that you would process separated anyway. If you have a fibonacci-up-to-n function like this:You can more easily write the function as this:The function is clearer. And if you use the function like this:in this example, if using the generator version, the whole 1000000 item list won't be created at all, just one value at a time. That would not be the case when using the list version, where a list would be created first.See the \"Motivation\" section in .A non-obvious use of generators is creating interruptible functions, which lets you do things like update UI or run several jobs \"simultaneously\" (interleaved, actually) while not using threads.Buffering. When it is efficient to fetch data in large chunks, but process it in small chunks, then a generator might help:The above lets you easily separate buffering from processing. The consumer function can now just get the values one by one without worrying about buffering.I find this explanation which clears my doubt. Because there is a possibility that person who don't know  also don't know about The return statement is where all the local variables are destroyed and the resulting value is given back (returned) to the caller.  Should the same function be called some time later, the function will get a fresh new set of variables.But what if the local variables aren't thrown away when we exit a function?  This implies that we can  where we left off.  This is where the concept of  are introduced and the  statement resumes where the  left off.So that's the difference between return and yield statements in Python.  So Generators are a simple and powerful tool for creating iterators. They are written like regular functions but use the yield statement whenever they want to return data. Each time next() is called, the generator resumes where it left-off (it remembers all the data values and which statement was last executed).The simple explanation:\nConsider a  statementA lot of the time, all the items in  doesn't need to be there from the start, but can be generated on the fly as they're required. This can be a lot more efficient in both Other times, you don't even know all the items ahead of time. For example:You have no way of knowing all the user's commands beforehand, but you can use a nice loop like this if you have a generator handing you commands:With generators you can also have iteration over infinite sequences, which is of course not possible when iterating over containers.I have found that generators are very helpful in cleaning up your code and by giving you a very unique way to encapsulate and modularize code.  In a situation where you need something to constantly spit out values based on its own internal processing and when that something needs to be called from anywhere in your code (and not just within a loop or a block for example), generators are  feature to use.An abstract example would be a fibonacci number generator that does not live within a loop and when it is called from anywhere will always return the next number in sequence:Now you have two fibonacci number generator objects which you can call from anywhere in your code and they will always return ever larger fibonacci numbers in sequence as follows:The lovely thing about generators is that they encapsulate state without having to go through the hoops of creating objects.  One way of thinking about them is as \"functions\" which remember their internal state.I got the fibonacci example from  and with a little imagination, you can come up with a lot of other situations where generators make for a great alternative to for-loops and other traditional iteration constructs.Lets say you have 100 million domains in your MySQL table and you would like to update alexa rank for each domain.First thing you need is to select your domain names from the database. Lets say your database name is  and table name is If you use  its going to return 100 million rows which is going to  consume lot of memory. So your server might crashSo you decided to run the program in batches. Let say our batch size is 1000.In our first batch we will query the first 1000 rows, check alexa rank for each domain and update the database row. In our second batch we will work on the next 1000 rows. In our third batch it will be from 2001 to 3000 and so on.Now we need a generator function which generates our batches. Here is our generator functionAs you can see our function keep ing the results. If you used the keyword  instead of , then the whole function will be ended once it reaches returnIf a function uses the keyword  then its a generator.Now you can iterate like thisMy favorite uses are \"filter\" and \"reduce\" operations.Let's say we're reading a file, and only want the lines which begin with \"##\".We can then use the generator function in a proper loopThe reduce example is similar.  Let's say we have a file where we need to locate blocks of  lines.  [Not HTML tags, but lines that happen to look tag-like.]Again, we can use this generator in a proper for loop.The idea is that a generator function allows us to filter or reduce a sequence, producing a another sequence one value at a time.A practical example where you could make use of a generator is if you have some kind of shape and you want to iterate over its corners, edges or whatever. For my own project (source code ) I had a rectangle:Now I can create a rectangle and loop over its corners:Instead of  you could have a method  and call that with . It's just more elegant to use  since then we can use the class instance name directly in the  expression.Basically avoiding call-back functions when iterating over input maintaining state.See  and  for an overview of what can be done using generators.Some good answers here, however, I'd also recommend a complete read of the python  which helps explain some of the more potent use-cases of generators.I use generators when our web server is acting as a proxy:Since the send method of a generator has not been mentioned here is an example:It shows the possibility to send a value to a running generator\nA more advanced course on generators in the video below (including yield from explination, generators for parallel processing, escaping recursion limit etc.)Piles of stuff. Any time you want to generate a sequence of items, but don't want to have to 'materialize' them all into a list at once. For example, you could have a simple generator that returns prime numbers:You could then use that to generate the products of subsequent primes:These are fairly trivial examples, but you can see how it can be useful for processing large (potentially infinite!) datasets without generating them in advance, which is only one of the more obvious uses."},
{"body": "Is there a built-in or standard library method in Python to calculate the arithmetic mean (average) of a list of numbers?I am not aware of anything in the standard library. However, you could use something like:In numpy, there's .NumPy has a  which is an arithmetic mean. Usage is as simple as this:In Python 3.4, there is a new  module. You can now use :For 3.1-3.3 users, the original version of the module is available on PyPI under the name . Just change  to .You don't even need numpy or scipy...Use scipy:Instead of casting to float you can do followingor using lambdaI always supposed  is omitted from the builtins/stdlib because it is as simple asand any caveats would be .Notable caveats:"},
{"body": "What I understand from reading the documentation is that Python has a separate namespace for functions, and if I want to use a global variable in that function, I need to use .I'm using Python 2.7 and I tried this little testIt seems things are working fine even without . I was able to access global variable without any problem.Am I missing anything? Also, following is from Python documentation:While formal parameters and class definition make sense to me, I'm not able to understand the restriction on for loop control target and function definition.The keyword  is only useful to change or create global variables in a local context, although creating global variables is seldom considered a good solution.The above will give you:While if you use the  statement, the variable will become available \"outside\" the scope of the function, effectively becoming a global variable.So the above code will give you:In addition, due to the nature of python, you could also use  to declare functions, classes or other objects in a local context. Although I would advise against it since it causes nightmares if something goes wrong or needs debugging.While you can access global variables without the  keyword, if you want to assign to them you have to use the  keyword. For example:In your case, you're just accessing the list .This is the difference between accessing the name and  it within a scope.If you're just looking up a variable to read its value, you've got access to global as well as local scope.However if you assign to a variable who's name isn't in local scope, you are  that name into this scope (and if that name also exists as a global, you'll hide that). If you want to be able to assign to the global name, you need to tell the parser to use the global name rather than bind a new local name - which is what the 'global' keyword does.Binding anywhere within a block causes the name everywhere in that block to become bound, which can cause some rather odd looking consequences (e.g. UnboundLocalError suddenly appearing in previously working code).The other answers answer your question. Another important thing to know about names in Python is that they are either local or global on a per-scope basis.Consider this, for example:You can probably guess that the  statement will be assigning to a local variable and not affect the value of the same variable declared outside the  function. You may be more surprised to discover that  The statement  inside the function produces an The reason is that Python has noticed that, elsewhere in the function, you assign the name , and also  is nowhere declared . That makes it a local variable. But when you try to print it, the local name hasn't been defined yet. Python in this case  to looking for the name as a global variable, as some other languages do. Essentially, you cannot access a global variable if you have defined a local variable of the same name  in the function.This tripped me up many times, and I hope to avoid you tripping over the same thing.Accessing a name and assigning a name are different. In your case, you are just accessing a name.If you assign to a variable within a function, that variable is assumed to be local unless you declare it global. In the absence of that, it is assumed to be global.Example:Any variable declared outside of a function is assumed to be global, it's only when declaring them from inside of functions (except constructors) that you must specify that the variable be global.It means that you should not do the following:Global makes the variable \"Global\"This makes 'x' act like a normal variable outside the function. If you took the global out then it would give an error since it cannot print a variable inside a function.I think global variables are great and easier to follow than \"return\", however I am only using small programs and just learning but can see it's value."},
{"body": "I am trying to create a SlugField in Django.I created this simple model:I then do this:I was expecting b-b-b-bYou will need to use the slugify function. You can call  automatically by overriding the  method:Be aware that the above will cause your URL to change when the  field is edited, which . It may be preferable to generate the slug only once when you create a new object:There is corner case with some utf-8 charactersExample:This can be solved with A small correction to Thepeer's answer: To override  function in model classes, better add arguments to it:Otherwise,  will result in a  error (unexpected argument).If you're using the admin interface to add new items of your model, you can set up a  in your  and utilize  to automate entering of a slug:Here, when the user enters a value in the admin form for the  field, the  will be automatically populated with the correct slugified . In most cases the slug should not change, so you really only want to calculate it on first save:If you don't want to set the slugfield to Not be editable, then I believe you'll want to set the Null and Blank properties to False.  Otherwise you'll get an error when trying to save in Admin.So a modification to the above example would be::Use prepopulated_fields in your admin class:I'm using Django 1.7Create a SlugField in your model like this:Then in  define ;"},
{"body": "I was wondering how to check whether a variable is a class (not an instance!) or not.I've tried to use the function  to do this, but I don't know what type would a class will have.For example, in the following codeI tried to substitute  \"\" with , but I realized that  is a keyword in python.Even better: use the  function.The inspect.isclass is probably the best solution, and it's really easy to see how it's actually implementedReturn  if  is class and  if not.class Foo: is called old style class and class X(object): is called new style class. Check this  . New style is recommended. Read about \"\"That's a little hackish, but an option if you don't want to avoid imports:There are some working solutions here already, but here's another one:"},
{"body": "I am looking over  but just can't seem to figure out how to do this as it's not working. I need to check if the current site user is logged in (authenticated), and am trying:despite being sure that the user is logged in, it returns just:I'm able to do other requests (from the first section in the url above), such as:which returns a successful response. is a function. You should call it likeAs Peter Rowell pointed out, what may be tripping you up is that in the default Django template language, you don't tack on parenthesis to call functions. So you may have seen something like this in template code:However, in Python code, it is indeed a method in the  class.:  is now an attribute in Django 1.10. The method still exists for backwards compatibility, but will be removed in Django 2.0.Following block should work: "},
{"body": "I want to change the key of an entry in a Python dictionary.Is there a straightforward way to do this?Easily done in 2 steps:Or in 1 step:which will raise  if  is undefined. Note that this  delete .if you want to change all the keys:if you want to change single key:\n   You can go with any of the above suggestion.pop'n'freshIn python 2.7 and higher, you can use dictionary comprehension:\nThis is an example I encountered while reading a CSV using a DictReader. The user had suffixed all the column names with ':'to get rid of the trailing ':' in the keys:Since keys are what dictionaries use to lookup values, you can't really change them. The closest thing you can do is to save the value associated with the old key, delete it, then add a new entry with the replacement key and the saved value. Several of the other answers illustrate different ways this can be accomplished.No direct way to do this, but you can delete-then-assignor do mass key changes:If you have a complex dict, it means there is a dict or list within the dict:You can associate the same value with many keys, or just remove a key and re-add a new key with the same value.For example, if you have keys->values:there's no reason you can't add  or remove  and add I haven't seen this exact answer:You can even do this to object attributes. \nMake them into a dictionary by doing this:Then you can manipulate the object attributes like you would a dictionary:"},
{"body": "I have the following string: How can I get the last four characters and store them in a string using Python?Like this:This slices the string's last 4 characters. The \"-\" before the 4 signals it starts from the end. Since it's before the \":\", it'll be the start of the string. If it were after the \":\", the string would end in 4 chars before actual end.See the Python docs for more information in slicing.See : "},
{"body": "How are \"keyword arguments\" different from regular arguments? Can't all arguments be passed as  instead of using positional syntax?there are two related concepts, both called \"keyword arguments\".On the calling side, which is what other commenters have mentioned, you have the ability to specify some function arguments by name. You have to mention them after all of the arguments without names (positional arguments), and there must be default values for any parameters which were not mentioned at all.The other concept is on the function definition side: You can define a function that takes parameters by name -- and you don't even have to specify what those names are. These are pure keyword arguments, and can't be passed positionally. The syntax isAny keyword arguments you pass into this function will be placed into a dictionary named kwargs. You can examine the keys of this dictionary at run-time, like this:There is one last language feature where the distinction is important. Consider the following function:The  argument will store all of the positional arguments passed to , with no limit to how many you can provide.The  argument will store any keyword arguments:And of course, you can use both at the same time:These features are rarely used, but occasionally they are very useful, and it's important to know which arguments are positional or keywords.Using keyword arguments is the same thing as normal arguments except order doesn't matter. For example the two functions calls below are the same:They have no keywords before them. The order is important!They have keywords in the front. They can be in any order!You should also know that if you use default arguments and neglect to insert the keywords, then the order will then matter!There are two ways to assign argument values to function parameters, both are used.Note that  have the option to use positional arguments.If  don't use positional arguments, then -- yes -- everything  wrote turns out to be a keyword argument.When  call a function you make a decision to use position or keyword or a mixture.  You can choose to do all keywords if you want.  Some of us do not make this choice and use positional arguments.I'm surprised that no one seems to have pointed out that one can pass a dictionary of keyed argument parameters, that satisfy the formal parameters, like so.I'm surprised no one has mentioned the fact that you can mix positional and keyword arguments to do sneaky things like this using  and  ():This allows you to use arbitrary keyword arguments that may have keys you don't want to define upfront.Using  you can have both required and non-required keyword :: (default value defined for 'b') (no default value defined for 'b'):This can help in cases where you have a many similar arguments next to each other especially when of the same type, in that case I prefer using named arguments or I create a custom class if arguments belong together."},
{"body": "this is i saw in someone's code: Using these magic methods (, ) allows you to implement objects which can be used easily with the  statement. The idea is that it makes it easy to build code which needs some 'cleandown' code executed (think of it as a  block). .A useful example could be a database connection object (which then automagically closes the connection once the corresponding 'with'-statement goes out of scope):As explained above, use this object with the  statement (you may need to do  at the top of the file if you're on Python 2.5). has a nice writeup as well.If you know what  are then you need nothing more to understand  and  magic methods. Lets see a very simple example.In this example I am opening  with help of  function. The  block ensures that even if an unexpected exception occurs  will be closed.Now I am opening same file with  statement:If you look at the code, I  didn't close the file & there is no  block. Because  statement automatically closes   . You can even check it by calling  attribute -- which returns .This is because the file objects (fp in my example) returned by  function has two built-in methods  and . It is  also known as context manager.  method is called at the start of  block and   method is called at the end.  Note:  statement only works with objects that support the context mamangement protocol i.e. they have  and  methods. A class which implement both methods is known as context manager class.Now lets define our own  class.I hope now you have basic understanding of both  and  magic methods.I found it oddly hard to locate the python docs for  and  methods by Googling, so to help others here is the link:I was hoping for a clear description of the  method arguments. This is lacking but we can deduce them...Presumably  is the class of the exception.It says you should not re-raise the passed-in exception. This suggests to us that one of the arguments might be an actual Exception instance ...or maybe you're supposed to instantiate it yourself from the type and value?We can answer by looking at this article:\n...so clearly  is an Exception instance.And presumably  is a Python traceback object."},
{"body": "I want to override the  method on a class to do something fancy but I don't want to break the default behavior.What's the correct way to do this?Overriding  should be fine --  is only called as a last resort i.e. if there are no attributes in the instance that match the name. For instance, if you access , then  will only be called if  has no attribute called . If the attribute is one you don't want to handle, raise :However, unlike ,  will be called first (only works for new style classes i.e. those that inherit from object). In this case, you can preserve default behaviour like so:See .To extend Michael answer, if you want to maintain the default behavior using , you can do it like so:  Now the exception message is more descriptive:"},
{"body": "I'm pulling data out of a Google doc, processing it, and writing it to a file (that eventually I will paste into a Wordpress page).It has some non-ASCII symbols. How can I convert these safely to symbols that can be used in HTML source? Currently I'm converting everything to Unicode on the way in, joining it all together in a Python string, then doing: There is an encoding error on the last line: This Python runs without an error:But then if I open the actual text file, I see lots of symbols like:Maybe I need to write to something other than a text file? Deal exclusively with unicode objects as much as possible by decoding things to unicode objects when you first get them and encoding them as necessary on the way out.If your string is actually a unicode object, you'll need to convert it to a unicode-encoded string object before writing it to a file:When you read that file again, you'll get a unicode-encoded string that you can decode to a unicode object:In Python 2.6+, you could  that is default () on Python 3:It might be more convenient if you need to write the text incrementally (you don't need to call  multiple times). Unlike  module,  module has a proper universal newlines support.The file opened by  is a file that takes  data, encodes it in  and writes it to the file. However, what you try to write isn't ; you take  and encode it in  . That's what the  method does, and the result of encoding a unicode string is a bytestring (a  type.)You should either use normal  and encode the unicode yourself, or (usually a better idea) use  and  encode the data yourself.Use  from the  module:Best practice, in general, use  for writing to files (we don't even have to worry about byte-order with utf-8).utf-8 is the most modern and univerally usable encoding - it works in all web browsers, most text-editors (see your settings if you have issues) and most terminals/shells.On Windows, you might try  if you're limited to viewing output in Notepad (or another limited viewer).And just open it with the context manager and write your unicode characters out:Here's an example that attempts to map every possible character up to three bits wide (4 is the max, but that would be going a bit far) from the digital representation (in integers) to an encoded printable output, along with its name, if possible (put this into a file called ):This should run in the order of about a minute, and you can view the data file, and if your file viewer can display unicode, you'll see it. Information about the categories can be found . Based on the counts, we can probably improve our results by excluding the Cn and Co categories, which have no symbols associated with them.It will display the hexadecimal mapping, , symbol (unless can't get the name, so probably a control character), and the name of the symbol. e.g.I recommend  on unix or cygwin (don't print/cat the entire file to your output):e.g. will display similar to the following lines which I sampled from it using Python 2 (unicode 5.2):My Python 3.5 from Anaconda has unicode 8.0, I would presume most 3's would.Save this to file: foo.py:Run it and pipe output to file:Open tmp.txt and look inside, you see this:Thus you have saved unicode e with a obfuscation mark on it to a file.That error arises when you try to encode a non-unicode string: it tries to decode it, assuming it's in plain ASCII. There are two possibilities:"},
{"body": "I have two existing dictionaries, and I wish to 'append' one of them to the other. By that I mean that the key,values of the other dictionary should be made into the first dictionary. For example:I think this all can be achieved through a  loop (maybe?), but is there some method of dictionaries or any other module that saves this job for me? The actual dictionaries I'm using are really big...You can door, if you don't want  to be modified, make a copy first:Note that if extra and orig have overlapping keys, the final value will be taken from extra. For example,Assuming that you do not want to change , you can either do a copy and update like the other answers, or you can create a new dictionary in one step by passing all items from both  dictionaries into the dict constructor:Or without itertools:Note that you only need to pass the result of  into  on Python 3, on 2.x  already returns a list so you can just do .As a more general use case, say you have a larger list of dicts that you want to combine into a single dict, you could do something like this: looks like it will do what you want...Perhaps, though, you don't want to update your original dictionary, but work on a copy:The most pythonic (and slightly faster) way to accomplish this is by:Or, depending on the problem to solve, maybe:There is the .update() method :)The answer I want to give is \"use collections.ChainMap\", but I just discovered that it was only added in Python 3.3: You can try to crib the class from the 3.3 source though: Here is a less feature-full Python 2.x compatible version (same author): Instead of expanding/overwriting one dictionary with another using dict.merge, or creating an additional copy merging both, you create a lookup chain that searches both in order. Because it doesn't duplicate the mappings it wraps ChainMap uses very little memory, and sees later modifications to any sub-mapping. Because order matters you can also use the chain to layer defaults (i.e. user prefs > config > env).A three-liner to combine or merge two dictionaries:This creates a new dictionary  without modifying  and .Note: If a key has different values in  and , then  overrides ."},
{"body": "What is the most efficient way to shift a list in python? \nRight now I have something like this:Is there a better way?A  is optimized for pulling and pushing on both ends. They even have a dedicated  method. What about just using ?It depends on what you want to have happen when you do this:You might want to change your:to:Numpy can do this using the  command:If you just want to iterate over these sets of elements rather than construct a separate data structure, consider using iterators to construct a generator expression:This also depends on if you want to shift the list in place (mutating it), or if you want the function to return a new list.  Because, according to my tests, something like this is at least twenty times faster than your implementation that adds two lists:In fact, even adding a  to the top of that to operate on a copy of the list passed in is still twice as fast.Various implementations with some timing at Simplest way I can think of:                                        I think you are looking for this:If efficiency is your goal, (cycles? memory?) you may be better off looking at the array module: Arrays do not have the overhead of lists.  As far as pure lists go though, what you have is about as good as you can hope to do.Possibly a ringbuffer is more suitable. It is not a list, although it is likely that it can behave enough like a list for your purposes.The problem is that the efficiency of a shift on a list is O(n), which becomes significant for large enough lists.Shifting in a ringbuffer is simply updating the head location which is O(1)For an immutable implementation, you could use something like this:I take this cost model as a reference:Your method of slicing the list and concatenating two sub-lists are linear-time operations. I would suggest using pop, which is a constant-time operation, e.g.:I don't know if this is 'efficient', but it also works:EDIT: Hello again, I just found a big problem with this solution!\nConsider the following code:The shift_classlist() method executes the same code as my x.insert(0,x.pop())-solution, otherlist is a list indipendent from the class. After passing the content of otherlist to the MyClass.classlist list, calling the shift_classlist() also changes the otherlist list:CONSOLE OUTPUT:I use Python 2.7. I don't know if thats a bug, but I think it's more likely that I missunderstood something here.Does anyone of you know why this happens?I have similar thing. For example, to shift by two...Another alternative:The following method is O(n) in place with constant auxiliary memory:Note that in python, this approach is horribly inefficient compared to others as it can't take advantage of native implementations of any of the pieces.for similar functionality as shift in other languages:"},
{"body": "I have terminal access to a VPS running centos 5.9 and default python 2.4.3 installed. I also installed python 2.7.3 via these commands: (I used  instead of )then I installed node.js from source via these commands:The problem is, when I use  and try to install a node.js package which requires python > 2.4.3 I get this error:how should I ?You can use  option to npm like so:or set it to be used always:Npm will in turn pass this option to node-gyp when needed.(note: I'm the one who opened an issue on Github to have this included in the docs, as there were so many questions about it ;-) )set python to python2.7 before running npm installLinux:Windows:For Windows users something like this should work:Ok, so you've found a solution already. Just wanted to share what has been useful to me so many times;I have created  alias which helps me switch python.Execute  before you run . The switch stays in effect until you quit the terminal, afterwards  is set back to system default.You can make use of this technique for any other command/tool as well."},
{"body": "Why is  faster than ? I am using CPython 3.5.2.I tried changing the power I raised by to see how it acts, and for example if I raise x to the power of 10 or 16 it's jumping from 30 to 35, but if I'm raising by  as a float, it's just moving around 24.1~4.I guess it has something to do with float conversion and powers of 2 maybe, but I don't really know.I noticed that in both cases powers of 2 are faster, I guess since those calculations are more native/easy for the interpreter/computer. But still, with floats it's almost not moving.   Python 3  objects are a full fledged object designed to support an arbitrary size; due to that fact,  (see how all variables are declared as  type in ). This also makes their exponentiation a lot more  and  since, you need to play around with the  array it uses to represent its value to perform it. ( -- See:  for more on s.) Python  objects, on the other hand,  to a C  type (by using ) and operations can be performed .  because, after checking for relevant edge-cases, it allows Python to  () to handle the actual exponentiation:where  and  are our original s as C s.The previous fact  the discrepancy between Python 2 and 3, so, I thought I'd address this comment too because it is interesting.In Python 2, you're using the old  object that differs from the  object in Python 3 (all  objects in 3.x are of  type). In 2, there's a distinction that depends on the value of the object (or, if you use the suffix ):The  you see here , it gets safely converted into a C   (The  also hints the compiler to put 'em in a register if it can do so, so that  make a difference):this allows for a good speed gain. To see how sluggish s are in comparison to s, if you wrapped the  name in a  call in Python 2 (essentially forcing it to use  as in Python 3), the speed gain disappears:Take note that, though the one snippet transforms the  to  while the other does not (as pointed out by @pydsinger), this cast is not the contributing force behind the slowdown. The implementation of  is. (Time the statements solely with  to see).This is CPython's peephole optimizer folding the constants for you. You get the same exact timings either case since there's no actual computation to find the result of the exponentiation, only loading of values:Identical byte-code is generated for  with the only difference being that the  loads the float  instead of the int .If we look at the bytecode, we can see that the expressions are purely identical. The only difference is a type of a constant that will be an argument of . So it's most certainly due to an  being converted to a floating point number down the line.Update: let's take a look at  in the CPython source code: calls , which is too long to paste here, so .It calls the  slot of , passing  as an argument.Finally, in  at line 686 of  we see that arguments are converted to a C  right before the actual operation:"},
{"body": "How to convert real numpy array to int numpy array?\nTried using map directly to array but it did not work.Use the  method.Some numpy functions for how to control the rounding: , ,, . depending how  u wish to round the floats, up, down, or to the nearest int. To make one of this in to int, or one of the  other types in numpy,  (as answered by BrenBern): you can use :If you're not sure your input is going to be a Numpy array, you can use  with  instead of :If the input array already has the correct dtype,  avoids the array copy while  does not (unless you specify ):"},
{"body": "I need help with setting the limits of y-axis on matplotlib. Here is the code that I tried, unsuccessfully.With the data I have for this plot, I get y-axis limits of 20 and 200. However, I want the limits 20 and 250.Try this . Works for subplots too .Your code works also for me. However, another workaround can be to get the plot's axis and then change only the y-values:This should work.  Your code works for me, like for Tam\u00e1s and Manoj Govindan.  It looks like you could try to update Matplotlib.  If you can't update Matplotlib (for instance if you have insufficient administrative rights), maybe using a different backend with  could help.To add to @Hima's answer, if you want to modify a current x or y limit you could use the following.I find this particularly useful when I want to zoom out or zoom in just a little from the default plot settings."},
{"body": "After you train a model in Tensorflow: I am improving my answer to add more details for saving and restoring models. In(and after) Tensorflow version 0.11:  This and some more advanced use-cases have been explained very well here.In( and After) TensorFlow version 0.11.0RC1, you can save and restore your model directly by calling  and  according to For TensorFlow version < 0.11.0RC1:The checkpoints that are saved contain values for the s in your model, not the model/graph itself, which means that the graph should be the same when you restore the checkpoint.Here's an example for a linear regression where there's a training loop that saves variable checkpoints and an evaluation section that will restore variables saved in a prior run and compute predictions. Of course, you can also restore variables and continue training if you'd like.Here are the  for s, which cover saving and restoring. And here are the  for the .There are two parts to the model, the model definition, saved by  as  in the model directory and the numerical values of tensors, saved into checkpoint files like .The model definition can be restored using , and the weights are restored using .However,  uses special collection holding list of variables that's attached to the model Graph, and this collection is not initialized using import_graph_def, so you can't use the two together at the moment (it's on our roadmap to fix). For now, you have to use approach of Ryan Sepassi -- manually construct a graph with identical node names, and use  to load the weights into it.(Alternatively you could hack it by using by using , creating variables manually, and using  for each variable, then using )You can also take this easier way.Step.1 - Initialize all your variablesStep.2 - Save the list inside Model Saver and Save itStep. 3 - Restore the modelStep. 4 - Check VariableWhile running in different python instance, useAs Yaroslav said, you can hack restoring from a graph_def and checkpoint by importing the graph, manually creating variables, and then using a Saver.I implemented this for my personal use, so I though I'd share the code here.Link: (This is, of course, a hack, and there is no guarantee that models saved this way will remain readable in future versions of TensorFlow.)If it is an internally saved model, you just specify a restorer for all variables asand use it to restore variables in a current session:For the external model you need to specify the mapping from the its variable names to your variable names. You can view the model variable names using the command The inspect_checkpoint.py script can be found in './tensorflow/python/tools' folder of the Tensorflow source.To specify the mapping, you can use my , which contains a set of classes and scripts to train and retrain different models. It includes an example of retraining ResNet models, located  In most cases, saving and restoring from disk using a  is your best option:You can also save/restore the graph structure itself (see the  for details). By default, the  saves the graph structure into a  file. You can call  to restore it. It restores the graph structure and returns a  that you can use to restore the model's state:However, there are cases where you need something much faster. For example, if you implement early stopping, you want to save checkpoints every time the model improves during training (as measured on the validation set), then if there is no progress for some time, you want to roll back to the best model.  If you save the model to disk every time it improves, it will tremendously slow down training. The trick is to save the variable states to , then just restore them later:A quick explanation: when you create a variable , TensorFlow automatically creates an assignment operation  to set the variable's initial value. Instead of creating placeholders and extra assignment ops (which would just make the graph messy), we just use these existing assignment ops. The first input of each assignment op is a reference to the variable it is supposed to initialize, and the second input () is the initial value. So in order to set any value we want (instead of the initial value), we need to use a  and replace the initial value. Yes, TensorFlow lets you feed a value for any op, not just for placeholders, so this works fine.Added the  code example.You can also check out  in , which offers  and  methods that can help you easily manage your models. It has parameters that you can also control how frequently you want to back up your model. Here's my simple solution for the two basic cases differing on whether you want to load the graph from file or build it during runtime.This answer holds for Tensorflow 0.12+ (including 1.0). Otherwise Tensorflow will make the names unique itself and they'll be thus different from the names stored in the file. It's not a problem in the previous technique, because the names are \"mangled\" the same way in both loading and saving.As described in issue :instead of If you use  as the default session, you don't need to add extra code to do save/restore things. Just pass a checkpoint dir name to MonitoredTrainingSession's constructor, it will use session hooks to handle these. "},
{"body": "Is there a library function that performs binary search on a list/tuple and return the position of the item if found and 'False' (-1, None, etc.) if not?I found the functions bisect_left/right in the , but they still return a position even if the item is not in the list. That's perfectly fine for their intended usage, but I just want to know if an item is in the list or not (don't want to insert anything).I thought of using  and then checking if the item at that position is equal to what I'm searching, but that seems cumbersome (and I also need to do bounds checking if the number can be larger than the largest number in my list). If there is a nicer method I'd like to know about it. To clarify what I need this for: I'm aware that a dictionary would be very well suited for this, but I'm trying to keep the memory consumption as low as possible. My intended usage would be a sort of double-way look-up table. I have in the table a list of values and I need to be able to access the values based on their index. And also I want to be able to find the index of a particular value or None if the value is not in the list.Using a dictionary for this would be the fastest way, but would (approximately) double the memory requirements.I was asking this question thinking that I may have overlooked something in the Python libraries. It seems I'll have to write my own code, as Moe suggested.Why not look at the code for bisect_left/right and adapt it to suit your purpose.like this:This is a little off-topic (since Moe's answer seems complete to the OP's question), but it might be worth looking at the complexity for your whole procedure from end to end.  If you're storing thing in a sorted lists (which is where a binary search would help), and then just checking for existence, you're incurring (worst-case, unless specified):Whereas with a , you're incurringThe thing a sorted list really gets you are \"next\", \"previous\", and \"ranges\" (including inserting or deleting ranges), which are O(1) or O(|range|), given a starting index.  If you aren't using those sorts of operations often, then storing as sets, and sorting for display might be a better deal overall.   incurs very little additional overhead in python.  It might be worth mentioning that the bisect docs now provide searching examples:\n(Raising ValueError instead of returning -1 or None is more pythonic \u2013 list.index() does it, for example. But of course you can adapt the examples to your needs.)Simplest is to use  and check one position back to see if the item is there:I agree that  using the bisect module is the correct approach.  He did not mention one important detail in his answer.From the   The bisection module does not require the search array to be precomputed ahead of time.  You can just present the endpoints to the  instead of it using the defaults of  and .Even more important for my use, looking for a value X such that the error of a given function is minimized.  To do that, I needed a way to have the bisect_left's algorithm call my computation instead.  This is really simple.Just provide an object that defines  as For example, we could use the bisect algorithm to find a square root with arbitrary precision!This is right from the manual:8.5.1. Searching Sorted ListsThe above bisect() functions are useful for finding insertion points but can be tricky or awkward to use for common searching tasks. The following five functions show how to transform them into the standard lookups for sorted lists:So with the slight modification your code should be:If you just want to see if it's present, try turning the list into a dict:On my machine, \"if n in l\" took 37 seconds, while \"if n in d\" took 0.4 seconds.While there's no explicit binary search algorithm in Python, there is a module -  - designed to find the insertion point for an element in a sorted list using a binary search. This can be \"tricked\" into performing a binary search. The biggest advantage of this is the same advantage most library code has - it's high-performing, well-tested and just works (binary searches in particular can be  - particularly if edge cases aren't carefully considered).For basic types like Strings or ints it's pretty easy - all you need is the  module and a sorted list:You can also use this to find duplicates:Obviously you could just return the index rather than the value at that index if desired.For custom types or objects, things are a little bit trickier: you have to make sure to implement rich comparison methods to get bisect to compare correctly.This should work in at least Python 2.7 -> 3.3This one is:Using a dict wouldn't like double your memory usage unless the objects you're storing are really tiny, since the values are only pointers to the actual objects:In that example, 'foo' is only stored once.  Does that make a difference for you?  And exactly how many items are we talking about anyway?This code works with integer lists in a recursive way. Looks for the simplest case scenario, which is: list length less than 2. It means the answer is already there and  a test is performed to check for the correct answer.\nIf not, a middle value is set and tested to be the correct, if not bisection is performed by calling again the function, but setting middle value as the upper or lower limit, by shifting it to the left or rightDave Abrahams' solution is good. Although I have would have done it minimalistic:Check out the examples on Wikipedia I guess this is much better and effective. please correct me :) . Thank youI needed binary search in python and generic for Django models. In Django models, one model can have foreign key to another model and I wanted to perform some search on the retrieved models objects. I wrote following function you can use this."},
{"body": "I'm having trouble parsing html elements with \"class\" attribute using Beautifulsoup. The code looks like thisI get an error on the same line \"after\" the script finishes. How do I get rid or this error?You can refine your search to only find those divs with a given class using BS3:From the documentation:Which in this case would be:It would also work for:Specific to BeautifulSoup 3:Will find all of these:Update: 2016\nIn the latest version of beautifulsoup, the method 'findAll' has been renamed to \n'find_all'. Hence the answer will be A straight forward way would be :Make sure you take of the casing of , its not Try to check if the div has a class attribute first, like this:This works for me to access the class attribute (on beautifulsoup 4, contrary to what the documentation says). The KeyError comes a list being returned not a dictionary.This worked for me:You can easily find by one class, but if you want to find by the intersection of two classes, it's a little more difficult,From the  (emphasis added):To be clear, this selects only the p tags that are both strikeout and body class. To find for the intersection of  in a set of classes (not the intersection, but the union), you can give a list to the  keyword argument (as of 4.1.2):Also note that findAll has been renamed from the camelCase to the more Pythonic ."},
{"body": "When I run the following command:I get this error:Any ideas how to fix this?The  says that python-ldap is based on OpenLDAP, so you need to have the development files (headers) in order to compile the Python module. If you're on Ubuntu, the package is called .::To install python-ldap successfully with pip, following development libraries are needed (package names taken from ubuntu environment):  On CentOS/RHEL 6, you need to install:and yum will also install  as a dependency.  Then you can run:In Ubuntu it looks like this :Windows: I completely agree with the accepted answer, but digging through the comments took a while to get to the meat of what I needed.  I ran across this specific problem with Reviewboard on Windows using the Bitnami.  To give an answer for windows then, I used this link mentioned in the comments:Then, executed the following commands (because I had python 2.7 and a 32bit install at that)On Fedora 22, you need to do this instead:On openSUSE you need to install the packages , ,  and .On OSX, you need the xcode CLI tools.  Just open a terminal and run:As a general solution to install Python packages with binary dependencies [1] on Debian/Ubuntu:You'll have to check the  versus PyPI. In this case they're the same.Obviously doesn't work if the Python package is not in the Ubuntu repos.[1] I learnt this trick when trying to  on Ubuntu.In FreeBSD 11:"},
{"body": "I need to add leading zeros to integer to make a string with defined quantity of digits ($cnt).\nWhat the best way to translate this simple function from PHP to Python:Is there a function that can do this?You can use the  method to pad a string with zeros:The standard way is to use format string modifiers. These format string methods are available in most programming languages (via the sprintf function in c for example) and are a handy tool to know about.which will output an string of length 5.Edit: In Python2.6 and above, there is also:See: You most likely just need to format your integer:For example,Python 2.6 allows this:You have at least two options:If on Python >2.5, see a third option in clorz's answer.For Python 3 and beyond:\nstr.zfill() is still the most readable optionBut it is a good idea to look into the new and powerful str.format(), what if you want to pad something that is not 0?A straightforward conversion would be (again with a function):This function takes  and converts it to a string, and adds zeros in the beginning only and only if the length is too short:To sum it up - build-in:  is good enough, but if someone is curious on how to implement this by hand, here is one more example.Python 3.6 f-strings allows us to add leading zeros easily:Have a look at  about.Just for the culture, on PHP, you have the function  which makes exactly the job of your function .This is my Python function:Initially, find the digits in the number of Cases /Folders. According to the length of the maximum / highest number of Cases / Folder, a format is created and added as suffix. For example. No of Cases = 9. Case_1. Case_2...Case_9 are generated. For No of Cases = 99, Case_01, Case_02...Case_99.. for 999, Case_001, Case_002....Case_999 and so on. Hope it helps"},
{"body": "How would I compare two dates to see which is later, using Python?For example, I want to check if the current date is past the last date in this list I am creating, of holiday dates, so that it will send an email automatically, telling the admin to update the holiday.txt file.Use the  method and the operator  and its kin. will return . will return . will return .see the .Use Let's say you have the initial dates as strings like these:\n\nYou can do the following:\n and  to convert them to python's date format. Then, the comparison is obvious:\n will return \n will return "},
{"body": "new to Python and had a question about dictionaries. I have a dictionary that I declared in a particular order and want to keep it in that order all the time. The keys/values can't really be kept in order based on their value, I just want it in the order that I declared it.So if I have the dictionary:It isn't in that order if I view it or iterate through it, is there any way to make sure Python will keep the explicit order that I declared the keys/values in?Using Python 2.6contains If the values are  (or any other immutable object), you can also use:Rather than explaining the theoretical part, I'll give a simple example.From Python 3.6 onwards, the standard  type maintains insertion order by default.Definingwill result in a dictionary with the keys in the order listed in the source code.This was achieved by using a simple array with integers for the sparse hash table, where those integers index into another array that stores the key-value pairs (plus the calculated hash). That latter array just happens to store the items in insertion order, and the whole combination actually uses less memory than the implementation used in Python 3.5 and before. See the  for details.This is still considered an implementation detail; future versions of Python  make it mandatory that  preserves order. See the :python dictionaries are unordered.  If you want an ordered dictionary, try .Note that OrderedDict was introduced into the standard library in python 2.7.  If you have an older version of python, you can find recipes for ordered dictionaries on .Dictionaries will use an order that makes searching efficient, and you cant change that,You could just use a list of objects (a 2 element tuple in a simple case, or even a class), and append items to the end. You can then use linear search to find items in it.Alternatively you could create or use a different data structure created with the intention of maintaining order.I came across this post while trying to figure out how to get OrderedDict to work. PyDev for Eclipse couldn't find OrderedDict at all, so I ended up deciding to make a tuple of my dictionary's key values as I would like them to be ordered. When I needed to output my list, I just iterated through the tuple's values and plugged the iterated 'key' from the tuple into the dictionary to retrieve my values in the order I needed them.example:It's a tad cumbersome, but I'm pressed for time and it's the workaround I came up with.note: the list of lists approach that somebody else suggested does not really make sense to me, because lists are ordered and indexed (and are also a different structure than dictionaries).You can't really do what you want with a dictionary. You already have the dictionary created. I found there was no way to keep in order once it is already created. What I did was make a json file instead with the object:I used:then used:to verify.Generally, you can design a class that behaves like a dictionary, mainly be implementing the methods , , ,  and some more.  That class can have any behaviour you like, for example prividing a sorted iterator over the keys ...I had a similar problem when developing a Django project. I couldn't use OrderedDict, because I was running an old version of python, so the simple solution was to use Django's SortedDict class:if you would like to have a dictionary  in a specific order, you can also create a list of lists, where the first item will be the key, and the second item will be the value\nand will look like this\nexample "},
{"body": "I recently came across this syntax, I am unaware of the difference. I would appreciate it if someone could tell me the difference.The answer is explained .To quote:Practically-speaking, there is not much difference since custom comparison operators are rare.  But you should use  as a general rule.In this case, they are the same.  is a singleton object (there only ever exists one ). checks to see if the object is the same object, while == just checks if they are equivalent.For example:But since there is only one , they will always be the same, and  will return True.If you use numpy,will give you error when numpy does elementwise comparison"},
{"body": "Is there a way to get  attributes/methods/fields/etc. of an object in Python? is  to what I want, but it doesn't work unless an object has a , which isn't always true (e.g. it's not true for a , a , etc.).Use the built-in function .What you probably want is .The catch is that classes are able to override the special  method, which causes  to return whatever the class wants (though they are encouraged to return an accurate list, this is not enforced). Furthermore, some objects may implement dynamic attributes by overriding , may be RPC proxy objects, or may be instances of C-extension classes. If your object is one these examples, they may not have a   be able to provide a comprehensive list of attributes via : many of these objects may have so many dynamic attrs it doesn't won't actually know  it has until you try to access it. In the short run, if  isn't sufficient, you could write a function which traverses  for an object, then  for all the classes in ; though this will only work for normal python objects. In the long run, you may have to use duck typing + assumptions - if it looks like a duck, cross your fingers, and hope it has .I use :"},
{"body": "I am getting the  error. Any idea how I can fix this?You need to use something like .See  for more information on iterating through dictionaries, such as using , across python versions.\nSince python 3 iteritems() is . User items() instead.For Python 3.x  has been removed. Use  instead.You want to use . This returns an iterator over the dictionary, which gives you a tuple(key, value)Your problem was that you were looping over fields, which returns the keys of the dictionary."},
{"body": "I'd like to create a file with path  using python. I've been using  where . I've looked for a non-directory version of , but I haven't been able to find anything. Is there a tool like this to create a file without opening it, or using system or popen/subprocess? There is  (). The system call to create a file is actually  with the  flag. So no matter how, you'll always open the file.So the easiest way to simply create a file without truncating it in case it exists is this:Actually you could omit the  since the refcounting GC of CPython will close it immediately after the  statement finished - but it's cleaner to do it explicitely and relying on CPython-specific behaviour is not good either.In case you want 's behaviour (i.e. update the mtime in case the file exists):You could extend this to also create any directories in the path that do not exist:Of course there IS a way to create files without opening. It's as easy as calling .  The only drawback is that this call requires root privileges on OSX."},
{"body": "Is there any reason to do anything more complicated than one of these two lines when you want to clear a list in Python?The reason I ask is that I just saw this in some running code:Clearing a list in place will affect all other references of the same list.For example, this method doesn't affect other references:But this one does:You could also do:There is a very simple way to delete a python list. Use .For example:This may be useful for large lists, but for small list it should be negligible.: As  Algorias, it doesn't  matter.Note that is equivalent toThere are two cases in which you might want to clear a list:In case 1 you just go on with the assigment:In case 2 the  statement would reduce the reference count to the list object the name  points at. If the list object is only pointed by the name  at, the reference count would be 0, and the object would be freed for garbage collection.Will delete the values of that list variable Will delete the variable itself from memory If you're clearing the list, you, obviously, don't need the list anymore. If so, you can just delete the entire list by simple del method.But in case, you need it again, you can reinitialize it. Or just simply clear its elements by To clear a list  in Python 3.3+ you can also use , which is consistent with the interfaces of other mutable containers (such as ,  or ).another solution that works fine is to create empty list as a reference empty list.for example you have a list as   . To clear it just make the following:this will make  an empty list just like the ."},
{"body": "Is there a way to widen the display of output in either interactive or script-execution mode?Specifically, I am using the describe() function on a Pandas .  When the  is 5 columns (labels) wide, I get the descriptive statistics that I want.  However, if the  has any more columns, the statistics are suppressed and something like this is returned:The \"8\" value is given whether there are 6 or 7 columns.  What does the \"8\" refer to?I have already tried dragging the IDLE window larger, as well as increasing the \"Configure IDLE\" width options, to no avail.My purpose in using Pandas and describe() is to avoid using a second program like STATA to do basic data manipulation and investigation.Thanks.Python/IDLE 2.7.3\nPandas 0.8.1\nNotepad++ 6.1.4 (UNICODE)\nWindows Vista SP2  As @bmu , pandas auto detects (by default) the size of the display area, a summary view will be used when an object repr does not fit on the display. You mentioned resizing the IDLE window, to no effect. If you do  does it fit on the IDLE window?The terminal size is determined by , this returns a tuple containing the  of the display. Does the output match the size of your IDLE window? There might be an issue (there was one before when running a terminal in emacs).Note that it is possible to bypass the autodetect,  will never switch to summary view if number of rows, columns does not exceed the given limits. is depracted. Instead, use . Like:Here is the help:Try this:From the documentation:See: You can adjust pandas print options with .However this will not work in all cases as pandas detects your console width and it will only use  if the output fits in the console (see the docstring of ). \nIn this case you can explicitly call  as answered by .With version 0.10 the way wide dataframes are printed :Further more the API for setting pandas options changed:If you want to set options temporarily for display one large df, you can use :Option values are restored automatically when you exit the  block. You can use  to force it to show the whole table.  (You can use  like this for any DataFrame.  The result of  is just a DataFrame itself.)The 8 is the number of rows in the DataFrame holding the \"description\" (because  computes 8 statistics, min, max, mean, etc.).You can set the output display to match your current terminal width:Set column max width using:This particular statement sets max width to 800px, per column.According to the , if you're running on a terminal (ie not iPython notebook, qtconsole or IDLE), it's a 2-liner to have Pandas auto-detect your screen width and adapt on the fly with how many columns it shows:"},
{"body": "I was doing a fun project: Solving a Sudoku from an input image using OpenCV (as in Google goggles etc). And I have completed the task, but at the end I found a little problem for which I came here.I did the programming using Python API of OpenCV 2.3.1.Below is what I did :And the method worked well.Check out Performing the step 4 on this image gives the result below:The red line drawn is the original contour which is the true outline of sudoku boundary.The green line drawn is approximated contour which will be the outline of warped image.Which of course, there is difference between green line and red line at the top edge of sudoku. So while warping, I am not getting the original boundary of the Sudoku. How can I warp the image on the correct boundary of the Sudoku, i.e. the red line OR how can I remove the difference between red line and green line? Is there any method for this in OpenCV?I have a solution that works, but you'll have to translate it to OpenCV yourself. It's written in Mathematica.The first step is to adjust the brightness in the image, by dividing each pixel with the result of a closing operation:The next step is to find the sudoku area, so I can ignore (mask out) the background. For that, I use connected component analysis, and select the component that's got the largest convex area:By filling this image, I get a mask for the sudoku grid:Now, I can use a 2nd order derivative filter to find the vertical and horizontal lines in two separate images:I use connected component analysis again to extract the grid lines from these images. The grid lines are much longer than the digits, so I can use caliper length to select only the grid lines-connected components. Sorting them by position, I get 2x10 mask images for each of the vertical/horizontal grid lines in the image:Next I take each pair of vertical/horizontal grid lines, dilate them, calculate the pixel-by-pixel intersection, and calculate the center of the result. These points are the grid line intersections:The last step is to define two interpolation functions for X/Y mapping through these points, and transform the image using these functions:All of the operations are basic image processing function, so this should be possible in OpenCV, too. The spline-based image transformation might be harder, but I don't think you really need it. Probably using the perspective transformation you use now on each individual cell will give good enough results.Nikie's answer solved my problem, but his answer was in Mathematica. So I thought I should give its OpenCV adaptation here. But after implementing I could see that OpenCV code is much bigger than nikie's mathematica code. And also, I couldn't find interpolation method done by nikie in OpenCV ( although it can be done using scipy, i will tell it when time comes.)Result :Result :Result :Result :Of course, this one is not so good.Result :Here, nikie does some kind of interpolation, about which I don't have much knowledge. And i couldn't find any corresponding function for this OpenCV. (may be it is there, i don't know). Check out this SOF which explains how to do this using SciPy, which I don't want to use : So, here I took 4 corners of each sub-square and applied warp Perspective to each.For that, first we find the centroids.But resulting centroids won't be sorted. Check out below image to see their order:So we sort them from left to right, top to bottom.Now see below their order :Finally we apply the transformation and create a new image of size 450x450.Result :The result is almost same as nikie's, but code length is large. May be, better methods are available out there, but until then, this works OK.Regards\nARK.You could try to use some kind of grid based modeling of you arbitrary warping. And since the sudoku already is a grid, that shouldn't be too hard.So you could try to detect the boundaries of each 3x3 subregion and then warp each region individually. If the detection succeeds it would give you a better approximation."},
{"body": "My python script uses subprocess to call a linux utility that is very noisy.  I want to store all of the output to a log file and show some of it to the user.  I thought the following would work, but the output doesn't show up in my application until the utility has produced a significant amount of output.The behavior I really want is for the filter script to print each line as it is received from the subprocess.  Sorta like what  does but with python code.What am I missing?  Is this even possible?If a  is added to fake_utility.py, the code has the desired behavior in python 3.1.  I'm using python 2.6.  You would think that using  would work the same as py3k, but it doesn't.Here is the minimal working code.It's been a long time since I last worked with Python, but I think the problem is with the statement , which reads the entire input before iterating over it. The solution is to use  instead:Of course you still have to deal with the subprocess' buffering.Note:  the solution with an iterator should be equivalent to using , except for the read-ahead buffer, but (or exactly because of this) the proposed change did produce different results for me (Python 2.5 on Windows XP).Indeed, if you sorted out the iterator then buffering could now be your problem. You could tell the python in the sub-process not to buffer its output.becomes I have needed this when calling python from within python.Bit late to the party, but was surprised not to see what I think is the simplest solution here:You want to pass these extra parameters to :Then you can iterate as in your example.  (Tested with Python 3.5)The following modification of R\u00f4mulo's answer works for me on Python 2 and 3 (2.7.12 and 3.6.1):All you need to do is run subprocess.call():\nutil.py:fake_util.py:Running ./util.py now produces output as it is generated:"},
{"body": "Which types of objects fall into the domain of \"subscriptable\"?It basically means that the object implements the  method. In other words, it describes objects that are \"containers\", meaning they contain other objects. This includes lists, tuples, and dictionaries.Off the top of my head, the following are the only built-ins that are subscriptable:But mipadi's answer is correct; any class that implements __getitem__ is subscriptableA scriptable object is an object that records the operations done to it and it can store them as a \"script\" which can be replayed.For example, see: Now, if Alistair didn't know what he asked and really meant \"subscriptable\" objects (as edited by others), then (as mipadi also answered) this is the correct one:A subscriptable object is any object that implements the  special method (think lists, dictionaries).I got this , what i was doing is So using \"[\" is causing error[Producing subscriptable error] , it should be "},
{"body": "I am looking for any resources that gives examples of Best Practices, Design patterns and the SOLID principles using Python.  Some overlap in theseAnother resource is by example at the .  A good number do not follow best practices but you can find some patterns in there that are usefulTypein a Python console.Although this is usually treated as a (fine!) joke, it contains a couple of valid python-specific axioms.Bruce Eckel's \"\" leans heavily on Design Patterns You can get started  and .  For a more in depth look at design pattners you should look at .  The source code is not in Python, but it doesn't need to be for you to understand the patterns. Something you can use to simplify your code when calling attributes on objects that might or might not exist is to use the  (to which I was introduced in ).This object just eats the lack of attribute error, and you can avoid checking for their existence.It's nothing more thanWith this, if you do  it won't explode, but just silently become the equivalent of .Normally you'd do something likeWith this, you just do:and forget about it. Beware that extensive use of the  object can potentially hide bugs in your code.You may also wish to read this  (select the .pdf file), which discusses Design Patterns in dynamic object oriented languages (i.e. Python). To quote the page:"},
{"body": "What is the best way to compare two instances of some object for equality in Python? I'd like to be able to do something likeExample:To further clarify the question. I'd like to compare by attribute values, and to make a more generic solution than Should the  method look something like this?As usual with Python, it's  :It outputs:N.B : be aware that before Python 3.0, you are more likely to use  instead of , working the same way.You override the  in your object.Implement the  method in your class; something like this:Edit: if you want your objects to compare equal if and only if they have equal instance dictionaries:As a summary : When comparing instances of objects, the  function is called.If the == operator is not working for you by default, you can always redefine the  function for the object.Edit:As has been pointed out, the  function is deprecated since 3.0.\nInstead you should use the  methods.I tried the initial example (see 7 above) and it did not work in ipython. Note that cmp(obj1,obj2) returns a \"1\" when implemented using two identical object instances. Oddly enough when I modify one of the attribute values and recompare, using  cmp(obj1,obj2) the object continues to return a \"1\".  (sigh...)Ok, so what you need to do is iterate two objects and compare each attribute using the == sign.Instance of a class when compared with == comes to non-equal. The best way is to ass the  function to your class which will do the stuff. If you want to do comparison by the content you can simply use cmp(obj1,obj2)In your case cmp(doc1,doc2) It will return -1 if the content wise they are same."},
{"body": "Model:views.py:On the template, when I call person.gender, I get 'M' or 'F' instead of 'Male' or 'Female'.\nHow to display the value ('Male' or 'Female') instead of the code(M/F)?It looks like you were on the right track -  is most certainly what you want:In templates, you don't include () in the name of a method. Do the following:you can use BooleanField instead of CharField,\n and in model:I've recently moved beyond Django, so I'm more familiar with Jinja2 templating, but here we go:Note that it is stored in the database as 'M' and 'F'Psuedo:Actual:For something more complex with more choices it might be worth writing some logic to search through the CATEGORY_CHOICES tuple of tuples.  As of now, with two choices for gender, I don't see that being the case.also see: "},
{"body": "In C# there's a  (written as ) that allows for easy (short) null checking during assignment:Is there a python equivalent?I know that I can do:But is there an even shorter way (where I don't need to repeat )?Ok, it must be clarified how the  operator works. It is a boolean operator, so it works in a boolean context. If the values are not boolean, they are converted to boolean for the purposes of the operator.Note that the  operator does not return only  or . Instead, it returns the first operand if the first operand evaluates to true, and it returns the second operand if the first operand evaluates to false.In this case, the expression  returns  if it is  or evaluates to true when converted to boolean. Otherwise, it returns . For most cases, this will serve for the very same purpose of C\u266f's null-coalescing operator, but keep in mind:If you use your variable  to hold something that is either a reference to the instance of a class or  (as long as your class does not define members  and ), it is secure to use the same semantics as the null-coalescing operator.In fact, it may even be useful to have this side-effect of Python. Since you know what values evaluates to false, you can use this to trigger the default value without using  specifically (an error object, for example).strictly,otherwise s=False will become \"default value\", which may not be what was intended.If you want to make this shorter, tryHere's a function that will return the first argument that isn't None:reduce() might needlessly iterate over all the arguments even if the first argument is not None, so you can also use this version:In addition to Juliano's answer about behavior of \"or\": \nit's \"fast\"So sometimes it's might be a useful shortcut for things likeThe two functions below I have found to be very useful when dealing with many variable testing cases. "},
{"body": "I am using python 3.2.1 and I can't import the  module. I use \n and it works but i can't use it with 's  like this:I get the following error:and when i write  it says there is no such module.From :.A possibly useful method of fixing some Python 2 code to also work in Python 3 (caveat emptor):On Python 3  expects a bytes stream. Use the following:Thank you OP for your question, and Roman for your answer. I had to search a bit to find this; I hope the following helps others.See: dtype=\"|Sx\", where x = any of { 1, 2, 3, ...}:\"The |S1 and |S2 strings are data type descriptors; the first means the array holds strings of length 1, the second of length 2. ...\"In my case I have used :You can use the  from the  module:In order to make examples from \nwork with Python 3.5.2, you can rewrite as follows :The reason for the change may be that the content of a file is in data (bytes) which do not make text until being decoded somehow.  may be a better name than .try thisfrom StringIO import StringIOx=\"1 3\\n 4.5 8\"  numpy.genfromtxt(StringIO(x))"},
{"body": "I'm making a website where users can log on and download files, using the  (based on ) which uses Python (2.6 in my case).I need to get the IP address of users when they log on (for logging purposes). \nDoes anyone know how to do this? Surely there is a way to do it with Python?See the documentation on  and then get from this same Request object, the attribute .For more information see the .Actually, what you will find is that when simply getting the following will get you the server's address: If you want the clients IP address, then use the following:Proxies can make this a little tricky, make sure to check out  () if you are using one. Take a look at request.environ in your particular environment. With nginx I will sometimes do something like this:When proxies, such as nginx, forward addresses, they typically include the original IP somewhere in the request headers. . Again, review the documentation about ProxyFix before implementing. Your solution may vary based on your particular environment.The user's IP address can be retrieved using the following snippet: uses this method:"},
{"body": "I ran into a problem while working with Selenium. For my project, I have to use Chrome. However, I can't connect to that browser after launching it with Selenium.For some reason, Selenium can't find Chrome by itself. This is what happens when I try to launch Chrome without including a path:To solve this problem, I then included the Chromium path in the code that launches Chrome. However, the interpreter fails to find a socket to connect to:I also tried solving the problem by launching chrome with:However, this did not work either.PS. Here's some information about my system:You need to make sure the standalone ChromeDriver binary (which is different than the Chrome browser binary) is either in your path or available in the webdriver.chrome.driver environment variable.see  for full information on how wire things up.Edit:Right, seems to be a bug in the Python bindings wrt reading the chromedriver binary from the path  the environment variable. Seems if chromedriver is not in your path you have to pass it in as an argument to the constructor.An easier way to get going (assuming you already have  installed, which you should, if not, go do that first and let homebrew make your life better) is to just run the following command:That should put the chromedriver in your path and you should be all set. For windows Download webdriver from:ordownload the latest chromedriver from Paste the chromedriver.exe file in  \"C:\\Python27\\Scripts\" Folder.This should work now.For windows, please have the  placed under For Windows' IDE:If your path doesn't work, you can try to add the  to your project, like in this project structure.Then you should load the  in your main file. As for me, I loaded the  in . means  upper directory means the directory where the  is locatedHope this will be helpful."},
{"body": "My code just scrapes a web page, then converts it to Unicode.But I get a :So I assume that means the HTML contains some wrongly-formed attempt at Unicode somewhere. Can I just drop whatever code bytes are causing the problem instead of getting an error?Can we get the actual value used for ?In addition, we usually encounter this problem here when we are trying to  an already encoded byte string. So you might try to decode it first as inAs an example:Fails withWhile:Succeeds without error. Do note that \"windows-1252\" is something I used as an . I got this from chardet and it had 0.5 confidence that it is right! (well, as given with a 1-character-length string, what do you expect) You should change that to the encoding of the byte string returned from  to what applies to the content you retrieved.Another problem I see there is that the  string method returns the modified string and does not modify the source in place. So it's kind of useless to have  as html is not the encoded string from html.encode (if that is what you were originally aiming for).As Ignacio suggested, check the source webpage for the actual encoding of the returned string from . It's either in one of the Meta tags or in the ContentType header in the response. Use that then as the parameter for .Do note however that it should not be assumed that other developers are responsible enough to make sure the header and/or meta character set declarations match the actual content. (Which is a PITA, yeah, I should know, I  one of those before).Decode the string you get back, using either the charset in the the appropriate  tag in the response or in the  header, then encode.As an extension to Ignacio Vazquez-Abrams' answerIt is sometimes desirable to remove accents from characters and print the base form. This can be accomplished withYou may also want to translate other characters (such as punctuation) to their nearest equivalents, for instance the RIGHT SINGLE QUOTATION MARK unicode character does not get converted to an ascii APOSTROPHE when encoding.Although there are more efficient ways to accomplish this. See this question for more details Use  - it even converts weird characters to ascii instantly, and even converts Chinese to phonetic ascii.then:I use this helper function throughout all of my projects. If it can't convert the unicode, it ignores it. This ties into a django library, but with a little research you could bypass it.I no longer get any unicode errors after using this.For broken consoles like  and HTML output you can always use:This will preserve all the non-ascii chars while making them printable in pure ASCII  in HTML.: . The only valid use case for this is printing to a non-unicode console or easy conversion to HTML entities in an HTML context. And finally, if you are on windows and use cmd.exe then you can type  to enable utf-8 output (works with Lucida Console font). You might need to add .You wrote \"\"\"I assume that means the HTML contains some wrongly-formed attempt at unicode somewhere.\"\"\"The HTML is NOT expected to contain any kind of \"attempt at unicode\", well-formed or not. It must of necessity contain Unicode characters encoded in some encoding, which is usually supplied up front ... look for \"charset\". You appear to be assuming that the charset is UTF-8 ... on what grounds? The \"\\xA0\" byte that is shown in your error message indicates that you may have a single-byte charset e.g. cp1252.If you can't get any sense out of the declaration at the start of the HTML, try using  to find out what the likely encoding is.Why have you tagged your question with \"regex\"? after you replaced your whole question with a non-question:If you have a string , you can use the  method for strings to convert encoding types.For more information about handling ASCII and unicode in Python, this is a really useful site: Works for meI think the answer is there but only in bits and pieces, which makes it difficult to quickly fix the problem such as Let's take an example, Suppose I have file which has some data in the following form ( containing ascii and non-ascii chars )1/10/17, 21:36 - Land : Welcome \u00ef\u00bf\u00bd\u00ef\u00bf\u00bdand we want to ignore and preserve only ascii characters. This code will do:and type(rline) will give you Looks like you are using python 2.x. \nPython 2.x defaults to ascii and it doesn\u2019t know about Unicode. Hence the exception.Just paste the below line after shebang, it will work"},
{"body": "Can someone explain this (straight from the - emphasis mine):Why would  and  return floats when they are by definition supposed to calculate integers?Well this got some very good arguments as to why they  return floats, and I was just getting used to the idea, when  pointed out that they in fact  return ints in Python 3...The range of floating point numbers usually exceeds the range of integers. By returning a floating point value, the functions can return a sensible value for input values that lie outside the representable range of integers.Consider: If  returned an integer, what should  return?Now, while Python's integers are now arbitrary precision, it wasn't always this way. The standard library functions are thin wrappers around the equivalent C library functions.As pointed out by other answers, in python they return floats probably because of historical reasons to prevent overflow problems. However, they return integers in python 3.You can find more information in .Because python's math library is a thin wrapper around the C math library which returns floats.The source of your confusion is evident in your comment:The point of the ceil and floor operations is to round floating-point data to .  Not to do a type conversion.  Users who need to get  values can do an explicit conversion following the operation.Note that it would not be possible to implement a round to integral value as trivially if all you had available were a ceil or float operation that returned an integer.  You would need to first check that the input is within the representable integer range, then call the function; you would need to handle NaN and infinities in a separate code path.Additionally, you must have versions of ceil and floor which return floating-point numbers if you want to conform to .Before Python 2.4, an integer couldn't hold the full range of truncated real numbers.Because the range for floats is greater than that of integers -- returning an integer could overflowThis is a very interesting question! As a float requires some bits to store the exponent (=) any floating point number greater than  will always be an integral value! At the other extreme a float with a negative exponent will give one of ,  or . This makes the discussion of  versus  moot because these functions will simply return the original number whenever the number is outside the range of the integer type.  The python functions are wrappers of the  function and so this is really a deficiency of the  functions where they should have returned an integer and forced the programer to do the range// check before calling ceil/floor.Thus the logical answer is the only time these functions are useful they would return a value within integer range and so the fact they return a float is a  and you are very smart for realizing this!Maybe because other languages do this as well, so it is generally-accepted behavior. (For good reasons, as shown in the other answers)Otherwise, you couldn't do that.If  would return integer in Python 2, this would return , not  you meant. In Python 3, it's safe, as  is always floating point division."},
{"body": "Why is 'cls' used instead of 'self'?\nAny help appreciatedThe distinction between  and  is defined in  . As Adrien said, this is not a mandatory. It's a coding style.  says:It's used in case of a class method. Check  for further details.EDIT: As clarified by Adrien, it's a convention. You can actually use anything but  and  are used ()."},
{"body": "What must I do to use my objects of a custom type as keys in a Python dictionary (where I don't want the \"object id\" to act as the key) , e.g. I'd want to use MyThing's as keys that are considered the same if name and location are the same.\nFrom C#/Java I'm used to having to override and provide an equals and hashcode method, and promise not to mutate anything the hashcode depends on.What must I do in Python to accomplish this ? Should I even ? (In a simple case, like here, perhaps it'd be better to just place a (name,location) tuple as key - but consider I'd want the key to be an object)You need to add , note  and  :The Python  defines these requirements on key objects, i.e. they must be .An alternative in Python 2.6 or above is to use  -- it saves you writing any special methods:You override  if you want special hash-semantics, and  or  in order to make your class usable as a key. Objects who compare equal need to have the same hash value.Python expects  to return an integer, returning  is not recommended :) User defined classes have  by default that calls , as you noted.There is some extra tips from the .:"},
{"body": "What event system for Python do you use? I'm already aware of , but I was wondering what else can be found, or is commonly used?I'm not interested in event managers that are part of large frameworks, I'd rather use a small bare-bones solution that I can easily extend.Wrapping up the various event systems that are mentioned in the answers here:The most basic style of event system is the 'bag of handler methods', which is a simple implementation of the .  Basically, the handler methods (callables) are stored in an array and are each called when the event 'fires'.The disadvantage of these event systems is that you can only register the handlers on the actual Event object (or handlers list).\nSo at registration time the event already needs to exist.That's why the second style of event systems exists: the .\nHere, the handlers don't register on an event object (or handler list), but on a central dispatcher.  Also the notifiers only talk to the dispatcher.  What to listen for, or what to publish is determined by 'signal', which is nothing more than a name (string).Note:  is not an 'event system' in the above sense.  It's a thread synchronization system where one thread waits until another thread 'signals' the Event object.I've been doing it this way:However, like with everything else I've seen, there is no auto generated pydoc for this, and no signatures, which really sucks.We use an EventHook as suggested from Michael Foord in his :Just add EventHooks to your classes with:We add the functionality to remove all listener from an object to Michaels class and ended up with this:I use . It's the most bare bones you can imagine. :-)\nIn fact, here is the complete source code:Note that you can't send messages between processes, for example. It's not a messaging system, just an event system, nothing more, nothing less.I found this small script on . It seems to have just the right simplicity/power ratio I'm after. Peter Thatcher is the author of following code (no licensing is mentioned).You may have a look at  (). Its a small single-file (~250 loc) approach\n\"providing namespaces, wildcards and TTL\".Here's a basic example:I made a variation of Longpoke's minimalistic approach that also ensures the signatures for both callees and callers:I created an  class (code at the end). The syntax is the following:Here is an Example:Output:EventManger Code:Here is a minimal design that should work fine. What you have to do is to simply inherit  in a class and afterwards use  to listen for a specific event. Whenever that specific event is fired anywhere in the code (ie. ), the corresponding callback will fire.Example:If I do code in pyQt I use QT sockets/signals paradigm, same is for djangoIf I'm doing async I/O I use native select moduleIf I'm usign a SAX python parser I'm using event API provided by SAX. So it looks like I'm victim of underlying API :-)Maybe you should ask yourself what do you expect from event framework/module. My personal preference is to use Socket/Signal paradigm from QT. more info about that can be found Here's another  for consideration. It seems a viable choice for more demanding applications."},
{"body": "I am using PyCharm on Windows. I am not getting the settings to change the maximum line length to 79 characters. I see that it is 120 characters by default. Where can I change it to 79 characters? I searched in the settings but didn't find.Here is screenshot of my Pycharm. Required settings is in following path: For File >> Settings >> Editor >> Code Style: Right margin (columns) Take a look at other options in that tab, they're very helpfulYou can even set a separate  for HTML. Under the specified path:File >> Settings >> Editor >> Code Style >> HTML >> Other Tab >> Right margin (columns)This is very useful because generally HTML and JS may be usually long in one line than Python. :) "},
{"body": "I have a dataframe df :Then I want to drop rows with certain sequence numbers which indicated in a list, suppose here is  then left:How or what function can do that ?Use  and pass it a Series of index labels:Note that it may be important to use the \"inplace\" command when you want to do the drop in line. Because your original question is not returning anything, this command should be used.\nYou can also pass to  the  (instead of Series of index labels):Which is equivalent to:If the DataFrame is huge, and the number of rows to drop is large as well, then simple drop by index  takes too much time. In my case, I have a multi-indexed DataFrame of floats with , and I need to remove  rows from it. The fastest method I found is, quite counterintuitively, to  the remaining rows.Let  be an array of positional indexes to drop ( in the question).In my case this took , while the simple  took  and consumed a lot of memory. The resulting DataFrame is the same.In a comment to @theodros-zelleke's answer, @j-jones asked about what to do if the index is not unique.  I had to deal with such a situation.  What I did was to rename the duplicates in the index before I called , a la:where  is a function I defined that went through the elements of index and renamed the duplicates.  I used the same renaming pattern as  uses on columns, i.e., , where  is the name of the row and  is how many times it has occurred previously."},
{"body": "Reading the Python cookbook at the minute and currently looking at generators. I'm finding it hard to get my head round.As I come from a Java background, is there a Java equivalent? The book was speaking about 'Producer / Consumer', however when I hear that I think of threading.Can anyone explain what a generator is and why you would use it? Without quoting any books, obviously (unless you can find a decent, simplistic answer direct from a book). Perhaps with examples, if you're feeling generous!A  is simply a function which returns an object on which you can call , such that for every call it returns some value, until it raises a  exception, signaling that all values have been generated. Such an object is called an .Normal functions return a single value using , just like in Java. In Python, however, there is an alternative, called . Using  anywhere in a function makes it a generator. Observe this code:As you can see,  is a function which yields  and . Every call to  yields a single value, until all values have been yielded.  loops call  in the background, thus:Likewise there are , which provide a means to succinctly describe certain common types of generators:Note that generator expressions are much like :Observe that a generator object is generated , but its code is  run all at once. Only calls to  actually execute (part of) the code. Execution of the code in a generator stops once a  statement has been reached, upon which it returns a value. The next call to  then causes execution to continue in the state in which the generator was left after the last . This is a fundamental difference with regular functions: those always start execution at the \"top\" and discard their state upon returning a value.There are more things to be said about this subject. It is e.g. possible to  data back into a generator (). But that is something I suggest you do not look into until you understand the basic concept of a generator.Now you may ask: why use generators? There are a couple of good reasons:\u00a0\u00a0  in the above examples  is a function which calls the method  on the given object. In Python <=2.6 one uses a slightly different technique, namely  instead of . Python 2.7 has  call  so you need not use the following in 2.7:A generator is effectively a function that returns (data) before it is finished, but it pauses at that point, and you can resume the function at that point.and so on.  The (or one) benefit of generators is that because they deal with data one piece at a time, you can deal with large amounts of data; with lists, excessive memory requirements could become a problem.   Generators, just like lists, are iterable, so they can be used in the same ways:Note that generators provide another way to deal with infinity, for exampleThe generator encapsulates an infinite loop, but this isn't a problem because you only get each answer every time you ask for it.Generators could be thought of as shorthand for creating an iterator.  They behave like a Java Iterator.  Example:Hope this helps/is what you are looking for.As many other answers are showing, there are different ways to create a generator.  You can use the parentheses syntax as in my example above, or you can use yield.  Another interesting feature is that generators can be \"infinite\" -- iterators that don't stop:First of all the term  originally was somewhat ill-defined in Python, leading to lots of confusion. What you probably mean are  and  (see ). Then in Python there are also  (which return a generator object),  (which are iterators) and  (which are evaluated to a generator object). According to  it seems that the official terminology is now that  is short for \"generator function\". In the past the documentation defined the terms inconsistently, but fortunately this has been fixed.It might still be a good idea to be precise and avoid the term \"generator\" without further specification.There is no Java equivalent.Here is a bit of a contrived example:There is a loop in the generator that runs from 0 to n, and if the loop variable is a multiple of 3, it yields the variable.During each iteration of the for loop the generator is executed. If it is the first time the generator executes, it starts at the beginning, otherwise it continues from the previous time it yieldedThe only thing I can add to Stephan202's answer is a recommendation that you take a look at David Beazley's PyCon '08 presentation \"Generator Tricks for Systems Programmers,\" which is the best single explanation of the how and why of generators that I've seen anywhere.  This is the thing that took me from \"Python looks kind of fun\" to \"This is what I've been looking for.\"  It's at .It helps to make a clear distinction between the function foo, and the generator foo(n):foo is a function. \nfoo(6) is a generator object.The typical way to use a generator object is in a loop:The loop printsThink of a generator as a resumable function.  behaves like  in the sense that values that are yielded get \"returned\" by the generator. Unlike return, however, the next time the generator gets asked for a value, the generator's function, foo, resumes where it left off -- after the last yield statement -- and continues to run until it hits another yield statement. Behind the scenes, when you call  the generator object bar is defined for you to have a  attribute.You can call it yourself to retrieve values yielded from foo:When foo ends (and there are no more yielded values), calling  throws a StopInteration error.I like to describe generators, to those with a decent background in programming languages and computing, in terms of stack frames.In many languages, there is a stack on top of which is the current stack \"frame\".  The stack frame includes space allocated for variables local to the function including the arguments passed in to that function.  When you call a function, the current point of execution (the \"program counter\" or equivalent) is pushed onto the stack, and a new stack frame is created.  Execution then transfers to the beginning of the function being called.With regular functions, at some point the function returns a value, and the stack is \"popped\".  The function's stack frame is discarded and execution resumes at the previous location.When a function is a generator, it can return a value  the stack frame being discarded, using the yield statement.  The values of local variables and the program counter within the function are preserved.  This allows the generator to be resumed at a later time, with execution continuing from the yield statement, and it can execute more code and return another value.Before Python 2.5 this was all generators did.  Python 2.5 added the ability to pass values back  to the generator as well.  In doing so, the passed-in value is available as an expression resulting from the yield statement which had temporarily returned control (and a value) from the generator.The key advantage to generators is that the \"state\" of the function is preserved, unlike with regular functions where each time the stack frame is discarded, you lose all that \"state\".  A secondary advantage is that some of the function call overhead (creating and deleting stack frames) is avoided, though this is a usually a minor advantage.I believe the first appearance of iterators and generators were in the Icon programming language, about 20 years ago.You may enjoy , which lets you wrap your head around them without concentrating on the syntax (since Icon is a language you probably don't know, and Griswold was explaining the benefits of his language to people coming from other languages).After reading just a few paragraphs there, the utility of generators and iterators might become more apparent.This post will use  as a tool to build up to explaining the usefulness of . This post will feature both C++ and Python code. Fibonacci numbers are defined as the sequence: 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, ....Or in general: This can be transferred into a C++ function extremely easily: But if you want to print the first 6 Fibonacci numbers, you will be recalculating a lot of the values with the above function.  For example: , but  also recalculates .  The higher the value you want to calculate, the worse off you will be. So one may be tempted to re-write the above by keeping track of the state in .But this is very ugly, and it complicates our logic in , it would be better to not have to worry about state in our  function.We could return a  of values and use an  to iterate over that set of values, but this requires a lot of memory all at once for a large number of return values. So back to our old approach, what happens if we wanted to do something else besides print the numbers? We'd have to copy and paste the whole block of code in  and change the output statements to whatever else we wanted to do.\nAnd if you copy and paste code, then you should be shot.  You don't want to get shot do you?To solve these problems, and to avoid getting shot, we may re-write this block of code using a callback function.  Every time a new Fibonacci number is encountered, we would call the callback function.This is clearly an improvement, your logic in  is not as cluttered, and you can do anything you want with the Fibonacci numbers, simply define new callbacks.But this is still not perfect.  What if you wanted to only get the first 2 Fibonacci numbers, and then do something, then get some more, then do something else. Well we could go on like we have been, and we could start adding state again into , allowing GetFibNumbers to start from an arbitrary point.\nBut this will further bloat our code, and it already looks too big for a simple task like printing Fibonacci numbers.We could implement a producer and consumer model via a couple threads.  But this complicates the code even more.Instead let's talk about generators.Python has a very nice language feature that solves problems like these called generators. A generator allows you to execute a function, stop at an arbitrary point, and then continue again where you left off.\nEach time returning a value.Consider the following code that uses a generator: Which gives us the results:The  statement is used in conjuction with Python generators.  It saves the state of the function and returns the yeilded value.  The next time you call the next() function on the generator, it will continue where the yield left off.This is by far more clean than the callback function code.  We have cleaner code, smaller code, and not to mention much more functional code (Python allows arbitrarily large integers)."},
{"body": "This is a question I have wondered about for quite some time, yet I have never found a suitable solution. If I run a script and I come across, let's say an IndexError, python prints the line, location and quick description of the error and exits. Is it possible to automatically start pdb when an error is encountered? I am not against having an extra import statement at the top of the file, nor a few extra lines of code.You can use  to print the exceptions traceback. Then use  to extract the traceback and finally call  with that tracebackIf you want to start an interactive command line with  using the locals of the frame where the exception originated you can doIf you don't provide the  flag then you'll need to enter 'c' (for Continue) when execution begins. Then it will run to the error point and give you control there.Use the following module:Name it  (or whatever you like) and put it somewhere in your python path.Now, at the start of your script, just add an .This isn't the debugger, but probably just as useful(?)I know I heard Guido mention this in a speech somewhere.I just checked python -?, and if you use the -i command you can interact where your script stopped.So given this script:You can get this output!To be honest I haven't used this, but I should be, seems very useful. has a command for toggling this behavior: . It does exactly what you described, maybe even a bit more (giving you more informative backtraces with syntax highlighting and code completion). It's definitely worth a try!IPython makes this simple on the command line:can be rewritten toOr, similarly, if calling a module:can be rewritten toNote the  to stop IPython from reading the script's arguments as its own.This also has the advantage of invoking the enhanced IPython debugger (ipdb) instead of pdb.You can put this line in your code:More info: To have it run without having to type c at the beginning use:Pdb has its own command line arguments: -c c will execute c(ontinue) command at start of execution and the program will run uninterrupted until the error.Put a breakpoint inside the constructor of topmost exception class in the hierarchy, and most of the times you will see where the error was raised.Putting a breakpoint means whatever you want it to mean : you can use an IDE, or , or whateverIf you are running a module:And now you want to enter  when an exception occurs, do this:(or  your ). The  is needed so that the module is found in the path, since you are running the  module now.If you are using the IPython environment, you can just use the %debug and the shell will take you back to the offending line with the ipdb environment for inspections etc. Another option as pointed above is to use the iPython magic %pdb which effectively does the same."},
{"body": "Is there a way to set up a global variable inside of a module? When I tried to do it the most obvious way as appears below, the Python interpreter said the variable  did not exist.And after importing the module in a different fileAnd the traceback was: Any ideas? I'm trying to set up a singleton by using a module, as per  recommendation.Here is what is going on.First, the only global variables Python really has are module-scoped variables.  You cannot make a variable that is truly global; all you can do is make a variable in a particular scope.  (If you make a variable inside the Python interpreter, and then import other modules, your variable is in the outermost scope and thus global within your Python session.)All you have to do to make a module-global variable is just assign to a name.Imagine a file called foo.py, containing this single line:Now imagine you import it.However, let's suppose you want to use one of your module-scope variables as a global inside a function, as in your example.  Python's default is to assume that function variables are local.  You simply add a  declaration in your function, before you try to use the global.By the way, for this example, the simple  test is adequate, because any string value other than an empty string will evaluate true, so any actual database name will evaluate true.  But for variables that might contain a number value that might be 0, you can't just say ; in that case, you should explicitly test for  using the  operator.  I modified the example to add an explicit  test.  The explicit test for  is never wrong, so I default to using it.Finally, as others have noted on this page, two leading underscores signals to Python that you want the variable to be \"private\" to the module.  If you ever do an , Python will not import names with two leading underscores into your name space.  But if you just do a simple  and then say  you will see the \"private\" variables in the list, and if you explicitly refer to  Python won't care, it will just let you refer to it.  The double leading underscores are a major clue to users of your module that you don't want them rebinding that name to some value of their own.It is considered best practice in Python not to do , but to minimize the coupling and maximize explicitness by either using  or by explicitly doing an import like .EDIT: If, for some reason, you need to do something like this in a very old version of Python that doesn't have the  keyword, there is an easy workaround.  Instead of setting a module global variable directly, use a mutable type at the module global level, and store your values inside it.In your functions, the global variable name will be read-only; you won't be able to rebind the actual global variable name.  (If you assign to that variable name inside your function it will only affect the local variable name inside the function.)  But you can use that local variable name to access the actual global object, and store data inside it.You can use a  but your code will be ugly:A  is better.  But the most convenient is a class instance, and you can just use a trivial class:(You don't really need to capitalize the database name variable.)I like the syntactic sugar of just using  rather than ; it seems the most convenient solution in my opinion.  But the  solution works fine also.With a  you can use any hashable value as a key, but when you are happy with names that are valid identifiers, you can use a trivial class like  in the above. The technique described here is the same as in , , that no artificial helper object is created to explicitly scope variables. .Think of it like  for the  instead of the current instance !, you can import  as often on as many clients as you want, manipulating the same, universal state:I have used this recipe to create simple and lightweight    ever since I found it. As an additional bonus I find it quite pythonic overall as it nicely fits Pythons policy of .Steveha's answer was helpful to me, but omits an important point (one that I think wisty was getting at). The global keyword is not necessary if you only access but do not assign the variable in the function.If you assign the variable without the global keyword then Python creates a new local var -- the module variable's value will now be hidden inside the function.  Use the global keyword to assign the module var inside a function.Pylint 1.3.1 under Python 2.7 enforces NOT using global if you don't assign the var.For this, you need to declare the variable as global. However, a global variable is also accessible from  the module by using . Add this as the first line of your module:You are falling for a subtle quirk. You cannot re-assign module-level variables inside a python function. I think this is there to stop people re-assigning stuff inside a function by accident. You can access the module namespace, you just shouldn't try to re-assign. If your function assigns something, it automatically becomes a function variable - and python won't look in the module namespace.You can do:but you cannot re-assign  inside a function. One workaround:Note, I'm not re-assigning , I'm just modifying its contents."},
{"body": "How do I retrieve the exit code when using Python's  module and the  method?Relevant code:Should I be doing this another way? will set the  attribute when it's done(*). Here's the relevant documentation section:So you can just do (I didn't test it but it should work):(*) This happens because of the way it's implemented: after setting up threads to read the child's streams, it just calls . You should first make sure that the process has completed running and the return code has been read out using the  method. This will return the code. If you want access to it later, it's stored as  in the  object. . The child process will be blocked If it writes to standard output/error, and/or reads from standard input, and there are no peers."},
{"body": "I'm new to Python, so this is probably a simple scoping question. The following code in a Python file (module) is confusing me slightly:In other languages I've worked in, this code would throw an exception, as the  variable is local to the  statement and should not exist outside of it. But this code executes, and prints 1. Can anyone explain this behavior? Are all variables created in a module global/available to the entire module?Python variables are scoped to the innermost function or module; control blocks like  and  blocks don't count. (IIUC, this is also how JavaScript's -declared variables work.)Scope in python follows this order:()Notice that  and other looping/branching constructs are not listed - only classes, functions, and modules provide scope in Python, so anything declared in an  block has the same scope as anything decleared outside the block. Variables aren't checked at compile time, which is why other languages throw an exception. In python, so long as the variable exists at the time you require it, no exception will be thrown.Yes, they're in the same \"local scope\", and actually code like this is common in Python:Note that  isn't declared or initialized before the condition, like it would be in C or Java, for example.Unlike languages such as C, a Python variable is in scope for the whole of the function (or class, or module) where it appears, not just in the innermost \"block\".  It is as though you declared  at the top of the function (or class, or module), except that in Python you don't have to declare variables.Note that the existence of the variable  is checked only at runtime -- that is, when you get to the  statement.  If  didn't equal  then you would get an exception: .As Eli said, Python doesn't require variable declaration. In C you would say:but in Python declaration is implicit, so when you assign to x it is automatically declared. It's because Python is dynamically typed - it wouldn't work in a statically typed language, because depending on the path used, a variable might be used without being declared. This would be caught at compile time in a statically typed language, but with a dynamically typed language it's allowed.The only reason that a statically typed language is limited to having to declare variables outside of  statements in because of this problem. Embrace the dynamic!Yes. It is also true for  scope. But not functions of course.In your example: if the condition in the  statement is false,  will not be defined though.you're executing this code from command line therefore  conditions is true and  is set. Compare:"},
{"body": "I want to send a value for  while requesting a webpage using Python Requests.  I am not sure is if it is okay to send this as a part of the header, as in the code below:The debug information isn't showing the headers being sent during the request.Is it acceptable to send this information in the header?  If not, how can I send it?The  should be specified as a field in the header.Here is a , and you'd probably be interested in , which includes .The simplest way to do what you want is to create a dictionary and specify your headers directly, like so:The above clobbers all the default headers that  specifies, which, as of version , is specified in the source code as:If you want to use the default headers and combine it with your own, you can do this:"},
{"body": "I'm using Python's logging module to log some debug strings to a file which works pretty well. Now in addition, I'd like to use this module to also print the strings out to stdout. How do I do this? In order to log my strings to a file I use following code:and then call a logger function likeThank you for some help here!Just get a handle to the root logger and add the StreamHandler. The StreamHandler writes to stderr. Not sure if you really need stdout over stderr, but this is what I use when I setup the Python logger and I also add the FileHandler as well. Then all my logs go to both places (which is what it sounds like you want).You could also add a Formatter to it so all your log lines have a common header.ie:Prints to the format of:Adding a StreamHandler without arguments goes to stderr instead of stdout. If some other process has a dependency on the stdout dump (i.e. when writing an NRPE plugin), then make sure to specify stdout explicitly or you might run into some unexpected troubles.Here's a quick example reusing the assumed values and LOGFILE from the question:Either run  with  as the argument prior to setting up any other handlers or logging any messages, or manually add a  that pushes messages to stdout to the root logger (or any other logger you want, for that matter).For 2.7, try the following:  "},
{"body": "How would I create a list with values between 2 values I put in? \nFor example, following list is generated for values between 11 and 16:Use . In Python 2.x it returns a list so all you need is,In Python 3.x it's a iterator so you need to convert it to a list,: Second number is exclusive so here it needs to be  = EDIT:To response to the question about incrementing by  the easiest option would probably be to use 's ,You seem to be looking for :For incrementing by  instead of , say:Try:That is a list in Python 2.x and behaves mostly like a list in Python 3.x.  If you are running Python 3 and need a list that you can modify, then use: Use list comprehension in python. Since you want 16 in the list too.. Use x2+1. Range function excludes the higher limit in the function.assuming you want to have a range between x to yuse list for 3.x supportIf you are looking for range like function which works for float type, then here is a very good .Output:"},
{"body": "This seems rather obvious, but I can't seem to figure out how do I convert an index of data frame to a column?For example:To,either:or, :so, if you have a multi-index frame with 3 levels of index, like:and you want to convert the 1st () and 3rd () levels in the index into columns, you would do:For MultiIndex you can extract its subindex using where  is the name of the subindex."},
{"body": "I am getting a response from the rest is an Epoch time format likeI want to convert that epoch seconds in MySQL format time so that I could store the differences in my MySQL database.I tried:The above result is not what I am expecting. I want it be like Please suggest how can I achieve this?Also,\nWhy I am getting  forTo convert your time value (float or int) to a formatted string, use:You can also use :This is what you needPlease input a  instead of an  and that other  should go away.Try this:Also in MySQL, you can  like:For your 2nd question, it is probably because  is a string.  You can convert it to numeric like: This is a little more wordy but it comes from date command in unix.First a bit of info in epoch from to understand how epoch should be.just ensure the arg you are passing to  is integer."},
{"body": "In python, is there a difference between calling  and assigning  to a dictionary? If yes, what is it?\nExample:If you have another variable also referring to the same dictionary, there is a big difference:This is because assigning  creates a new, empty dictionary and assigns it to the  variable. This leaves  pointing at the old dictionary with items still in it. However,  clears the same dictionary that  and  both point at. will create a new instance for  but all other references will still point to the old contents. \n will reset the contents, but all references to the same instance will still be correct.In addition to the differences mentioned in other answers, there also is a speed difference.  d = {} is over twice as fast:In addition to @odano 's answer, it seems using  is faster if you would like to clear the dict for many times.The result is:As an illustration for the things already mentioned before:One thing not mentioned is scoping issues. Not a great example, but here's the case where I ran into the problem:The solution is to replace  with If someone thinks up a more practical example, feel free to edit this post. methods are always useful if the original object is not in scope:Re-assigning the dictionary would create a new object and wouldn't modify the original one."},
{"body": "I know that I can use something like  to get a substring in Python, but what does the 3 mean in ?it means 'nothing for the first argument, nothing for the second, and jump by three'. It gets every third item of the sequence sliced.\n is what you want. New in Python 2.3Python sequence slice addresses can be written as a[start:end:step] and any of start, stop or end can be dropped.   is every third element of the sequence. is a sequence of each -th item in the entire sequence.Example:The syntax is:So you can do: is, , \"slice of s from i to j with step k\".  When  and  are absent, the whole sequence is assumed and thus  means \"every k-th item\".First, let's initialize a list:Let's take every 3 item from :Let's take every 3 item from :Let's take every 3 item from :Let's take every 3 item from :When slicing in Python the third parameter is the step. As others mentioned, see  for a nice overview.With this knowledge,  just means that you have not specified any start or end indices for your slice. Since you have specified a step, , this will take every third entry of  starting at the first index. For example:The third parameter is the step.  So [::3] would return every 3rd element of the list/string.Python uses the :: to separate the End, the Start, and the Step value.It depends on what class you are using it on exactly because:We can then open up slice objects as:This is notably used in Numpy to slice multi-dimensional arrays in any direction.Of course, any sane API should use  with the usual \"every 3\" semantic.This visual example will show you how to a neatly select elements in a NumPy Matrix (2 dimensional array) in a pretty entertaining way (I promise). (Caution: this is a NumPy array specific example with the aim of illustrating the a use case of \"double colons\"  for jumping of elements in multiple axes. This example does not cover native Python data structures like ).Say we have a NumPy matrix that looks like this:Say for some reason, your boss wants you to select the following elements:\"But How???\"... Read on! (We can do this in a 2-step approach)Specify the \"start index\" and \"end index\" in both row-wise and column-wise directions.In code:Notice now we've just obtained our subset, with the use of simple start and end indexing technique. Next up, how to do that \"jumping\"... (read on!)We can now specify the \"jump steps\" in both row-wise and column-wise directions (to select elements in a \"jumping\" way) like this:In code (note the double colons):We have just selected all the elements as required! :)Now we know the concept, we can easily combine step 1 and step 2 into one consolidated step - for compactness:Done!"},
{"body": "I am creating a program that will download a .jar (java) file from a web server, by reading the URL that is specified in the .jad file of the same game/application. I'm using Python 3.2.1I've managed to extract the URL of the JAR file from the JAD file (every JAD file contains the URL to the JAR file), but as you may imagine, the extracted value is type() string. Here's the relevant function:However I always get an error saying that the type in the function above has to be bytes, and not string. I've tried using the URL.encode('utf-8'), and also bytes(URL,encoding='utf-8'), but I'd always get the same or similar error.So basically my question is how to download a file from a server when the URL is stored in a string type?If you want to obtain the contents of a web page into a variable, just  the response of :The easiest way to download and save a file is to use the  function:But keep in mind that  is considered  and might become deprecated (not sure why, though).So the most  way to do this would be to use the  function to return a file-like object that represents an HTTP response and copy it to a real file using .If this seems too complicated, you may want to go simpler and store the whole download in a  object and then write it to a file. But this works well only for small files.It is possible to extract  (and maybe other formats) compressed data on the fly, but such an operation probably requires the HTTP server to support random access to the file.I use  package whenever I want something related to HTTP requests because its API is very easy to start with:first, install then the code:I hope I understood the question right, which is:  how to download a file from a server when the URL is stored in a string type?I download files and save it locally using the below code:"},
{"body": "I was going through the tensorflow api docs . In tensorflow docs they used a keyword called . What is it? In a lot of methods in the api docs it is written like, Now what it is written is that  are only . Well why keep a different name like ? I almost thought that it was . . Another thing is that there are two methods i could not differentiate. They wereWhat are the differences between them? The docs are not clear to me. I know what  does. But not the other. An example will be really helpful.Logits simply means that the function operates on the unscaled output of earlier layers and that the relative scale to understand the units is linear.  It means, in particular, the sum of the inputs may not equal 1, that the values are  probabilities (you might have an input of 5). produces just the result of applying the  to an input tensor.  The softmax \"squishes\" the inputs so that sum(input) = 1;  it's a way of normalizing.  The shape of output of a softmax is the same as the input - it just normalizes the values.  The outputs of softmax  be interpreted as probabilities.In contrast,  computes the cross entropy of the result after applying the softmax function (but it does it all together in a more mathematically careful way).  It's similar to the result of:The cross entropy is a summary metric - it sums across the elements.  The output of  on a shape  tensor is of shape  (the first dimension is treated as the batch).If you want to do optimization to minimize the cross entropy, AND you're softmaxing after your last layer, you should use  instead of doing it yourself, because it covers numerically unstable corner cases in the mathematically right way.  Otherwise, you'll end up hacking it by adding little epsilons here and there.(Edited 2016-02-07: If you have single-class labels, where an object can only belong to one class, you might now  consider using  so that you don't have to convert your labels to a dense one-hot array.  This function was added after release 0.6.0.)Suppose you have two tensors, where  contains computed scores for each class (for example, from y = W*x +b) and  contains one-hot encoded true labels. If you interpret the scores in  as unnormalized log probabilities, then they are .Additionally, the total cross-entropy loss computed in this manner:is essentially equivalent to the total cross-entropy loss computed with the function :In the output layer of your neural network, you will probably compute an array that contains the class scores for each of your training instances, such as from a computation . To serve as an example, below I've created a  as a 2 x 3 array, where the rows correspond to the training instances and the columns correspond to classes. So here there are 2 training instances and 3 classes.Note that the values are not normalized (i.e. the rows don't add up to 1). In order to normalize them, we can apply the softmax function, which interprets the input as unnormalized log probabilities (aka ) and outputs normalized linear probabilities. It's important to fully understand what the softmax output is saying. Below I've shown a table that more clearly represents the output above. It can be seen that, for example, the probability of training instance 1 being \"Class 2\" is 0.619. The class probabilities for each training instance are normalized, so the sum of each row is 1.0.So now we have class probabilities for each training instance, where we can take the argmax() of each row to generate a final classification. From above, we may generate that training instance 1 belongs to \"Class 2\" and training instance 2 belongs to \"Class 1\". Are these classifications correct? We need to measure against the true labels from the training set. You will need a one-hot encoded  array, where again the rows are training instances and columns are classes. Below I've created an example  one-hot array where the true label for training instance 1 is \"Class 2\" and the true label for training instance 2 is \"Class 3\".Is the probability distribution in  close to the probability distribution in ? We can use  to measure the error.We can compute the cross-entropy loss on a row-wise basis and see the results. Below we can see that training instance 1 has a loss of 0.479, while training instance 2 has a higher loss of 1.200. This result makes sense because in our example above,  showed that training instance 1's highest probability was for \"Class 2\", which matches training instance 1 in ; however, the prediction for training instance 2 showed a highest probability for \"Class 1\", which does not match the true class \"Class 3\".What we really want is the total loss over all the training instances. So we can compute:We can instead compute the total cross entropy loss using the  function, as shown below. Note that  and  produce essentially equivalent results with some small differences in the very final digits. However, you might as well use the second approach: it takes one less line of code and accumulates less numerical error because the softmax is done for you inside of .tf.nn.softmax computes the forward propagation through a softmax layer. You use it during evaluation of the model when you compute the probabilities that the model outputs.tf.nn.softmax_cross_entropy_with_logits computes the cost for a softmax layer. It is only used during training. The logits are the unnormalized log probabilities output the model (the values output before the softmax normalization is applied to them)."},
{"body": "I'm currently working on a computation in python shell. What I want to have is Matlab style listout where you can see all the variables that have been defined up to a point (so I know which names I've used, their values and such).Is there a way, and how can I do that?Couple of things you could use:If this is an option for you, you might want to look at :To get a list of all currently defined variables, type :You can type  for more detail:There are a wealth of other functions available - basically it is the Python interpreter on steroids. One convenient one is  command, which lets you save variables between sessions (kind of like a super easy )If you have a few minutes, check out I am in no way associated with the team behind IPython, just a satisfied user.To get the names:To get the values:vars() also takes an optional argument to find out which vars are defined within an object itself. As RedBlueThing and analog said:Using the interactive shell (version 2.6.9), after creating variables  and , running  givesrunning  givesRunning  gives exactly the same answer as  in this case.I haven't gotten into any modules, so all the variables are available as both local and global variables.   and  list the values of the variables as well as the names;  only lists the names.If I import a module and run  or  inside the module,  still gives only a small number of variables; it adds  to the variables listed above.  and  also list the same variables, but in the process of printing out the dictionary value for , it lists a far larger number of variables: built-in functions, exceptions, and types such as \"\", rather than just the brief  as shown above.For more about  see  at New Mexico Tech or  at ibiblio.org.For more about  and  see  at Dive Into Python and a page about  at New Mexico Tech.[Comment:  @Kurt:  You gave a link to  but that answer has a mistake in it. The problem there is:   in that example will always return . You do get a list of the variables, which answers the question, but with incorrect types listed beside them.  This was not obvious in your example because all the variables happened to be strings anyway; however, what it's returning is the type of the name of the variable instead of the type of the variable.  To fix this: instead of  use . I apologize for posting a comment in the answer section but I don't have comment posting privileges, and the other question is closed.]print locals()edit continued from comment.To make it look a little prettier when printing:That should give you a more vertical printout.keep in mind dir() will return all current imports,  variables.if you just want your variables, I would suggest a naming scheme that is easy to extract from dir, such as varScore, varNames, etc.that way, you can simply do this:if you want to list all variables, but exclude imported modules and variables such as:you can use something like so:as you can see, it will show the variable \"imports\" though, because it is a variable (well, a tuple). A quick workaround is to add the word \"imports\" into the imports tuple itself!In my Python 2.7 interpreter, the same  command that exists in MATLAB exists in Python. It shows the same details as the MATLAB analog (variable name, type, and value/data).In the Python interpreter,  lists all variables in the \"interactive namespace\".globals(), locals(), vars(), and dir() may all help you in what you want.a bit more smart (python 3) way:This has to be defined in the interactive shell.import os, a = 10, import sys, MyWho()  then gives['a', 'MyWho', 'sys', 'os']"},
{"body": "According to :However, if I define two files:and:when i run  it prints out  without giving any exception. Is diveintopython wrong, or did I misunderstand something? And is there some way to  define a module's function as private?In Python, \"privacy\" depends on \"consenting adults'\" levels of agreement - you can't  it (any more than you can in real life;-).  A single leading underscore means you're not  to access it \"from the outside\" --  leading underscores (w/o trailing underscores) carry the message even more forcefully... but, in the end, it still depends on social convention and consensus: Python's introspection is forceful enough that you can't  every other programmer in the world to respect you wish.((Btw, though it's a closely held secret, much the same holds for C++: with most compilers, a simple  line before ing your  file is all it takes for wily coders to make hash of your \"privacy\"...!-))There may be confusion between  and .A  starts with \nSuch a element is not copied along when using the  form of the import command;  it is however imported if using the  syntax  ()\nSimply remove one underscore from the a.__num of the question's example and it won't show in modules that import a.py using the  syntax.A  starts with   (aka dunder i.e. d-ouble under-score) \nSuch a variable has its name \"mangled\" to include the classname etc.\nIt can still be accessed outside of the class logic, through the mangled name.\nAlthough the name mangling can serve as a mild prevention device against unauthorized access, its main purpose is to prevent possible name collisions with class members of the ancestor classes. \nSee Alex Martelli's funny but accurate reference to  as he describes the convention used in regards to these variables.This question was not fully answered, since module privacy is not purely conventional, and since using  may or may not recognize module privacy, depending on how it is used.If you define private names in a module, those names  be imported into any script that uses the syntax, 'import module_name'. Thus, assuming you had correctly defined in your example the module private, _num, in a.py, like so....you would be able to access it in b.py with the module name symbol:To import only non-privates from a.py, you must use the  syntax:For the sake of clarity, however, it is better to be explicit when importing names from modules, rather than importing them all with a '*':Python allows for private  members with the double underscore prefix.  This technique doesn't work at a module level so I am thinking this is a mistake in Dive Into Python.Here is an example of private class functions:Python has three modes via., private, public and protected .While importing a module only public mode is accessible .So private and protected modules cannot be called from outside of the module i.e., when it is imported ."},
{"body": "I've been wondering this for some time. As the title say, which is faster, the actual function or simply raising to the half power?This is not a matter of premature optimization. This is simply a question of how the underlying code actually works. What is the theory of how Python code works?I sent Guido van Rossum an email cause I really wanted to know the differences in these methods.As per comments, I've updated the code:Now the  function is directly in a local argument, meaning it has the fastest lookup possible.  The python version seems to matter here. I used to think that  would be faster, since when python parses \"i**.5\" it knows, syntactically, which method to call ( or some variant), so it doesn't have to go through the overhead of lookup that the  variant does. But I might be wrong: 0.191000 vs. 0.224000 0.195000 vs. 0.139000Also psyco seems to deal with  better: 0.109000 vs. 0.043000 0.128000 vs. 0.067000Table results produced on machine:To reproduce results:How many square roots are you really performing?  Are you trying to write some 3D graphics engine in Python?  If not, then why go with code which is cryptic over code that is easy to read?  The time difference is would be less than anybody could notice in just about any application I could forsee.  I really don't mean to put down your question, but it seems that you're going a little too far with premature optimization.Here's some timings (Python 2.5.2, Windows):This test shows that  is slightly faster than .For the Python 3.0 the result is the opposite: is always faster than  on another machine (Ubuntu, Python 2.6 and 3.1):In these micro-benchmarks, math.sqrt will be slower, because of the slight time it takes to lookup the sqrt in the math namespace. You can improve it slightly with Even then though, running a few variations through timeit, show a slight (4-5%) performance advantage for \"x**.5\"interestingly, doingsped it up even more, to within 1% difference in speed, with very little statistical significance.I will repeat Kibbee, and say that this is probably a premature optimization.Most likely math.sqrt(x), because it's optimized for square rooting.Benchmarks will provide you the answer you are looking for.using Claudiu's code, on my machine even with \"from math import sqrt\" x**.5 is faster but using psyco.full() sqrt(x) becomes much faster, at least by 200%In python 2.6 the   function uses the C  function and the  functions uses the C  function.In glibc compiler the implementation of  is quite complex and it is well optimized for various exceptional cases. For example, calling C  simply calls the  function.The difference in speed of using  or  is caused by the wrappers used around the C functions and the speed strongly depends on optimization flags/C compiler used on the system. Here are the results of Claudiu's algorithm on my machine. I got different results:For what it's worth (see Jim's answer). On my machine, running python 2.5:Someone commented about the \"fast Newton-Raphson square root\" from Quake 3... I implemented it with ctypes, but it's super slow in comparison to the native versions. I'm going to try a few optimizations and alternate implementations.Here's another method using struct, comes out about 3.6x faster than the ctypes version, but still 1/10 the speed of C.You might want to benchmark the  as well. Shouldn't take much to convert to Python.Claudiu's results differ from mine. I'm using Python 2.6 on Ubuntu on an old P4 2.4Ghz machine... Here's my results:sqrt is consistently faster for me... Even Codepad.org NOW seems to agree that sqrt, in the local context, is faster (). Codepad seems to be running Python 2.5 presently. Perhaps they were using 2.4 or older when Claudiu first answered?In fact, even using math.sqrt(i) in place of arg(i), I still get better times for sqrt. In this case timeit2() took between 0.53 and 0.55 seconds on my machine, which is still better than the 0.56-0.60 figures from timeit1.I'd say, on modern Python, use math.sqrt and definitely bring it to local context, either with somevar=math.sqrt or with from math import sqrt.What would be even faster is if you went into math.py and copied the function \"sqrt\" into your program. It takes time for your program to find math.py, then open it, find the function you are looking for, and then bring that back to your program. If that function is faster even with the \"lookup\" steps, then the function itself has to be awfully fast. Probably will cut your time in half. IN summary:"},
{"body": "What is python-3 using instead of PIL for manipulating Images?The \"friendly PIL fork\" . Check out the  for support matrix and so on.Christoph Gohlke managed to build PIL (for Windows only) for python versions up to 3.3: I tried his version of PIL with Python 3.2, and image open/create/pixel manipulation/save all work.Qt works very well with graphics. In my opinion it is more versatile than PIL.You get all the features you want for graphics manipulation, but there's also vector graphics and even support for real printers. And all of that in one uniform API, .To use Qt you need a Python binding for it:  or .\nThey both support Python 3.Here is a simple example that loads a JPG image, draws an antialiased circle of radius  at coordinates  with the color of the pixel that was at those coordinates and saves the modified image as a PNG file:But please note that this solution is quite 'heavyweight', because Qt is a large framework for making GUI applications.As of March 30, 2012, I have tried and failed to get the sloonz fork on GitHub to open images.  I got it to compile ok, but it didn't actually work.  I also tried building gohlke's library, and it compiled also but failed to open any images.  Someone mentioned PythonMagick above, but it only compiles on Windows.  See .PIL was last updated in 2009, and while it's website says they are working on a Python 3 port, it's been 3 years, and the mailing list has gone cold.To solve my Python 3 image manipulation problem, I am using  to execute ImageMagick shell commands.  This method works.See .You can use my package  on Python 3. It is numpy-based rather than PIL based.The original PIL author planned to port to Python 3. Check this message in the mailing list: However, for the last 3 years, there has been no further work by the original author.As to what you can use NOW, I've no idea. I'm sticking with Python 2.6 until 3.0 has more widespread support from third-party libraries.Maybe you could do the image work in some other language.You want the , here is how to install it on Python 3:If that does not work for you (it should), try normal :"},
{"body": "What is the pythonic way of writing the following code?I have a vague memory that the explicit declaration of the  loop can be avoided and be written in the  condition. Is this true?Though not widely known,  also accepts a tuple. You don't need to loop.Just use:Take an extension from the file and see if it is in the set of extensions:Using a set because time complexity for lookups in sets is O(1) ().I have this:"},
{"body": "I see a lot on converting a  string to an  object in Python, but I want to go the other way.\nI've got  and I would like to convert it to string like .You can use  to help you format your date.E.g.,will yield:More information about formatting see  and  objects (and  as well) support a , and there are two ways to access it:So your example could look like:orFor completeness' sake: you can also directly access the attributes of the object, but then you only get the numbers:The time taken to learn the mini-language is worth it.For reference, here are the codes used in the mini-language:You could use simple string formatting methods: can be used as well:Output:Another option:String concatenation, , can be used to build the string.You can convert datetime to string.Output --> 5/23/2017"},
{"body": "How do I use a progress bar when my script is doing some task that is likely to take time?For example, a function which takes some time to complete and returns  when done. How can I display a progress bar during the time the function is being executed?Note that I need this to be in real time, so I can't figure out what to do about it. Do I need a  for this? I have no idea.Right now I am not printing anything while the function is being executed, however a progress bar would be nice. Also I am more interested in how this can be done from a code point of view.There are specific libraries () but maybe something very simple would do:Note: this progressbar is a fork of  which hasn't been maintained in years.With  you can add a progress meter to your loops in a second:The above suggestions are pretty good, but I think most people just want a ready made solution, with no dependencies on external packages, but is also reusable. I got the best points of all the above, and made it into a function, along with a test cases.To use it, just copy the lines under \"def update_progress(progress)\" but not the test script. Don't forget to import sys. Call this whenever you need to display or update the progress bar.This works by directly sending the \"\\r\" symbol to console to move cursor back to the start. \"print\" in python does not recongise the above symbol for this purpose, hence we need 'sys'This is what the result of the test script shows (The last progress bar animates):for a similar application (keeping track of the progress in a loop) I simply used the :Their example goes something like this,Try progress from .The result will be a bar like the following:I really like the , as it is very simple to use.For the most simple case, it is just:The appearance can be customized and it can display the estimated remaining time. For an example use the same code as above but with:I've just made a simple progress class for my needs after searching here for a equivalent solution. I tough I might a well post it.Example :Will print the following:Use this library:  ().Usage:Have fun!Many of the answers above rely on external packages, but I also think (as some above stated) that most people just want a ready made solution. The code bellow can be adapted to fit your needs by customizing the string part. It is simpler and works without the need of a second thread to update the bar. Some packages above do that. A second thread can be a problem for example for a ipython notebook.The code bellow only works with iterators that provide a length (i.e. len(iterator) must be defined).Example:Output:...... can be any iterable object with a , e.g.  ['a', 'b', 'c']` works just fine.If your work can't be broken down into measurable chunks, you could call your function in a new thread and time how long it takes:You can obviously increase the timing precision as required.Here's a short solution that builds the loading bar programmatically (you must decide how long you want it).I like this .Starts with simple example and moves onto a multi-threaded version. Works out of the box. No 3rd party packages required.The code will look something like this:Or here is example to use threads in order to run the spinning loading bar while the program is running:You should link the progress bar to the task at hand (so that it measures the progress :D). For example, if you are FTPing a file, you can tell ftplib to grab a certain size buffer, let's say 128K, and then you add to your progress bar whatever percentage of the filesize 128k represents. If you are using the CLI, and your progress meter is 20 characters long, you would add one character when 1/20th of the file had transferred.@Massagran: It works well in my programs. Furthermore, we need to add a counter to indicate the loop times. This counter plays as the argument of the method .\nFor example: read all lines of a test file and treat them on something. Suppose that the function  do not concern in the variable .The variable  controls the status of  via the method "},
{"body": "I'm unsure about (1).As for (2), I believe Python flushes to stdout after every new line.  But, if you overload stdout to be to a file, does it flush as often?For file operations, Python uses the operating system's default buffering unless you configure it do otherwise.  You can specify a buffer size, unbuffered, or line buffered.For example, the open function takes a buffer size argument.\"The optional buffering argument specifies the file\u2019s desired buffer size: \" You can also force flush the buffer to a file programmatically with the  method.I have found this useful when tailing an output file with .I don't know if this applies to python as well, but I think it depends on the operating system that you are running.On Linux for example, output to terminal flushes the buffer on a newline, whereas for output to files it only flushes when the buffer is full (by default).  This is because it is more efficient to flush the buffer fewer times, and the user is less likely to notice if the output is not flushed on a newline in a file.  You might be able to auto-flush the output if that is what you need.EDIT:  I think you would auto-flush in python this way (based \nfrom )You can also check the default buffer size by calling the read only DEFAULT_BUFFER_SIZE attribute from io module.Here is another approach, up to the OP to choose which one he prefers.When including the code below in the .py file before any other code, messages printed with  and any errors will no longer be logged to Ableton's Log.txt but to separate files on your disk:(for Mac, change  to the name of your user folder. On Windows the path to your user folder will have a different format)When you open the files in a text editor that refreshes its content when the file on disk is changed (example for Mac: TextEdit does not but TextWrangler does), you will see the logs being updated in real-time."},
{"body": "My Django unit tests take a long time to run, so I'm looking for ways to speed that up. I'm considering installing an , but I know that has its downsides too. Of course, there are things I could do with my code, but I'm looking for a structural fix. Even running a single test is slow since the database needs to be rebuilt / south migrated every time. So here's my idea...Since I know the test database will always be quite small, why can't I just configure the system to always keep the entire test database in RAM? Never touch the disk at all. How do I configure this in Django?  I'd prefer to keep using  since that's what I use in production, but if \u00a03 or something else makes this easy, I'd go that way.Does SQLite or MySQL have an option to run entirely in memory? It should be possible to configure a RAM disk and then configure the test database to store its data there, but I'm not sure how to tell Django / MySQL to use a different data directory for a certain database, especially since it keeps getting erased and recreated each run. (I'm on a Mac FWIW.)If you set your database engine to sqlite3 when you run your tests, .I'm using code like this in my  to set the engine to sqlite when running my tests:Or in Django 1.2:And finally in Django 1.3 and 1.4:(The full path to the backend isn't strictly necessary with Django 1.3, but makes the setting forward compatible.)You can also add the following line, in case you are having problems with South migrations:I usually create a separate settings file for tests and use it in test command e.g.It has two benefits:MySQL supports a storage engine called \"MEMORY\", which you can configure in your database config () as such:Note that the MEMORY storage engine doesn't support blob / text columns, so if you're using  this won't work for you.I can't answer your main question, but there are a couple of things that you can do to speed things up.Firstly, make sure that your MySQL database is set up to use InnoDB. Then it can use transactions to rollback the state of the db before each test, which in my experience has led to a massive speed-up. You can pass a database init command in your settings.py (Django 1.2 syntax):Secondly, you don't need to run the South migrations each time. Set  in your settings.py and the database will be created with plain syncdb, which will be much quicker than running through all the historic migrations.You can do double tweaking:I'm using both tricks and I'm quite happy.How to set up it for MySQL on Ubuntu:Beware, it's just for testing, after reboot your database from memory is lost!Extending on Anurag's answer I simplified the process by creating the same test_settings and adding the following to manage.pyseems cleaner since sys is already imported and manage.py is only used via command line, so no need to clutter up settingsAnother approach: have another instance of MySQL running in a tempfs that uses a RAM Disk. Instructions in this blog post:.Advantages: "},
{"body": "I am looking to format a number like 188518982.18 to \u00a3188,518,982.18 using Python.How can I do this?See the  module.This does currency (and date) formatting.Not quite sure why it's not mentioned more online (or on this thread), but the  package (and Django utilities) from the Edgewall guys is awesome for currency formatting (and lots of other i18n tasks).  It's nice because it doesn't suffer from the need to do everything globally like the core Python locale module.The example the OP gave would simply be:My locale settings seemed incomplete, so I had too look beyond this SO answer and found: OS-independentJust wanted to share here.If you are using OSX and have yet to set your locale module setting this first answer will not work you will receive the following error:To remedy this you will have to do use the following:Oh, that's an interesting beast.I've spent considerable time of getting that right, there are three main issues that differs from locale to locale:\n - currency symbol and direction\n - thousand separator\n - decimal pointI've written my own rather extensive implementation of this which is part of the kiwi python framework, check out the LGPL:ed source here:The code is slightly Linux/Glibc specific, but shouldn't be too difficult to adopt to windows or other unixes.Once you have that installed you can do the following:Which will then give you:orDepending on the currently selected locale.The main point this post has over the other is that it will work with older versions of python. locale.currency was introduced in python 2.5.This is an ancient post, but I just implemented the following solution which:Code:Output:And for the original poster, obviously, just switch  for I've come to look at the same thing and found  not really used it yet but maybe a mix of the two would be goodA lambda for calculating it inside a function, with help from and then,#printing  the variable 'Total:' in a format that looks like this '9,348.237'where the '{:7,.3f}' es the number of spaces for formatting the number in this case is a million with 3 decimal points.\nThen you add the '.format(zum1).  The zum1 is tha variable that has the big number for the sum of all number in my particular program.  Variable can be anything that you decide to use."},
{"body": "Edit: Since it appears that there's either no solution, or I'm doing something so non-standard that nobody knows - I'll revise my question to also ask: What is the best way to accomplish logging when a python app is making a lot of system calls?My app has two modes.  In interactive mode, I want all output to go to the screen as well as to a log file, including output from any system calls.  In daemon mode, all output goes to the log.  Daemon mode works great using os.dup2().  I can't find a way to \"tee\" all output to a log in interactive mode, without modifying each and every system call.In other words, I want the functionality of the command line 'tee' for any output generated by a python app, .To clarify: To redirect all output I do something like this, and it works great:The nice thing about this is that it requires no special print calls from the rest of the code.  The code also runs some shell commands, so it's nice not having to deal with each of their output individually as well.Simply, I want to do the same, except  instead of redirect.At first blush, I thought that simply reversing the dup2's should work.  Why doesn't it?  Here's my test: The file \"a.log\" should be identical to what was displayed on the screen.Since you're comfortable spawning external processes from your code, you could use  itself.  I don't know of any Unix system calls that do exactly what  does.You could also emulate  using the  package (or use  if you're using Python 2.5 or earlier).I had this same issue before and found this snippet very useful:from:   I would spin up a small class to write to two places at once...This is obviously quick-and-dirty.  Some notes:These are all straightforward enough that I'm comfortable leaving them as exercises for the reader.  The key insight here is that  just calls a \"file-like object\" that's assigned to .What you really want is  module from standard library.  Create a logger and attach two handlers, one would be writing to a file and the other to stdout or stderr.See   for detailsI wrote a  implementation in Python that should work for most cases, and it works on Windows also.Also, you can use it in combination with  module from Python if you want.(Ah, just re-read your question and see that this doesn't quite apply.)Here is a sample program that makes uses the .  This logging module has been in all versions since 2.3.  In this sample the logging is configurable by command line options.  In quite mode it will only log to a file, in normal mode it will log to both a file and the console.As described elsewhere, perhaps the best solution is to use the logging module directly:However, there are some (rare) occasions where you  to redirect stdout. I had this situation when I was extending django's runserver command which uses print: I didn't want to hack the django source but needed the print statements to go to a file.This is a way of redirecting stdout and stderr away from the shell using the logging module:You should only use this LogFile implementation if you really cannot use the logging module directly.Here is another solution, which is more general than the others -- it supports splitting output (written to ) to any number of file-like objects. There's no requirement that  itself is included.NOTE: This is a proof-of-concept. The implementation here is not complete, as it only wraps  of the file-like objects (e.g. ), leaving out members/properties/setattr, etc.  However, it is probably good enough for most people as it currently stands.What I like about it, other than its generality, is that it is clean in the sense it doesn't make any direct calls to , , , etc.To complete John T answer: I added an  and  method to use it as a context manager with the 'with' keyword, which gives this codeIt can then be used asanother solution using logging module:I know this question has been answered repeatedly, but for this I've taken the main answer from  answer and modified it so it contains the suggested flush and followed its linked revised version. I've also added the enter and exit as mentioned in  answer for use with the with statement. In addition, the  mentions to flush files using  so I've added that as well. I don't know if you  need that but its there.You can then use itor None of the answers above really seems to answer the problem posed.  I know this is an old thread, but I think this problem is a lot simpler than everyone is making it:Now this will repeat everything to the normal sys.stderr handler and your file.  Create another class  for .As per a request by @user5359531 in the comments under @John T's , here's a copy of the referenced post to the revised version of the linked discussion in that answer:I'm writing a script to run cmd-line scripts.  ( Because in some cases, there just is no viable substitute for a Linux command -- such as the case of rsync. )What I really wanted was to use the default python logging mechanism in every case where it was possible to do so, but to still capture any error when something went wrong that was unanticipated.This code seems to do the trick.  It may not be particularly elegant or efficient ( although it doesn't use string+=string, so at least it doesn't have that particular potential bottle-\nneck ).  I'm posting it in case it gives someone else any useful ideas.Obviously, if you're not as subject to whimsy as I am, replace LOG_IDENTIFIER with another string that you're not like to ever see someone write to a log."},
{"body": "I want to run a script, which basicly shows things like:Now, at the moment, I use print to print the whole line AFTER the function has succeeded. However, I now want it to print \"Installing xxx...\" first, and AFTER the function has run, to add the \"DONE\" tag; but on the same line.Any ideas?You can use the  statement to do this without importing .The comma on the end of the  line prevents  from issuing a new line (you should note that there will be an extra space at the end of the output).\nSince the above does not work in Python 3, you can do this instead (again, without importing ):The print function accepts an  parameter which defaults to . Setting it to an empty string prevents it from issuing a new line at the end of the line.You can simply use this:and the output will beno need to overkill by . Pay attention to comma symbol at the end.\n to remove the newline insert at the end. Read more by Use  and . In this way, you have to add the new line by hand with  if you want to recreate the print functionality. I think that it might be unnecessary to use curses just for this. You must use backspace '' or ('') char to go back on previous position in console outputPython 3:This code will count from 0% to 100% on one line. Final value will be:Additional info about flush in this case here: None of the answers worked for me since they all paused until a new line was encountered. I wrote a simple helper:To test it: \"hello \" will first print out and flush to the screen before the sleep. After that you can use standard print.   will print without return carriagePrint has an optional end argument, it is what printed in the end.\nThe default is newline but you can change it to empty string. eg: print(\"hello world!\", end=\"\")If you want to overwrite the previous line (rather than continually adding to it), you can combine  with   at the end of the print statement. For example,will count 0 to 9, replacing the old number in the console. The  will print on the same line as the last counter, 9.In your case for the OP, this would allow the console to display percent complete of the install as a \"progress bar\", where you can define a begin and end character position, and update the markers in between.Just in case you have pre-stored the values in an array, you can call them in the following format:I found this solution, and it's working on Python 2.7"},
{"body": "I have a list with numeric strings, like so:I would like to convert every list element to integer, so it would look like this:I could do it using a loop, like so:Does it have to be so ugly? I'm sure there is a more pythonic way to do this in a one line of code. Please help me out.This is what list comprehensions are for:In Python 2.x another approach is to use :Note: in Python 3.x  returns a map object which you can convert to a list if you want:just a point,the list comprehension is more natural, whileis faster. Useful read: If you are intending on passing those integers to a function or method, consider this example:This construction is intentionally remarkably similar to list comprehensions mentioned by adamk. Without the square brackets, it's called a , and is a very memory-efficient way of passing a list of arguments to a method. A good discussion is available here: Another way,Another way to make it in Python 3:"},
{"body": "Lets say I have an numpy array a:And I would like to add a column of zeros to get array b:How can I do this easily in numpy?I think a more straightforward solution and faster to boot is to do the following:And timings: and \nare useful alternatives to  and ,\nwith square brackets [] instead of round ().\nA couple of examples:(The reason for square brackets [] instead of round ()\nis that Python expands e.g. 1:4 in square --\nthe wonders of overloading.)Use :While writing the question I came up with one way, using hstackAny other (more elegant solutions) welcome!I think:is more elegant.What I find most elegant is the following:An advantage of  is that it also allows you to insert columns (or rows) at other places inside the array. Also instead of inserting a single value you can easily insert a whole vector, for instance doublicate the last column:Which leads to:For the timing,  might be slower than JoshAdel's solution: also worksI like JoshAdel's answer because of the focus on performance. A minor performance improvement is to avoid the overhead of initializing with zeros, only to be overwritten. This has a measurable difference when N is large, empty is used instead of zeros, and the column of zeros is written as a separate step:A bit late to the party, but nobody posted this answer yet, so for the sake of completeness: you can do this with list comprehensions, on a plain Python array:I was also interested in this question and compared the speed ofwhich all to the same thing for any input vector . WithI foundLooks like / is what you want if you're looking for speed.Assuming  is a (100,3) ndarray and  is a (100,) ndarray  can be used as follows:The trick is to use This converts  to a (100, 1) 2D array.now gives"},
{"body": "I have to search through a list and replace all occurrences of one element with another. So far my attempts in code are getting me nowhere, what is the best way to do this?For example, suppose my list has the following integersand I need to replace all occurrences of the number 1 with the value 10 so the output I need is Thus my goal is to replace all instances of the number 1 with the number 10.Try using a  and the .List comprehension works well--and looping through with enumerate can save you some memory (b/c the operation's essentially be doing in place).There's also functional programming...see usage of :If you have several values to replace, you can also use a dictionary:The following is a very direct methodThis method works. Comments are welcome. Hope it helps :)"},
{"body": "This is a slightly.. vain question, but BuildBot's output isn't particularly nice to look at..For example, compared to....and others,  looks rather.. archaicI'm currently playing with Hudson, but it is very Java-centric (although with , I found it easier to setup than BuildBot, and produced more info)Basically: is there any Continuous Integration systems aimed at python, that produce lots of shiny graphs and the likes? Since this time the Jenkins project has replaced Hudson as the community version of the package. The original authors have moved to this project as well. Jenkins is now a standard package on Ubuntu/Debian, RedHat/Fedora/CentOS, and others. The following update is still essentially correct. The starting point to do this with  is different. After trying a few alternatives, I think I'll stick with Hudson.  was nice and simple, but quite limited. I think  is better suited to having numerous build-slaves, rather than everything running on a single machine like I was using it.Setting Hudson up for a Python project was pretty simple:That's all that's required. You can setup email notifications, and  are worth a look. A few I'm currently using for Python projects:You might want to check out  and .  You can have it run your unit tests, and coverage checks with this command:That'll be helpful if you want to go the Jenkins route, or if you want to use another CI server that has support for JUnit test reporting.Similarly you can capture the output of pylint using the Don't know if it would do :  is made by the guys who write Trac and is integrated with Trac.  is the CI tool used by Apache. It is written in Python.We've had great success with  as our CI server and using nose as our test runner.   gives you count pass/fail, readable display for failed test( that can be E-Mailed).  You can even see details of the test failures while you stack is running.  If of course supports things like running on multiple machines, and it's much simpler to setup and maintain than buildbot.Buildbot's waterfall page can be considerably prettified. Here's a nice example Atlassian's  is also definitely worth checking out.  The entire Atlassian suite (JIRA, Confluence, FishEye, etc) is pretty sweet.I guess this thread is quite old but here is my take on it with hudson:I decided to go with pip and set up a repo (the painful to get working but nice looking eggbasket), which hudson auto uploads to with a successful tests. Here is my rough and ready script for use with a hudson config execute script like: /var/lib/hudson/venv/main/bin/hudson_script.py -w $WORKSPACE -p my.package -v $BUILD_NUMBER, just put in **/coverage.xml, pylint.txt and nosetests.xml in the config bits:When it comes to deploying stuff you can do something like:And then people can develop stuff using:This stuff assumes you have a repo structure per package with a setup.py and dependencies all set up then you can just check out the trunk and run this stuff on it.I hope this helps someone out.------update---------I've added epydoc which fits in really nicely with hudson. Just add javadoc to your config with the html folderNote that pip doesn't support the -E flag properly these days, so you have to create your venv separatelyanother one :  is a hosted tool for pythonIf you're considering hosted CI solution, and doing open source, you should look into  as well - it has very nice integration with GitHub.  While it started as a Ruby tool, they have  a while ago.Signal is another option. You can know more about it and watch a video also .I would consider  - it has great Python support, and very pretty output.continuum's  now is able to trigger builds from github and can compile for linux, osx and windows ( 32 / 64 ). the neat thing is that it really allows you to closely couple distribution and continuous integration. That's crossing the t's and dotting the I's of Integration. The site, workflow and tools are really polished and AFAIK conda is the most robust and pythonic way to distributing complex python modules, where you need to wrap  distribute C/C++/Fotran libraries.We have used bitten quite a bit.  It is pretty and integrates well with Trac, but it is a pain in the butt to customize if you have any nonstandard workflow.  Also there just aren't as many plugins as there are for the more popular tools.  Currently we are evaluating Hudson as a replacement.Check . As  explains, it uses Docker for every build. Thanks to that, you can configure whatever you like inside your Docker image, including Python.Little disclaimer, I've actually had to build a solution like this for a client that wanted a way to automatically test and deploy  code on a git push plus manage the issue tickets via git notes. This also lead to my work on the .One could easily just setup a bare node system that has a build user and manage their build through , , /, and . One could even go a step further and use ansible and celery for distributed builds with a gridfs/nfs file store.Although, I would not expect anyone other than a Graybeard UNIX guy or Principle level engineer/architect to actually go this far. Just makes for a nice idea and potential learning experience since a build server is nothing more than a way to arbitrarily execute scripted tasks in an automated fashion."},
{"body": "Is there a way to specify the running directory of command in python's  ?For example:My python script is located in Is is possible to run  in the directory  ?  How do I set the working directory for subprocess?  to set the Current Working Directory; you'll also want to escape your backslashes (), or use  so that the backslashes aren't interpreted as escape sequences by Python.  The way you have it written, the  part will be translated to a .So, your new line should look like:To use your Python script path as cwd,  and define cwd using this:"},
{"body": "Why does this code not input integers?  Everything on the web says to use , but I read on Stack Overflow (on a thread that did not deal with integer input) that  was renamed to  in Python 3.x.There were two functions to get user input, called  and . The difference between them is,  doesn't evaluate the data and returns as it is, in string form. But,  will evaluate whatever you entered and the result of evaluation will be returned. For example,The data  is evaluated and the result is . When it evaluates the expression , it detects that you are adding two numbers and so the result will also be of the same  type. So, the type conversion is done for free and  is returned as the result of  and stored in  variable. You can think of  as the  composed with an  call. you should be careful when you are using  in Python 2.x. I explained why one should be careful when using it, in .But,  doesn't evaluate the input and returns as it is, as a string.Python 3.x's  and Python 2.x's  are similar and  is not available in Python 3.x. To answer your question, since Python 3.x doesn't evaluate and convert the data type, you have to explicitly convert to s, with , like thisYou can accept numbers of any base and convert them directly to base-10 with the  function, like thisThe second parameter tells what is the base of the numbers entered and then internally it understands and converts it. If the entered data is wrong it will throw a .Apart from that, your program can be changed a little bit, like thisYou can get rid of the  variable by using  and . : Python doesn't expect  at the end of the line :)In Python 3.x,  was renamed to  and the Python 2.x  was removed.  This means that, just like ,  in Python 3.x always returns a string object.To fix the problem, you need to explicitly make those inputs into integers by putting them in :Also, Python does not need/use semicolons to end lines.  So, having them doesn't do anything positive. (Python 3) and  (Python 2)  return strings. Convert the result to integer explicitly with .Pro tip: semi-colons are not needed in Python.For multiple integer in a single line,  might be better.If the number is already known, (like 2 integers), you can useMultiple questions require input for several integers on single line.  The best way is to input the whole string of numbers one one line and then split them to integers.Python 3.x has  function which returns always string.So you must convert to \n\nIn python 2.x  and  functions always return string so you must convert them to int too.I encountered a problem of taking integer input while solving a problem on , where two integers - separated by space - should be read from one line.While  is sufficient for a single integer, I did not find a direct way to input two integers.  I tried this:Now I use num1 and  num2 as integers.  Hope this helps.Yes, in python 3.x,  is replaced with . In order to revert to old behavior of  use: This will let python know that entered input is integerConvert to integers:Similarly for floating point numbers:While in your example,  does the trick in any case, 's  is worth consideration since that makes sure your code works for both Python 2 and 3  disables Python2's default behaviour of  trying to be \"clever\" about the input data type ( basically just behaves like ).Taking int as input in python:\nwe take a simple string input using:now we want int as input.so we typecast this string to int. simply using:"},
{"body": "I'm thinking about putting the virtualenv for a Django web app I am making inside my git repository for the app. It seems like an easy way to keep deploy's simple and easy. Is there any reason why I shouldn't do this?I'm totally new to virtualenv, so there is a good chance this is a really stupid question.I use  to get the packages I need into a  file and add that to my repository.  I tried to think of a way of why you would want to store the entire virtualenv, but I could not.I used to do the same until I started using libraries that are compiled differently depending on the environment such as PyCrypto. My PyCrypto mac wouldn't work on Cygwin wouldn't work on Ubuntu.It becomes an utter nightmare to manage the repository.Either way I found it easier to manage the pip freeze & a requirements file than having it all in git. It's cleaner too since you get to avoid the commit spam for thousands of files as those libraries get updated...  Storing the virtualenv directory inside git will, as you noted, allow you to deploy the whole app by just doing a git clone (plus installing and configuring Apache/mod_wsgi).  One potentially significant issue with this approach is that on Linux the full path gets hard-coded in the venv's activate, django-admin.py, easy_install, and pip scripts.  This means your virtualenv won't entirely work if you want to use a different path, perhaps to run multiple virtual hosts on the same server.  I think the website may actually work with the paths wrong in those files, but you would have problems the next time you tried to run pip.The solution, already given, is to store enough information in git so that during the deploy you can create the virtualenv and do the necessary pip installs.  Typically people run  to get the list then store it in a file named requirements.txt.  It can be loaded with .  RyanBrady already showed how you can string the deploy statements in a single line:  Personally, I just put these in a shell script that I run after doing the git clone or git pull.Storing the virtualenv directory also makes it a bit trickier to handle pip upgrades, as you'll have to manually add/remove and commit the files resulting from the upgrade.  With a requirements.txt file, you just change the appropriate lines in requirements.txt and re-run .  As already noted, this also reduces \"commit spam\".I think one of the main problems which occur is that the virtualenv might not be usable by other people. Reason is that it always use absolute path's. So if you virtualenv was for example in  it will assume the same for all other people using this repository (it must be exactly the same absolute path). You can't presume people using the same directory structure as you.Better practice is that everybody is setting up their own environment (be it with or without virtualenv) and installing libraries there. That also makes you code more usable over different platforms (Linux/Windows/Mac), also because virtualenv is installed different in each of them.If you know which operating systems your application will be running on, I would create one virtualenv for each system and include it in my repository. Then I would make my application detect which system it is running on and use the corresponding virtualenv.The system could e.g. be identified using the  module.In fact, this is what I do with an in-house application I have written, and to which I can quickly add a new system's virtualenv in case it is needed. This way, I do not have to rely on that pip will be able to successfully download the software my application requires. I will also not have to worry about compilation of e.g.  which I use.If you do not know which operating system your application may run on, you are probably better off using  as suggested in other answers here.If you just setting up development env, then use pip freeze file, caz that makes the git repo clean.Then if doing production deployment, then checkin the whole venv folder. That will make your deployment more reproducible, not need those libxxx-dev packages, and avoid the internet issues.So there are two repos. One for your main source code, which includes a requirements.txt. And a env repo, which contains the whole venv folder. "},
{"body": "I have form with one input for email and two submit buttons to subscribe and unsubscribe from newsletter:I have also class form:I must write my own clean_email method and I need to know by which button was form submited. But the value of submit buttons aren't in  dictionary.\nCould I get values of buttons otherwise?You can use  in the  method to access the POST data before validation. It should contain a key called  or  depending on which button was pressed.Eg:You can also do like this, It's an old question now, nevertheless I had the same issue and found a solution that works for me: I wrote MultiRedirectMixin.one url to the same view! \nlike so!"},
{"body": "I've read every other google source and SO thread, with nothing working. installed on . Download, extracting, and then trying to install PyCrypto results in So I install MinGW and tack that on the install line as the compiler of choice. But then I get the error How in the world do I get around this? I've tried using pip, which gives the same result. I found a prebuilt PyCrypto 2.3 binary and installed that, but it's nowhere to be found on the system (not working).Any ideas?If you don't already have a C/C++ development environment installed that is compatible with the Visual Studio binaries distributed by Python.org, then you should stick to installing only pure Python packages or packages for which a Windows binary is available.Fortunately, there are PyCrypto binaries available for Windows:\n\nAs @Udi suggests in the comment below, the following command also installs  and can be used in  as well:Notice to choose the relevant link for your setup from If you're looking for builds for Python 3.5, see Microsoft has recently recently released a standalone, dedicated . If you're using Python 2.7, simply install that compiler and Setuptools 6.0 or later, and most packages with C extensions will now compile readily. is part of the Visual C++ compiler, you need that to install what you are trying to install. Don't even try to deal with MingGW if your Python was compiled with Visual Studio toolchain and vice versa. Even the  of the Microsoft tool chain is important. Python compiled with VS 2008 won't work with extensions compiled with VS 2010!You have to compile PyCrypto with the same compiler that the version of Python was compiled with. Google for  because that is the root of your problem,  As far as I know the following is still true. This was posted in the link above in June, 2010 referring to trying to build extensions with VS 2010 Express against the Python installers available on python.org.For VS2010:For VS2012:then Call:, python finally agreed for a binary disribution called  which allows to install even binary extensions on Windows without having a compiler with simple . There is a  with their status. Pycrypto is not there yet, but lxml, PySide and Scrapy for example. : . It is a  fork with new features and it supports wheel. It replaces  (see ) is an almost-compatible fork of PyCrypto with Windows wheels available on .You can install it with a simple:The website includes instructions to build it from sources with the Microsoft compilers too.I have managed to get  to compile by using  and . This presumes that you have  or  installed.Here's how I did it:1) Install . For the sake of this explanation, let's assume it's installed in . When using the installer, which I recommend, select the C++ compiler  MSYS should install with 2) Add  to your . If you aren't familiar,  is very helpful.3) From the search bar, run  and the MSYS terminal will open. For those familiar with , it works in a similar fashion.4) From within the MSYS terminal  should run without error after this.  It's possible to build PyCrypto using the Windows 7 SDK toolkits. There are two versions of the Windows 7 SDK. The original version (for .Net 3.5) includes the VS 2008 command-line compilers. Both 32- and 64-bit compilers can be installed.The first step is to compile  to provide fast arithmetic. I've documented the process I use in the  library. Detailed instructions for building mpir using the SDK compiler can be found at The key steps to use the SDK compilers from a DOS prompt are:1) Run either vcvars32.bat or vcvars64.bat as appropriate.2) At the prompt, execute \"set MSSdk=1\"3) At the prompt, execute \"set DISTUTILS_USE_SDK=1\"This should allow \"python setup.py install\" to succeed assuming there are no other issues  with the C code. But I vaaguely remember that I had to edit a couple of PyCrypto files to enable mpir and to find the mpir libraries but I don't have my Windows system up at the moment. It will be a couple of days before I'll have time to recreate the steps. If you haven't reported success by then, I'll post the PyCrypto steps. The steps will assume you were able to compile mpir.I hope this helps. For those of you looking for python 3.4 I found a  with an installer that just works. Here are the direct links for  and To install Pycrypto in Windows,Try this in Command Prompt,Set path=C:\\Python27\\Scripts (i.e path where easy_install is located)Then execute the following,easy_install pycryptoTry this,Download Pycrypto from \"\"Then change your current path to downloaded path using your terminal and user should be root:Eg: root@xyz-virtual-machine:~/pycrypto-2.6.1#Then execute the following using the terminal:python setup.py installIt's worked for me. Hope works for all..This probably isn't the optimal solution but you might download and install the free Visual C++ Express package from MS. This will give you the C++ compiler you need to compile the PyCrypto code.This error  occurs because the install script didn't find the  command.You only need to .(N.B: Note that MinGW comes with MSYS so )For example, if we are in folder \nOptional: you might need to clean before you re-run the script:You need to install msys package under MinGW and add following entries in your PATH env variable.Then run your command from normal windows command prompt.Due to weird legal reasons, binaries are not published the normal way. Voidspace is normally the best second source. But since quite some time, voidspace maintainer did not update. \nUse the zip from [Try just using:or:Source: "},
{"body": "I need some help on declaring a regex. My inputs are like the following:The required output is:I've tried this:I've also tried this (but it seems like I'm using the wrong regex syntax):I dont want to hard-code the  from 1 to 99 . . .This tested snippet should do it: Here's a commented version explaining how it works:Regexes are  But I would strongly recommend spending an hour or two studying the basics. For starters, you need to learn which characters are special:  which need to be escaped (i.e. with a backslash placed in front - and the rules are different inside and outside character classes.) There is an excellent online tutorial at: . The time you spend there will pay for itself many times over. Happy regexing! does fixed replacements. Use  instead.The easiest wayI would go like this (regex explained in comments):If you want to learn more about regex I recomend to read  by  Jan Goyvaerts and Steven Levithan.replace method of string objects does not accept regular expressions but only fixed strings (see documentation: ).You have to use  module:don't have to use regular expression (for your sample string)"},
{"body": "I need to remove all special characters, punctuation and spaces from a string so that I only have letters and numbers.This can be done without :You can use :If you insist on using , other solutions will do fine. However note that if it can be done without using a regular expression, that's the best way to go about it.Here is a regex to match a string of characters that are not a letters or numbers:Here is the Python command to do a regex substitution:Shorter way  :If you want spaces between words and numbers substitute '' with ' 'you can add more special character and that will be replaced by '' means nothing i.e they will be removed.I think just  worksThe most generic approach is using the 'categories' of the unicodedata table which classifies every single character. E.g. the following code filters only printable characters based on their category:Look at the given URL above for all related categories. You also can of course filter\nby the punctuation categories.After seeing this, I was interested in expanding on the provided answers by finding out which executes in the least amount of time, so I went through and checked some of the proposed answers with  against two of the example strings:The above results are a product of the lowest returned result from an average of:  can be 3x faster than .Assuming you want to use a regex and you want/need Unicode-cognisant 2.x code that is 2to3-ready:Use translate:Caveat: Only works on ascii strings.and you shall see your result as 'askhnlaskdjalsdk"},
{"body": "Is there any short way to achieve what the APT () command line interface does in Python?I mean, when the package manager prompts a yes/no question followed by , the script accepts  or  (defaults to  as hinted by the capital letter).The only thing I find in the official docs is  and ...I know it's not that hard to emulate, but it's annoying to rewrite :|As you mentioned, the easiest way is to use . There is no built-in way to do this. From :Usage example:I'd do it this way:There is a function  in Python's standard library: You can use it to check user's input and transform it to  or  value.A very simple (but not very sophisticated) way of doing this for a single choice would be:You could also write a simple (slightly improved) function around this:as mentioned by @Alexander Artemenko, here's a simple solution using strtoboolI know this has been answered a bunch of ways and this may not answer OP's specific question (with the list of criteria) but this is what I did for the most common use case and it's far simpler than the other responses:You can also use .Shamelessly taken from the README:I modified fmark's answer to by python 2/3 compatible more pythonic.See  if you are interested in something with more error handlingYou could try something like the code below to be able to work with choices from the variable 'accepted' show here:Here is the code ..on 2.7, is this too non-pythonic?it captures any variation of Yes at least.Doing the same with python 3.x, where  doesn't exist:How about this: This is what I use:This is how I'd do it.OutputHere's my take on it, I simply wanted to abort if the user did not affirm the action.You can use 's  method.This will print:Should work for  on Linux, Mac or Windows.Docs: "},
{"body": "I have a  as part of an object, and wish to encode this to a JSON string which should look like . I don't care about precision on the client side, so a float is fine.Is there a good way to serialize this? JSONDecoder doesn't accept Decimal objects, and converting to a float beforehand yields  which is wrong, and will be a big waste of bandwidth.How about subclassing ?Then use it like so: and higher has native support for Decimal type:Note that  is  by default:So:Hopefully, this feature will be included in standard library.I would like to let everyone know that I tried Micha\u0142 Marczyk's answer on my web server that was running Python 2.6.5 and it worked fine. However, I upgraded to Python 2.7 and it stopped working. I tried to think of some sort of way to encode Decimal objects and this is what I came up with:This should hopefully help anyone who is having problems with Python 2.7. I tested it and it seems to work fine. If anyone notices any bugs in my solution or comes up with a better way, please let me know.I tried switching from simplejson to builtin json for GAE 2.7, and had issues with the decimal. If default returned str(o) there were quotes (because _iterencode calls _iterencode on the results of default), and float(o) would remove trailing 0.If default returns an object of a class that inherits from float (or anything that calls repr without additional formatting) and has a custom __repr__ method, it seems to work like I want it to. can not be exactly represented in IEEE floats, it will always come as , e.g. try  , you can read more about it here:  \n  So if you don't want float, only option you have to send it as string, and to allow automatic conversion of decimal objects to JSON, do something like this:In my Flask app, Which uses python 2.7.11, flask alchemy(with 'db.decimal' types), and Flask Marshmallow ( for 'instant' serializer and deserializer), i had this error, every time i did a GET or POST. The serializer and deserializer, failed to convert  Decimal types into any JSON identifiable format. I did a  \"pip install simplejson\", then \nJust by addingthe serializer and deserializer starts to purr again. I did nothing else...\nDEciamls are displayed as '234.00' float format.This is what I have, extracted from our classWhich passes unittest:My $.02!I extend a bunch of the JSON encoder since I am serializing tons of data for my web server. Here's some nice code. Note that it's easily extendable to pretty much any data format you feel like and will reproduce 3.9 as Makes my life so much easier...this can be done by adding in , but I was hoping for a better solution"},
{"body": "I want to do something similar to this:But this is not supported by python lists\nWhat is the best way of doing it?Use a list comprehension:If you want to use the  infix syntax, you can just do:you can then use it like: But if you don't absolutely need list properties (for example, ordering), just use sets as the other answers recommend.Use Or you might just have x and y be sets so you don't have to do any conversions.That is a \"set subtraction\" operation. Use the set data structure for that.In Python 2.7:Output:if duplicate and ordering items are problem :For many use cases, the answer you want is:This is a hybrid between  and .aaronasterling's version does  item comparisons for each element in , so it takes quadratic time. quantumSoup's version uses sets, so it does a single constant-time set lookup for each element in \u2014but, because it converts   and  into sets, it loses the order of your elements.By converting only  into a set, and iterating  in order, you get the best of both worlds\u2014linear time, and order preservation.*However, this still has a problem from quantumSoup's version: It requires your elements to be hashable. That's pretty much built into the nature of sets.** If you're trying to, e.g., subtract a list of dicts from another list of dicts, but the list to subtract is large, what do you do?If you can decorate your values in some way that they're hashable, that solves the problem. For example, with a flat dictionary whose values are themselves hashable:If your types are a bit more complicated (e.g., often you're dealing with JSON-compatible values, which are hashable, or lists or dicts whose values are recursively the same type), you can still use this solution. But some types just can't be converted into anything hashable.If your items aren't, and can't be made, hashable, but they are comparable, you can at least get log-linear time (, which is a lot better than the  time of the list solution, but not as good as the  time of the set solution) by sorting and using :If your items are neither hashable nor comparable, then you're stuck with the quadratic solution.Try this.Looking up values in sets are faster than looking them up in lists:I believe this will scale slightly better than:Both preserve the order of the lists.This example subtracts two lists:"},
{"body": "Why such structuregives an error ?Default argument values are evaluated at function define-time, but  is an argument only available at function call time. Thus arguments in the argument list cannot refer each other.It's a common pattern to default an argument to  and add a test for that in code:For cases where you also wish to have the option of setting 'b' to None:"},
{"body": "I'm wondering if there's any way to tell pip, specifically in a requirements file, to install a package with both a minimum version () and a maximum version which should never be installed (theoretical api: ).I ask because I am using a third party library that's in active development. I'd like my pip requirements file to specify that it should always install the most recent minor release of the 0.5.x branch, but I don't want pip to ever try to install any newer major versions (like 0.6.x) since the API is different.  This is important because even though the 0.6.x branch is available, the devs are still releasing patches and bugfixes to the 0.5.x branch, so I don't want to use a static  line in my requirements file.Is there any way to do that?You can do:And  will look for the best match, assuming the version is at least 0.2, and less than 0.3.This also applies to pip .  See the full details on version specifiers in ."},
{"body": "In python 2.7, we got the  available.Now, I know the pro and cons of the following:What are  (and the like) for? What are their benefits? How does it work? What is a view after all?I read that the view is always reflecting the changes from the dictionary. But how does it behave from the perf and memory point of view? What are the pro and cons?Dictionary views are essentially what their name says:  on the keys and values (or items) of a dictionary.  Here is an excerpt from the  for Python\u00a03:(The Python\u00a02 equivalent uses  and .)This example shows the : the keys view is  a copy of the keys at a given point in time, but rather a simple window that shows you the keys; if they are changed, then what you see through the window does change as well.  This feature can be useful in some circumstances (for instance, one can work with a view on the keys in multiple parts of a program instead of recalculating the current list of keys each time they are needed).One advantage is that  at, say, the keys uses only  and requires , as there is no creation of a list of keys (Python\u00a02, on the other hand, often unnecessarily creates a new list, as quoted by Rajendran T, which takes memory and time in an amount proportional to the length of the list).  To continue the window analogy, if you want to see a landscape behind a wall, you simply make an opening in it (you build a window); copying the keys into a list would correspond to instead painting a copy of the landscape on your wall\u2014the copy takes time, space, and does not update itself.To summarize, views are simply\u2026 views (windows) on your dictionary, which show the contents of the dictionary even after it changes.  They offer features that differ from those of lists: a list of keys contain a  of the dictionary keys at a given point in time, while a view is dynamic and is much faster to obtain, as it does not have to copy any data (keys or values) in order to be created.The view methods return a list(not a copy of the list, compared to ,  and ), so it is more lightweight, but reflects the current contents of dictionary.From As you mentioned  returns a copy of the dictionary\u2019s list of (key, value) pairs which is wasteful and  returns an iterator over the dictionary\u2019s (key, value) pairs.Now take the following example to see the difference between an interator of dict and a view of dictWhereas a view simply shows you what's in the dict. It doesn't care if it changed:A view is simply a what the dictionary looks like now. After deleting an entry  would have been out-of-date and  would have thrown an error.Just from reading the docs I get this impression:So I guess the key usecase is if you're keeping a dictionary around and repeatedly iterating over its keys/items/values with modifications in between. You could just use a view instead, turning  into . But if you're just iterating over the dictionary once, I think the iter- versions are still preferable."},
{"body": "I've recently moved over to using IPython notebooks as part of my workflow.  However, I've not been successful in finding a way to import .py files into the individual cells of an open IPython notebook so that they can edited, run and then saved. Can this be done?I've found  in the documentation which tells me how to import .py files as new notebooks but this falls short of what I want to achieve.Any suggestions would be much appreciated,\nthanks in advance.A text file can be loaded in a notebook cell with the magic command .If you execute a cell containing:the content of  will be loaded in the next cell. You can edit and execute it as usual.To save the cell content back into a file add the cell-magic  at the beginning of the cell and run it. Beware that if a file with the same name already exists .To see the help for any magic command add a : like  or .For general help on magic functions type \"%magic\"\nFor a list of the available magic functions, use %lsmagic. For a description\nof any of them, type %magic_name?, e.g. '%cd?'.: Starting from IPython 3 (now Jupyter project), the notebook has a text editor that can be used as a more convenient alternative to load/edit/save text files.  Beside the cell magic commands, IPython notebook (now Jupyter notebook) is so cool that it allows you to use any unix command right from the cell (this is also equivalent to using the  cell magic command).To run a unix command from the cell, just precede your command with  mark. for example:Also, see  for further explanation with examples.\nHope this helps.Drag and drop a Python file in the Ipython notebooks \"home\" notebooks table, click upload. This will create a new notebook with only one cell containing your .py file contentElse copy/paste from your favorite editor ;)I have found it satisfactory to use ls and cd within ipython notebook to find the file. Then type cat your_file_name into the cell, and you'll get back the contents of the file, which you can then paste into the cell as code."},
{"body": "I keep important settings like the hostnames and ports of development and production servers in my version control system. But I know that it's  to keep  (like private keys and database passwords) in a VCS repository.But passwords--like any other setting--seem like they should be versioned. So what  the proper way to keep passwords version controlled?I imagine it would involve keeping the  in their own \"secrets settings\" file and having  file encrypted and version controlled. But what technologies? And how to do this properly? Is there a better way entirely to go about it?I ask the question generally, but in my specific instance I would like to store secret keys and passwords for a  site using  and .Also, an  solution would do something magical when I push/pull with git--e.g., if the encrypted passwords file changes a script is run which asks for a password and decrypts it into place.You're exactly right to want to encrypt your sensitive settings file while still maintaining the file in version control. As you mention, the best solution would be one in which Git will transparently encrypt certain sensitive files when you push them so that locally (i.e. on any machine which has your certificate) you can use the settings file, but Git or Dropbox or whoever is storing your files under VC does not have the ability to read the information in plaintext.This gist  shows a tutorial on how to use the Git's smudge/clean filter driver with openssl to transparently encrypt pushed files. You just need to do some initial setup.You'll basically be creating a  folder containing 3 bash scripts,which are used by Git for decryption, encryption, and supporting Git diff. A master passphrase and salt (fixed!) is defined inside these scripts and you MUST ensure that .gitencrypt is never actually pushed.\nExample  script:Similar for  and . See Gist.Your repo with sensitive information should have a .gitattribute file (unencrypted and included in repo) which references the .gitencrypt directory (which contains everything Git needs to encrypt/decrypt the project transparently) and which is present on your local machine. contents:Finally, you will also need to add the following content to your  fileNow, when you push the repository containing your sensitive information to a remote repository, the files will be transparently encrypted. When you pull from a local machine which has the .gitencrypt directory (containing your passphrase), the files will be transparently decrypted.I should note that this tutorial does not describe a way to only encrypt your sensitive settings file. This will transparently encrypt the entire repository that is pushed to the remote VC host and decrypt the entire repository so it is entirely decrypted locally. To achieve the behavior you want, you could place sensitive files for one or many projects in one sensitive_settings_repo. You could investigate how this transparent encryption technique works with Git submodules  if you really need the sensitive files to be in the same repository.The use of a fixed passphrase could theoretically lead to brute-force vulnerabilities if attackers had access to many encrypted repos/files. IMO, the probability of this is very low. As a note at the bottom of this tutorial mentions, not using a fixed passphrase will result in local versions of a repo on different machines always showing that changes have occurred with 'git status'.Heroku pushes  for settings and secret keys:With Foreman and  files Heroku provide an enviable toolchain to export, import and synchronise environment variables.Personally, I believe it's wrong to save secret keys alongside code. It's fundamentally inconsistent with source control, because the keys are for services . The one boon would be that a developer can clone HEAD and run the application without any setup. However, suppose a developer checks out a historic revision of the code. Their copy will include last year's database password, so the application will fail against today's database.With the Heroku method above, a developer can checkout last year's app, configure it with today's keys, and run it successfully against today's database. The cleanest way in my opinion is to use environment variables. You won't have to deal with  files for example, and the project state on the production environment would be the same as your local machine's. I recommend reading 's config chapter, the others too if you're interested.   An option would be to put project-bound credentials into an encrypted container (TrueCrypt or Keepass) and push it.Interesting question btw. I just found this:  which looks very promising for automatic encryptionI suggest using configuration files for that and to not version them.You can however version examples of the files.I don't see any problem of sharing development settings. By definition it should contain no valuable data. was recently released by StackExchange and while I have yet to use it, it seems to exactly address the problems and support the features requested in this question.From the description on :I think GnuPG is the best way to go - it's already used in one git-related project (git-annex) to encrypt repository contents stored on cloud services. GnuPG (gnu pgp) provides a  strong key-based encryption.Now if your 'mypassword' file did not change then encrypting it will result with same ciphertext and it won't be added to the index (no redundancy). Slightest modification of mypassword results in radically different ciphertext and mypassword.gpg in staging area differs a lot from the one in repository, thus will be added to the commit. Even if the attacker gets a hold of your gpg key he still needs to bruteforce the password. If the attacker gets an access to remote repository with ciphertext he can compare a bunch of ciphertexts, but their number won't be sufficient to give him any non-negligible advantage.Later on you can use .gitattributes to provide an on-the-fly decryption for quit git diff of your password.Also you can have separate keys for different types of passwords etc.No, just don't, even if it's your private repo and you never intend to share it, don't.You should create a local_settings.py put it on VCS ignore and in your settings.py do something likeIf your secrets settings are that versatile, I am eager to say you're doing something wrongThis is the best way to manage a set of sane defaults for the config you checkin without requiring the config be complete, or contain things like hostnames and credentials.  There are a few ways to override default configs.Environment variables (as others have already mentioned) are one way of doing it.The best way is to look for an external config file that overrides the default config values.  This allows you to manage the external configs via a configuration management system like Chef, Puppet or Cfengine.  Configuration management is the standard answer for the management of configs separate from the codebase so you don't have to do a release to update the config on a single host or a group of hosts. Encrypting creds is not always a best practice, especially in a place with limited resources.  It may be the case that encrypting creds will gain you no additional risk mitigation and simply add an unnecessary layer of complexity.  Make sure you do the proper analysis before making a decision.Usually, i seperate password as a config file. and make them dist.And when i run , put the real password in  that copied.ps. when you work with git or hg. you can ignore  files to make  or Encrypt the passwords file, using for example GPG. Add the keys on your local machine and on your server. Decrypt the file and put it outside your repo folders.I use a passwords.conf, located in my homefolder. On every deploy this file gets updated. No, private keys and passwords do not fall under revision control. There is no reason to  burden everyone with read access to your repository with knowing sensitive service credentials used in production, when most likely not all of them should have access to those services.Starting with Django 1.4, your Django projects now ship with a  module that defines the  and it's a perfect place to start enforcing the use of a  settings module that contains site-specific configurations. This settings module is ignored from revision control, but it's presence is required when running your project instance as a WSGI application, typical for production environments. This is how it should look like:Now you can have a  module who's owner and group can be configured so that only authorized personnel and the Django processes can read the file's contents.If you need VCS for your secrets you should at least keep them in a second repository seperated from you actual code. So you can give your team members access to the source code repository and they won't see your credentials. Furthermore host this repository somewhere else (eg. on your own server with an encrypted filesystem, not on github) and for checking it out to the production system you could use something like .Another approach could be to completely avoid saving secrets in version control systems and instead use a tool like , a secret storage with key rolling and auditing, with an API and embedded encryption.Since asking this question I have settled on a solution, which I use when developing small application with a small team of people.git-crypt uses GPG to transparently encrypt files when their names match certain patterns. For intance, if you add to your  file......then a file like  will always be pushed to remote repos with encryption, but remain unencrypted on your local file system.If I want to add a new GPG key (a person) to your repo which can decrypt the protected files then run . This creates a new commit. The new user will be able to decrypt subsequent commits.You could use EncFS if your system provides that. Thus you could keep your encrypted data as a subfolder of your repository, while providing your application a decrypted view to the data mounted aside. As the encryption is transparent, no special operations are needed on pull or push. It would however need to mount the EncFS folders, which could be done by your application based on an password stored elsewhere outside the versioned folders (eg. environment variables)."},
{"body": "Why in this millenium should Python  specify a maximum line length of 79 characters?Pretty much every code editor under the sun can handle longer lines.  What to do with wrapping should be the choice of the content consumer, not the responsibility of the content creator.Are there any (legitimately) good reasons for adhering to 79 characters in this age?Much of the value of PEP-8 is to stop people arguing about inconsequential formatting rules, and get on with writing good, consistently formatted code. Sure, no one really thinks that 79 is optimal, but there's no obvious gain in changing it to 99 or 119 or whatever your preferred line length is. I think the choices are these: follow the rule and find a worthwhile cause to battle for, or provide some data that demonstrates how readability and productivity vary with line length. The latter would be extremely interesting, and would have a good chance of changing people's minds I think.Keeping your code human readable not just machine readable. A lot of devices still can only show 80 characters at a time. Also it makes it easier for people with larger screens to multi-task by being able to set up multiple windows to be side by side.Readability is also one of the reasons for enforced line indentation.I believe those who study typography would tell you that 66 characters per a line is supposed to be the most readable width for length. Even so, if you need to debug a machine remotely over an ssh session, most terminals default to 80 characters, 79 just fits, trying to work with anything wider becomes a real pain in such a case. You would also be suprised by the number of developers using vim + screen as a day to day environment.I am a programmer who has to deal with a lot of code on a daily basis. Open source and what has been developed in house.As a programmer, I find it useful to have many source files open at once, and often organise my desktop on my (widescreen) monitor so that two source files are side by side. I might be programming in both, or just reading one and programming in the other.I find it dissatisfying and frustrating when one of those source files is >120 characters in width, because it means I can't comfortably fit a line of code on a line of screen. It upsets formatting to line wrap.I say '120' because that's the level to which I would get annoyed at code being wider than. After that many characters, you should be splitting across lines for readability, let alone coding standards.I write code with 80 columns in mind. This is just so that when I do leak over that boundary, it's not such a bad thing.Printing a monospaced font at default sizes is (on A4 paper) 80 columns by 66 lines.79 characters (well, actually 72 characters) is where most text-based email readers linewrap.  So code cut-and-pasted into an email is a lot more readable.because if you push it beyond the 80th column it means that either you are writing a very long and complex line of code that does too much (and so you should refactor), or that you indented too much (and so you should refactor).Here's why I like the 80-character with: at work I use Vim and work on two files at a time on a monitor running at, I think, 1680x1040 (I can never remember).  If the lines are any longer, I have trouble reading the files, even when using word wrap.  Needless to say, I hate dealing with other people's code as they love long lines.Since whitespace has semantic meaning in Python, some methods of word wrapping could produce incorrect or ambiguous results, so there needs to be some limit to avoid those situations.  An 80 character line length has been standard since we were using teletypes, so 79 characters seems like a pretty safe choice.I agree with Justin.  To elaborate, overly long lines of code are harder to read by humans  and some people might have console widths that only accommodate 80 characters per line.  The style recommendation is there to ensure that the code you write can be read by as many people as possible on as many platforms as possible and as comfortably as possible."},
{"body": "I need your help about matplotlib. Yes, I did not forget calling the pyplot.show().It returns  as the output.There is nothing to happen. No error message. No new window. Nothing. I install  by using pip and I didn't take any error messages. Details:I use,If I set my backend to  in ,\nthen I can reproduce your symptoms:~/.matplotlib/matplotlibrc:Note that the file  may not be in directory . In this case, the following code shows where it is:If you edit  and change the backend to something like , you should see a plot. You can list all the backends available on your machine withIt should return a list like:Reference:I ran into the exact same problem on Ubuntu 12.04, because I installed matplotlib (within a virtualenv) usingTo make long story short, my advice is: don't try to install matplotlib using pip or by hand; let a real package manager (e.g. apt-get / synaptic) install it and all its dependencies for you.Unfortunately, matplotlib's backends (alternative methods for actually rendering your plots) have all sorts of dependencies that pip will not deal with.  Even worse, it fails silently; that is,  appears to install matplotlib successfully.  But when you try to use it (e.g. ), no plot window will appear.  I tried all the different backends that people on the web suggest (Qt4Agg, GTK, etc.), and they all failed (i.e. when I tried to import matplotlib.pyplot, I get  because it's trying to import some dependency that's missing).  I then researched how to install those dependencies, but it just made me want to give up using pip (within virtualenv) as a viable installation solution for any package that has non-Python package dependencies.  The whole experience sent me crawling back to apt-get / synaptic (i.e. the Ubuntu package manager) to install software like matplotlib.  That worked perfectly.  Of course, that means you can only install into your system directories, no virtualenv goodness, and you are stuck with the versions that Ubuntu distributes, which may be way behind the current version...%matplotlib inlineFor me working with notebook, adding the above line before the plot works.For future reference,I have encountered the same problem -- pylab was not showing under ipython.  The problem was fixed by changing ipython's config file {ipython_config.py}.  In the config fileI changed 'auto' to 'qt' and now I see graphsJust type:See  at 23:30 ! is used because of my import:  as pltI'm using python2.7 on a mac with iTerm2.I had to install matplotlib from source to get this to work. The key instructions (from ) are:By changing the backend, as @unutbu says, I just ran into loads more problems with all the different backends not working either.Adding the following two lines before importing pylab seems to work for me What solved my problem was just using the below two lines in ipython notebook at the topAnd it worked. I'm using Ubuntu16.04 and ipython-5.1Be sure to have this startup script enabled :\n( Preferences > Console > Advanced Options ) If the standard PYTHONSTARTUP is enabled you won't have an interactive plotSimilar to @Rikki, I solved this problem by upgrading  with . If you can't upgrade uninstalling and reinstalling may work.For Ubuntu 12.04:"},
{"body": "I'm basically looking for a python version of Given a list of lists, I need a new list that gives all the possible combinations of items between the lists.The number of lists is unknown, so I need something that works for all cases. Bonus points for elegance!you need :The most elegant solution is to use  in python 2.6.If you aren't using Python 2.6, the docs for itertools.product actually show an equivalent function to do the product the \"manual\" way:I hope you find that as elegant as I did when I first encountered it.Numpy can do it:Nothing wrong with straight up recursion for this task, and if you need a version that works with strings, this might fit your needs:"},
{"body": "I could use some pseudo-code, or better, Python.  I am trying to implement a rate-limiting queue for a Python IRC bot, and it partially works, but if someone triggers less messages than the limit (e.g., rate limit is 5 messages per 8 seconds, and the person triggers only 4), and the next trigger is over the 8 seconds (e.g., 16 seconds later), the bot sends the message, but the queue becomes full and the bot waits 8 seconds, even though it's not needed since the 8 second period has lapsed.Here the , if you want just to drop messages when they arrive too quickly (instead of queuing them, which makes sense because the queue might get arbitrarily large):There are no datastructures, timers etc. in this solution and it works cleanly :) To see this, 'allowance' grows at speed 5/8 units per seconds at most, i.e. at most five units per eight seconds. Every message that is forwarded deducts one unit, so you can't send more than five messages per every eight seconds.Note that  should be an integer, i.e. without non-zero decimal part, or the algorithm won't work correctly (actual rate will not be ). E.g.  does not work because  will never grow to 1.0. But  works fine.Use this decorator @RateLimited(ratepersec) before your function that enqueues.Basically, this checks if 1/rate secs have passed since the last time and if not, waits the remainder of the time, otherwise it doesn't wait. This effectively limits you to rate/sec. The decorator can be applied to any function you want rate-limited.In your case, if you want a maximum of 5 messages per 8 seconds, use @RateLimited(0.625) before your sendToQueue function.A Token Bucket is fairly simple to implement.Start with a bucket with 5 tokens.Every 5/8 seconds: If the bucket has less than 5 tokens, add one.Each time you want to send a message: If the bucket has \u22651 token, take one token out and send the message. Otherwise, wait/drop the message/whatever.(obviously, in actual code, you'd use an integer counter instead of real tokens and you can optimize out the every 5/8s step by storing timestamps)Reading the question again, if the rate limit is fully reset each 8 seconds, then here is a modification:Start with a timestamp, , at a time long ago (e.g., at the epoch). Also, start with the same 5-token bucket.Strike the every 5/8 seconds rule.Each time you send a message: First, check if  \u2265 8 seconds ago. If so, fill the bucket (set it to 5 tokens). Second, if there are tokens in the bucket, send the message (otherwise, drop/wait/etc.). Third, set  to now.That should work for that scenario.I've actually written an IRC bot using a strategy like this (the first approach). Its in Perl, not Python, but here is some code to illustrate:The first part here handles adding tokens to the bucket. You can see the optimization of adding tokens based on time (2nd to last line) and then the last line clamps bucket contents to the maximum (MESSAGE_BURST)$conn is a data structure which is passed around. This is inside a method that runs routinely (it calculates when the next time it'll have something to do, and sleeps either that long or until it gets network traffic). The next part of the method handles sending. It is rather complicated, because messages have priorities associated with them.That's the first queue, which is run no matter what. Even if it gets our connection killed for flooding. Used for extremely important things, like responding to the server's PING. Next, the rest of the queues:Finally, the bucket status is saved back to the $conn data structure (actually a bit later in the method; it first calculates how soon it'll have more work)As you can see, the actual bucket handling code is very small \u2014 about four lines. The rest of the code is priority queue handling. The bot has priority queues so that e.g., someone chatting with it can't prevent it from doing its important kick/ban duties.to block processing until the message can be sent, thus queuing up further messages,  antti's beautiful solution may also be modified like this:it just waits until enough allowance is there to send the message. to not start with two times the rate, allowance may also initialized with 0.Keep the time that the last five lines were sent.  Hold the queued messages until the time the fifth-most-recent message (if it exists) is a least 8 seconds in the past (with last_five as an array of times):One solution is to attach a timestamp to each queue item and to discard the item after 8 seconds have passed. You can perform this check each time the queue is added to.This only works if you limit the queue size to 5 and discard any additions whilst the queue is full.If someone still interested, I use this simple callable class in conjunction with a timed LRU key value storage to limit request rate per IP. Uses a deque, but can rewritten to be used with a list instead.How about this:I needed a variation in Scala. Here it is:Here is how it can be used:Just a python implementation of a code from accepted answer."},
{"body": "Suppose you have three objects you acquire via context manager, for instance A lock, a db connection and an ip socket.\nYou can acquire them by:But is there a way to do it in one block? something likeFurthermore, is it possible, given an array of unknown length of objects that have context managers, is it possible to somehow do:If the answer is \"no\", is it because the need for such a feature implies bad design, or maybe I should suggest it in a pep? :-P, you can use :is equivalent to:Note that this isn't exactly the same as normally using nested , because , , and  will all be called initially, before entering the context managers. This will not work correctly if one of these functions may raise exceptions, but will work for the examples in the question., syntax has been added for this, and  has been deprecated:, you can also enter an unknown-length list of context managers by using :This allows you to create the context managers as you are adding them to the , which prevents the possible problem with .The first part of your question is possible in .The second part of your question is solved with  in ."},
{"body": "I've got a timedelta. I want the days, hours and minutes from that - either as a tuple or a dictionary... I'm not fussed.I must have done this a dozen times in a dozen languages over the years but Python usually has a simple answer to everything so I thought I'd ask here before busting out some nauseatingly simple (yet verbose) mathematics.Mr Fooz raises a good point.I'm dealing with \"listings\" (a bit like ebay listings) where each one has a duration. I'm trying to find the time left by doing Am I right in saying that wouldn't account for DST? If not, what's the simplest way to add/subtract an hour?If you have a  value ,  already gives you the \"days\" you want.  values keep fraction-of-day as seconds (not directly hours or minutes) so you'll indeed have to perform \"nauseating simple mathematics\", e.g.:This is a bit more compact, you get the hours, minutes and seconds in two lines.As for DST, I think the best thing is to convert both  objects to seconds. This way the system calculates DST for you.I don't understand how about thisYou get minutes and seconds of a minute as a float.timedeltas have a  and  attribute .. you can convert them yourself with ease."},
{"body": "I am trying to pass a list as an argument to a command line program. Is there an  option to pass a list as option?Script is called like belowUse the  option. takes 1 or more arguments,  takes zero or more.Let's take a look in more detail at some of the different ways one might try to do this, and the end result.Here is the output you can expect::: I don't mean in general.. I mean using quotes to  is not what you want. I prefer passing a delimited string which I parse later in the script. The reasons for this are; the list can be of any type  or , and sometimes using  I run into problems if there are multiple optional arguments and positional arguments.Then,or,will work fine. The delimiter can be a space, too, which would though enforce quotes around the argument value like in the example in the question.Additionally to , you might want to use  if you know the list in advance:If you are intending to make a single switch take multiple parameters, then you use . If your example '-l' is actually taking integers:ProducesIf you specify the same argument multiple times, the default action () replaces the existing data.The alternative is to use the  action:Which producesOr you can write a custom handler/action to parse comma-separated values so that you could doUsing  in argparse's add_argument methodI use nargs='' to the option to pick defaults if I am not passing any explicit argumentsIncluding a code snippet as example: The below sample code is written in python3. By changing the print statement format, can run in python2Note: I am collecting multiple string arguments that gets stored in the list - opts.alist\nIf you want list of integers, change the type parameter on parser.add_argument to int"},
{"body": "I'm trying to use the sort feature when querying my mongoDB, but it is failing.  The same query works in the MongoDB console but not here.  Code is as follows:The error I get is as follows:I found a link elsewhere that says I need to place a 'u' infront of the key if using pymongo, but that didn't work either.  Anyone else get this to work or is this a bug., in pymongo, takes  and  as parameters.So if you want to sort by, let's say,  then you should You can try this:This also works:I'm using this in my code, please comment if i'm doing something wrong here, thanks.Python uses key,direction. You can use the above way.So in your case you can do this"},
{"body": "Assuming  is a Python dictionary, what's the best, most elegant, most \"pythonic\" way of refactoring code like this:Like this:You can also use the  like so:You can pass any ordinary function instead of lambda:While  is a nice idiom, it's slower than  (and slower than  if presence of the key in the dictionary can be expected most of the time):For multiple different defaults try this:There is a method in python dictionaries to do this: However this method sets the value of  to  if key  is not already defined, unlike what the question asked.(this is a late answer)An alternative is to subclass the  class and implement the  method, like this:Examples:"},
{"body": "I know that Python does not support method overloading, but I've run into a problem that I can't seem to solve in a nice Pythonic way.I am making a game where a character needs to shoot a variety of bullets, but how do I write different functions for creating these bullets? For example suppose I have a function that creates a bullet travelling from point A to B with a given speed. I would write a function like this:But I want to write other functions for creating bullets like:And so on with many variations. Is there a better way to do it without using so many keyword arguments cause its getting kinda ugly fast. Renaming each function is pretty bad too because you get either , , or .To address some answers:Python does support \"method overloading\" as you present it. In fact, what you just describe is trivial to implement in Python, in so many different ways, but I would go with:In the above code,  is a plausible default value for those arguments, or . You can then call the method with only the arguments you are interested in, and Python will use the default values. You could also do something like this:Another alternative is to directly hook the desired function directly to the class or instance:Yet another way is to use an abstract factory pattern:You can use \"roll-your-own\" solution for function overloading. This one is copied from  about multimethods (because there is little difference between mm and overloading in python):The usage would beMost restrictive limitations  are:What you are asking for, is called . See  language examples which demonstrates different types of dispatches.However, before looking at that, we'll first tackle why  is not really what you want in python.First one needs to understand the concept of overloading and why it's not applicable to python.Python is a  typed language, so the concept of overloading simply does not apply to it. However, all is not lost, since we can create such  at run-time:So we should be able to do  in python or, as it is alternatively called, .The multimethods are also called :Python does not support this out of the box. But, as it happens, there is an excellent python package called  that does exactly that. Here is how we might use  package to implement your methods:A possible option is to use the multipledispatch module as detailed here:\nInstead of doing this:You can do this:With the resulting usage:This type of behaviour is typically solved (in OOP languages) using Polymorphism. Each type of bullet would be responsible for knowing how it travels. For instance:Pass as many arguments to the c_function that exist, then do the job of determining which c function to call based on the values in the initial c function. So, python should only ever be calling the one c function. That one c function looks at the arguments, and then can delegate to other c functions appropriately.You're essentially just using each subclass as a different data container, but by defining all the potential arguments on the base class, the subclasses are free to ignore the ones they do nothing with.When a new type of bullet comes along, you can simply define one more property on the base, change the one python function so that it passes the extra property, and the one c_function that examines the arguments and delegates appropriately. Doesn't sound too bad I guess.In Python 3.4 was added .Here is short API description from PEP.To define a generic function, decorate it with the @singledispatch decorator. Note that the dispatch happens on the type of the first argument. Create your function accordingly:To add overloaded implementations to the function, use the register() attribute of the generic function. This is a decorator, taking a type parameter and decorating a function implementing the operation for that type:I think your basic requirement is to have a C/C++ like syntax in python with the least headache possible.  Although I liked Alexander Poluektov's answer it doesn't work for classes.  The following should work for classes.  It works by distinguishing by the number of non keyword arguments (but doesn't support distinguishing by type):And it can be used simply like this:Output:Either use multiple keyword arguments in the definition, or create a  hierarchy whose instances are passed to the function.I think aclass hierarchy with the associated polymorphism is the way to go. You can effectively overload the base class constructor by using a metaclass so that calling the base class results in the creation of the appropriate subclass object. Below is some sample code to illustrate the essence of what I mean.Use keyword arguments with defaults. E.g.In the case of a straight bullet versus a curved bullet, I'd add two functions:  and .By passing keyword args, overloading methods is tricky in python. However, there could be usage of passing the dict, list or primitive variables.I have tried something for my use cases, this could help here to understand people to overload the methods.Let's take your example:a class overload method with call the methods from different class.pass the arguments from remote class:OR \nSo, handling is being achieved for list, Dictionary or primitive variables from method overloading.try it out for your codes."},
{"body": "There is a lot of documentation on how to serialize a Model QuerySet but how do you just serialize to JSON the fields of a Model Instance?You can easily use a list to wrap the required object and that's all what django serializers need to correctly serialize it, eg.:To avoid the array wrapper, remove it before you return the response:I found this interesting post on the subject too:It uses django.forms.models.model_to_dict, which looks like the perfect tool for the job.If you're dealing with a list of model instances the best you can do is using , it gonna fit your need perfectly. However, you are to face an issue with trying to serialize a  object, not a  of objects. That way, in order to get rid of different hacks, just use Django's  (if I'm not mistaken,  relies on it, too): You now just need one straight  call to serialize it to json:That's it! :)It sounds like what you're asking about involves serializing the data structure of a Django model instance for interoperability.  The other posters are correct: if you wanted the serialized form to be used with a python application that can query the database via Django's api, then you would wan to serialize a queryset with one object.  If, on the other hand, what you need is a way to re-inflate the model instance somewhere else without touching the database or without using Django, then you have a little bit of work to do.Here's what I do:First, I use  for the conversion.  It happened to be what I found first, but it might not be the best.  My implementation depends on one of its features, but there should be similar ways with other converters.Second, implement a  method on all models that you might need serialized.  This is a magic method for , but it's probably something you're going to want to think about no matter what implementation you choose.  The idea is that you return an object that is directly convertible to  (i.e. an array or dictionary).  If you really want to do this automatically:This will not be helpful to you unless you have a completely flat data structure (no , only numbers and strings in the database, etc.).  Otherwise, you should seriously think about the right way to implement this method.Third, call  and you have what you want.If you're asking how to serialize a single object from a model and you  you're only going to get one object in the queryset (for instance, using objects.get), then use something like:which would get you something of the form:I solved this problem by adding a serialization method to my model:Here's the verbose equivalent for those averse to one-liners: is an ordered list of model fields which can be accessed from instances and from the model itself.I return the dict of my instanceso it return something like {'field1':value,\"field2\":value,....}how about this way:or exclude anything you don't want.Here's my solution for this, which allows you to easily customize the JSON as well as organize related recordsFirstly implement a method on the model. I call is  but you can call it whatever you like, e.g.:Then in the view I do:It doesn't seem you can serialize an instance, you'd have to serialize a QuerySet of one object.I run out of the  release of django, so this may not be in earlier versions.To serialize and deserialze, use the following:Use list, it will solve problemStep1:Step2:Step3:"},
{"body": "I would like to increase the width of the ipython notebook in my browser. I have a high-resolution screen, and I would like to expand the cell width/size to make use of this extra space.Thanks!edit: 5/2017I now use jupyterthemes: and this command:which sets the width to 100% with a nice theme.That  solution didn't actually work on my IPython, however luckily someone suggested a working solution for new IPythons:Create a file  (iPython) or  (Jupyter) with contentThen restart iPython/Jupyter notebooks. Note that this will affect all notebooks.If you don't want to change your default settings, and you only want to change the width of the current notebook you're working on, you can enter the following into a cell:To get this to work with jupyter (version 4.0.6) I created  containing:You can set the CSS of a notebook by calling a stylesheet from any cell. As an example, take a look at the .In particular, creating a file containingshould give you a starting point. However, it may be necessary to also adjust e.g  to deal with markdown as well as code cells.If that file is  then add a cell containing:This will apply all the stylings, and, in particular, change the cell width.I recommend using . This way you can override css for all notebooks, without adding any code to notebooks. \nWe don't like to change configuration in .ipython/profile_default, since  we are running a shared Jupyter server for the whole team.I made a  style specifically for vertically-oriented high-res screens, that makes cells wider and adds a bit of empty-space in the bottom, so you can position the last cell in the centre of the screen.\n\nYou can, of course, modify my css to your liking, if you have a different layout, or you don't want extra empty-space in the end.Last but not least, Stylish is a great tool to have in your toolset, since you can easily customise other sites/tools to your liking (e.g. Jira, Podio, Slack, etc.)"},
{"body": "I am looking for a function that takes as input two lists, and returns the , and the significance of the correlation.You can have a look at :The Pearson correlation can be calculated with numpy's .If you don't feel like installing scipy, I've used this quick hack, slightly modified from :(Edited for correctness.)The following code is a straight-up interpretation of :Test:returnsThis agrees with Excel, ,  (also ), which return 0.981980506 and 0.9819805060619657, and 0.98198050606196574, respectively.:: Fixed a bug pointed out by a commenter.An alternative can be a native scipy function from  which calculates:And here is an example:will return you: Rather than rely on numpy/scipy, I think my answer should be the easiest to code and  in calculating the Pearson Correlation Coefficient (PCC) .The  of PCC is basically to show you how  the two variables/lists are. \nIt is important to note that the PCC value ranges .\nA value between 0 to 1 denotes a positive correlation.\nValue of 0 = highest variation (no correlation whatsoever).\nA value between -1 to 0 denotes a negative correlation.Hmm, many of these responses have long and hard to read code...I'd suggest using numpy with its nifty features when working with arrays:Here is an implementation for pearson correlation based on sparse vector. The vectors here are expressed as a list of tuples expressed as (index, value). The two sparse vectors can be of different length but over all vector size will have to be same. This is useful for text mining applications where the vector size is extremely large due to most features being bag of words and hence calculations are usually performed using sparse vectors. Unit tests:You can do this with , too:This givesThis is a implementation of Pearson Correlation function using numpy:Here's a variant on mkh's answer that runs much faster than it, and scipy.stats.pearsonr, using numba.You can take a look at this article. This is a well-documented example for calculating correlation based on historical forex currency pairs data from multiple files using pandas library (for Python), and then generating a heatmap plot using seaborn library.You may wonder how to interpret your probability in the context of looking for a correlation in a particular direction (negative or positive correlation.)  Here is a function I wrote to help with that.  It might even be right!It's based on info I gleaned from  and , thanks to other answers posted here."},
{"body": "I have a dictionary of points, say:I want to create a new dictionary with all the points whose x and y value is smaller than 5, i.e. points 'a', 'b' and 'd'.According to the , each dictionary has the  function, which returns a list of   tuple:So I have written this:Is there a more elegant way? I was expecting Python to have some super-awesome  function...Nowadays, in Python 2.7 and up, you can use a dict comprehension:And in Python 3:You could choose to call  instead of  if you're in Python 2 and  may have a  of entries. may be overkill if you know for sure each point will always be 2D only (in that case you might express the same constraint with an ) but it will work fine;-).I think that Alex Martelli's answer is definitely the most elegant way to do this, but just wanted to add a way to satisfy your want for a super awesome  method in a Pythonic sort of way:Basically we create a class that inherits from , but adds the filter method. We do need to use  for the the filtering, since using  while destructively iterating will raise exception."},
{"body": "What are the relative merits / downsides of various Python bundles (EPD / Anaconda) vs. a manual install?I have installed EPD academic, and I have no issues with it. It provides more packages that I think I will ever need, and it is very easy to update using enpkg enstaller. The EPD academic licence requires yearly renewal however and the free version does not do updates as easily.At the moment I really only use a handful of packages such as , , , , ,  and their respective dependencies. For such limited use am I better off with manual install and  or do the bundles offer anything over and above this?: Nowadays I always recommend Anaconda. It includes lots of Python packages for scientific computing, data science, web development, etc. It also provides a superior environment tool, , which allows to easily switch between environments, even between Python 2 and 3. It is also updated very quickly as soon as a new version of a package is released, and you can just do  to update it.:On Windows, what is complicated is to compile the math packages, so I think a manual install is a viable option only if you are interested only in , without other packages.Therefore better chose either EPD (now Canopy) or Anaconda.Anaconda has around 270 packages, including the most important for most scientific applications and data analysis, that is, , , , , , .\nSo if this is enough for you, I would choose Anaconda.Instead, if you are interested in other packages, and even more if you use any of the Enthought packages ( for example is very useful for realtime data visualization), then EPD/Canopy is probably a better choice. The Academic version has a larger number of packages in the base install, and many more in the repository. Anaconda also includes Chaco.I have tried various Windows distributions in the last year, trying to find one sutable for my work environment (behind a proxy, but without access to proxy configuration).Here is my feedback from experience:\nWe had a license of EPD, but it was old and we were unable to update becasue of the weird proxy situation. In order to add some packages (such as recent version of ), I compiled from source. To update  and , I used the precompiled installer from , but it would sometimes screw up compatibility. I loved having a fully configured  and , and it simply worked out of the box.After a while, I tried installing the free version of Canopy, but it lacks Cython and py2exe and some specific advanced packaged I needed, so I never really used it.\nSome of my colleagues bought the full Canopy license, but we're still not sure how they're going to update...\nNot wanting to struggle with licenses, I installed Python(x,y) at home. The only downside I noticed right now is that the standard installation requires you to select which packages you want. It's both a good and a bad point, because I can't be sure that my clients will have the exact same configuration as I do when I install. (The Enthought tool suite can be installed in Python(x,y).)\nAfter using Python(x,y) for a while, I just noticed I installed the 32 bit version. Although it is not clear on their website, it seems they don't have a 64 bit version as of July 2015. I'm going to uninstall it and get a 64 bit distribution.\nWhen I first wrote this, Anaconda didn't seem to have enough packages yet. A couple of years later, it seems much better, I'm going to give it a try!\nIn order to avoid version compatibility issues with our old EPD version, I ended up using manual Python installation and adding additional packages from the LFD website linked above. It works great, but I would still suggest Canopy to a new user who requires advanced packages (like  or ). If you go for Canopy, get the full licence (Academic or purchased). Else, go with Python(x,y), it will end up being the same.\nNo need for a distribution. It's all relatively recent (+/- 6 months is tolerable) and pre-compiled. You just need to execute  and it's there! Most advanced packages are there as well.The other answers cover the ground quite nicely, so I just want to remark on one particular aspect that nobody has mentioned yet. It is probably fairly niche, but it  potentially make or break Anaconda or Canopy for some people under Linux systems:Anaconda Python builds use the UCS4 Unicode mode, whereas Enthought Canopy uses UCS2.What this means in practical terms is that if you rely on any extensions which you cannot compile yourself for whatever reason (e.g. pre-compiled proprietary libraries), if they happen not to be built for a Python version with the same mode, you may sooner or later run into errors that look something like .According to , UCS4 seems to currently be more popular and recommended. Also, the whole UCS compatibility issues seem to only affect 2.x and < 3.3 versions.I used Anaconda for years and liked it quite a bit. Unfortunately,  (now ) is unavailable without the enterprise edition. I want to use Jupyter notebooks in the classroom, so I switched to Canopy. It seems easy enough to install all of the packages we need. Admittedly, we haven't tested them all."},
{"body": "I'm just curious if anyone knows if there's good reason why django's orm doesn't call 'full_clean' on a model unless it is being saved as part of a model form.  (NOTE: quote updated for Django 1.6... previous django docs had a caveat about ModelForms as well.)Are there good reasons why people wouldn't want this behavior? I'd think if you took the time to add validation to a model, you'd want that validation run every time the model is saved.I know how to get everything to work properly, I'm just looking  for an explanation.AFAIK, this is because of backwards compatibility. There are also problems with ModelForms with excluded fields, models with default values, pre_save() signals, etc.Sources you might be intrested in:Because of the compatibility considering, the auto clean on save is not enabled in django kernel.If we are starting a new project and want the default  method on Model could clean automatically, we can use the following signal to do clean before every model was saved. The simpliest way to call the  method is just to override  method in your :Instead of inserting a piece of code that declares a receiver, we can use an app as  section in Before that, you may need to install  using PyPI:"},
{"body": "I guess it could be something like , but that is more like a frozen-keys dict (a half-frozen dict). Isn't it?A \"frozendict\" should be a frozen dictionary, it should have , , , etc., and support , , etc.Python doesn't have a builtin frozendict type. It turns out this wouldn't be useful too often (though it would still probably be useful more often than  is).The most common reason to want such a type is when memoizing function calls for functions with unknown arguments. The most common solution to store a hashable equivalent of a dict (where the values are hashable) is something like . This depends on the sorting not being a bit insane. Python cannot positively promise sorting will result in something reasonable here. (But it can't promise much else, so don't sweat it too much.)You could easily enough make some sort of wrapper that works much like a dict. It might look something likeIt should work great:Curiously, although we have the seldom useful  in python, there's still no frozen mapping.  The idea was rejected in .  So the python 2 solution to this:Still seems to be the somewhat lame:In python3 you have the option of :Now the default config  be updated dynamically, but remain immutable where you want it to be immutable by passing around the proxy instead.  So changes in the  will update  as expected, but you can't write to the mapping proxy object itself.  Admittedly it's not quite the same thing as an \"immutable, hashable dict\" - but it's a decent substitute given the same kind of use cases for which we might want a frozendict.    Assuming the keys and values of the dictionary are themselves immutable (e.g. strings) then:Here is the code I've been using.  I subclassed frozenset.  The advantages of this are the following.Update Jan 21 2015:  The original piece of code I posted in 2014 used a for-loop to find a key that matched.  That was incredibly slow.  Now I've put together an implementation which takes advantage of frozenset's hashing features.  Key-value pairs are stored in special containers where the  and  functions are based on the key only.  This code has also been formally unit-tested, unlike what I posted here in August 2014.MIT-style license.I think of frozendict everytime I write a function like this:Yes, this is my second answer, but it is a completely different approach.  The first implementation was in pure python.  This one is in Cython.  If you know how to use and compile Cython modules, this is just as fast as a regular dictionary.  Roughly .04 to .06 micro-sec to retrieve a single value.This is the file \"frozen_dict.pyx\"Here's the file \"setup.py\"If you have Cython installed, save the two files above into the same directory.  Move to that directory in the command line.And you should be done.There appears to be a nearly trivial solution that the other answers have missed. The most trivial answer ought to be something likeexcept that  is a read-only attribute in , so we have to override it:You can add a  method to your class in the same way that @MikeGraham's answer suggests.You can make the class even more immutable by having  and  raise errors, especially the latter one. Alternatively, you could define an empty  attribute on the class. None of this is strictly necessary, however, since user-defined attributes will not affect the result of :Another thing to keep in mind is that you can always modify a dict defined like this if you are willing to jump through the hoops:will bypass the frozenness. Whether this is a bug or a feature is for you to decide.The main disadvantage of  is that it needs to be specified before it is used, so it's less convenient for single-use cases.However, there is a practical workaround that can be used to handle many such cases. Let's say that you want to have an immutable equivalent of the following dict:This can be emulated like this:It's even possible to write an auxiliary function to automate this:Of course this works only for flat dicts, but it shouldn't be too difficult to implement a recursive version.Another option is the  class from the  package.You may use  from  package as:As per the :"},
{"body": "In Python 2.5.2, the following code raises a TypeError:If I replace the  with , it will work. What's the explanation for this?The reason is that super() only operates on new-style classes, which in the 2.x series means extending from object.In addition, don't use super() unless you have to. It's not the general-purpose \"right thing\" to do with new-style classes that you might suspect.There are times when you're expecting multiple inheritance and you might possibly want it, but until you know the hairy details of the MRO, best leave it alone and stick to:I tried the various X.a() methods; however, they seem to require an instance of X in order to perform a(), so I did X().a(self), which seems more complete than the previous answers, at least for the applications I've encountered. It doesn't seem to be a good way of handling the problem as there is unnecessary construction and destruction, but it works fine.My specific application was Python's cmd.Cmd module, which is evidently not a NewStyle object for some reason.Final Result:In case none of the above answers mentioned it clearly. Your parent class needs to inherit from \"object\", which would essentially turn it into a new style class. "},
{"body": "I'm starting new Google App Engine application and currently considering two frameworks:  and . I'm rather satisfied with built-in webapp framework that I've used for my previous App Engine application, so I think webapp2 will be even better and I won't have any problems with it.However, there are a lot of good reviews of Flask, I really like its approach and all the things that I've read so far in the documentation and I want to try it out. But I'm a bit concerned about limitations that I can face down the road with Flask.So, the question is -  By \"problem\" I mean something that I can't work around in several lines of code (or any reasonable amount of code and efforts) or something that is completely impossible.And as a follow-up question: are there any killer-features in Flask that you think can blow my mind and make me use it despite any problems that I can face? I'm the author of tipfy and webapp2.A big advantage of sticking with webapp (or its natural evolution, webapp2) is that you don't have to create your own versions for existing SDK handlers for your framework of your choice.For example,  uses a webapp handler. To use it in a pure Flask view, using werkzeug.Request and werkzeug.Response, you'll need to implement deferred for it (like I did  for tipfy).The same happens for other handlers: blobstore (Werkzeug still doesn't support range requests, so you'll need to use WebOb even if you create your own handler -- see ), mail, XMPP and so on, or others that are included in the SDK in the future.And the same happens for libraries created with App Engine in mind, like , which is based on webapp and would need a port or adapter to work with other frameworks, if you don't want to mix webapp and your-framework-of-choice handlers in the same app.So, even if you choose a different framework, you'll end a) using webapp in some special cases or b) having to create and maintain your versions for specific SDK handlers or features, if you'll use them.I much prefer Werkzeug over WebOb, but after over one year porting and maintaining versions of the SDK handlers that work natively with tipfy, I realized that this is a lost cause -- to support GAE for the long term, best is to stay close to webapp/WebOb. It makes support for SDK libraries a breeze, maintenance becomes a lot easier, it is more future-proof as new libraries and SDK features will work out of the box and there's the benefit of a large community working around the same App Engine tools.A specific webapp2 defense is summarized . Add to those that  and is  and you have a good set of compelling reasons to go for it. Also, webapp2 has a big chance to be included in a future SDK release (this is extra-official, don't quote me :-) which will push it forward and bring new developers and contributions.That said, I'm a big fan of Werkzeug and the Pocoo guys and borrowed a lot from Flask and others (web.py, Tornado), but -- and, you know, I'm biased -- the above webapp2 benefits should be taken into account.Your question is extremely broad, but there appears to be no big problems using Flask on Google App Engine.This mailing list thread links to several templates:And here is a tutorial specific to the Flask / App Engine combination:Also, see , , and  for issues people have had with Flask and Google App Engine.I think google app engine officially supports flask framework. There is a sample code and tutorial here -> For me the decision for webapp2 was easy when I discovered that flask is not an object-oriented framework (from the beginning), while webapp2 is a pure object oriented framework. webapp2 uses Method Based Dispatching as standard for all RequestHandlers (as flask documentation calls it and implements it since V0.7 in MethodViews). While in flask MethodViews are an add-on it is a core design principle for webapp2. So your software design will look different using both frameworks. Both frameworks use nowadays jinja2 templates and are fairly feature identical. I prefer to add security checks to a base-class RequestHandler and inherit from it. This is also good for utility functions, etc. As you can see for example in link [3] you can override methods to prevent dispatching a request.If you are an OO-person, or if you need to design a REST-server, I would recommend webapp2 for you. If you prefer simple functions with decorators as handlers for multiple request-types, or you are uncomfortable with OO-inheritance then choose flask. I think both frameworks avoid the complexity and dependencies of much bigger frameworks like pyramid.I didn't try webapp2 and found that tipfy was a bit difficult to use since it required setup scripts and builds that configure your python installation to other than default. F\u00f6r these and other reasons I havn't made my largest project depend on a framework and I use the plain webapp instead, add the library called beaker to get session capability and django already has builtin translations for words common to many usecases so when building a localized application django was the right choice for my largest project. The 2 other frameworks I actually deployed with projects to a production environment were GAEframework.com and web2py and generally it seems that adding a framework which changes its template engine could lead to incompatibilities between old and new versions. So my experience is that I'm being reluctant to adding a framework to my projects unless they solve the more advanced use cases (file upload, multi auth, admin ui are 3 examples of more advanced use cases that no framework for gae at the moment handles well. "},
{"body": "The typical ConfigParser generated file looks like:Now, is there a way to index lists like, for instance:Related question: ? Thanks in advanceThere is nothing stopping you from packing the list into a delimited string and then unpacking it once you get the string from the config.  If you did it this way your config section would look like:It's not pretty but it's functional for most simple lists.Also a bit late, but maybe helpful for some.\nI am using a combination of ConfigParser and JSON:just read it with:You can even break lines if your list is long (thanks @peter-smit):Of course i could just use JSON, but i find config files much more readable, and the [DEFAULT] Section very handy.Coming late to this party, but I recently implemented this with a dedicated section in a config file for a list:and using  to get an iterable list of path items, like so:Hope this helps other folk Googling this question ;)One thing a lot of people don't know is that multi-line configuration-values are allowed. For example:The value of  will now be:Which you easily can split with the splitlines method (don't forget to filter empty items).If we look to a big framework like Pyramid they are using this technique:Myself, I would maybe extend the ConfigParser if this is a common thing for you:Note that there are a few things to look out for when using this techniqueIf you want to  pass in a list then you can use:For example configuration:The code is:output:I landed here seeking to consume this...The answer is to split it on the comma and strip the spaces:To get a  list result:It may not answer the OP's question exactly but might be the simple answer some people are looking for.This is what I use for lists:config file content: code : it work for stringsin case of numbers config content:code:thanks.Only primitive types are supported for serialization by config parser. I would use JSON or YAML for that kind of requirement.I faced the same problem in the past. If you need more complex lists, consider creating your own parser by inheriting from ConfigParser. Then you would overwrite the get method with that:With this solution you will also be able to define dictionaries in your config file.But be careful! This is not as safe: this means anyone could run code through your config file. If security is not an issue in your project, I would consider using directly python classes as config files. The following is much more powerful and expendable than a ConfigParser file:So now my  file, which could look like this:Can be parsed into fine-grained-enough objects for my small project.This is for very quick parsing of simple configs, you lose all ability to fetch ints, bools, and other types of output without either transforming the object returned from , or re-doing the parsing job accomplished by the Parser class elsewhere."},
{"body": "What is the difference between Numpy's  and  functions? When should you use one rather than the other? They seem to generate identical output for all the inputs I can think of.The  is:So it is like , except it has fewer options, and .  has  by default. I think the main difference is that  (by default) will make a copy of the object, while  will not unless necessary.The difference can be demonstrated by this example:Hope this helps!The differences are mentioned quite clearly in the documentation of  and . The differences lie in the argument list and hence the action of the function depending on those parameters. The function definitions are :and The following arguments are those that may be passed to  and   as mentioned in the documentation :"},
{"body": "I'm trying to test a complicated javascript interface with Selenium (using the Python interface, and across multiple browsers).  I have a number of buttons of the form:I'd like to be able to search for buttons based on \"My Button\" (or non-case-sensitive, partial matches such as \"my button\" or \"button\")I'm finding this amazingly difficult, to the extent to which I feel like I'm missing something obvious.  The best thing I have so far is:This is case-sensitive, however.  The other thing I've tried is iterating through all the divs on the page, and checking the element.text property.  However, every time you get a situation of the form:div.outer also has \"My Button\" as the text.  To fix THAT, I've tried looking to see if div.outer is the parent of div.inner, but couldn't figure out how to do that (element.get_element_by_xpath('..') returns an element's parent, but it tests not equal to div.outer).  Also, iterating through all the elements on the page seems to be really slow, at least using the Chrome webdriver.Ideas?Edit: This question came out a little vague.  Asked (and answered) a more specific version here: Try the following:you could try an xpath like:You can also use it with Page Object Pattern, e.g:@FindBy(xpath = \"//*[contains(text(), 'Best Choice')]\")WebElement buttonBestChoice;Try this. Its very easy:This really worked for me in selenium web driver."},
{"body": "I'm trying to use Sphinx to document a 5,000+ line project in Python. It has about 7 base modules. As far as I know, In order to use autodoc I need to write code like this for each file in my project:This is way too tedious because I have many files. It would be much easier if I could just specify that I wanted the 'mods' package to be documented. Sphinx could then recursively go through the package and make a page for each submodule.Is there a feature like this? If not I could write a script to make all the .rst files, but that would take up a lot of time.You can check this  that I've made. I think it can help you.This script parses a directory tree looking for python modules and packages and creates ReST files appropriately to create code documentation with Sphinx. It also creates a modules index.This script is now part of Sphinx 1.1 as .I do not know whether Sphinx had had  extension at the time original question was asked, but for now it is quite possible to set up automatic generation of that kind without using  or similar script. Below there are settings which work for one of my projects.In conclusion, there is no need to keep  directory under version control. Also, you may name it anything you want and place it anywhere in the source tree (putting it below  will not work, though).In each package, the  file can have  components for each part of the package.Then you can  and it mostly does what you want.Maybe what you're looking for is  and this ."},
{"body": "I am having trouble with some of pandas functionalities. How do I check what is my installation version?Check :Pandas also provides a utility function, , which reports the version of its dependencies as well:open the console and type the code below.Code:Output:Run:You should get a list of packages (including panda) and their versions, e.g.:"},
{"body": "Here's the simplest way to explain this. Here's what I'm using:Here's what I want:The reason is that I want to split a string into tokens, manipulate it, then put it back together again.If you are splitting on newline, use .(Not a general solution, but adding this here in case someone comes here not realizing this method existed.)You can also split a string with an array of strings instead of a regular expression, like this:If one wants to split string while keeping separators by regex without capturing group:If one assumes that regex is wrapped up into capturing group:Both ways also will remove empty groups which are useless and annoying in most of the cases."},
{"body": "In Flask, How do I extract parameters from a URL? \nHow can I extract named parameters from a URL using flask and python?When the user accesses this URL running on my flask app, I want the web service to be able to handle the parameters specified after the question mark:Use  to get parsed contents of query string:"},
{"body": "What is the name of the method to override the  operator (subscript notation) for a class in Python?You need to use the .And if you're going to be setting values you'll need to implement the  too, otherwise this will happen:To fully overload it you also need to implement the and  methods.I almost forgot... if you want to completely emulate a list, you also need .There are all documented in You are looking for the  method. See , section 3.4.6"},
{"body": "In other words, what's the sprintf equivalent to pprint?  The  module has a command named , for just that purpose.From the documentation:Example:Assuming you really do mean  from the ,  then you want\nthe  method.If you just mean  , then you want Are you looking for ?Something like this:"},
{"body": "I'm building a simple helper script for work that will copy a couple of template files in our code base to the current directory. I don't, however, have the absolute path to the directory where the templates are stored. I do have a relative path from the script but when I call the script it treats that as a path relative to the current working directory. Is there a way to specify that this relative url is from the location of the script instead?In the file that has the script, you want to do something like this:This will give you the absolute path to the file you're looking for.  Note that if you're using setuptools, you should probably use its  instead.:  I'm responding to a comment here so I can paste a code sample.  :-)I'm assuming you mean the  script when you mention running the file directly.  If so, that doesn't appear to be the case on my system (python 2.5.1 on OS X 10.5.7):However, I do know that there are some quirks with  on C extensions.  For example, I can do this on my Mac:However, this raises an exception on my Windows machine.you need  (sample below adds the parent directory to your path)As mentioned in the accepted answer I just want to add thatIt should be something likeThe accepted answer can be misleading in some cases , please refer to  link for detailsSee \nAs initialized upon program startup, the first item of this list, path[0], is the directory containing the script that was used to invoke the Python interpreter. Use this path as the root folder from which you Consider my code:Instead of using as in the accepted answer, it would be more robust to use:because using __file__ will return the file from which the module was loaded, if it was loaded from a file, so if the file with the script is called from elsewhere, the directory returned will not be correct. These answers give more detail:  and This code will return the absolute path to the main script.This will work even in a module.Hi first of all you should understand functions  and  In short  makes a  to . And if the path provided is itself a absolute path then the function returns the same path.similarly  makes a  to . And if the path provided is itself a relative path then the function returns the same path.:suppose i have a file  which contains list of input files to be processed by my python script.D:\\conc\\input1.dicD:\\conc\\input2.dicD:\\Copyioconc\\input_file_list.txtIf you see above folder structure,  is present in  folder and the files to be processed by the python script are present in  folderBut the content of the file  is as shown below:..\\conc\\input1.dic..\\conc\\input2.dicAnd my python script is present in  drive.And the relative path provided in the  file are relative to the path of  file.So when python script shall executed the current working directory (use  to get the path)As my relative path is relative to , that is , i have to change the current working directory to .So i have to use , so the current working directory shall be .Now to get the files  and , i will read the lines \"..\\conc\\input1.dic\" then shall use the command  (to change relative path to absolute path. Here as current working directory is \"D:\\Copyofconc\", the file \".\\conc\\input1.dic\" shall be accessed relative to \"D:\\Copyofconc\")so  shall be \"D:\\conc\\input1.dic\"An alternative which works for me: What worked for me is using . Then I specified the  directory I needed to go. For example I just needed to go up one directory.I'm not sure if this applies to some of the older versions, but I believe Python 3.3 has native relative path support.For example the following code should create a text file in the same folder as the python script:(note that there shouldn't be a forward or backslash at the beginning if it's a relative path)"},
{"body": "I am using Python 2.x.My editor gives me a 'warning' underline when I compare , but no warning when I use .  I did a test in the Python shell and determined both are valid syntax, but my editor seems to be saying that  is preferred. Is this the case, and if so, why?Use  when you want to check against an object's  (e.g. checking to see if  is ).  Use  when you want to check  (e.g. Is  equal to ?).You can have custom classes where  will return e.g: checks for object .  There is only 1 object , so when you do , you're checking whether they actually are the same object (not just  objects) In other words,  is a check for equivalence (which is defined from object to object) whereas  checks for object identity: is generally preferred when comparing arbitrary objects to singletons like  because it is faster and more predictable.  always compares by object identity, whereas what  will do depends on the exact type of the operands and even on their ordering.This recommendation is supported by , which  that \"comparisons to singletons like None should always be done with  or , never the equality operators.\"PEP 8 defines that it is better to use the is operator when comparing singletons."},
{"body": "I want to move away from PHP a little and learn Python. In order to do web development with Python I'll need a framework to help with templating and other things. I have a non-production server that I use to test all of web development stuff on. It is a Debian 7.1 LAMP stack that runs MariaDB instead of the common MySQL-server package.Yesterday I installed Django and created my first project called . I have not changed any settings yet.Here is my first big piece of confusion. In the tutorial I followed the guy installed Django, started his first project, restarted Apache, and Django just worked from then on. He went to his browser and went to the Django default page with no problems.Me however, I have to cd into my firstweb folder and runAnd it works. No problem. But I'm wondering if it is supposed to work like this, and if this will cause problems down the line?My  is that I want to set it up so it uses my MySQL database. I go into my settings.py under /firstweb/firstweb and I see ENGINE and NAME but I'm not sure what to put here.And then in the USER, PASSWORD, and HOST areas is this my database and its credentials? If I am using  can I just put  in the HOST area? is simple to add. In your  dictionary, you will have an entry like this:You also have the option of utilizing MySQL , as of Django 1.7. You can accomplish this by setting your  array like so:You also need to create the  file with similar settings from aboveWith this new method of connecting in Django 1.7, it is important to know the order connections are established:If you are just testing your application on your local machine, you can useAdding the  argument allows machines other than your own to access your development application. Once you are ready to deploy your application, I recommend taking a look at the chapter on  on the Mysql default character set is often not utf-8, therefore make sure to create your database using this sql:  If you are using  your  line should look like this:To the very first please run the below commands to install python dependencies otherwise python runserver command will throw error.Then configure the settings.py file as defined by #Andy and at the last execute :Have fun..!! As all said above, you can easily install xampp first from \nThen follow the instructions as:Actually, there are many issues with different environments, python versions, so on. You might also need to install python dev files, so to 'brute-force' the installation I would run all of these:You should be good to go with the accepted answer. And can remove the unnecessary packages if that's important to you.If you are using python3.x then Run below commandThen change setting.py likeAndy's answer helps but if you have concern on exposing your database password in your django setting, I suggest to follow django official configuration on mysql connection: Quoted here as: To replace 'HOST': '127.0.0.1' in setting, simply add it in my.cnf:Another OPTION that is useful, is to set your storage engine for django, you might want it in your setting.py:sudo apt-get install python-dev python3-devsudo apt-get install libmysqlclient-devpip install MySQL-pythonpip install pymysqlpip install mysqlclientYou must create a MySQL database first. Then go to  file and edit the  dictionary with your MySQL credentials:Here is a complete installation guide for setting up Django to use MySQL on a virtualenv:"},
{"body": "I have about 10million values that I need to put in some type of look up table, so I was wondering which would be more efficient a  or ?I know you can do something like this for both:andMy thought is the dict will be faster and more efficient.Thanks for your help. \nLittle more info on what I'm trying to do.  .  I'm making a look up table to see if a value calculated has all ready been calculated.  \nEfficiency for look up. \nThere are no values assosiated with the value...so would a  be better?Lookups in lists are O(n), lookups in dictionaries are amortized O(1), with regard to the number of items in the data structure. If you don't need to associate values, use sets.Both dictionaries and sets use hashing and they use much more memory than only for object storage. According to A.M. Kuchling in , the implementation tries to keep the hash 2/3 full, so you might waste quite some memory. If you do not add new entries on the fly (which you do, based on your updated question), it might be worthwhile to sort the list and use binary search. This is O(log n), and is likely to be slower for strings, impossible for objects which do not have a natural ordering.A dict is a hash table, so it is really fast to find the keys. So between dict and list, dict would be faster. But if you don't have a value to associate, it is even better to use a set. It is a hash table, without the \"table\" part.EDIT: for your new question, YES, a set would be better. Just create 2 sets, one for sequences ended in 1 and other for the sequences ended in 89. I have sucessfully solved this problem using sets. is exactly what you want.  O(1) lookups, and smaller than a dict.I did some benchmarking and it turns out that dict is faster than both list and set for large data sets, running python 2.7.3 on an i7 CPU on linux:As you can see, dict is considerably faster than list and about 3 times faster than set. In some applications you might still want to choose set for the beauty of it, though. And if the data sets are really small (< 1000 elements) lists perform pretty well. if data are unique set() will be the most efficient, but of two - dict (which also requires uniqueness, oops :)You want a dict.  For (unsorted) lists in Python, the \"in\" operation requires O(n) time---not good when you have a large amount of data.  A dict, on the other hand, is a hash table, so you can expect O(1) lookup time.  As others have noted, you might choose a set (a special type of dict) instead, if you only have keys rather than key/value pairs.Related:You don't actually need to store 10 million values in the table, so it's not a big deal either way.Hint: think about how large your result can be after the first sum of squares operation. The largest possible result will be much smaller than 10 million..."},
{"body": "Many third-party Python modules have an attribute which holds the version information for the module (usually something like  or ), however some do not.Particular examples of such modules are libxslt and libxml2.I need to check that the correct version of these modules are being used at runtime. Is there a way to do this?A potential solution wold be to read in the source at runtime, hash it, and then compare it to the hash of the known version, but that's nasty. Is there a better solutions?I'd stay away from hashing.  The version of libxslt being used might contain some type of patch that doesn't effect your use of it.  As an alternative, I'd like to suggest that you don't check at run time (don't know if that's a hard requirement or not).  For the python stuff I write that has external dependencies (3rd party libraries), I write a script that users can run to check their python install to see if the appropriate versions of modules are installed.  For the modules that don't have a defined 'version' attribute, you can inspect the interfaces it contains (classes and methods) and see if they match the interface they expect.  Then in the actual code that you're working on, assume that the 3rd party modules have the interface you expect.Use . Anything installed from PyPI at least should have a version number.Some ideas:You can use to see the installed packages in requirements format."},
{"body": "Hay, I'm trying to save a object to my database, but it's throwing a MultiValueDictKeyError error.The problems lies within the form, the is_private is represented by a checkbox. If the check box is NOT selected, obvously nothing is passed. This is where the error gets chucked.How do i properly deal with this exception, and catch it? The line isThankUse the MultiValueDict's  method. This is also present on standard dicts and is a way to fetch a value while providing a default if it does not exist.Generally,Choose what is best for you:If  key is present in request.POST the  variable will be equal to it, if not, then it will be equal to False.You get that because you're trying to get a key from a dictionary when it's not there. You need to test if it is in there first.try:or depending on the values you're using.Why didn't you try to define  in your models as ?"},
{"body": "I don't understand how looping over a dictionary or set in python is done by 'arbitrary' order.I mean, it's a programming language so everything in the language must be 100% determined, correct? Python must have some kind of algorithm that decides which part of the dictionary or set is chosen, 1st, second and so on. What am I missing?The order is not arbitrary, but depends on the insertion and deletion history of the dictionary or set, as well as on the specific Python implementation. For the remainder of this answer, for 'dictionary', you can also read 'set'; sets are implemented as dictionaries with just keys and no values.Keys are hashed, and hash values are assigned to slots in a dynamic table (it can grow or shrink based on needs). And that mapping process can lead to collisions, meaning that a key will have to be slotted in a  slot based on what is already there.Listing the contents loops over the slots, and so keys are listed in the order they  reside in the table.Take the keys  and , for example, and lets assume the table size is 8 slots. In Python 2.7,  is ,  is . Modulo 8, that means these two keys are slotted in slots 3 and 4 then:This informs their listing order:All slots except 3 and 4 are empty, looping over the table first lists slot 3, then slot 4, so  is listed before . and , however, have hash values that are exactly 8 apart and thus map to the exact same slot, :Their order now depends on which key was slotted first; the second key will have to be moved to a next slot:The table order differs here, because one or the other key was slotted first.The technical name for the underlying structure used by CPython (the most commonly used Python implemenation) is a , one that uses open addressing. If you are curious, and understand C well enough, take a look at the  for all the (well documented) details. You could also watch this  about how CPython  works, or pick up a copy of , which includes a chapter on the implementation written by Andrew Kuchling.Note that as of Python 3.3, a random hash seed is used as well, making hash collisions unpredictable to prevent certain types of denial of service (where an attacker renders a Python server unresponsive by causing mass hash collisions). This means that the order of a given dictionary is then  dependent on the random hash seed for the current Python invocation.Other implementations are free to use a different structure for dictionaries, as long as they satisfy the documented Python interface for them, but I believe that all implementations so far use a variation of the hash table.CPython 3.6 introduces a   implementation that maintains insertion order, and is faster and more memory efficient to boot. Rather than keep a large sparse table where each row references the stored hash value, and the key and value objects, the new implementation adds a smaller hash  that only references indices in dense table (one that only contains as many rows as there are actual key-value pairs), and it is the dense table that happens to list the contained items in order. See the . Note that this is considered an , Python-the-language does not specify that other implementations have to retain order.Python 2.7 and newer also provides an , a subclass of  that adds an additional data structure to record key order. At the price of some speed and extra memory, this class remembers in what order you inserted keys; listing keys, values or items will then do so in that order. It uses a doubly-linked list stored in an additional dictionary to keep the order up-to-date efficiently. See the . Note that the  type is still unordered.If you wanted an ordered set, you can install the ; it works on Python 2.5 and up.This is more a response to  before it was closed as a duplicate.The others are right: don't rely on the order. Don't even pretend there is one.That said, there is  thing you can rely on:That is, the order is .Understanding why there is a  order requires understanding a few things:From the top:A  is a method of storing random data with really fast lookup times.It has a backing array:We shall ignore the special dummy object, which exists only to make removes easier to deal with, because we won't be removing from these sets.In order to have really fast lookup, you do some magic to calculate a hash from an object. The only rule is that two objects which are equal have the same hash. (But if two objects have the same hash they can be unequal.)You then make in index by taking the modulus by the array length:This makes it really fast to access elements.Hashes are only most of the story, as  and  can result in the same number. In that case, several different strategies can try and resolve the conflict. CPython uses \"linear probing\" 9 times before doing complicated things, so it will look  for up to 9 places before looking elsewhere.CPython's hash sets are stored like this:So when you create an array the backing store is length 8. When it is 5 full and you add an element, it will briefly contain 6 elements.  so this triggers a resize, and the backing store quadruples to size 32.Finally,  just returns  for numbers (except  which is special).So, let's look at the first one: is 10, so the backing store is at least 15(+1) . The relevant power of 2 is 32. So the backing store is:We haveso these insert as:So we would expect an order likewith the 1 or 33 that isn't at the start somewhere else. This will use linear probing, so we will either have:orYou might expect the 33 to be the one that's displaced because the 1 was already there, but due to the resizing that happens as the set is being built, this isn't actually the case. Every time the set gets rebuilt, the items already added are effectively reordered.Now you can see whymight be in order. There are 14 elements, so the backing store is at least 21+1, which means 32:1 to 13 hash in the first 13 slots. 20 goes in slot 20.55 goes in slot  which is 23:If we chose 50 instead, we'd expectAnd lo and behold: is implemented quite simply by the looks of things: it traverses the list and pops the first one.\"Arbitrary\" isn't the same thing as \"non-determined\".What they're saying is that there are no useful properties of dictionary iteration order that are \"in the public interface\". There almost certainly are many properties of the iteration order that are fully determined by the code that currently implements dictionary iteration, but the authors aren't promising them to you as something you can use. This gives them more freedom to change these properties between Python versions (or even just in different operating conditions, or completely at random at runtime) without worrying that your program will break.Thus if you write a program that depends on  of dictionary order, then you are \"breaking the contract\" of using the dictionary type, and the Python developers are not promising that this will always work, even if it appears to work for now when you test it. It's basically the equivalent of relying on \"undefined behaviour\" in C.The other answers to this question are excellent and well written.  The OP asks \"how\" which I interpret as \"how do they get away with\" or \"why\".The Python documentation says  are not ordered because the Python dictionary implements the  .  As they say In other words, a computer science student cannot assume that an associative array is ordered.  The same is true for sets in  and  Implementing a dictionary using a hash table is an  that is interesting in that it has the same properties as associative arrays as far as order is concerned.Python use  for storing the dictionaries, so there is no order in dictionaries or other iterable objects that use hash table.But regarding the indices of items in a hash object, python calculate the indices based on following code :Therefor, as the hash value of integers is the integer itself the index  is based on the number ( is a constant) so the index calculated by  between  and the number itself (expect for -1 which it's hash is -2) , and for other objects with their hash value. consider the following example with  that use hash-table :For number  we have : That actually it's : in this case  is  or . And for  :And for  :For more details about python hash function its good to read the following quotes from  :"},
{"body": "I'm trying to build two functions using PyCrypto that accept two parameters: the message and the key, and then encrypt/decrypt the message.I found several links on the web to help me out, but each one of them has flaws: uses os.urandom, which is discouraged by PyCrypto.Moreover, the key I give to the function is not guaranteed to have the exact length expected. What can I do to make that happen ?Also, there are several modes, which one is recommended? I don't know what to use :/Finally, what exactly is the IV? Can I provide a different IV for encrypting and decrypting, or will this return in a different result?Here's what I've done so far:Here is my implementation and works for me with some fixes and enhances the alignment of the key and secret phrase with 32 bytes and iv to 16 bytes:You may need the following two functions to pad(when do encryption) and unpad(when do decryption) when the length of input is not a multiple of BLOCK_SIZE.So you're asking the length of key? You can use the md5sum of the key rather than use it directly.More, according to my little experience of using PyCrypto, the IV is used to mix up the output of a encryption when input is same, so the IV is chosen as a random string, and use it as part of the encryption output, and then use it to decrypt the message.And here's my implementation, hope it will be useful for you:You can get a passphrase out of an arbitrary password by using a cryptographic hash function ( Python's builtin ) like SHA-1 or SHA-256. Python includes support for both in its standard library:You can truncate a cryptographic hash value just by using  or  and it will retain its security up to the length you specify.For someone who would like to use urlsafe_b64encode and urlsafe_b64decode, here are the version that're working for me (after spending some time with the unicode issue)For the benefit of others, here is my decryption implementation which I got to by combining the answers of @Cyril and @Marcus.  This assumes that this coming in via HTTP Request with the encryptedText quoted and base64 encoded.Another take on this (heavily derived from solutions above) butIt's little late but i think this will be very helpful. No one mention about use  scheme like PKCS#7 padding. You can use it instead the previous functions to pad(when do encryption) and unpad(when do decryption).i will provide the full Source Code below."},
{"body": "I want to put output information of my program to a folder. if given folder does not exist, then the program should create a new folder with folder name as given in the program. Is this possible? If yes, please let me know how.Suppose I have given folder path like  and  folder doesn't exist then program should create  folder and should put output information in the  folder.You can create a folder with \nand use  to see if it already exists:If you're trying to make an installer:  does a lot of work for you.You probably want  as it will create intermediate directories as well, if needed.Have you tried os.mkdir?You might also try this little code snipped:makedirs creates multiple levels of directories, if needed."},
{"body": "I am trying to create a graphical spectrum analyzer in python.I am currently reading 1024 bytes of a 16 bit dual channel 44,100 Hz sample rate audio stream and averaging the amplitude of the 2 channels together.  So now I have an array of 256 signed shorts.  I now want to preform a fft on that array, using a module like numpy, and use the result to create the graphical spectrum analyzer, which, to start will just be 32 bars.I have read the wikipedia articles on Fast Fourier Transform and Discrete Fourier Transform but I am still unclear of what the resulting array represents.  This is what the array looks like after I preform an fft on my array using numpy:I am wondering what exactly these numbers represent and how I would convert these numbers into a percentage of a height for each of the 32 bars.  Also, should I be averaging the 2 channels together?The array you are showing is the Fourier Transform coefficients of the audio signal. These coefficients can be used to get the frequency content of the audio. The FFT is defined for complex valued input functions, so the coefficients you get out will be imaginary numbers even though your input is all real values. In order to get the amount of power in each frequency, you need to calculate the magnitude of the FFT coefficient for each frequency. This is  just the real component of the coefficient, you need to calculate the square root of the sum of the square of its real and imaginary components. That is, if your coefficient is a + b*j, then its magnitude is sqrt(a^2 + b^2).Once you have calculated the magnitude of each FFT coefficient, you need to figure out which audio frequency each FFT coefficient belongs to. An N point FFT will give you the frequency content of your signal at N equally spaced frequencies, starting at 0. Because your sampling frequency is 44100 samples / sec. and the number of points in your FFT is 256, your frequency spacing is 44100 / 256 = 172 Hz (approximately)The first coefficient in your array will be the 0 frequency coefficient. That is basically the average power level for all frequencies. The rest of your coefficients will count up from 0 in multiples of 172 Hz until you get to 128. In an FFT, you only can measure frequencies up to half your sample points. Read these links on the  and  if you are a glutton for punishment and need to know why, but the basic result is that your lower frequencies are going to be replicated or  in the higher frequency buckets. So the frequencies will start from 0, increase by 172 Hz for each coefficient up to the N/2 coefficient, then decrease by 172 Hz until the N - 1 coefficient.That should be enough information to get you started. If you would like a much more approachable introduction to FFTs than is given on Wikipedia, you could try . It was very helpful for me.So that is what those numbers represent. Converting to a percentage of height could be done by scaling each frequency component magnitude by the sum of all component magnitudes. Although, that would only give you a representation of the relative frequency distribution, and not the actual power for each frequency. You could try scaling by the maximum magnitude possible for a frequency component, but I'm not sure that that would display very well. The quickest way to find a workable scaling factor would be to experiment on loud and soft audio signals to find the right setting.Finally, you should be averaging the two channels together if you want to show the frequency content of the entire audio signal as a whole. You are mixing the stereo audio into mono audio and showing the combined frequencies. If you want two separate displays for right and left frequencies, then you will need to perform the Fourier Transform on each channel separately.Although this thread is years old, I found it very helpful. I just wanted to give my input to anyone who finds this and are trying to create something similar.As for the division into bars this should not be done as antti suggest, by dividing the data equally based on the number of bars. The most useful would be to divide the data into octave parts, each octave being double the frequency of the previous. (ie. 100hz is one octave above 50hz, which is one octave above 25hz). Depending on how many bars you want, you divide the whole range into 1/X octave ranges.\nBased on a given center frequency of A on the bar, you get the upper and lower limits of the bar from:To calculate the next adjoining center frequency you use a similar calculation:You then average the data that fits into these ranges to get the amplitude for each bar.For example:\nWe want to divide into 1/3 octaves ranges and we start with a center frequency of 1khz.Given 44100hz and 1024 samples (43hz between each data point) we should average out values 21 through 26. ( 890.9 / 43 = 20.72 ~ 21 and 1122.5 / 43 = 26.10 ~ 26 )(1/3 octave bars would get you around 30 bars between ~40hz and ~20khz).\nAs you can figure out by now, as we go higher we will average a larger range of numbers. Low bars typically only include 1 or a small number of data points. While the higher bars can be the average of hundreds of points. The reason being that 86hz is an octave above 43hz... while 10086hz sounds almost the same as 10043hz.what you have is a sample whose length in time is 256/44100 = 0.00580499 seconds. This means that your frequency resolution is 1 / 0.00580499 = 172 Hz. The 256 values you get out from Python correspond to the frequencies, basically, from 86 Hz to 255*172+86 Hz = 43946 Hz. The numbers you get out are complex numbers (hence the \"j\" at the end of every second number).EDITED: FIXED WRONG INFORMATIONYou need to convert the complex numbers into amplitude by calculating the sqrt(i + j) where i and j are the real and imaginary parts, resp.If you want to have 32 bars, you should as far as I understand take the average of four successive amplitudes, getting 256 / 4 = 32 bars as you want."},
{"body": "Is there a built-in function that can round like this:I don't know of a standard function in Python, but this works for me:It is easy to see why the above works.  You want to make sure that your number divided by 5 is an integer, correctly rounded.  So, we first do exactly that (), and then since we divided by 5, we multiply by 5 as well.  The final conversion to  is because  returns a floating-point value in Python.I made the function more generic by giving it a  parameter, defaulting to 5.For rounding to non-integer values, such as 0.05:I found this useful since I could just do a search and replace in my code to change \"round(\" to \"myround(\", without having to change the parameter values.It's just a matter of scaling    Removing the 'rest' would work:If the value is aready an integer:As a function:round(x[, n]): values are rounded to the closest multiple of 10 to the power minus n. So if n is negative...Since 10 = 5 * 2, you can use integer division and multiplication with 2, rather than float division and multiplication with 5.0. Not that that matters much, unless you like bit shiftingModified version of divround :-)Sorry, I wanted to comment on Alok Singhai's answer, but it won't let me due to a lack of reputation =/Anyway, we can generalize one more step and go:This allows us to use non-integer bases, like  or any other fractional base.I realise I'm late to the party, but it seems that this solution has not been mentioned:It does not use multiplication and will not convert from/to floats.Rounding to the nearest multiple of 10:As you can see, it works for both negative and positive numbers. Ties (e.g. -15 and 15) will always be rounded upwards.Similar example that rounds no the nearest multiple of 5, demonstrating that it also behaves as expected for a different \"base\":What about this:In case someone needs \"financial rounding\" (0.5 rounds always up):As per documentation other rounding options are:By default Python uses ROUND_HALF_EVEN as it has some statistical advantages (the rounded results are not biased).**next multiple of 5 **consider 51 need to converted to 55You can \u201ctrick\u201d  into rounding off instead of rounding down by adding  to the \nnumber you pass to ."},
{"body": "I have data, in which I want to find number of NaN, so that if it is less than some threshold, I will drop this columns. I looked, but didn't able to find any function for this. there is count_values(), but it would be slow for me, because most of values are distinct and I want count of NaN only.You can use the  method and then sum to count the nan values. For one column:For several columns, it also works:You could subtract the total length from the  of non-nan values:You should time it on your data. For small Series got a 3x speed up in comparison with the  solution.Since pandas 0.14.1 my suggestion  to have a keyword argument in the value_counts method has been implemented:if you are using Jupyter Notebook, How about....oror, are there anywhere NaNs in the data, if yes, where?if its just counting nan values in a pandas column here is a quick wayBased on the second and most voted answer we can easily define a function that gives us a dataframe to preview the missing values and the % of missing values in each column:Used the solution proposed by @sushmit in my code. \nA possible variation of  the same can also be -\n\nAdvantage of this is that it returns the result for each of the columns in the df henceforth.You can use value_counts method and print values of np.nanbased to the answer that was given and some improvements this is my approach "},
{"body": "So I tried the \"evil\" thing Ned Deily mentioned in his answer . Now I have that the type True is now always False. How would I reverse this within the interactive window?Thing to not do:Since True has now been completely overridden with False, there doesn't seem to be an obvious way to back-track. Is there a module that True comes from that I can do something like:You can simply  your custom name to set it back to the default:This works:but fails if  has been fiddled with as well. Therefore this is better:as  cannot be reassigned.These also evaluate to  regardless of whether  has been reassigned to , , , , etc:Another way:For completeness: Kevin mentions that you could also fetch the real  from :But that  can also be overriden:So better to go with one of the other options.Just do this:Or, because booleans are essentially integers:Solutions that use no object literals but are as durable as . Of course, you can define False once True is defined, so I'll supply solutions as half pairs."},
{"body": "I am trying to plot some data from a camera in real time using OpenCV. However the real-time plotting (using matplotlib) doesn't seem to be working.I've isolated the problem into this simple example:I would expect this example to plot 1000 points individually. What actually happens is that the window pops up with the first point showing (ok with that), then waits for the loop to finish before it populates the rest of the graph.Any thoughts why I am not seeing points populated one at a time?Here's the working version of the code in question (requires at least version Matplotlib 1.1.0 from 2011-11-14):Note some of the changes: The while loop at the end is to keep the window up after all data is plotted.If you're interested in realtime plotting, I'd recommend looking into . In particular, using  to avoid redrawing the background on every frame can give you substantial speed gains (~10x):Output: is probably not the best choice for this.  What I would do is use  instead.  You also might want to include a small time delay (e.g., ) in the loop so that you can see the plots happening.  If I make these changes to your example it works for me and I see each point appearing one at a time.None of the methods worked for me.\nBut I have found this\nAll you need is to addand than you could see the new plot.I know I'm a bit late to answer this question. Nevertheless, I've made some code a while ago to plot live graphs, that I would like to share:Just try it out. Copy-paste this code in a new python-file, and run it. You should get a beautiful, smoothly moving graph:The problem seems to be that you expect  to show the window and then to return. It does not do that. The program will stop at that point and only resume once you close the window. You should be able to test that: If you close the window and then another window should pop up.To resolve that problem just call  once after your loop. Then you get the complete plot. (But not a 'real-time plotting')You can try setting the keyword-argument  like this:  once at the beginning and then use  to update.Here is a version that I got to work on my system.The drawnow(makeFig) line can be replaced with a makeFig(); plt.draw() sequence and it still works OK.I know this question is old, but there's now a package available called  on GitHub as \"python-drawnow\". This provides an interface similar to MATLAB's drawnow -- you can  update a figure.An example for your use case:python-drawnow is a thin wrapper around  but provides the ability to confirm (or debug) after figure display.If you want draw and not freeze your thread as more point are drawn you should use plt.pause() not time.sleep()im using the following code to plot a series of xy coordinates."},
{"body": "Is there a way to upgrade the version of python used in a virtualenv (e.g. if a bugfix release comes out)?I could  then remove the directory and  but this requires a lot of reinstallation of large libraries, for instance numpy which I use a lot. I can see this is an advantage when upgrading from e.g. 2.6 -> 2.7, but what about 2.7.x -> 2.7.y?ThanksDid you see ? If I haven't misunderstand that answer, you may try to create a new virtualenv on top of the old one. You just need to know which python is going to use your virtualenv (you will need to see your virtualenv version). If your virtualenv is installed with the same python version of the old one and upgrading your virtualenv package is not an option, you may want to read  in order to install a virtualenv with the python version you want.I've tested this approach (the one that create a new virtualenv on top of the old one) and it worked fine for me. I think you may have some problems if you change from python 2.6 to 2.7 or 2.7 to 3.x but if you just upgrade inside the same version (staying at 2.7 as you want) you shouldn't have any problem, as all the packages are held in the same folders for both python versions (2.7.x and 2.7.y packages are inside your_env/lib/python2.7/).If you change your virtualenv python version, you will need to install all your packages again for that version (or just link the packages you need into the new version packages folder, i.e: your_env/lib/python_newversion/site-packages) I changed the answer 5 months after I originally answered. The following method is more convenient and robust. it also fixes the  exception when you do  in a virtual environment after upgrading Python to v2.7.8. Currently, .If you're using Homebrew Python on OS X, first  all virtualenv, then upgrade Python:Run the following commands ( is path of your virtual environment):Finally, re-create your virtual environment:By doing so, old Python core files and standard libraries (plus  and ) are removed, while the custom libraries installed in  are preserved and working, as soon as they are in pure Python. Binary libraries may or may not need to be reinstalled to function properly.This worked for me on 5 virtual environments with Django installed.BTW, if  is not working afterwards, try this:If you happen to be using the venv module that comes with Python 3.3+, it supports an  option. \nPer the :I wasn't able to create a new virtualenv on top of the old one.  But there are tools in pip which make it much faster to re-install requirements into a brand new venv.  Pip can build each of the items in your requirements.txt into a wheel package, and store that in a local cache.  When you create a new venv and run pip install in it, pip will automatically use the prebuilt wheels if it finds them.  Wheels install much faster than running setup.py for each module.My ~/.pip/pip.conf looks like this:I install wheel (), then run .  This stores the built wheels in the wheel-dir in my pip.conf.From then on, any time I pip install any of these requirements, it installs them from the wheels, which is pretty quick.  I moved my home directory from one mac to another (Mountain Lion to Yosemite) and didn't realize about the broken virtualenv until I lost hold of the old laptop. I had the virtualenv point to Python 2.7 installed by  and since Yosemite came with Python 2.7, I wanted to update my virtualenv to the system python. When I ran  on top of the existing directory, I was getting  error. By trial and error, I worked around this issue by removing a few links and fixing up a few more manually. This is what I finally did (similar to what @Rockalite did, but simpler):After this, I was able to just run virtualenv on top of the existing directory.I'm adding an answer for anyone using Doug Hellmann's excellent  specifically since the existing answers didn't do it for me.Some context:Directions:Let's say your existing project is named  and is currently running Python 2 (), though the commands are the same whether upgrading from 2.x to 3.x, 3.6.0 to 3.6.1, etc.  I'm also assuming you're currently inside the activated virtual environment.1. Deactivate and remove the old virtual environment:Note that if you've added any custom commands to the hooks (e.g., ) you'd need to save those before removing the environment.2. Stash the real project in a temp directory:3. Create the new virtual environment (and project dir) and activate:4. Replace the empty generated project dir with real project, change back into project dir:5. Re-install dependencies, confirm new Python version, etc:If this is a common use case, I'll consider opening a PR to add something like  /  to virtualenvwrapper."},
{"body": "When initializing a dictionary with  Pycharm's code inspector generates a warning, sayingIf I rewrite it  the warning goes away. Since  already  a dictionary literal, I'm pretty sure the message is erroneous. Furthermore, it seems like both  and  are valid and Pythonic. This related question seems to conclude that the choice is just a matter of style/preference:\nWhy would Pycharm complain about ?Mac nailed it. The warning actually applied to multiple lines, not just the one that was flagged.Pycharm seems to look for a sequence of consecutive statements where you initialize a dictionary and then set values in the dictionary. For example, this will trigger the warning:But this code will not:I think pycharm will trigger the error if you have something like:as you could have writtenBTW: The fact that the error goes away if you use the function doesn't necessarily mean that pycharm believes  is a literal. It could just mean that it doesn't complain for:HTH!"},
{"body": "I am getting TransactionManagementError when trying to save a Django User model instance and in its post_save signal, I'm saving some models that have the user as the foreign key. The context and error is pretty similar to this question\nHowever, in this case, the error occurs .It works well in manual testing, but unit tests fails.Is there anything that I'm missing?views.pysignal.pytests.pyTraceback:I ran into this same problem myself. This is caused by a quirk in how transactions are handled in the newer versions of Django coupled with a unittest that intentionally triggers an exception.I had a unittest that checked to make sure a unique column constraint was enforced by purposefully triggering an IntegrityError exception:In Django 1.4, this works fine. However, in Django 1.5/1.6, each test is wrapped in a transaction, so if an exception occurs, it breaks the transaction until you explicitly roll it back. Therefore, any further ORM operations in that transaction, such as my , will fail with that  exception.Like caio mentioned in the comments, the solution is to capture your exception with  like:That will prevent the purposefully-thrown exception from breaking the entire unittest's transaction.Since @mkoistinen never made , an answer, I'll post his suggestion so people won't have to dig through comments.From the : A TransactionTestCase may call commit and rollback and observe the effects of these calls on the database.I was getting this error on running unit tests in my create_test_data function using django 1.9.7.  It worked in earlier versions of django.It looked like this:My solution was to use update_or_create instead:I had the same issue, while testing and went searching google and stackoverflow.In My Case I was doing thisso converting it toRemoved that error."},
{"body": "Having spent a decent amount of time watching both the  and  tags on SO, the impression that I get is that  questions are less likely to contain reproducible data. This is something that the R community has been pretty good about encouraging, and thanks to guides like , newcomers are able to get some help on putting together these examples. People who are able to read these guides and come back with reproducible data will often have much better luck getting answers to their questions.How can we create good reproducible examples for  questions? Simple dataframes can be put together, e.g.:But many example datasets need more complicated structure, e.g.:For datasets that are hard to mock up using a few lines of code, is there an equivalent to R's  that allows you to generate copy-pasteable code to regenerate your datastructure?This is to mainly to expand on @AndyHayden's answer by providing examples of how you can create sample dataframes.  Pandas and (especially) numpy give you a variety of tools for this such that you can generally create a reasonable facsimile of any real dataset with just a few lines of code.After importing numpy and pandas, be sure to provide a random seed if you want folks to be able to exactly reproduce your data and results.Here's an example showing a variety of things you can do.  All kinds of useful sample dataframes could be created from a subset of this:This produces:Some notes:In addition to taking subsets of the above code, you can further combine the techniques to do just about anything.  For example, here's a short example that combines  and  to create sample ticker data for 4 stocks covering the same dates:Now we have a sample dataset with 100 lines (25 dates per ticker), but we have only used 4 lines to do it, making it easy for everyone else to reproduce without copying and pasting 100 lines of code.  You can then display subsets of the data if it helps to explain your question:My best advice for asking questions would be to play on the psychology of the people who answer questions.  Being one of those people, I can give insight into why I answer certain questions and why I don't answer others.I'm motivated to answer questions for several reasonsAll my purest intentions are great and all, but I get that satisfaction if I answer 1 question or 30.   for which questions to answer has a huge component of point maximization.I'll also spend time on interesting problems but that is few and far between and doesn't help an asker who needs a solution to a non-interesting question.  Your best bet to get me to answer a question is to serve that question up on a platter ripe for me to answer it with as little effort as possible.  If I'm looking at two questions and one has code I can copy paste to create all the variables I need... I'm taking that one!  I'll come back to the other one if I have time, maybe.Make it easy for the people answering questions.I like points (I mentioned that above).  But those points aren't really really my reputation.  My real reputation is an amalgamation of what others on the site think of me.  I strive to be fair and honest and I hope others can see that.  What that means for an asker is, we remember the behaviors of askers.  If you don't select answers and upvote good answers, I remember.  If you behave in ways I don't like or in ways I do like, I remember.  This also plays into which questions I'll answer.Anyway, I can probably go on, but I'll spare all of you who actually read this. One of the most challenging aspects of responding to SO questions is the time it takes to recreate the problem (including the data).  Questions which don't have a clear way to reproduce the data are less likely to be answered.  Given that you are taking the time to write a question and you have an issue that you'd like help with, you can easily help yourself by providing data that others can then use to help solve your problem.The instructions provided by @Andy for writing good Pandas questions are an excellent place to start.  For more information, refer to  and how to create .  After taking the time to write your question and any sample code, try to read it and provide an 'Executive Summary' for your reader which summarizes the problem and clearly states the question.:Depending on the amount of data, sample code and error stacks provided, the reader needs to go a long way before understanding what the problem is.  Try restating your question so that the question itself is on top, and then provide the necessary details.:Sometimes just the head or tail of the DataFrame is all that is needed.  You can also use the methods proposed by @JohnE to create larger datasets that can be reproduced by others.  Using his example to generate a 100 row DataFrame of stock prices:If this was your actual data, you may just want to include the head and/or tail of the dataframe as follows (be sure to anonymize any sensitive data):You may also want to provide a description of the DataFrame (using only the relevant columns).  This makes it easier for others to check the data types of each column and identify other common errors (e.g. dates as string vs. datetime64 vs. object):If your DataFrame has a multiindex, you must first reset before calling .  You then need to recreate the index using :Here is my version of  - the standard R tool to produce reproducible reports - for Pandas s.\nIt will probably fail for more complex frames, but it seems to do the job in simple cases:now,  that this produces a much more verbose output than , e.g.,vsfor  above, but it preserves column types. \nE.g., in the above test case,because  is  and  is ."},
{"body": "Both function return the same list.\nThen what is the need of two different functions performing same job.The difference is that flatten always returns a copy and ravel returns a view of the original array whenever possible. This isn't visible in the printed output, but if you modify the array returned by ravel, it may modify the entries in the original array. If you modify the entries in an array returned from flatten this will never happen. ravel will often be faster since no memory is copied, but you have to be more careful about modifying the array it returns.As explained  a key difference is that  is a method of an ndarray object and hence can only be called for true numpy arrays. In contrast  is a library-level function and hence can be called on any object that can successfully be parsed. For example  will work on a list of ndarrays, while flatten is not available for that type of object.@IanH also points out important differences with memory handling in his answer."},
{"body": "I need to lock a file for writing in Python. It will be accessed from multiple Python processes at once. I have found some solutions online, but most fail for my purposes as they are often only Unix based or Windows based.Alright, so I ended up going with the code I wrote  (). I can use it in the following fashion:There is a cross-platform file locking module here: Although as Kevin says, writing to a file from multiple processes at once is something you want to avoid if at all possible.If you can shoehorn your problem into a database, you could use SQLite.  It supports concurrent access and handles its own locking.I prefer  \u2014 Platform-independent file lockingCoordinating access to a single file at the OS level is fraught with all kinds of issues that you probably don't want to solve.Your best bet is have a separate process that coordinates read/write access to that file.Locking is platform and device specific, but generally, you have a few options:For all these methods, you'll have to use a spin-lock (retry-after-failure) technique for acquiring and testing the lock.  This does leave a small window for mis-synchronization, but its generally small enough to not be an major issue.If you're looking for a solution that is cross platform, then you're better off logging to another system via some other mechanism (the next best thing is the NFS technique above).  Note that sqlite is subject to the same constraints over NFS that normal files are, so you can't write to an sqlite database on a network share and get synchronization for free.Locking a file is usually a platform-specific operation, so you may need to allow for the possibility of running on different operating systems. For example:I have been looking at several solutions to do that and my choice has been\nIt's powerful and relatively well documented. It's based on fasteners.Other solutions:I found a simple and worked(!)  from grizzled-python.Simple use os.open(..., O_EXCL) + os.close() didn't work on windows.I have been working on a situation like this where I run multiple copies of the same program from within the same directory/folder and logging errors.  My approach was to write a \"lock file\" to the disc before opening the log file. The program checks for the presence of the \"lock file\" before proceeding, and waits it's turn if the \"lock file\" exists.Here is the code:EDIT---\nAfter thinking over some of the comments about stale locks above I edited the code to add a check for staleness of the \"lock file.\"  Timing several thousand iterations of this function on my system gave and average of 0.002066... seconds from just before:to just after:so I figured I will start with 5 times that amount to indicate staleness and monitor the situation for problems.Also, as I was working with the timing, I realized that I had a bit of code  that was not really necessary:which I had immediately following the open statement, so I have removed it in this edit.Kernel-level file locking is an extremly complex subject, since different types of locks have very different semantic, and the main lock of unix systems (fcntl) has horrible flaws.But I've just released a python library which deals with the issue in a very portable way. It can achieve shared/exclusive file record locking in a system-wide manner, and provides workaround for fcntl flaws.Enjoy RSFile - You may find  very useful. It can be used to lock a file or for locking mechanisms in general and can be accessed from multiple Python processes at once. If you simply  want to lock a file here's how it works: "},
{"body": "Every day I love python more and more. Today, I was writing some code like:I had to do something N times. But each time didn't depend on the value of  (index variable).\nI realized that I was creating a variable I never used (), and I thought \"There surely is a more pythonic way of doing this without the need for that useless index variable.\"So... the question is: do you know how to do this simple task in a more (pythonic) beautiful way?A slightly faster approach than looping on  is:Use the _ variable, as I learned when I asked this , for example:I just use , it's straight to the point. It's going to generate the entire list for huge numbers in Python 2, but if you're using Python 3 it's not a problem.since function is first-class citizen, you can write small wrapper (from Alex answers)then you can pass function as argument.Assume that you've defined  as a function, and you'd like to perform it  times.\nMaybe you can try the following:The _ is the same thing as x. However it's a python idiom that's used to indicate an identifier that you don't intend to use. In python these identifiers don't takes memor or allocate space like variables do in other languages.  It's easy to forget that.  They're just names that point to objects, in this case an integer on each iteration.What about a simple while loop?You already have the variable; why not use it?"},
{"body": "I am trying to save a csv to a folder after making some edits to the file. Every time I use  the csv file has a separate column of indexes. I want to avoid printing the index to csv.I tried: And to save the file...However, I still got the unwanted index column. How can I avoid this when I save my files?Use .There are two ways to handle the situation where we do not want the index to be stored in csv file."},
{"body": "Is there a generally accepted way to do this?  Is this acceptable:The correct way to do it is to provide a docstring. That way,  will also spit out your comment.That's three double quotes to open the comment and another three double quotes to end it. You can also use any valid Python string. It doesn't need to be multiline and double quotes can be replaced by single quotes. See: Use a docstring, as others have already written. You can even go one step further and add a  to your docstring, making automated testing of your functions a snap.Use a :Oh boy! Consider a can of worms opened :)The principles of good commenting are fairly subjective, but here are some guidlines:Read about using  in your python code.As per the Python Docstring Conventions:There will be no golden rule, but rather provide comments that mean something to the other developers on your team (if you have one) or even to yourself when you come back to it six months down the road.I would go for a documentation practice that integrates with a Documentation tool such as .  The first step is to use a :I would go a step further than just saying \"use a docstring\". Pick a documentation generation tool, such as pydoc or epydoc (I use epydoc in pyparsing), and use the markup syntax recognized by that tool.  Run that tool often while you are doing your development, to identify holes in your documentation.  In fact, you might even benefit from writing the docstrings for the members of a class  implementing the class."},
{"body": "I have a script reading in a csv file with very huge fields:However, this throws the following error on some csv files:The csv file might contain very huge fields, therefore increase the : works for Python 2.x and 3.x.  would only work with Python 2.x ()As Geoff pointed out, the code above might result in the following error: . \nTo circumvent this, you could use the following  code:This could be because your CSV file has embedded single or double quotes. If your CSV file is tab-delimited try opening it as:For Python 3.4 I found this code and it works for me:"},
{"body": "the following code worked until today when I imported from a Windows machine and got this error:How can I fix this issue?It'll be good to see the csv file itself, but this might work for you, give it a try, replace:with:Or, open a file with  and pass it to , like:Or, use , like this:I realize this is an old post, but I ran into the same problem and don't see the correct answer so I will give it a tryPython Error:Caused by trying to read Macintosh (pre OS X formatted) CSV files. These are text files that use CR for end of line. If using MS Office make sure you select either plain  format or .  as save-as type. My preferred EOL version would be LF (Unix/Linux/Apple), but I don't think MS Office provides the option to save in this format.For Mac OS X, save your CSV file in \"Windows Comma Separated (.csv)\" format.If this happens to you  (as it did to me):Try to run  on your windows imported files firstThis worked for me on OSX.This is an error that I faced. I had saved .csv file in MAC OSX.While saving, save it as \"Windows Comma Separated Values (.csv)\" which resolved the issue."},
{"body": "I was looking for an elegant way to change a specified column name in a .play data ...The most elegant solution I have found so far ...I was hoping for a simple one-liner ... this attempt failed ...Any hints gratefully received.A one liner does exist:Following is the docstring for the  method.Since  argument is available, you don't need to copy and assign the original data frame back to itself, but do as follows:What about?"},
{"body": "Someone suggested I use an ORM for a project I'm designing but I'm having trouble finding information on what it is or how it works. Can anyone give me a brief explanation or a link as to where I can learn more about it? (ORM) is a technique that lets you query and manipulate data from a database using an object-oriented paradigm. When talking about ORM, most people are referring to a  that implements the Object-Relational Mapping technique, hence the phrase \"an ORM\".An ORM library is a completely ordinary library written in your language of choice that encapsulates the code needed to manipulate the data, so you don't use SQL anymore; you interact directly with an object in the same language you're using.For example, here is a completely imaginary case with a pseudo language:You have a book class, you want to retrieve all the books of which the author is \"Linus\". Manually, you would do something like that:With an ORM library, it would look like this:The mechanical part is taken care of automatically via the ORM library.Well, use one. Whichever ORM library you choose, they all use the same principles. There are a lot of ORM libraries around here:If you want to try an ORM library in Web programming, you'd be better off using an entire framework stack like:Do not try to write your own ORM, unless you are trying to learn something. This is a gigantic piece of work, and the old ones took a lot of time and work before they became reliable.Sure.ORM stands for  \"Object to Relational Mapping\" where In applications where you don't use a ORM framework you do this by hand. Using an ORM framework would allow you do reduce the boilerplate needed to create the solution.So let's say you have this object.and the tableUsing an ORM framework would allow you to map that object with a db record automagically and write something like:And have the employee inserted into the DB.An ORM (Object Relational Mapper) is a piece/layer of software that helps map your code Objects to your database.Some handle more aspects than others...but the purpose is to take some of the weight of the Data Layer off of the developer's shoulders.Here's a brief clip from Martin Fowler (Data Mapper):The first chapter of the Hibernate book  has an excellent overview of general ORM concepts and discusses motivation and design of ORMs. Highly recommended, even if you don't work with Java. is a great option for those using python.Like all acronyms it's ambiguous, but I assume they mean  -- a way to cover your eyes and make believe there's no SQL underneath, but rather it's all objects;-). Not really true, of course, and not without problems -- the always colorful Jeff Atwood has described ORM as ;-). But, if you know little or no SQL, and have a pretty simple / small-scale problem, they can save you time!-)This is a huge topic. Pick up a good hibernate book and it should explain ORM in detail before getting to the nitty gritty hibernate material.DALMP   could be a good one for php/mysql currently supporting many caches backends like redis/memcache/apcObject Model is concerned with the following three concepts\nData Abstraction\nEncapsulation\nInheritance\nThe relational model used the basic concept of a relation or table.\nObject-relational mapping (OR mapping) products integrate object programming language capabilities with relational databases."},
{"body": "How do I get the probability of a string being similar to another string in Python?I want to get a decimal value like:etc.Preferably with standard Python and library.e.g.There is a built in.Using it:I think maybe you are looking for an algorithm describing the distance between strings. Here are some you may refer to:   is a  that implements Levenshtein distance in python, with some helper functions to help in certain situations where you may want two distinct strings to be considered identical. For example:You can create a function like:Package  includes Levenshtein distance:"},
{"body": "I am using jinja2, and I want to call a python function as a helper, using a similar syntax as if I were calling a macro. jinja2 seems intent on preventing me from making a function call, and insists I repeat myself by copying the function into a template as a macro.    Is there any straightforward way to do this?  And, is there any way to import a whole set of python functions and have them accessible from jinja2, without going through a whole lot of rigamarole (such as writing an extension)?For those using Flask, put this in your :and in your template call it with I know this post is quite old, but there are better methods of doing this in the newer versions of Flask using context processors.The above can be used in a Jinja2 template with Flask like so:(Which outputs )The above when used like so:(Which outputs the input price with the currency symbol)Or, without decorators, and manually registering the function:Filters applied with the above two methods can be used like this:I think jinja deliberately makes it difficult to run 'arbitrary' python within a template. It tries to enforce the opinion that less logic in templates is a good thing.You can manipulate the global namespace within an  instance to add references to your functions. It must be done  you load any templates. For example:Will output:Works with Jinja2 version 2.7.3Use a lambda to connect the template to your main codeThen you can seamlessly call the function in the templateTo call a python function from Jinja2, you can use custom filters which work similarly as the globals:\nIt's quite simple and useful.\nIn a file myTemplate.txt, I wrote:And in a python script:"},
{"body": "I have two dictionaries, but for simplification, I will take these two:Now, I want to compare whether each  pair in  has the same corresponding value in . So I wrote this:And it works since a  is returned and then compared for equality.My questions:Is this correct? Is there a  way to do this? Better not in speed, I am talking about code elegance.UPDATE: I forgot to mention that I have to check how many  pairs are equal. If you want to know how many values match in both the dictionaries, you should have said that :) Maybe something like this:What you want to do is simply  What you do is not a good idea, because the items in a dictionary are not supposed to have any order. You might be comparing  with  (same dictionaries, different order).For example, see this:The difference is only one item, but your algorithm will see that  items are differentI'm new to python but I ended up doing something similar to @mouadThe XOR operator () should eliminate all elements of the dict when they are the same in both dicts.Just use:@mouad 's answer is nice if you assume both dictionaries just contain simple values. However if you have dictionaries that contain dictionaries you'll get an exception as dictionaries are not hashable.Off the top of my head, something like this might work:Note that I'm not yet happy about the line:as the values of these could be objects. It would be nice to be able to check if the two objects were implementing a comparable interface.To check if two dictionaries are equal, simply use:Yet another possibility, up to the last note of the OP, is to compare the hashes ( or ) of the dicts dumped as JSON. The way hashes are constructed guarantee that if they are equal, the source strings are equal as well. This is very fast and mathematically sound. Here's another option:So as you see the two id's are different. But the  seem to do the trick:In PyUnit there's a method which compares dictionaries beautifully. I tested it using the following two dictionaries, and it does exactly what you're looking for.I'm not recommending importing  into your production code. My thought is the source in PyUnit could be re-tooled to run in production. It uses  which \"pretty prints\" the dictionaries. Seems pretty easy to adapt this code to be \"production ready\"."},
{"body": "What is the difference between  and  in Numpy? And where can I find the implementations in the numpy source code?Well,  is just a convenience function to create an , it is not a class itself.  You can also create an array using , but it is not the recommended way.  From the docstring of :  Most of the meat of the implementation is in C code, , but you can start looking at the ndarray interfaces here: is a function that returns a .  There is no object type numpy.array."},
{"body": "I am writing a Python (Python 3.3) program to send some data to a webpage using POST method.  Mostly for debugging process I am getting the page result and displaying it on the screen using  function.The code is like this:the   method returns a  element encoding the page (which is a well formated UTF-8 document)  It seemed okay until I stopped using IDLE GUI for Windows and used the Windows console instead.  The returned page has a U+2014 character (em-dash) which the print function translates well in the Windows GUI (I presume Code Page 1252) but does not in the Windows Console (Code Page 850).  Given the  default behavior I get the following error:I could fix it using this quite ugly code:Now it replace the offending character \"\u2014\" with a .  Not the ideal case (a hyphen should be a better replacement) but good enough for my purpose.There are several things I do not like from my solution.The problem is not the emdash (I can think of several ways to solve that particularly problem) but I need to write robust code.  I am feeding the page with data from a database and that data can come back.  I can anticipate many other conflicting cases: an '\u00c1' U+00c1 (which is possible in my database) could translate into CP-850 (DOS/Windows Console encodign for Western European Languages) but not into CP-437 (encoding for US English, which is default in many Windows instalations).So, the question:Is there a nicer solution that makes my code agnostic from the output interface encoding?I see three solutions to this:Based on Dirk St\u00f6cker's answer, here's a neat wrapper function for Python 3's print function. Use it just like you would use print.As an added bonus, compared to the other answers, this won't print your text as a bytearray ('b\"content\"'), but as normal strings ('content'), because of the last decode step.For debugging purposes, you could use .To display text, always print Unicode. Don't hardcode the character encoding of your environment such as  inside your script. To decode the http response, see .To print Unicode to Windows console, you could .I dug deeper into this and found the best solutions are here.In my case I solved \"UnicodeEncodeError: 'charmap' codec can't encode character \"original code:New code:If you use Python 3.6 (possibly 3.5 or later), it doesn't give that error to me anymore.  I had a similar issue, because I was using v3.4, but it went away after I uninstalled and reinstalled. If you are using Windows command line to print the data, you should use This worked for me!"},
{"body": "In Python, the where and when of using string concatenation versus string substitution eludes me. As the string concatenation has seen large boosts in performance, is this (becoming more) a stylistic decision rather than a practical one?For a concrete example, how should one handle construction of flexible URIs:Edit: There have also been suggestions about joining a list of strings and for using named substitution. These are variants on the central theme, which is, which way is the Right Way to do it at which time? Thanks for the responses!Concatenation is (significantly) faster according to my machine. But stylistically, I'm willing to pay the price of substitution if performance is not critical. Well, and if I need formatting, there's no need to even ask the question... there's no option but to use interpolation/templating.Don't forget about named substitution:\"As the string concatenation has seen large boosts in performance...\"If performance matters, this is good to know.However, performance problems I've seen have never come down to string operations.  I've generally gotten in trouble with I/O, sorting and O()  operations being the bottlenecks.Until string operations are the performance limiters, I'll stick with things that are obvious.  Mostly, that's substitution when it's one line or less, concatenation when it makes sense, and a template tool (like Mako) when it's large.  The cost of string concatenation is proportional to the length of the result.  Looping leads you straight to the land of N-squared.  Some languages will optimize concatenation to the most recently allocated string, but it's risky to count on the compiler to optimize your quadratic algorithm down to linear.  Best to use the primitive (?) that takes an entire list of strings, does a single allocation, and concatenates them all in one go.What you want to concatenate/interpolate and how you want to format the result should drive your decision. Also, know that Python 2.6 introduces a new version of string interpolation, called :String templating is slated to eventually replace %-interpolation, but that won't happen for quite a while, I think.I was just testing the speed of different string concatenation/substitution methods out of curiosity. A google search on the subject brought me here. I thought I would post my test results in the hope that it might help someone decide....After running , I found that the % method was about twice as fast as the others on these small strings. The concat method was always the slowest (barely). There were very tiny differences when switching the positions in the  method, but switching positions was always at least .01 slower than the regular format method.Sample of test results:I ran these because I do use string concatenation in my scripts, and I was wondering what the cost was. I ran them in different orders to make sure nothing was interfering, or getting better performance being first or last. On a side note, I threw in some longer string generators into those functions like  and regular concat was almost 3 times as fast (1.1 vs 2.8) as using the  and  methods. I guess it depends on the strings, and what you are trying to achieve. If performance really matters, it might be better to try different things and test them. I tend to choose readability over speed, unless speed becomes a problem, but thats just me. SO didn't like my copy/paste, i had to put 8 spaces on everything to make it look right. I usually use 4.Remember, stylistic decisions  practical decisions, if you ever plan on maintaining or debugging your code :-)  There's a famous quote from Knuth (possibly quoting Hoare?): \"We should forget about small efficiencies, say about 97% of the time: premature optimization is the root of all evil.\"As long as you're careful not to (say) turn a O(n) task into an O(n) task, I would go with whichever you find easiest to understand..I use substitution wherever I can. I only use concatenation if I'm building a string up in say a for-loop.Actually the correct thing to do, in this case (building paths) is to use . Not string concatenation or interpolation"},
{"body": "I know that artificial benchmarks are evil. They can show results only for very specific narrow situation. I don't assume that one language is better than the other because of the some stupid bench. However I wonder why results is so different. Please see my questions at the bottom.Benchmark is simple math calculations to find pairs of prime numbers which differs by 6 (so called )\nE.g. sexy primes below 100 would be: \nRunning: all except Factor was running in VirtualBox (Debian unstable amd64 guest, Windows 7 x64 host)\nCPU: AMD A4-3305M[*1] - I'm afraid to imagine how much time will it takeC:Ruby:Scala:Scala opimized  (the same idea like in Clojure optimization):Clojure:Clojure optimized :PythonFactorBash(zsh):Rough answers:Most important optimisation in the Clojure code would be to use typed primitive maths within , something like:With this improvement, I get Clojure completing 10k in 0.635 secs (i.e. the second fastest on your list, beating Scala) note that you have printing code inside your benchmark in some cases - not a good idea as it will distort the results, especially if using a function like  for the first time causes initialisation of IO subsystems or something like that!I'll answer just #2, since it's the only one I've got anything remotely intelligent to say, but for your Python code, you're creating an intermediate list in , whereas you're using  in your  in Ruby which is just iterating.If you change your  to:they're on par.I could optimize the Python further, but my Ruby isn't good enough to know when I've given more of an advantage (e.g., using  makes Python win on my machine, but I don't remember if the Ruby range you used creates an entire range in memory or not). Without being too silly, making the Python code look like:which doesn't change much more, puts it at 1.5s for me, and, with being extra silly, running it with PyPy puts it at .3s for 10K, and 21s for 100K.Here's a fast Clojure version, using the same basic algorithms:It runs about 20x faster than your original on my machine. And here's a version that leverages the new reducers library in 1.5 (requires Java 7 or JSR 166):This runs about 40x faster than your original. On my machine, that's 100k in 1.5 seconds.You can make the Scala a lot faster by modifying your  method toNot quite as concise but the program runs in 40% of the time! We cut out the superfluous  and anonymous  objects, the Scala compiler recognizes the tail-recursion and turns it into a while-loop, which the JVM can turn into more or less optimal machine code, so it shouldn't be too far off the C version.See also: Here is my scala version in both parallel and no-parallel, just for fun:\n(In my dual core compute, the parallel version takes 335ms while the no-parallel version takes 655ms)EDIT: According to 's suggestion, I have changed my code to avoid the effects of IO and jvm warmup:The result shows in my compute:Never mind the benchmarks; the problem got me interested and I made some fast tweaks.  This uses the  decorator, which memoizes a function; so when we call  we basically get that prime check for free.  This change cuts the work roughly in half.  Also, we can make the  calls step through just the odd numbers, cutting the work roughly in half again.This requires Python 3.2 or newer to get , but could work with an older Python if you install a Python recipe that provides .  If you are using Python 2.x you should really use  instead of .The above took only a very short time to edit.  I decided to take it one step further, and make the primes test only try prime divisors, and only up to the square root of the number being tested.  The way I did it only works if you check numbers in order, so it can accumulate all the primes as it goes; but this problem was already checking the numbers in order so that was fine.On my laptop (nothing special; processor is a 1.5 GHz AMD Turion II \"K625\") this version produced an answer for 100K in under 8 seconds.The above code is pretty easy to write in Python, Ruby, etc. but would be more of a pain in C.You can't compare the numbers on this version against the numbers from the other versions without rewriting the others to use similar tricks.  I'm not trying to prove anything here; I just thought the problem was fun and I wanted to see what sort of easy performance improvements I could glean.Don't forget Fortran!  (Mostly joking, but I would expect similar performance to C).  The statements with exclamation points are optional, but good style. ( is a comment character in fortran 90)I couldn't resist to do a few of the most obvious optimizations for the C version which made the 100k test now take 0.3s on my machine (5 times faster than the C version in the question, both compiled with MSVC 2010 /Ox).Here is the identical implemention in Java:With Java 1.7.0_04 this runs almost exactly as fast as the C version. Client or server VM doesn't show much difference, except that JIT training seems to help the server VM a bit (~3%) while it has almost no effect with the client VM. The output in Java seems to be slower than in C. If the output is replaced with a static counter in both versions, the Java version runs a little faster than the C version.These are my times for the 100k run:and the 1M run (16386 results):While this does not really answer your questions, it shows that small tweaks can have a noteworthy impact on performance. So to be able to really compare languages you should try to avoid all algorithmic differences as much as possible.It also gives a hint why Scala seems rather fast. It runs on the Java VM and thus benefits from its impressive performance.In Scala try using Tuple2 instead of List, it should go faster. Just remove the word 'List' since (x, y) is a Tuple2.Tuple2 is specialized for Int, Long and Double meaning it won't have to box/unbox those raw datatypes. . List isn't specialized. .Here's the code for the Go (golang.org) version:It ran just as fast as the C version.\nIntel Core 2 Duo T6500 2.1GHz, 2MB L2 cache, 800MHz FSB.\n4GB RAM The 100k version:  With 1000000 (1M instead of 100K):   But I think that it would be fair to use Go's built in multithreading capabilities and compare that version with the regular C version (without multithreading), just because it's almost too easy to do multithreading with Go.Update: I did a parallel version using Goroutines in Go:The parallelized version completed in 1.706 seconds. It used less than 1.5 Mb RAM. Fixed with a call to Update: I ran the paralellized version up to 1M numbers.  With 1000000 (1M instead of 100K):    The 100k version:   Based on , I wrote a scala version using recursion, and I improved on it by only going to the sqrt instead of x/2 for the prime check function. I get ~250ms for 100k, and ~600ms for 1M. I went ahead and went to 10M in ~6s.I also went back and wrote a CoffeeScript (V8 JavaScript) version, which gets ~15ms for 100k, 250ms for 1M, and 6s for 10M, by using a counter (ignoring I/O). If I turn on the output it takes ~150ms for 100k, 1s for 1M, and 12s for 10M. Couldn't use tail recursion here, unfortunately, so I had to convert it back into loops.Just for the fun of it, here is a parallel Ruby version. On my 1.8GHz Core i5 MacBook Air, the performance results are:It looks like the JVM's JIT is giving Ruby a nice performance boost in the default case, while true multithreading helps JRuby perform 50% faster in the threaded case. What's more interesting is that JRuby 1.7 improves the JRuby 1.6 score by a healthy 17%!The answer to your question #1 is that Yes, the JVM is incredably fast and yes static typing helps.The JVM should be faster than C in the long run, possibly even faster than \"Normal\" assembly language--Of course you can always hand optimize assembly to beat anything by doing manual runtime profiling and creating a separate version for each CPU, you just have to be amazingly good and knowledgable.The reasons for Java's speed are: The JVM can analyze your code while it runs and hand-optimize it--for instance, if you had a method that could be statically analyzed at compile time to be a true function and the JVM noticed that you were often calling it with the same parameters, it COULD actually eliminate the call completely and just inject the results from the last call (I'm not sure if Java actually does this exactly, but it doest a lot of stuff like this). Due to static typing, the JVM can know a lot about your code at compile time, this lets it pre-optimize quite a bit of stuff.  It also lets the compiler optimize each class individually without knowledge of how another class is planning to use it.  Also Java doesn't have arbitrary pointers to memory location, it KNOWS what values in memory may and may not be changed and can optimize accordingly.Heap allocation is MUCH more efficient than C, Java's heap allocation is more like C's stack allocation in speed--yet more versatile.  A lot of time has gone into the different algroithims used here, it's an art--for instance, all the objects with a short lifespan (like C's stack variables) are allocated to a \"known\" free location (no searching for a free spot with enough space) and are all freed together in a single step (like a stack pop). The JVM can know quirks about your CPU architecture and generate machine code specifically for a given CPU.The JVM can speed your code long after you shipped it.  Much like moving a program to a new CPU can speed it up, moving it to a new version of the JVM can also give you huge speed performances taylored to CPUs that didn't even exist when you initially compiled your code, something c physically cannot do without a recomiple.By the way, most of the bad rep for java speed comes from the long startup time to load the JVM (Someday someone will build the JVM into the OS and this will go away!) and the fact that many developers are really bad at writing GUI code (especially threaded) which caused Java GUIs to often become unresponsive and glitchy.  Simple to use languages like Java and VB have their faults amplified by the fact that the capibilities of the average programmer tends to be lower than more complicated languages."},
{"body": "I am setting out to do a side project that has the goal of translating code from one programming language to another. The languages I am starting with are PHP and Python (Python to PHP should be easier to start with), but ideally I would be able to add other languages with (relative) ease. The plan is:Then I believe I can start outputting code. . I'll still have to review the generated code and fix problems. Ideally the translator should flag problematic translations.Before you ask \"What the hell is the point of this?\" The answer is... It'll be an interesting learning experience. If you have any insights on how to make this less daunting, please let me know.I am more interested in knowing what kinds of patterns I could enforce on the code to make it easier to translate (ie: IoC, SOA ?) the code than how to do the translation.I've been  to do general purpose program manipulation (with language translation being a special case) since 1995, supported by a strong team of computer scientists.  DMS provides generic parsing, AST building, symbol tables, control and data flow analysis, application of translation rules, regeneration of source text with comments, etc., all parameterized by explicit definitions of computer languages.The amount of machinery you need to do this  is vast (especially if you want to be able to do this for multiple languages in a general way), and then you need reliable parsers for languages with unreliable definitions (PHP is perfect example of this).There's nothing wrong with you thinking about building a language-to-language translator or attempting it, but I think you'll find this a much bigger task for real languages than you expect.  We have some 100 man-years invested in just DMS, and another 6-12 months in each \"reliable\" language definition (including the one we painfully built for PHP), much more for nasty languages such as C++.  It will be a \"hell of a learning experience\"; it has been for us.  (You might find the technical Papers section at the above website interesting to jump start that learning).People often attempt to build some kind of generalized machinery by starting with some piece of technology with which they are familiar, that does a part of the job. (Python ASTs are great example).  The good news, is that part of the job is done.  The bad news is that machinery has a zillion assumptions built into it, most of which you won't discover until you try to wrestle it into doing something else.  At that point you find out the machinery is wired to do what it originally does, and will really, really resist your attempt to make it do something else. (I suspect trying to get the Python AST to model PHP is going to be a lot of fun).The reason I started to build DMS originally was to build foundations that had very few such assumptions built in.   It has some that give us headaches.  So far, no black holes. (The hardest part of my job over the last 15 years is to try to prevent such assumptions from creeping in).Lots of folks also make the mistake of assuming that if they can parse (and perhaps get an AST), they are well on the way to doing something complicated.  One of the hard lessons is that you need symbol tables and flow analysis to do good program analysis or transformation.   ASTs are necessary but not sufficient.  This is the reason that Aho&Ullman's compiler book doesn't stop at chapter 2.  (The OP has this right in that he is planning to build additional machinery beyond the AST).  For more on this topic, see .The remark about \"I don't need a perfect translation\" is troublesome.  What weak translators do is convert the \"easy\" 80% of the code, leaving the hard 20% to do by hand.  If the application you intend to convert are pretty small, and you only intend to convert it once well, then that 20% is OK. If you want to convert many applications (or even the same one with minor changes over time), this is not nice.  If you attempt to convert 100K SLOC then 20% is 20,000 original lines of code that are hard to translate, understand and modify in the context of another 80,000 lines of translated program you already don't understand.  That takes a huge amount of effort.  At the million line level, this is simply impossible in practice.  (Amazingly there are people that distrust automated tools and insist on translating million line systems by hand; that's even  and they normally find out painfully with long time delays, high costs and often outright failure.)What you have to shoot for to translate large-scale systems is high nineties percentage  conversion rates, or it is likely that you can't complete the manual part of the translation activity.Another key consideration is size of code to be translated.  It takes a lot of energy to build a working, robust translator, even with good tools.  While it seems sexy and cool to build a translator instead of simply doing a manual conversion, for small code bases (e.g., up to about 100K SLOC in our experience) the economics simply don't justify it. Nobody likes this answer, but if you really have to translate just 10K SLOC of code, you are probably better off just biting the bullet and doing it.  And yes, that's painful.I consider our tools to be extremely good (but then, I'm pretty biased).   And it is still very hard to build a good translator; it takes us about 1.5-2 man-years and we know how to use our tools.  The difference is that with this much machinery, we  considerably more often than we fail.My answer will address the specific task of parsing Python in order to translate it to another language, and not the higher-level aspects which Ira addressed well in his answer.In short: The  module, available since Python 2.6 is much more suitable for your needs, since it gives you a ready-made AST to work with. I've written an  last year, but in short, use the  method of  to parse Python source code into an AST. The  module will give you a parse tree, not an AST. . Now, since Python's ASTs are quite detailed, given an AST the front-end job isn't terribly hard. I suppose you can have a simple prototype for some parts of the functionality ready quite quickly. However, getting to a complete solution will take more time, mainly because the semantics of the languages are different. A simple subset of the language (functions, basic types and so on) can be readily translated, but once you get into the more complex layers, you'll need heavy machinery to emulate one language's core in another. For example consider Python's generators and list comprehensions which don't exist in PHP (to my best knowledge, which is admittedly poor when PHP is involved).To give you one final tip, consider the  tool created by the Python devs to translate Python 2 code to Python 3 code. Front-end-wise, it has most of the elements you need to translate Python to . However, since the cores of Python 2 and 3 are similar, no emulation machinery is required there.Writing a translator isn't impossible, especially considering that Joel's Intern If you want to do one language, it's easy. If you want to do more, it's a little more difficult, but not too much. The hardest part is that, while any turing complete language can do what another turing complete language does, built-in data types can change what a language does phenomenally.For instance:takes a  of C++ code to duplicate (ok, well you can do it fairly short with some looping constructs, but still).That's a bit of an aside, I guess. Have you ever written a tokenizer/parser based on a language grammar? You'll probably want to learn how to do that if you haven't, because that's the main part of this project. What I would do is come up with a basic Turing complete syntax - something fairly similar to Python  . Then you create a lexer/parser that takes a language grammar (perhaps using ), and based on the grammar, compiles the language into your intermediate language. Then what you'll want to do is do the reverse - create a parser from your language into target languages based on the grammar.The most obvious problem I see is that at first you'll probably create  inefficient code, especially in more powerful* languages like Python.But if you do it this way then you'll probably be able to figure out ways to optimize the output as you go along. To summarize:*by powerful I mean that this takes 4 lines:Show me another language that can do something like that in 4 lines, and I'll show you a language that's as powerful as Python.There are a couple answers telling you not to bother. Well, how helpful is that? You want to learn? You can learn. This is compilation. It just so happens that your target language isn't machine code, but another high-level language. This is done all the time.There's a relatively easy way to get started. First, go get  (if you want to work in PHP) or some such and go through the example code. Next, you can write a lexical analyzer using a sequence of regular expressions and feed tokens to the parser you generate. Your semantic actions can either output code directly in another language or build up some data structure (think objects, man) that you can massage and traverse to generate output code.You're lucky with PHP and Python because in many respects they are the same language as each other, but with different syntax. The hard part is getting over the semantic differences between the grammar forms and data structures. For example, Python has lists and dictionaries, while PHP only has assoc arrays.The \"learner\" approach is to build something that works OK for a restricted subset of the language (such as only print statements, simple math, and variable assignment), and then progressively remove limitations. That's basically what the \"big\" guys in the field all did.Oh, and since you don't have static types in Python, it might be best to write and rely on PHP functions like \"python_add\" which adds numbers, strings, or objects according to the way Python does it.Obviously, this can get much bigger if you let it.I will second @EliBendersky point of view regarding using ast.parse instead of parser (which I did not know about before). I also warmly recommend you to review his blog. I used ast.parse to do Python->JavaScript translator (@). I've come up with Pythonium design by  reviewing other implementations and trying them on my own. I forked Pythonium from  which I also started, It's actually a complete rewrite . The overall design is inspired from PyPy and  paper.Everything I tried, from beginning to the best solution, even if it looks like Pythonium marketing it really isn't (don't hesitate to tell me if something doesn't seem correct to the netiquette):I never actually succeed at running Pyjamas #fail and never tried to read the code #fail again. But in my mind PyJamas was doing API->API tranlation (or framework to framework) and not Python to JavaScript translation. The JavaScript framework consume data that is already in the page or data from the server. Python code is only \"plumbing\". After that I discovered  that pyjamas was actually a real python->js translator. Still I think it's possible to do API->API (or framework->framework) translation and that's basicly what I do in Pythonium but at lower level. Probably Pyjamas use the same algorithm as Pythonium...Then I discovered brython fully written in Javascript like Skulpt, no need for compilation and lot of fluff... but written in JavaScript.Since the initial line written in the course of this project, I knew about PyPy, even the JavaScript backend for PyPy. Yep, you can, if you find it, directly generate a Python interpreter in JavaScript from PyPy. People say, it was a disaster. I read no where why. But I think the reason is that the intermediate language they use to implement the interpreter, RPython, is a subset of Python tailored to be translated to C (and maybe asm). Ira Baxter says you always make assumptions when you build something and probably you fine tune it to be the best at what it's meant to do in the case of PyPy: Python->C translation. Those assumptions might not be relevant in another context worse they can infere overhead otherwise said direct translation will most likely always be better.Having the interpreter written in Python sounded like a (very) good idea. But I was more interested in a compiler for performance reasons also it's actually more easy to compile Python to JavaScript than interpret it.I started PythonJS with the idea of putting together a subset of Python that I could easily translate to JavaScript. At first I didn't even bother to implement OO system because of past experience. The subset of Python that I achieved to translate to JavaScript are:This seems like a lot but actually very narrow compared to full blown semantic of Python. It's really JavaScript with a Python syntax. The generated JS is perfect ie. there is no overhead, it can not be improved in terms of performance by further editing it. If you can improve the generated code, you can do it from the Python source file too. Also, the compiler did not rely on any JS tricks that you can find in .js written by , so it's very readable.The direct descendant of this part of PythonJS is the Pythonium Veloce mode. The full implementation can be found @  793 SLOC + around 100 SLOC of shared code with the other translator.An adapted version of pystones.py can be translated in Veloce mode cf. After having setup basic Python->JavaScript translation I choosed another path to translate full Python to JavaScript. The way of glib doing object oriented class based code except the target language is JS so you have access to arrays, map-like objects and many other tricks and all that part was written in Python. IIRC there is no javascript code written by in Pythonium translator. Getting single inheritance is not difficult here are the difficult parts making Pythonium fully compliant with Python:This part is factored in  It's written in Python compatible with Python Veloce.The actual compliant translator  doesn't generate JavaScript code directly and most importantly doesn't do ast->ast transformation. I tried the ast->ast thing and ast even if nicer than cst is not nice to work with even with ast.NodeTransformer and more importantly I don't need to do ast->ast.Doing python ast to python ast in my case at least would maybe be a performance improvement since I sometime inspect the content of a block before generating the code associated with it, for instance:So I don't really visit each node once for each phase of the translation.The overall process can be described as:Python builtins are written in Python code (!), IIRC there is a few restrictions related to bootstraping types, but you have access to everything that can translate Pythonium in compliant mode. Have a look at Reading JS code generated from pythonium compliant can be understood but source maps will greatly help.The valuable advice I can give you in the light of this experience are kind old farts:With Python Veloce mode only, I'm very happy! But along the way I discovered that what I was really looking for was liberating me and others from Javascript but more importantly being able to  in a comfortable way. This lead me to Scheme, DSL, Models and eventually domain specific models (cf. ).About what Ira Baxter response:The estimations are not helpful at all. I took me more or less 6 month of free time for both PythonJS and Pythonium. So I can expect more from full time 6 month. I think we all know what 100 man-year in an enterprise context can mean and not mean at all...When someone says something is hard or more often impossible, I answer that \"it only takes time to find a solution for a problem that is impossible\" otherwise said nothing is impossible except if it's proven impossible in this case a math proof...If it's not proven impossible then it leaves room for imagination:andorIt's not just optimistic thinking. When I started Python->Javascript everybody was saying it was impossible. PyPy impossible. Metaclasses too hard. etc... I think that the only revolution that brings PyPy over Scheme->C paper (which is 25 years old) is some automatic JIT generation (based hints written in the RPython interpreter I think).Most people that say that a thing is \"hard\" or \"impossible\" don't provide the reasons. C++ is hard to parse? I know that, still they are (free)  C++ parser. Evil is in the detail? I know that. Saying it's impossible alone is not helpful, It's even worse than \"not helpful\" it's discouraging, and some people mean to discourage others. I heard about this question via .What would be perfection ? That's how you define next goal and maybe reach the overall goal.I see no patterns that can not be translated from one language to another language at least in a less than perfect way. Since language to language translation is possible, you'd better aim for this first. Since, I think according to , translation between two computer languages is a tree or DAG isomorphism. Even if we already know that they are both turing complete, so...Framework->Framework which I better visualize as API->API translation might still be something that you might keep in mind as a way to improve the generated code. E.g: Prolog as very specific syntax but still you can do Prolog like computation by describing the same graph in Python... If I was to implement a Prolog to Python translator I wouldn't implement unification in Python but in a C library and come up with a \"Python syntax\" that is very readable for a Pythonist. In the end, syntax is only \"painting\" for which we give a meaning (that's why I started scheme). Evil is in the detail of the language and I'm not talking about the syntax. The concepts that are used in the language  hook (you can live without it) but required VM features like tail-recursion optimisation can be difficult to deal with. You don't care if the initial program doesn't use tail recursion and even if there is no tail recursion in the target language you can emulate it using greenlets/event loop. For target and source languages, look for:From this will emerge:You will also probably be able to know what will be translated to fast and slow code.There is also the question of the stdlib or any library but there is no clear answer, it depends of your goals.Idiomatic code or readable generated code have also solutions...Targeting a platform like PHP is much more easy than targeting browsers since you can provide C-implementation of slow and/or critical path.Given you first project is translating Python to PHP, at least for the PHP3 subset I know of, customising veloce.py is your best bet. If you can implement veloce.py for PHP then probably you will be able to run the compliant mode... Also if you can translate PHP to the subset of PHP you can generate with php_veloce.py it means that you can translate PHP to the subset of Python that veloce.py can consume which would mean that you can translate PHP to Javascript. Just saying...You can also have a look at those libraries:Also you might be interested by this blog post (and comments): You could take a look at the , which translates Vala (a C#-like language) into C."},
{"body": "Let's say that I have a class Suit and four subclasses of suit: Heart, Spade, Diamond, Club.I have a method which receives a suit as a parameter, which is a class object, not an instance. More precisely, it may receive only one of the four values: Heart, Spade, Diamond, Club. How can I make an assertion which ensures such a thing? Something like:I'm using Python 3.You can use  like this .  You can use  if you have an instance, or  if you have a class. Normally thought its a bad idea. Normally in Python you work out if an object is capable of something by attempting to do that thing to it.The  boolean function returns true if the given subclass  is indeed a subclass of the superclass .You can use the builtin issubclass. But type checking is usually seen as unneccessary because you can use duck-typing.Using issubclass seemed like a clean way to write loglevels. It kinda feels odd using it... but it seems cleaner than other options. "},
{"body": "I am running PyLint on a Python project. PyLint makes many complaints about being unable to find numpy members. How can I avoid this while avoiding skipping membership checks.From the code:Which, when ran, I get the expected:However, pylint gives me this error:For versions, I am using pylint 1.0.0 (astroid 1.0.1, common 0.60.0) and trying to work with numpy 1.8.0 .I had the same issue here, even with the latest versions of all related packages (, , ).The following solution worked like a charm: I added  to the list of ignored modules by modifying my  file, in the  section:Depending on the error, you might also need to add the following line (still in the ):In recent versions of pylint you can add  to your pylint command. They had fixed this problem in an earlier version in an unsafe way. Now if you want them to look more carefully at a package outside of the standard library, you must explicitly whitelist it. I was getting the same error for a small numpy project I was working on and decided that ignoring the numpy modules would do just fine. I created a  file with:and following paduwan's and j_houg's advice I modified the following sectors:andand it \"fixed\" my issue.Since this is the top result in google and it gave me the impression that you have to ignore those warnings in all files:The problem has actually been fixed in the sources of pylint/astroid last month \nbut are not yet in the Ubuntu packages. To get the sources, justwhereby the last step will most likely require a  and of course you need mercurial to clone.If using  with Don Jayamanne's excellent , add a user setting to whitelist numpy:There have been many different bugs reported about this over the past few years i.e. I'd suggest disabling for the lines where the complaints occur.Probably, it's confused with numpy's abstruse method of methods import. Namely,  is in fact , imported in numpy with statementin turn imported withand in numeric you'll findI guess I would be confused in place of PyLint! See  for PyLint side of view.This seems to work on at least Pylint 1.1.0:I had this problem with numpy, scipy, sklearn, nipy, etc., and I solved it by wrapping epylint like so:$ cat epylint.pyThis script simply runs epylint, then scrapes its output to filter out false-positive warnings and errors. You can extend it by added more elif cases.N.B.: If this applies to you, then you'll want to modify your pychechers.sh so it likes like this(Of course, you have to make epylint.py executable first)Here is a link to my .emacs . Hope this is useful to someone.This is the pseudo-solution I have come up with for this problem.Then, in your code, instead of calling  functions as  and  and so on, \nyou would write , , etc. \nAdvantages of this approach vs. other approaches suggested in other answers:The clear disadvantage is that you have to explicitely import every numpy function you use. \nThe approach could be elaborated on further. \nYou could define your own module, call it say,  as followsThen, your application code could import this module only (instead of numpy) as and use the names as usual: ,  etc.The advantage of this is that you will have a single module in which all  related imports are done once and for all, and then you import it with that single line, wherever you want. Still you have to be careful that  does not import names that don\u00b4t exist in  as those errors won't be caught by pylint.  In Extension to j_hougs answer, you can now add the modules in question to this line in .pylintrc, which is already prepared empty on generation:you can generate a sample .pylintrc by doing:and then edit the mentioned lineI had to add this at the top of any file where I use numpy a lot. Just in case someone in eclipse is having trouble with Pydev and pylint...I've been working on a patch to pylint to solve the issue with dynamic members in libraries such as numpy. It adds a \"dynamic-modules\" option which forces to check if members exist during runtime by making a real import of the module. See . There is also a pull request, see link in one of the comments.A little bit of copy paste from the previous answer to summarize what is working (at least for me: debian-jessie)Basic command to run:\n    # ONLY if you do not already have a .pylintrc file in your home\n    $ pylint --generate-rcfile > .pylintrcThen open the file and add the packages you want after  separated by comma. You can have the same behavior using the option  from the command line.If you ignore some packages in the  section that means that  will never show error related to that packages. In practice,  will not tell you anything about those packages."},
{"body": "Recently I wrote a function to generate certain sequences with nontrivial constraints. The problem came with a natural recursive solution. Now it happens that, even for relatively small input, the sequences are several thousands, thus I would prefer to use my algorithm as a generator instead of using it to fill a list with all the sequences.Here is an example. Suppose we want to compute all the permutations of a string with a recursive function. The following naive algorithm takes an extra argument 'storage' and appends a permutation to it whenever it finds one:(Please don't care about inefficiency, this is only an example.)Now I want to turn my function into a generator, i.e. to yield a permutation instead of appending it to the storage list:This code does  work (the function behaves like an empty generator).Am I missing something?\nIs there a way to turn the above recursive algorithm into a generator ?Or without an accumulator:This avoids the -deep recursion, and is in general a nice way to handle generators-inside-generators: allows us to continue progress in another generator by simply ing it, instead of iterating through it and ing each item manually.Python 3.3 will add  to the syntax, which allows for natural delegation to a sub-generator:The interior call to getPermutations -- it's a generator, too.You need to iterate through that with a for-loop (see @MizardX posting, which edged me out by seconds!)"},
{"body": "I have a shell that runs CentOS.For a project I'm doing, I need python 2.5+, but centOS is pretty dependent on 2.4.From what I've read, a number of things will break if you upgrade to 2.5.I want to install 2.5 separately from 2.4, but I'm not sure how to do it. So far I've downloaded the source tarball, untarred it, and did a  which is where I want it to end up. Can I now just  ? Or is there more?You could also use the , and then do  to install python 2.6Try epelThe python executable will be available at Now,  command will execute When I've run into similar situations, I generally avoid the package manager, especially if it would be embarrassing to break something, i.e. a production server. Instead, I would go to Activestate and download their binary package:This is installed by running a script which places everything into a folder and does not touch any system files. In fact, you don't even need root permissions to set it up. Then I change the name of the binary to something like apy26, add that folder to the end of the PATH and start coding. If you install packages with or if you use virtualenv and easyinstall, then you have just as flexible a python environment as you need without touching the system standard python.Edits...\nRecently I've done some work to build a portable Python binary for Linux that should run on any distro with no external dependencies. This means that any binary shared libraries needed by the portable Python module are part of the build, included in the tarball and installed in Python's private directory structure. This way you can install Python for your application without interfering with the system installed Python. has a build script which has been thoroughly tested on Ubuntu Lucid 10.04 LTS both 32 and 64 bit installs. I've also built it on Debian Etch but that was a while ago and I can't guarantee that I haven't changed something. The easiest way to do this is you just put your choice of Ubuntu Lucid in a virtual machine, checkout the script with  and then run the script. Once you have it built, use the tarball on any recent Linux distro. There is one little wrinkle with moving it to a directory other than  which is that you have to run the included  to set the interpreter path BEFORE you move the directory. This affects any binaries in All of this is based on building with RUNPATH and copying the dependent shared libraries. Even though the script is in several files, it is effectively one long shell script arranged in the style of /etc/rc.d directories.No, that's it. You might want to make sure you have all optional library headers installed too so you don't have to recompile it later. They are listed in the documentation I think.Also, you can install it even in the standard path if you do . That way it won't override your current default \"python\".No need to do yum or make your own RPM. Build  from source.There can be a  use Add the install path ( by default) to .It will not break  or any other things which are dependent on . provides a YUM repository for python26 RPMs that can co-exist with the 'native' 2.4 that is needed for quite a few admin tools on CentOS.Quick instructions that worked at least for me:If you want to make it easier on yourself, there are CentOS RPMs for new Python versions floating around the net. E.g. see:When you install your python version (in this case it is python2.6) then issue this command to create your :Late to the party, but the OP should have gone with  or , and sidestepped the problem completely.I am currently working on a Centos server, well, toiling away would be the proper term and I can assure everyone that the only way I am able to blink back the tears whilst using the software equivalents of fire hardened spears, is  buildout.  you can always make your own RPM:Missing Dependency: libffi.so.5  is here : for dos that just dont know :=)I unistalled the original version of python (2.6.6) and install 2.7(with option ) but when I tried install something with yum didn't work.So I solved this issue as follow:Done"},
{"body": "I need to be able to switch back and forth between Python 2 and 3. How do I do that using Homebrew as I don't want to mess with path and get into trouble.\nRight now I have 2.7 installed through Homebrew.I would use  You can install it:To enable pyenv in your Bash shell, you need to run:To do this automatically for Bash upon startup, add that line to your . Once you have installed pyenv and activated it, you can install different versions of python and choose which one you can use. Example:You can check the versions you have installed with:And you can switch between python versions with the command:Also you can set a python version for the current directory with:You can check by running :You can have both versions installed at the same time.For Python 2.x:For Python 3.x:Now, you will have both the versions installed in your machine. When you want to use version 2, use the  executable. When you want to use version 3, use the  executable.Alternatively, you probably can just enter \"python3\" to run your most current version of python3.x and \"python\" or \"python2\" to run the latest installed 2.x version."},
{"body": "What I'm trying to do would look like this in the command line:How can I get a reference to all the names defined in  from within  itself?Something like this:Just use globals()As previously mentioned, globals gives you a dictionary as opposed to dir() which gives you a list of the names defined in the module.  The way I typically see this done is like this:Also check out the built-in  module.  It can be very handy."},
{"body": "In :I don't understand mro.\nWhat does  do?\nWhat does mro mean?Follow along...:As long as we have single inheritance,  is just the tuple of: the class, its base, its base's base, and so on up to  (only works for new-style classes of course).Now, with  inheritance...:...you also get the assurance that, in , no class is duplicated, and no class comes after its ancestors, save that classes that first enter at the same level of multiple inheritance (like B and C in this example) are in the  left to right.Every attribute you get on a class's instance, not just methods, is conceptually looked up along the , so, if more than one class among the ancestors defines that name, this tells you where the attribute will be found -- in the first class in the  that defines that name. stands for Method Resolution Order.  It returns a list of types the class is derived from, in the order they are searched for methods.This would perhaps show the order of resolution.and response would be The rule is dept first which in this case would mean D, B, A, CPython normally uses a  order when searching inheriting classes\nBut when two classes inherit from the same class, Python removes the first mention of that class from mro."},
{"body": "I'm writing a Django Middleware class that I want to execute only once at startup, to initialise some other arbritary code. I've followed the very nice solution posted by sdolan , but the \"Hello\" message is output to the terminal . E.g.and in my Django settings file, I've got the class included in the  list.But when I run Django using runserver and request a page, I get in the terminalAny ideas why \"Hello world\" is printed twice? Thanks.Don't do it this way.You don't want \"middleware\" for a one-time startup thing.You want to execute code in the top-level .  That module is imported and executed once.file: file: The number one answer does not seem to work anymore, urls.py is loaded upon first request.What has worked lately is to put the startup code in any one of your INSTALLED_APPS .py e.g. When using  ... this gets executed twice, but that is because runserver has some tricks to validate the models first etc ... normal deployments or even when runserver auto reloads, this is only executed once.This question is well-answered in the blog post , which will work for Django >= 1.4.Basically, you can use  to do that, and it will be run only once, when the server starts, but not when you run commands or import a particular module.If it helps someone, in addition to  answer, \"--noreload\" option prevents runserver from executing command on startup twice:But that command won't reload runserver after other code's changes as well."},
{"body": "Python provides a nice method for getting length of an eager iterable,  that is. But I couldn't find anything similar for lazy iterables represented by generator comprehensions and functions. Of course, it is not hard to write something like:But I can't get rid of a feeling that I'm reimplementing a bicycle.(While I was typing the function, a thought struck my mind: maybe there really is no such function, because it \"destroys\" its argument. Not an issue for my case, though).P.S.: concerning the first answers - yes, something like  would work too, but that drastically increases the usage of memory.P.P.S.: re-checked... Disregard the P.S., seems I made a mistake while trying that, it works fine. Sorry for the trouble.There isn't one because you can't do it in the general case - what if you have a lazy infinite generator?  For example:This never terminates but will generate the Fibonacci numbers.  You can get as many Fibonacci numbers as you want by calling .If you really need to know the number of items there are, then you can't iterate through them linearly one time anyway, so just use a different data structure such as a regular list.The easiest way is probably just  where gen is your generator. Or better yet:If it's not iterable, it will throw a .Or, if you want to count something specific in the generator:You can use enumerate() to loop through the generated data stream, then return the last number -- the number of items.I tried to use itertools.count() with itertools.izip() but no luck. This is the best/shortest answer I've come up with:Kamil Kisiel's solution is way better:Use  for a memory efficient purely functional solution:By definition, only a subset of generators will return after a certain number of arguments (have a pre-defined length), and even then, only a subset of these finite generators have a predictable end (accessing the generator can have side-effects which could stop the generator earlier).If you wish to implement length methods for your generator, you have to first define what you consider the \"length\" (is it the total number of elements? the number of remaining elements?), then wrap your generator in a class. Here's an example:Here is how to use it:This is a hack, but if you really want to have  work on a general iterable (consuming it in the way), you can create your own version of .The  function is essentially equivalent to the following (though implementations usually provide some optimizations to avoid the extra lookup):Therefore we can define our  to try that, and if  does not exist, count the number of elements ourselves by consuming the iterable:The above works in Python 2/3, and (as far as I know) should cover every conceivable kind of iterable.Try the  package for a simple solution.  Example:See  for another applied example.I think when considering only length, you should discard the result (content).always does the work for you in this scenario."},
{"body": "When using os.system() it's often necessary to escape filenames and other arguments passed as parameters to commands.  How can I do this?  Preferably something that would work on multiple operating systems/shells but in particular for bash.I'm currently doing the following, but am sure there must be a library function for this, or at least a more elegant/robust/efficient option: I've accepted the simple answer of using quotes, don't know why I didn't think of that; I guess because I came from Windows where ' and \" behave a little differently.Regarding security, I understand the concern, but, in this case, I'm interested in a quick and easy solution which os.system() provides, and the source of the strings is either not user-generated or at least entered by a trusted user (me).This is what I use:The shell will always accept a quoted filename and remove the surrounding quotes before passing it to the program in question. Notably, this avoids problems with filenames that contain spaces or any other kind of nasty shell metacharacter.: If you are using Python 3.3 or later, use  instead of rolling your own. does what you want since python 3.(Use  to support both python 2 and python 3)Perhaps you have a specific reason for using . But if not you should probably be using the . You can specify the pipes directly and avoid using the shell.The following is from :Maybe  is a better shot?Note that pipes.quote is actually broken in Python 2.5 and Python 3.1 and not safe to use--It doesn't handle zero-length arguments.See ; it has been fixed in Python 2.6 and 3.2 and newer.I believe that os.system just invokes whatever command shell is configured for the user, so I don't think you can do it in a platform independent way.  My command shell could be anything from bash, emacs, ruby, or even quake3.  Some of these programs aren't expecting the kind of arguments you are passing to them and even if they did there is no guarantee they do their escaping the same way.: This is an answer for Python 2.7.x.According to the ,  is a way to \"\". (Although it is  and finally exposed publicly in Python 3.3 as the  function.)On ,  is a way to \"\".Here we are, the platform independent way of quoting strings for command lines.Usage:The function I use is:that is: I always enclose the argument in double quotes, and then backslash-quote the only characters special inside double quotes.The real answer is: Don't use  in the first place. Use  instead and supply the unescaped arguments.If you do use the system command, I would try and whitelist what goes into the os.system() call.. For example..The subprocess module is a better option, and I would recommend trying to avoid using anything like os.system/subprocess wherever possible."},
{"body": "I have a model that looks like this:I want select count (just the count) of items for each category, so in SQL it would be as simple as this:Is there an equivalent of doing this \"the Django way\"? Or is plain SQL the only option? I am familiar with the  method in Django, however I don't see how  would fit there.Here, as I just discovered, is how to do this with the Django 1.1 aggregation API:(: Full ORM aggregation support is now included in . True to the below warning about using private APIs, the method documented here no longer works in post-1.1 versions of Django.  I haven't dug in to figure out why; if you're on 1.1 or later you should use the real  anyway.)The core aggregation support was already there in 1.0; it's just undocumented, unsupported, and doesn't have a friendly API on top of it yet.  But here's how you can use it anyway until 1.1 arrives (at your own risk, and in full knowledge that the query.group_by attribute is not part of a public API and could change):If you then iterate over query_set, each returned value will be a dictionary with a \"category\" key and a \"count\" key.You don't have to order by -count here, that's just included to demonstrate how it's done (it has to be done in the .extra() call, not elsewhere in the queryset construction chain).  Also, you could just as well say count(id) instead of count(1), but the latter may be more efficient.Note also that when setting .query.group_by, the values must be actual DB column names ('category_id') not Django field names ('category').  This is because you're tweaking the query internals at a level where everything's in DB terms, not Django terms.Since I was a little confused about how grouping in Django 1.1 works I thought I'd elaborate here on how exactly you go about using it. First, to repeat what Michael said:Note also that you need to !This will select only the categories and then add an annotation called . Depending on the default ordering this may be all you need, . The reason for this is that the fields required for ordering are also selected and make each row unique, so you won't get stuff grouped how you want it. One quick way to fix this is to reset the ordering:This should produce exactly the results you want. To set the name of the annotation you can use:Then you will have an annotation called  in the results.Everything else about grouping was very straightforward to me. Be sure to check out the  for more detailed info.How's this?  (Other than slow.)It has the advantage of being short, even if it does fetch a lot of rows.Edit.The one query version.  BTW, this is often  than SELECT COUNT(*) in the database.  Try it to see."},
{"body": "How do you execute raw SQL in SQLAlchemy?I have a python web app that runs on flask and interfaces to the database through SQLAlchemy. I need a way to run the raw SQL. The query involves multiple table joins along with Inline views. I've tried:But I keep getting gateway errors.Have you tried:or:If you want to use a session (as your question suggests), use its  method directly:The following might be specific to my database driver (psycopg2); I'm not sure. Regardless, it's how I pull out my values.The key point about is the  call. The  part is just something that I find makes my life easier by giving name based access.Furthermore, this is transactional  managing it manually. Say  is a function that creates a session:docs: example:You can get the results of SELECT SQL queries using  and  as shown . You don't have to deal with tupules this way. As an example for a class User having the tablename 'users' you can try, executes the  but doesn't commit it unless you're on autocommit mode. So, inserts and updates wouldn't reflect in the database.To commit after the changes, doHave you tried using  and bind parameters as described ? This can help solve many parameter formatting and performance problems. Maybe the gateway error is a timeout? Bind parameters tend to make complex queries execute substantially faster."},
{"body": "I looked at other questions and can't figure it out... I did the following to install django-debug-toolbar:3 Added INTERNAL_IPS:4 Added debug_toolbar to installed appsI am not getting any errors or anything, and the toolbar doesn't show up on any page, not even admin.I even added the directory of the debug_toolbar templates to my Stupid question, but you didn't mention it, so... What is  set to? It won't load unless it's .If it's still not working, try adding '127.0.0.1' to  as well.This is a last-ditch-effort move, you shouldn't  to do this, but it will clearly show if there's merely some configuration issue or whether there's some larger issue.Add the following to settings.py:That will effectively remove all checks by debug toolbar to determine if it should or should not load itself; it will always just load. Only leave that in for testing purposes, if you forget and launch with it, all your visitors will get to see your debug toolbar too.For explicit configuration, also see the official install docs .EDIT(6/17/2015):Apparently the syntax for the nuclear option has changed. It's now in its own dictionary:Their  use this dictionary.Debug toolbar wants the ip address in request.META['REMOTE_ADDR'] to be set in the INTERNAL_IPS setting. Throw in a print statement in one of your views like such:And then load that page. Make sure that IP is in your INTERNAL_IPS setting in settings.py. Normally I'd think you would be able to determine the address easily by looking at your computer's ip address, but in my case I'm running the server in a Virtual Box with port forwarding...and who knows what happened. Despite not seeing it anywhere in ifconfig on the VB or my own OS, the IP that showed up in the REMOTE_ADDR key was what did the trick of activating the toolbar.If everything else is fine, it could also be that your template lacks an explicit closing  tag\u2014The current stable version 0.11.0 requires the following things to be true for the toolbar to be shown: If you are serving static content make sure you collect the css, js and html by doing: Newer, development versions have added defaults for settings points 2, 3 and 4 which makes life a bit simpler, however, as with any development version it has bugs. I found that the latest version from git resulted in an  error when running through nginx/uwsgi. Either way, if you want to install the latest version from github run:You can also clone a specific commit by doing: I have the toolbar working just perfect. With this configurations:I hope it helpsAdd  to your INTERNAL_IPS on Windows, it is used with vagrant internallyINTERNAL_IPS = (\n    '10.0.2.2',\n)This should work.I had the same problem and finally resolved it after some googling. In INTERNAL_IPS, you need to have the  IP address.I tried everything, from setting , to settings  to my client's IP address, and even configuring Django Debug Toolbar manually (note that recent versions make all configurations automatically, such as adding the middleware and URLs). Nothing worked in a remote development server (though it did work locally).\nThe ONLY thing that worked was configuring the toolbar as follows:This replaces the default method that decides if the toolbar should be shown, and always returns true.Another thing that can cause the toolbar to remain hidden is if it cannot find the required static files.  The debug_toolbar templates use the {{ STATIC_URL }} template tag, so make sure there is a folder in your static files called debug toolbar.The collectstatic management command should take care of this on most installations.I tried the configuration from  and it worked for me:I just modified it by adding  instead of  as mentioned in the , as I'm using Django 1.7.An addition to previous answers:if the toolbar doesn't show up, but it loads in the html (check your site html in a browser, scroll down)the issue can be that debug toolbar static files are not found (you can also see this in your site's access logs then, e.g. 404 errors for /static/debug_toolbar/js/toolbar.js)It can be fixed the following way then (examples for nginx and apache):nginx config:apache config:Or:more on collectstatic here: Or manualy move debug_toolbar folder of debug_toolbar static files to your set static files folderI had to add the following to the project url.py file to get the debug toolbar display. After that debug tool bar is displayed. and higher:Also don't forget to include the debug_toolbar to your middleware.\nThe Debug Toolbar is mostly implemented in a middleware. Enable it in your settings module as follows:\n(django newer versions)Old-style middleware:(need to have _CLASSES keywork in the Middleware)This wasn't the case for this specific author but I just have been struggling with the Debug Toolbar not showing and after doing everything they pointed out, I found out it was a problem with MIDDLEWARE order. So putting the middleware early in the list could work. Mine is first:In my case, it was another problem that hasn't been mentioned here yet: I had GZipMiddleware in my list of middlewares.As the automatic configuration of debug toolbar puts the debug toolbar's middleware at the top, it only gets the \"see\" the gzipped HTML, to which it can't add the toolbar.I removed GZipMiddleware in my development settings. Setting up the debug toolbar's configuration manually and placing the middleware  GZip's should also work.In my case I just needed to remove the python compiled files ()you have to make sure there is a closing  tag in your templates. My problem is that there is no regular html tags in my templates, I just display content in plain text. I solved  it by inheriting every html file from base.html, which has a  tag.For me this was as simple as typing  into the address bar, rather than  which apparently was not matching the INTERNAL_IPS.I got the same problem, I solved it by looking at the Apache's error log.\nI got the apache running on mac os x with mod_wsgi\nThe debug_toolbar's tamplete folder wasn't being loadLog sample:I just add this line to my VirtualHost file:I had the same problem using Vagrant. I solved this problem by adding  to the INTERNAL_IPS as below example. Remembering that  is the IP in my private network in Vagrantfile.I had this problem and had to install the debug toolbar from source.Version 1.4 has a problem where it's hidden if you use PureCSS and apparently other CSS frameworks. is the commit which fixes that.The docs explain how to install from source.For anyone who is using Pycharm 5 - template debug is not working there in some versions. Fixed in 5.0.4, affected vesions - 5.0.1, 5.0.2\nCheck out  Spend A LOT time to find that out. Maybe will help someoneOne stupid thing got me.. that if you use apache wsgi, remember to touch the .wsgi file to force your code recompile. just waste 20 minutes of my time to debug the stupid error :("},
{"body": "How can I download a webpage with a user agent other than the default one on urllib2.urlopen? from everyone's favorite .The short story: You can use  to do this.You can also pass the headers as a dictionary when creating the Request itself, :I  a  a couple weeks ago.There is example code in that question, but basically you can do something like this: (Note the capitalization of  as of , section 14.43.)Or, a bit shorter:For python 3, urllib is split into 3 modules...All these should work in theory, but (with Python 2.7.2 on Windows at least) any time you send a custom User-agent header, urllib2 doesn't send that header.  If you don't try to send a User-agent header, it sends the default Python / urllib2 None of these methods seem to work for adding User-agent but they work for other headers:Another solution in  and Python 2.7:For  you can use:Try this :"},
{"body": "Is there any Django function which will let me get an object form the database, or None if nothing matches?Right now I'm using something like:But that's not very clear, and it's messy to have everywhere.In Django  you can use the  Queryset method. It returns the first object matched by the queryset, or None if there is no matching object.Usage:There are two ways to do this;Or you can use a wrapper:Call it like thisTo add some sample code to sorki's answer (I'd add this as a comment, but this is my first post, and I don't have enough reputation to leave comments), I implemented a get_or_none custom manager like so:And now I can do this:You can also try to use django annoying (it has another useful functions!)install it with:Give Foo . It's pretty easy - just put your code into function in custom manager, set custom manager in your model and call it with .If you need generic function (to use it on any model not just that with custom manager) write your own and place it somewhere on your python path and import, not messy any more.Whether doing it via a manager or generic function, you may also want to catch 'MultipleObjectsReturned' in the TRY statement, as the get() function will raise this if your kwargs retrieve more than one object.Building on the generic function:and in the manager:Here's a variation on the helper function that allows you to optionally pass in a  instance, in case you want to get the unique object (if present) from a queryset other than the model's  objects queryset (e.g. from a subset of child items belonging to a parent instance):This can be used in two ways, e.g.:I think that in most cases you can just use:Only if it is not critical that a new entry will be added in Foo table ( other columns will have the None/default values )"},
{"body": "So it's a new millennium; Apple has waved their hand; it's now legal to include a Python interpreter in an iPhone (App Store) app.How does one go about doing this? All the existing discussion (unsurprisingly) refers to jailbreaking. (Older question: )My goal here isn't to write a PyObjC app, but to write a regular ObjC app that runs Python as an embedded library. The Python code will then call back to native Cocoa code. It's the \"control logic is Python code\" pattern.Is there a guide to getting Python built in XCode, so that my iPhone app can link it? Preferably a stripped-down Python, since I won't need 90% of the standard library.I can probably figure out the threading and Python-extension API; I've done that on MacOS. But only using command-line compilers, not XCode.It doesn't really matter how you build Python -- you don't need to build it in Xcode, for example -- but what does matter is the product of that build.Namely, you are going to need to build something like libPython.a that can be statically linked into your application.  Once you have a .a, that can be added to the Xcode project for your application(s) and, from there, it'll be linked and signed just like the rest of your app.IIRC (it has been a while since I've built python by hand) the out-of-the-box python will build a libPython.a (and a bunch of other libraries), if you configure it correctly.Of course, your second issue is going to be cross-compiling python for ARM from your  box.  Python is an autoconf based project and autoconf is a pain in the butt for cross-compilation.As you correctly state, making it small will be critical.Not surprising, either, is that you aren't the first person to want to do this, but not for iOS.   Python has been squeezed into devices much less capable than those that run iOS.  I found a thread with a bunch of links when googling about;  it .Also, you might want to join the  list.   While you aren't targeting a PyObjC based application (which, btw, is a good idea -- PyObjC has a long way to go before it'll be iOS friendly), the PyObjC community has been discussing this and Ronald, of anyone, is probably the most knowledgeable person in this particular area.  Note that PyObjC will have to solve the embedded Python on iOS problem prior to porting PyObjC.  Their prerequisite is your requirement, as it were.I've put a very rough script up on github that fetches and builds python2.6.5 for iPhone and simulator.Work in progressSomewhat depressing update nearly 2 years later: (copied from README on github)I also started such a project. It comes with its own simplified compile script so there is no need to mess around with autoconf to get your cross compiled static library. It is able to build a completely dependency-free static library of Python with some common modules. It should be easily extensible."},
{"body": "I can write something myself by finding zero-crossings of the first derivative or something, but it seems like a common-enough function to be included in standard libraries.  Anyone know of one?My particular application is a 2D array, but usually it would be used for finding peaks in FFTs, etc.Specifically, in these kinds of problems, there are multiple strong peaks, and then lots of smaller \"peaks\" that are just caused by noise that should be ignored.  These are just examples; not my actual data:1-dimensional peaks:2-dimensional peaks:The peak-finding algorithm would find the location of these peaks (not just their values), and ideally would find the true inter-sample peak, not just the index with maximum value, probably using  or something.Typically you only care about a few strong peaks, so they'd either be chosen because they're above a certain threshold, or because they're the first  peaks of an ordered list, ranked by amplitude.As I said, I know how to write something like this myself.  I'm just asking if there's a pre-existing function or package that's known to work well.I  and it works decently for the 1-D case, but could be better.sixtenbe  for the 1-D case.I do not think that what you are looking for is provided by SciPy.  I would write the code myself, in this situation.The spline interpolation and smoothing from scipy.interpolate are quite nice and might be quite helpful in fitting peaks and then finding the location of their maximum.I'm looking at a similar problem, and I've found some of the best references come from chemistry (from peaks finding in mass-spec data).  For a good thorough review of peaking finding algorithms read .  This is one of the best clearest reviews of peak finding techniques that I've run across.  (Wavelets are the best for finding peaks of this sort in noisy data.).It looks like your peaks are clearly defined and aren't hidden in the noise.  That being the case I'd recommend using smooth savtizky-golay derivatives to find the peaks (If you just differentiate the data above you'll have a mess of false positives.).  This is a very effective technique and is pretty easy to implemented (you do need a matrix class w/ basic operations).  If you simply find the zero crossing of the first S-G derivative I think you'll be happy.There is a function in scipy named  which sounds like is suitable for your needs, however I don't have experience with it so I cannot recommend..For those not sure about which peak-finding algorithms to use in Python, here a rapid overview of the alternatives: Wanting myself an equivalent to the MatLab  function, I've found that the  from Marcos Duarte is a good catch.Pretty easy to use:Which will give you:Detecting peaks in a spectrum in a reliable way has been studied quite a bit, for example all the work on sinusoidal modelling for music/audio signals in the 80ies. Look for \"Sinusoidal Modeling\" in the literature.If your signals are as clean as the example, a simple \"give me something with an amplitude higher than N neighbours\" should work reasonably well. If you have noisy signals, a simple but effective way is to look at your peaks in time, to track them: you then detect spectral lines instead of spectral peaks. IOW, you compute the FFT on a sliding window of your signal, to get a set of spectrum in time (also called spectrogram). You then look at the evolution of the spectral peak in time (i.e. in consecutive windows).There are standard statistical functions and methods for finding outliers to data, which is probably what you need in the first case. Using derivatives would solve your second. I'm not sure for a method which solves both continuous functions and sampled data, however.First things first, the definition of \"peak\" is vague if without further specifications.  For example, for the following series, would you call 5-4-5 one peak or two?1-2-1-2-1-1-5-4-5-1-1-5-1In this case, you'll need at least two thresholds: 1) a high threshold only above which can an extreme value register as a peak; and 2) a low threshold so that extreme values separated by small values below it will become two peaks.Peak detection is a well-studied topic in Extreme Value Theory literature, also known as \"declustering of extreme values\".  Its typical applications include identifying hazard events based on continuous readings of environmental variables e.g. analysing wind speed to detect storm events."},
{"body": "Copying a shuffled  list ten times takes me about 0.18 seconds: (these are five runs)Copying the unshuffled list ten times takes me about 0.05 seconds:Here's my testing code:I also tried copying with , the results were similar (i.e., big speed difference)Why the big speed difference? I know and understand the speed difference in the famous  example, but here my processing has no decisions. It's just blindly copying the references inside the list, no?I'm using Python 2.7.12 on Windows 10. Tried Python 3.5.2 as well now, the results were almost the same (shuffled consistently around 0.17 seconds, unshuffled consistently around 0.05 seconds). Here's the code for that:The interesting bit is that it depends on the order in which the integers are  created. For example instead of  create a random sequence with :This is as fast as copying your  (first and fast example).However when you shuffle - then your integers aren't in the order they were first created anymore, that's what makes it slow. A quick intermezzo:So when you copy your list you get each item of that list and put it \"as is\" in the new list. When your next item was created shortly after the current one there is a good chance (no guarantee!) that it's saved next to it on the heap. Let's assume that whenever your computer loads an item in the cache it also loads the  next-in-memory items (cache locality). Then your computer can perform the reference count increment for  items on the same cache!With the shuffled sequence it still loads the next-in-memory items but these aren't the ones next-in-list. So it can't perform the reference-count increment without \"really\" looking for the next item. The actual speed depends on what happened before the copy: in what order were these items created and in what order are these in the list.You can verify this by looking at the :Just to show a short excerpt:So these objects are really \"next to each other on the heap\". With  they aren't:Which shows these are not really next to each other in memory:I haven't thought this up myself. Most of the informations can be found in the .This answer is based on the \"official\" CPython implementation of Python. The details in other implementations (Jython, PyPy, IronPython, ...) may be different. Thanks @J\u00f6rgWMittag .When you shuffle the list items, they have worse locality of reference, leading to worse cache performance.You might think that copying the list just copies the references, not the objects, so their locations on the heap shouldn't matter.  However, copying still involves accessing each object in order to modify the refcount.As explained by others, it's not just copying the references but also increases the reference counts inside the objects and thus the objects  accessed and the cache plays a role.Here I just want to add more experiments. Not so much about shuffled vs unshuffled (where accessing one element might miss the cache but get the following elements into the cache so they get hit). But about repeating elements, where later accesses of the same element might hit the cache because the element is still in the cache.Testing a normal range:A list of the same size but with just one element repeated over and over again is faster because it hits the cache all the time:And it doesn't seem to matter what number it is:Interestingly, it gets even faster when I instead repeat the same two or four elements:I guess something doesn't like the same single counter increased all the time. Maybe some  because each increase has to wait for the result of the previous increase, but this is a wild guess.Anyway, trying this for even larger numbers of repeated elements:The output (first column is the number of different elements, for each I test three times and then take the average):So from about 2.8 seconds for a single (repeated) element it drops to about 2.2 seconds for 2, 4, 8, 16, ... different elements and stays at about 2.2 seconds until the hundred thousands. I think this uses my L2 cache (4 \u00d7 256 KB, I have an ).Then over a few steps, the times go up to 3.5 seconds. I think this uses a mix of my L2 cache and my L3 cache (8 MB) until that's \"exhausted\" as well.At the end it stays at around 3.5 seconds, I guess because my caches don't help with the repeated elements anymore."},
{"body": "Django has some good automatic serialization of ORM models returned from DB to JSON format.How to serialize SQLAlchemy query result to JSON format? I tried  but it encodes query object itself.\nI tried  but it returnsIs it really so hard to serialize SQLAlchemy ORM objects to JSON /XML? Isn't there any default serializer for it? It's very common task to serialize ORM query results nowadays.What I need is just to return JSON or XML data representation of SQLAlchemy query result.SQLAlchemy objects query result in JSON/XML format is needed to be used in javascript datagird (JQGrid )You could use something like this:and then convert to JSON using:It will ignore fields that are not encodable (set them to 'None').It doesn't auto-expand relations (since this could lead to self-references, and loop forever).If, however, you'd rather loop forever, you could use:And then encode objects using:This would encode all children, and all their children, and all their children... Potentially encode your entire database, basically. When it reaches something its encoded before, it will encode it as 'None'.Another alternative, probably better, is to be able to specify the fields you want to expand:You can now call it with:To only expand SQLAlchemy fields called 'parents', for example.You could just output your object as a dict:And then you use User.as_dict() to serialize your object.As explained in You can convert a RowProxy to a dict like this:Then serialize that to JSON ( you will have to specify an encoder for things like  values )\nIt's not that hard if you just want one record ( and not a full hierarchy of related records ).I recommend using a recent surfaced library . It allows you to create serializers to represent your model instances with support to relations and nested objects.Have a look at theier . package has an implementation of  Base class for your models.Usage:Now the  model is magically serializable.If your framework is not Flask, you can just For security reasons you should never return all the model's fields. I prefer to selectively choose them.Flask's json encoding now supports UUID, datetime and relationships (and added  and  for flask_sqlalchemy  class). I've updated the encoder as follows:With this I can optionally add a  property that returns the list of fields I wish to encode:I add @jsonapi to my view, return the resultlist and then my output is as follows:It is not so straighforward. I wrote some code to do this. I'm still working on it, and it uses the MochiKit framework. It basically translates compound objects between Python and Javascript using a proxy and registered JSON converters. Browser side for database objects is \nIt needs the basic Python proxy source in .On the Python side there is the base .\nThen finally the SqlAlchemy object encoder in .\nIt also depends on metadata extractors found in the  file.Custom serialization and deserialization. (class method) builds a Model object based on json data. could be called only on instance, and merge all data from json into Model instance. - recursive serialization property is needed to define write only properties (\"password_hash\" for example).Use the  in SQLAlchemy:If you're transferring the object between sessions, remember to detach the object from the current session using .\nTo attach it again, just do .Here is a solution that lets you select the relations you want to include in your output as deep as you would like to go.\nNOTE: This is a complete re-write taking a dict/str as an arg rather than a list. fixes some stuff..so for an example using person/family/homes/rooms... turning it into json all you need isI thought I'd play a little code golf with this one. FYI: I am using  since we have a separately designed schema according to business requirements. I just started using SQLAlchemy today but the documentation states that automap_base is an extension to declarative_base which seems to be the typical paradigm in the SQLAlchemy ORM so I believe this should work.It does not get fancy with following foreign keys per 's solution, but it simply matches columns to values and handles Python types by str()-ing the column values. Our values consist Python datetime.time and decimal.Decimal class type results so it gets the job done.Hope this helps any passers-by!I know this is quite an older post. I took solution given by @SashaB and modified as per my need. I added following things to it:My code is as follows:Hope it helps someone!My take utilizing (too many?) dictionaries:Running with flask (including jsonify) and flask_sqlalchemy to print outputs as JSON.Call the function with jsonify(serialize()).Works with all SQLAlchemy queries I've tried so far (running SQLite3)"},
{"body": "What is a good way to find the index of an element in an array in python? \nNote that the array may not be sorted.\nIs there a way to specify what comparison operator to use?The best way is probably to use the list method .index. For the objects in the list, you can do something like:with any special processing you need.You can also use a for/in statement with enumerate(arr)Example of finding the index of an item that has value > 100.From :If you just want to find out if an element is contained in the list or not:Here is another way using list comprehension (some people might find it debatable). It is very approachable for simple tests, e.g. comparisons on object attributes (which I need a lot):Of course this assumes the existence (and, actually, uniqueness) of a suitable element in the list.assuming you want to find a value in a numpy array,\nI guess something like this might work:There is the  method, , but I don't think you can specify a custom comparison operator. It wouldn't be hard to write your own function to do so, though:The index method of a list will do this for you. If you want to guarantee order, sort the list first using . Sorted accepts a cmp or key parameter to dictate how the sorting will happen:Or:I use function for returning index for the matching element (Python 2.6):Then use it via lambda function for retrieving needed element by any required equation e.g. by using element name.If i need to use it in several places in my code i just define specific find function e.g. for finding element by name:And then it is quite easy and readable:I found this by adapting some tutos. Thanks to google, and to all of you ;)A very simple use:P.S. scuse my englishhow's this one?Usage:"},
{"body": "If I want to use the results of , which is a  object, with a method that expects a dictionary or mapping-like object (see ), what is the right way to do it?I would think the answer is no:  smells like a convention for implementation, but not for an interface, the way  or  or  seem to be.You can access the namespace's dictionary with :You can modify the dictionary directly if you wish:Yes, it is okay to access the __dict__ attribute.  It is a well-defined, tested, and guaranteed behavior.:If you prefer to have dict-like view of the attributes, you can use the standard Python idiom, :In general, I would say \"no\". However  has struck me as over-engineered, possibly from when classes couldn't inherit from built-in types. On the other hand,  does present a task-oriented approach to argparse, and I can't think of a situation that would call for grabbing the , but the limits of my imagination are not the same as yours."},
{"body": "Is it possible to get some information out of the .pyc file that is generated from a .py file? worked well for me with Python 2.7 to decompile the .pyc bytecode into .py, whereas unpyclib crashed with an exception.For Python 3 and Python 2.7, you can try the more recent .See  for some other comments.Yes, you can get it with  that can be found on .Than you can decompile your .pyc fileYou may try . It's based on Decompyle++ and Uncompyle2.\nIt's supports decompiling python versions 1.0-3.3Note: I am the author of the above tool.Decompyle++ (pycdc) was the only one that worked for me: was suggested in Yes, it is possible.There is a perfect open-source Python (.PYC) decompiler, called Decompyle++  Decompyle++ aims to translate compiled Python byte-code back into valid and human-readable Python source code. \nWhile other projects have achieved this with varied success, Decompyle++ is unique in that it seeks to support byte-code from any version of Python. Online version is available here: \nI have no idea if it's any good, but a quick google search turned up ."},
{"body": "Python sorts by byte value by default, which means \u00e9 comes after z and other equally funny things. What is the best way to sort alphabetically in Python?Is there a library for this? I couldn't find anything. Preferrably sorting should have language support so it understands that \u00e5\u00e4\u00f6 should be sorted after z in Swedish, but that \u00fc should be sorted by u, etc. Unicode support is thereby pretty much a requirement.If there is no library for it, what is the best way to do this? Just make a mapping from letter to a integer value and map the string to a integer list with that?IBM's  library does that (and a lot more). It has Python bindings: . : The core difference in sorting between ICU and  is that ICU uses the full  while  uses .The differences between those two algorithms are briefly summarized here: . These are rather exotic special cases, which should rarely matter in practice.I don't see this in the answers. My Application sorts according to the locale using python's standard library. It is pretty easy.Question to Lennart and other answerers: Doesn't anyone know 'locale' or is it not up to this task?Try James Tauber's . It may not do exactly as you want, but seems well worth a look. For a bit more information about the issues, see  by Christopher Lenz.A summary and extended answer: under Python 2, and  will in fact solve the problem, and does a good job, assuming that you have the locale in question installed. I tested it under Windows too, where the locale names confusingly are different, but on the other hand it seems to have all locales that are supported installed by default. doesn't necessarily do this better in practice, it however does way . Most notably it has support for splitters that can split texts in different languages into words. This is very useful for languages that doesn't have word separators. You'll need to have a corpus of words to use as a base for the splitting, because that's not included, though.It also has long names for the locales so you can get pretty display names for the locale, support for other calendars than Gregorian (although I'm not sure the Python interface supports that) and tons and tons of other more or less obscure locale supports. If you want to sort alphabetically and locale-dependent, you can use the  module, unless you have special requirements, or also need more locale dependent functionality, like words splitter.You might also be interested in :Though it is certainly not the most exact way, it is a very simple way to at least get it somewhat right. It also beats locale in a webapp as locale is not threadsafe and sets the language settings process-wide. It also easier to set up than PyICU which relies on an external C library. I uploaded the script to github as the original was down at the time of this writing and I had to resort to web caches to get it:I successfully used this script to sanely sort German/French/Italian text in a plone module.I see the answers have already done an excellent job, just wanted to point out one coding inefficiency in . To apply a selective char-by-char translation to a unicode string s, it uses the code:Python has a much better, faster and more concise way to perform this auxiliary task (on Unicode strings -- the analogous method for byte strings has a different and somewhat less helpful specification!-):The dict you pass to the  method has Unicode ordinals (not strings) as keys, which is why we need that rebuilding step from the original char-to-char . (Values in the dict you pass to translate [as opposed to keys, which must be ordinals] can be Unicode ordinals, arbitrary Unicode strings, or None to remove the corresponding character as part of the translation, so it's easy to specify \"ignore a certain character for sorting purposes\", \"map \u00e4 to ae for sorting purposes\", and the like).In Python 3, you can get the \"rebuilding\" step more simply, e.g.:See  for other ways you can use this  static method in Python 3.The simplest, easiest, and most straightforward way to do this it to make a callout to the Perl library module, , which is  a subclass of the standard  module. All you need do is pass the constructor a locale value of  for Sweden.  (You may not neccesarily appreciate this for Swedish text, but because Perl uses abstract characters, you can use any Unicode code point you please \u2014 no matter the platform or build! Few languages offer such convenience. I mention it because I\u2019ve fighting a losing battle with Java a lot over this maddening problem lately.)The problem is that I do not know how to access a Perl module from Python \u2014 apart, that is, from using a shell callout or two-sided pipe. To that end,  that you can call to do exactly what you have asked for with perfect ease. , with all tailoring options supported!!  And if you have an optional module installed or run Perl 5.13 or better, then you have full access to easy-to-use CLDR locales.  See below.Imagine an input set ordered this way:A default sort by code point yields:which is incorrect by everybody\u2019s book.  Using my script, which uses the Unicode Collation Algorithm, you get this order:That is the default UCA sort.  To get the Swedish locale, call  this way:Here is a better input demo. First, the input set:By code point, that sorts this way:But using the default UCA makes it sort this way:But in the Swedish locale, this way:If you prefer uppercase to sort before lowercase, do this:You can do many other things with . For example, here is how to sort titles in English:You will need Perl 5.10.1 or better to run the script in general. For locale support, you must either install the optional CPAN module . Alternately, you can install a development versions of Perl, 5.13+, which include that module standardly.This is a rapid prototype, so  is mostly un(der)documented. But this is its SYNOPSIS of what switches/options it accepts on the command line:Yeah, ok: that\u2019s really the argument list I use for the call to , but you get the idea. :) If you can figure out how to call Perl library modules from Python directly without calling a Perl script, by all means do so.  I just don\u2019t know how myself.  I\u2019d love to learn how.In the meantime, I believe this script will do what you need done in all its particular \u2014   I now use this for all of text sorting. It  does what I\u2019ve needed for a long, long time. The only downside is that  argument causes performance to go down the tubes, although it\u2019s plenty fast enough for regular, non-locale  sorting.  Since it loads everything in memory, you probably don\u2019t want to use this on gigabyte documents. I use it many times a day, and it sure it great having sane text sorting at last.To implement it you will need to read about \"Unicode collation algorithm\"\nsee\na sample implementation is hereJeff Atwood wrote a good post on , in it he linked to a script which does .It's not a trivial script, by any means, but it does the trick.Lately I've been using zope.ucol () for this task. For example, sorting the german \u00df:zope.ucol also wraps ICU, so would be an alternative to PyICU.It is far from a complete solution for your use case, but you could take a look at the  script from effbot.org. What it basically does is remove all accents from a text. You can use that 'sanitized' text to sort alphabetically. (For a better description see  page.)"},
{"body": "I am trying to find out the version of Visual Studio that is used to compile the Python on my computerIt says What I do not understand is this  designation. Does it mean it is compiled with Visual Studio 2005? I cannot find this information on . appears to be Visual C++ 2008 according to  (of all places).  The  indicates 1500 to be the result of the  macro. mentions that The above MSDN link said that 1600 indicates VS2010.Strangely, I wasn't able to find that info about the earlier  values on MSDN."},
{"body": "I can't get my head around Python's  module. My needs are very simple: I just want to log everything to syslog. After reading documentation I came up with this simple test script:But this script does not produce any log records in syslog. What's wrong?Change the line to this:This works for meYou should  use the local host for logging, whether to /dev/log or localhost  through the TCP stack. This allows the fully RFC compliant and featureful system logging daemon to handle syslog. This eliminates the need for the remote daemon to be functional and provides the enhanced capabilities of syslog daemon's such as rsyslog and syslog-ng for instance. The same philosophy goes for SMTP. Just hand it to the local SMTP software. In this case use 'program mode' not the daemon, but it's the same idea. Let the more capable software handle it. Retrying, queuing, local spooling, using TCP instead of UDP for syslog and so forth become possible. You can also [re-]configure those daemons separately from your code as it should be.Save your coding for your application, let other software do it's job in concert.I add a little extra comment just in case it helps anyone because I found this exchange useful but needed this little extra bit of info to get it all working.To log to a specific facility using SysLogHandler you need to specify the facility value.\nSay for example that you have defined:in syslog, then you'll want to use:and you also need to have syslog listening on UDP to use localhost instead of /dev/log.Piecing things together from here and other places, this is what I came up with that works on unbuntu 12.04 and centOS6Create an file in  that ends in .conf and add the following textRestart , reloading did NOT seem to work for the new log files. Maybe it only reloads existing conf files?Then you can use this test program to make sure it actually works.Is your syslog.conf set up to handle facility=user?You can set the facility used by the python logger with the facility argument, something like this:I found  to make it quite easy to get the basic logging behavior you describe:There are other things you could do, too, but even just the first two lines of that will get you what you've asked for as I understand it.From the above script will log to LOCAL0 facility with our custom \"LOG_IDENTIFIER\"...\nyou can use LOCAL[0-7] for local purpose.You can also add a file handler or rotating file handler to send your logs to a local file:\n"},
{"body": "I recently installed Python 3.1 and the Pygame module for Python 3.1  When I type import python in the console I get the following error:Please help!It could be due to the architecture of your OS. Is your OS 64 Bit and have you installed 64 bit version of Python? It may help to install both 32 bit version  and , which is available officially only in 32 bit and you won't face this problem.I see that 64 bit pygame is maintained , you might also want to try uninstalling Pygame only and install the 64 bit version on your existing python3.1, if not choose go for both 32-bit version.Looks like the question has been long ago answered but the solution did not work for me.  When I was getting that error, I was able to fix the problem by downloading I had installed Python 32 bit version and psycopg2 64 bit version to get this problem. I installed psycopg2 32 bit version and then it worked.Had this issue on Python 2.7.9, solved by updating to Python 2.7.10 (unreleased when this question was asked and answered).Another possible cause of similar issue could be wrong  in the cx_freeze manifest, trying to load x86 common controls dll in x64 process - should be fixed by this patch:"},
{"body": "I am new to Python's logging package and plan to use it for my project. I would like to customize the time format to my taste. Here is a short code I copied from a tutorial:And here is the output:I would like to shorten the time format to just: '', dropping the mili-second suffix. I looked at the Formatter.formatTime, but confused. I appreciate your help to achieve my goal. Thank you.From the  regarding the Formatter class:So changetoUsing logging.basicConfig, the following example works for me:This allows you to format & config all in one line.  A resulting log record looks as follows:2014-05-26 12:22:52.376 CRITICAL historylistener - main: History log failed to startif using logging.config.fileConfig with a configuration file use something like:To add to the other answers, here are the  from Python Documentation."},
{"body": "How do I get the index column name in python pandas?  Here's an example dataframe:What I'm trying to do is get/set the dataframe index title.  Here is what i tried:Anyone know how to do this? You can just get/set the index via its  property should do the trick.Python has a  function that let's you query object attributes.  was helpful here.From version  you can use :The new functionality works well in method chains.You can also rename column names with parameter :If you do not want to create a new row but simply put it in the empty cell then use:Otherwise use:  also give us the column namesUse  to set the index name. Seems this api is available since ."},
{"body": "I'm looking for an open source implementation, preferably in python, of  (). Is anyone familiar with such open source implementation I can use?I'm writing an application that searches twitter for some search term, say \"youtube\", and counts \"happy\" tweets vs. \"sad\" tweets. \nI'm using Google's appengine, so it's in python. I'd like to be able to classify the returned search results from twitter and I'd like to do that in python.\nI haven't been able to find such sentiment analyzer so far, specifically not in python. \nAre you familiar with such open source implementation I can use? Preferably this is already in python, but if not, hopefully I can translate it to python.Note, the texts I'm analyzing are VERY short, they are tweets. So ideally, this classifier is optimized for such short texts.BTW, twitter does support the \":)\" and \":(\" operators in search, which aim to do just this, but unfortunately, the classification provided by them isn't that great, so I figured I might give this a try myself.Thanks!BTW, an early demo is  and the code I have so far is  and I'd love to opensource it with any interested developer.With most of these kinds of applications, you'll have to roll much of your own code for a statistical classification task. As Lucka suggested, NLTK is the perfect tool for natural language manipulation in Python, so long as your goal doesn't interfere with the non commercial nature of its license.  However, I would suggest other software packages for modeling.  I haven't found many strong advanced machine learning models available for Python, so I'm going to suggest some standalone binaries that easily cooperate with it.You may be interested in , which can be easily interfaced with Python.  This has been used for classification tasks in various areas of natural language processing.  You also have a pick of a number of different models.  I'd suggest starting with Maximum Entropy classification so long as you're already familiar with implementing a Naive Bayes classifier.  If not, you may want to look into it and code one up to really get a decent understanding of statistical classification as a machine learning task.The University of Texas at Austin computational linguistics groups have held classes where most of the projects coming out of them have used this great tool.  You can look at the course page for  to get an idea of how to make it work and what previous applications it has served.Another great tool which works in the same vein is .  The difference between Mallet is that there's a bit more documentation and some more models available, such as decision trees, and it's in Java, which, in my opinion, makes it a little slower.   is a whole suite of different machine learning models in one big package that includes some graphical stuff, but it's really mostly meant for pedagogical purposes, and isn't really something I'd put into production.Good luck with your task.  The real difficult part will probably be the amount of knowledge engineering required up front for you to classify the 'seed set' off of which your model will learn.  It needs to be pretty sizeable, depending on whether you're doing binary classification (happy vs sad) or a whole range of emotions (which will require even more).  Make sure to hold out some of this engineered data for testing, or run some tenfold or remove-one tests to make sure you're actually doing a good job predicting before you put it out there. And most of all, have fun!  This is the best part of NLP and AI, in my opinion.Good luck with that.Sentiment is enormously contextual, and tweeting culture makes the problem worse because  for most tweets.  The whole point of twitter is that you can leverage the huge amount of shared \"real world\" context to pack meaningful communication in a very short message.If they say the video is bad, does that mean bad, or ?Thanks everyone for your suggestions, they were indeed very useful!\nI ended up using a Naive Bayesian classifier, which I borrowed from . \nI started by feeding it with a list of good/bad keywords and then added a \"learn\" feature by employing user feedback. It turned out to work pretty nice.The full details of my work as in a .Again, your help was very useful, so thank you!I have constructed a word list labeled with sentiment. You can access it from here:You will find a short Python program on my blog: This post displays how to use the word list with single sentences as well as with Twitter.Word lists approaches have their limitations. You will find a investigation of the limitations of my word list in the article \"A new ANEW: Evaluation of a word list for sentiment analysis in microblogs\". That article is available from my homepage.Please note a  is missing from the code (for paedagogic reasons).A lot of research papers indicate that a good starting point for sentiment analysis is looking at adjectives, e.g., are they positive adjectives or negative adjectives. For a short block of text this is pretty much your only option... There are papers that look at entire documents, or sentence level analysis, but as you say tweets are quite short... There is no real magic approach to understanding the sentiment of a sentence, so I think your best bet would be hunting down one of these research papers and trying to get their data-set of positively/negatively oriented adjectives.Now, this having been said, sentiment is domain specific, and you might find it difficult to get a high-level of accuracy with a general purpose data-set.Good luck.I think you may find it difficult to find what you're after. The closest thing that I know of is , which has some  and is available under a limited kind of open-source licence, but is written in Java.Also, sentiment analysis systems are usually developed by training a system on product/movie review data which is significantly different from the average tweet. They are going to be optimised for text with several sentences, all about the same topic. I suspect you would do better coming up with a rule-based system yourself, perhaps based on a lexicon of sentiment terms like .Check out  for an implementation of similar idea with a really beautiful interface (and ).Take a look at . It's written in python, and it uses Naive Bayes classifier with semi-supervised machine learning. The source can be found .Maybe  (based on NLTK and pattern) is the right sentiment analysis tool for you.I came across  a while ago. You could probably use it as a starting point. It also has a lot of modules and addons, so maybe they already have something similar.Somewhat wacky thought: you could try using the Twitter API to download a large set of tweets, and then classifying a subset of that set using emoticons: one positive group for  \":)\", \":]\", \":D\", etc, and another negative group with \":(\", etc.Once you have that crude classification, you could search for more clues with frequency or ngram analysis or something along those lines.It may seem silly, but serious research has been done on this (search for \"sentiment analysis\" and emoticon). Worth a look.  There's a Twitter Sentiment API by TweetFeel that does advanced linguistic analysis of tweets, and can retrieve positive/negative tweets. See For those interested in coding Twitter Sentiment Analyis from scratch, there is a Coursera course \"\" with python code on GitHub (as part of assignment 1 - ). The sentiments are part of the .You can find working solutions, for example . In addition to the AFINN-111 sentiment list, there is a simple implementation of builing a dynamic term list based on frequency of terms in tweets that have a pos/neg score (see )."},
{"body": "In python, how do I check if an object is a generator object?Trying this - gives the error -(I know I can check if the object has a  method for it to be a generator, but I want some way using which I can determine the type of any object, not just generators.)You can use GeneratorType from types:You mean generator functions ? use .if you want a generator object you can use  as pointed out by JAB in his comment.The  function is fine if you want to check for pure generators (i.e. objects of class \"generator\"). However it will return  if you check, for example, a  iterable. An alternative way for checking for a generalised generator is to use this function:I think it is important to make distinction between  and  (generator function's result):calling generator_function won't yield normal result, it even won't execute any code in the function itself, the result will be special object called :so it is not generator function, but generator:and generator function is not generator:just for a reference, actual call of function body will happen by consuming generator, e.g.:See also Don't do this.  It's simply a very, very bad idea.Instead, do this:In the unlikely event that the body of the  loop also has s, there are several choices: (1) define a function to limit the scope of the errors, or (2) use a nested  block.Or (3) something like this to distinguish all of these s which are floating around.Or (4) fix the other parts of your application to provide generators appropriately.  That's often simpler than all of this.If you are using tornado webserver or similar you might have found that server methods are actually generators and not methods. This makes it difficult to call other methods because yield is not working inside the method and therefore you need to start managing pools of chained generator objects. A simple method to manage pools of chained generators is to create a help function such as Now writing chained generators such asProduces outputWhich is probably what you want if your looking to use generators as a thread alternative or similar."},
{"body": "I notice that it is often suggested to use queues with multiple treads, instead of lists and .pop(). Is this because lists are not thread-safe, or for some other reason?Lists themselves are thread-safe. In CPython the GIL protects against concurrent accesses to them, and other implementations take care to use a fine-grained lock or a synchronized datatype for their list implementations. However, while lists  can't go corrupt by attempts to concurrently access, the lists's  is not protected. For example:is not guaranteed to actually increase L[0] by one if another thread does the same thing, because  is not an atomic operation. (Very, very few operations in Python are actually atomic, because most of them can cause arbitrary Python code to be called.) You should use Queues because if you just use an unprotected list, you may get or delete  because of race conditions.To clarify a point in Thomas' excellent answer, it should be mentioned that   thread safe.This is because there is no concern that data being  will be in the same place once we go to  to it. The  operation does not read data, it only writes data to the list. of  operations and whether or not they are thread safe.\nHoping to get an answer regarding the  language construct ."},
{"body": "I'm working with Python v2, and I'm trying to find out if you can tell if a word is in a string.I have found some information about identifying if the word is in the string - using .find, but is there a way to do an IF statement. I would like to have something like the following:Thanks for any help.What is wrong with:but keep in mind that this matches a sequence of characters, not necessarily a whole word - for example,  is True. If you only want to match whole words, you ought to use regular expressions:find returns an integer representing the index of where the search item was found.  If it isn't found, it returns -1.If you want to find out whether a whole word is in a space-separated list of words, simply use:This elegant method is also the fastest. Compared to Hugh Bothwell's and daSong's approaches:This small function compares all search words in given text. If all search words are found in text, returns length of search, or  otherwise.Also supports unicode string search.usage:If matching a sequence of characters is not sufficient and you need to match whole words, here is a simple function that gets the job done. It basically appends spaces where necessary and searches for that in the string:This assumes that commas and other punctuations have already been stripped out.You can split string to the words and check the result list.You could just add a space before and after \"word\".This way it looks for the space before and after \"word\".Advanced way to check the exact word, that we need to find in a long string:"},
{"body": "I am running a  Python app on a Heroku Cedar dyno. The app returns  to its clients (it's an , really).Once in a while clients get 0-byte responses. It's not me returning them, however. Here is a snippet of my app's log:The first line above is me starting to handle the request. The second line is me returning a value (to Flask -- it's a Flask \"Response\" object).The third line is Gnicorn's, where you can see the Gunicorn got the 200 status and 22 bytes HTTP body (\"\").However, the client got 0 bytes. Here is the Heroku router log:Why does Gunicorn return 22 bytes, but Heroku sees 0, and indeed passes back 0 bytes to the client? Is this a Heroku bug?I know I may be considered a little off the wall here but there is another option.We know that from time to time there is a bug that happens on transit.We know that there is not much we can do right now to stop the problem. If you are only providing the API then stop reading however if you write the client too, keep going.The error is a known case, and known cause. The result of an empty return value means that something went wrong. However the value is available and was fetched, calculated, whatever... My instinct as a developer would be to treat an empty result as an HTTP error and request the data be resent. You could then track the resend requests and see how often this happens.I would suggest (although you strike me as the kind of developer to think of this too) that you count the requests and set a sane value for responding \"network error\" to the user. My instinct would be to retry right away and then to wait a little while before retrying some more.From what you describe the first retry would probably pick up the data properly. Of course this could mean keeping older requests hanging about in cache for a few minutes or running the request a second time depending on what seemed most appropriate.This would also route around any number of other point-to-point networking errors and leave the app far more robust even in the face of connectivity problems.I know our instinct as developers is to fix the known fault but sometimes it is better to work towards a system that is able to operate despite faults. That said it never hurts to log errors and problems and try to fix them anyway."},
{"body": "I am looking for best practices for function/class/module documentation, i.e. comments in the code itself. Ideally I would like a comment template which is both human readable and consumable by Python documentation utilities.I have read the .I understand this part:This sentence needs a bit more explanation:Specifically, I am looking for examples of well-commented functions and classes.You should  and check out the . All the cool kids are doing it.You should . i.e.You should avoid repeating yourself unnecessarily or explaining the eminently obvious. Example of what not to do:If you wanted to describe something non-obvious it would be a different story; for example, that verbose causes messages to occur on  or a  stream. This is not specific to Python, but follows from more hand-wavy ideals such as  and .Try to avoid mentioning specific types if possible (abstract or interface-like types are generally okay). Mentioning  is typically more helpful from a duck typing perspective (i.e. \"iterable\" instead of , or \"mutable ordered sequence\" instead of ). I've seen some code that is very literal and heavy WRT the  and the  function-level documentation, which I've found to be at odds with the duck typing mentality.As Emji said, Django is a good project to follow for clear, consistent style guides.For example, their  even goes as far as describing how they'd like to see documentation.  Specifically they mention:I think the best resource will be Quote:: The official Python documentation generatorThe best way to learn documentation practices is probably to look at the source code of a well known project. Like the ."},
{"body": "I'd like to make a Python package containing some  code. I've got the the Cython code working nicely. However, now I want to know how best to package it.For most people who just want to install the package, I'd like to include the  file that Cython creates, and arrange for  to compile that to produce the module. Then the user doesn't need Cython installed in order to install the package.But for people who may want to modify the package, I'd also like to provide the Cython  files, and somehow also allow for  to build them using Cython (so those users  need Cython installed).How should I structure the files in the package to cater for both these scenarios?The . But it doesn't say how to make a single  that handles both the with/without Cython cases.I've done this myself now, in a Python package  ( - EDIT: now ) (I don't expect this to be a popular package, but it was a good chance to learn Cython).This method relies on the fact that building a  file with  (at least with Cython version 0.14) always seems to create a  file in the same directory as the source  file.Here is a cut-down version of  which I hope shows the essentials:I also edited  to ensure that  is included in a source distribution (a source distribution that is created with ):I don't commit  to version control 'trunk' (or 'default' for Mercurial). When I make a release, I need to remember to do a  first, to ensure that  is present and up-to-date for the source code distribution. I also make a release branch, and commit the C file into the branch. That way I have a historical record of the C file that was distributed with that release.Adding to Craig McQueen's answer: see below for how to override the  command to have Cython automatically compile your source files before creating a source distribution.That way your run no risk of accidentally distributing outdated  sources. It also helps in the case where you have limited control over the distribution process e.g. when automatically creating distributions from continuous integration etc.The easiest is to include both but just use the c-file? Including the .pyx file is nice, but it's not needed once you have the .c file anyway. People who want to recompile the .pyx can install Pyrex and do it manually.Otherwise you need to have a custom build_ext command for distutils that builds the C file first. Cython already includes one. What that documentation doesn't do is say how to make this conditional, but Should handle it.Including (Cython) generated .c files are pretty weird. Especially when we include that in git. I'd prefer to use . When Cython is not available, it will build an egg which has built-in Cython environment, and then build your code using the egg.A possible example: Update(2017-01-05):Since , there's no need to use .  is an example to build Cython project from scratch without .The simple hack I came up with:Just install Cython if it could not be imported. One should probably not share this code, but for my own dependencies it's good enough.This is a setup script I wrote which makes it easier to include nested directories inside the build. One needs to run it from folder within a package. Givig structure like this:setup.pyHappy compiling ;) "},
{"body": "Suppose I have written a decorator that does something very generic. For example, it might convert all arguments to a specific type, perform logging, implement memoization, etc.Here is an example:Everything well so far. There is one problem, however. The decorated function does not retain the documentation of the original function:Fortunately, there is a workaround:This time, the function name and documentation are correct:But there is still a problem: the function signature is wrong. The information \"*args, **kwargs\" is next to useless.What to do? I can think of two simple but flawed workarounds:1 -- Include the correct signature in the docstring:This is bad because of the duplication. The signature will still not be shown properly in automatically generated documentation. It's easy to update the function and forget about changing the docstring, or to make a typo. []2 -- Not use a decorator, or use a special-purpose decorator for every specific signature:This works fine for a set of functions that have identical signature, but it's useless in general. As I said in the beginning, I want to be able to use decorators entirely generically.I'm looking for a solution that is fully general, and automatic.So the question is: is there a way to edit the decorated function signature after it has been created?Otherwise, can I write a decorator that extracts the function signature and uses that information instead of \"*kwargs, **kwargs\" when constructing the decorated function? How do I extract that information? How should I construct the decorated function -- with exec?Any other approaches? preserves signatures since Python 3.4: is available  but it does not preserve the signature there:Notice:  instead of .There is a  with  decorator you can use:Then the signature and help of the method is preserved:EDIT: J. F. Sebastian pointed out that I didn't modify  function -- it is fixed now.Take a look at the  module - specifically the  decorator, which solves this problem.This is solved with Python's standard library  and specifically  function, which is designed to \"\". It's behaviour depends on Python version, however, as shown below. Applied to the example from the question, the code would look like:When executed in Python 3, this would produce the following:Its only drawback is that in Python 2 however, it doesn't update function's argument list. When executed in Python 2, it will produce:Second option:$ easy_install wraptwrapt have a bonus, preserve class signature."},
{"body": "I am trying to write a wrapper script for a command line program (svnadmin verify) that will display a nice progress indicator for the operation.  This requires me to be able to see each line of output from the wrapped program as soon as it is output.I figured that I'd just execute the program using , use , then read each line as it came in and act on it accordingly.  However, when I ran the following code, the output appeared to be buffered somewhere, causing it to appear in two chunks, lines 1 through 332, then 333 through 439 (the last line of output)After looking at the documentation on subprocess a little, I discovered the  parameter to , so I tried setting bufsize to 1 (buffer each line) and 0 (no buffer), but neither value seemed to change the way the lines were being delivered.At this point I was starting to grasp for straws, so I wrote the following output loop:but got the same result.Is it possible to get 'realtime' program output of a program executed using subprocess?  Is there some other option in Python that is forward-compatible (not )?I tried this, and for some reason while the codebuffers aggressively, the variantdoes not. Apparently this is a known bug: You can try this:If you use readline instead of read, there will be some cases where the input message is not printed. Try it with a command the requires an inline input and see for yourself.I ran into the same problem awhile back.  My solution was to ditch iterating for the  method, which will return immediately even if your subprocess isn't finished executing, etc.Using pexpect [  ] with non-blocking readlines will resolve this problem. It stems from the fact that pipes are buffered, and so your app's output is getting buffered by the pipe, therefore you can't get to that output until the buffer fills or the process dies.I used this solution to get realtime output on a subprocess. This loop will stop as soon as the process completes leaving out a need for a break statement or possible infinite loop. Complete solution:Real Time Output Issue resolved:\nI did encountered similar issue in Python, while capturing the real time output from c program.  I added \";\" in my C code. It worked for me. Here is the snip the code << C Program >><< Python Program >><< OUTPUT>>\nPrint: Count  1\nPrint: Count  2\nPrint: Count  3Hope it helps.~sairamFound this \"plug-and-play\" function . Worked like a charm!This is what I did:It continuously prints new lines as they are output by the subprocess. "},
{"body": "Python seems to have functions for copying files (e.g. ) and functions for copying directories (e.g. ) but I haven't found any function that handles both. Sure, it's trivial to check whether you want to copy a file or a directory, but it seems like a strange omission.Is there really no standard function that works like the unix  command, i.e. supports both directories and files and copies recursively? What would be the most elegant way to work around this problem in Python?I suggest you first call , and if an exception is thrown, then retry with .Unix  doesn't 'support both directories and files':To make cp copy a directory, you have to manually tell cp that it's a directory, by using the '-r' flag.There is some disconnect here though -  when passed a filename as the source will happily copy just the single file; copytree won't.To add on  and  answers, here's an alternative way of copying files and folders recursively. (Python 3.X)Should it be your first time and you have no idea how to copy files and folders recursively, I hope this helps.The python shutil.copytree method its a mess. I've done one that works correctly: and  are copying files. copies a folder with all the files and all subfolders.  is using  to copy the files.So the analog to  you are saying is the  because  targets and copies a folder and its files/subfolders like . Without the   copies files like  and  do."},
{"body": "I'm having a few issues trying to encode a string to UTF-8. I've tried numerous things, including using  and , but I get the error:This is my string:I don't see what's going wrong, any idea?Edit: The problem is that printing the string as it is does not show properly. Also, this error when I try to convert it:This is to do with the encoding of your terminal not being set to UTF-8.  Here is my terminalOn my terminal the example works with the above, but if I get rid of the  setting then it won't workConsult the docs for your linux variant to discover how to make this change permanent.try:edit: gives , which is correct.so your problem must be at some oter place, possibly if you try to do something with it were there is an implicit conversion going on (could be printing, writing to a stream...)to say more we'll need to see some code.My +1 to mata's comment at  and to the Nick Craig-Wood's demonstration.  You have decoded the string correctly.  The problem is with the  command as it converts the Unicode string to the console encoding, and the console is not capable to display the string.  Try to write the string into a file and look at the result using some decent editor that supports Unicode:Then you will see .If you are working on a  host, look at  on your  PC.When this file contains a line:comment it out with adding  at the head of line. It might help.With this line,  sends language related environment variables of your PC to the  host. It causes  problems.No problems with my terminal. The above answers helped me looking in the right directions but it didn't work for me until I added :As indicated in the comment below, this may lead to undesired results. OTOH it also may just do the trick well enough to get things working and you don't care about losing some characters.It looks like your string is encoded to , so what exactly is the problem?  Or what are you trying to do here..?this works for ubuntu 15.10:I was getting the same type of error, and I found that the console is not capable of displaying the string in another language. Hence I made the below code changes to set default_charset as UTF-8. This is the best answer:\nin linux:so  is OK.i solve that problem changing in the file settings.py with 'ENGINE': 'django.db.backends.mysql',   don\u00b4t use 'ENGINE': 'mysql.connector.django',Just convert the text explicitly to string using . Worked for me."},
{"body": "Python 2.x has two ways to overload comparison operators,  or the \"rich comparison operators\" such as .  Rich comparison operators are simpler to implement each, but you must implement several of them with nearly identical logic.  However, if you can use the builtin  and tuple ordering, then  gets quite simple and fulfills all the comparisons:This simplicity seems to meet my needs much better than overloading all 6(!) of the rich comparisons.  (However, you can get it down to \"just\" 4 if you rely on the \"swapped argument\"/reflected behavior, but that results in a net increase of complication, in my humble opinion.)I understand the , , , etc. operators can be overloaded for other purposes, and can return any object they like.  I am not asking about the merits of that approach, but only about differences when using these operators for comparisons in the same sense that they mean for numbers. As Christopher ,  is disappearing in 3.x. Yep, it's easy to implement everything in terms of e.g.  with a mixin class (or a metaclass, or a class decorator if your taste runs that way).For example:Now your class can define just  and multiply inherit from ComparableMixin (after whatever other bases it needs, if any). A class decorator would be quite similar, just inserting similar functions as attributes of the new class it's decorating (the result might be microscopically faster at runtime, at equally minute cost in terms of memory).Of course, if your class has some particularly fast way to implement (e.g.)  and , it should define them directly so the mixin's versions are not use (for example, that is the case for ) -- in fact  might well be defined to facilitate that as:but in the code above I wanted to keep the pleasing symmetry of only using ;-).\nAs to why  had to go, since we  have  and friends, why keep another, different way to do exactly the same thing around?  It's just so much dead-weight in every Python runtime (Classic, Jython, IronPython, PyPy, ...).  The code that  won't have bugs is the code that isn't there -- whence Python's principle that there ought to be ideally one obvious way to perform a task (C has the same principle in the \"Spirit of C\" section of the ISO standard, btw).This doesn't mean we go out of our way to prohibit things (e.g., near-equivalence between mixins and class decorators for some uses), but it definitely  mean that we don't like to carry around code in the compilers and/or runtimes that redundantly exists just to support multiple equivalent approaches to perform exactly the same task.Further edit: there's actually an even better way to provide comparison AND hashing for many classes, including that in the question -- a  method, as I mentioned on my comment to the question. Since I never got around to writing the PEP for it, you must currently implement it with a Mixin (&c) if you like it:It's a very common case for an instance's comparisons with other instances to boil down to comparing a tuple for each with a few fields -- and then, hashing should be implemented on exactly the same basis. The  special method addresses that need directly.To simplify this case there's a class decorator in Python 2.7+/3.2+, , that can be used to implement what Alex suggests. Example from the docs:This is covered by Also,  goes away in python 3.0.  ( Note that it is not present on  but it IS on  )I tried out the comparable mixin answer above.  I ran into trouble with \"None\".  Here is a modified version that handles equality comparisons with \"None\".  (I saw no reason to bother with inequality comparisons with None as lacking semantics):Inspired by Alex Martelli's  &  answers, I came up with the following mixin.\nIt allows you to implement a single  method, which uses key-based comparisons\nsimilar to , but allows your class to pick the most efficient comparison key based on the type of . (Note that this mixin doesn't help much for objects which can be tested for equality but not order). "},
{"body": "I would like to limit the X and Y axis in matplotlib but for a speific subplot. As I can see\nsubplot figure itself doesn't have any axis property. I want for example to change only the limits for the second plot!You should learn a bit of the OO interface to matplotlib, not just the state machine interface.  Almost all of the  function are thin wrappers that basically do . () return an  () object.  Once you have a referance to the axes object you can plot directly to it, change it's limits, etc.and so on for as many axes as you want.or better, wrap it all up in a loop:"},
{"body": "For example, I have a string like this(return value of ):Whatever I did to it, it is always printed with the annoying  before the string:Does anyone have any ideas about how to use it as a normal string or convert it into a normal string? Decode it.To get bytes from string, encode it.If the answer from  didn't work you could also try:"},
{"body": "I have found that both of the following work:Should all Python classes extend object? Are there any potential problems with not extending object?In Python 2, not inheriting from  will create an old-style class, which, amongst other effects, causes  to give different results:vs.Also the rules for multiple inheritance are  in ways that I won't even try to summarize here. All good documentation that I've seen about MI describes new-style classes.Finally, old-style classes have disappeared in Python 3, and inheritance from  has become implicit. So, always prefer new style classes unless you need backward compat with old software.In Python 3, classes extend  implicitly, whether you say so yourself or not.In Python 2, there's  classes. To signal a class is new-style, you have to inherit explicitly from . If not, the old-style implementation is used.You generally want a new-style class. Inherit from  explicitly.In python 3 you can create a class in three different ways & internally they are all equal (see examples). It doesn't matter how you create a class, all classes in python 3 inherits from special class called  . The  class   is fundamental class in python and provides lot of functionality like double-underscore methods, descriptors, super() method, property() method etc. Example 1.Example 2.Example 3. Yes, all Python classes should extend (or rather subclass, this is Python here) object. While normally no serious problems will occur, in some cases (as with multiple inheritance trees) this will be important. This also ensures better compatibility with Python 3.in python3 there isn't a differance, but in python2 not extending  gives you an old-style classes; you'd like to use a new-style class over an old-style class."},
{"body": "I get this pep8 warning whenever I use lambda expressions. Are lambda expressions not recommended? If not why?The recommendation in  you are running into is:Assigning lambdas to names basically just duplicates the functionality of  - and in general, it's best to do something a single way to avoid confusion and increase clarity.The legitimate use case for lambda is where you want to use a function without assigning it, e.g:Here is the story, I had a simple lambda function which I was using twice. This is just for the representation, I have faced couple of different versions of this.Now, to keep things DRY, I start to reuse this common lambda. At this point my code quality checker complains about lambda being a named function so I convert it into a function. Now the checker complains that a function has to be bounded by one blank line before and after. Here we have now 6 lines of code instead of original 2 lines with no increase in readability and no increase in being pythonic. At this point the code checker complains about the function not having docstrings. In my opinion this rule better be avoided and broken when it makes sense, use your judgement. Lattyware is absolutely right: Basically  wants you to avoid things likeand instead useHowever, as addressed in a recent  (Aug 2014), statements such as the following are now compliant:Since my PEP-8 checker doesn't implement this correctly yet, I turned off E731 for the time being.I also encountered a situation in which it was even impossible to use a def(ined) function.In this case, I really wanted to make a mapping which belonged to the class. Some objects in the mapping needed the same function. It would be illogical to put the a named function outside of the class. \nI have not found a way to refer to a method (staticmethod, classmethod or normal) from inside the class body. SomeClass does not exist yet when the code is run. So referring to it from the class isn't possible either. "},
{"body": "From my understanding : :  : Now, coming back to python, i am bit confused about this. Every where you can learn that python is an interpreted language but it's not interpreted to the machine code, instead to some intermediate code(like byte-code or IL). So which program then executes the IM code. Please help me understand how is a python script handled and run.  First off, interpreted/compiled is not a property of the language but a property of the implementation. For most languages, most if not all implementations fall in one category, so one might save a few words saying the language is interpreted/compiled too, but it's still an important distinction, both because it aids understanding and because there are quite a few languages with usable implementations of both kinds (mostly in the realm of functional languages, see Haskell and ML). In addition, there are C interpreters and projects that attempt to compile a subset of Python to C or C++ code (and subsequently to machine code).Second, compilation is not restricted to ahead-of-time compilation to native machine code. A compiler is, more generally, a program that converts a program in one programming language into a program in another programming language (arguably, you can even have a compiler with the same input and output language if significant transformations are applied). And JIT compilers compile to native machine code , which can give speed very close to or even better than ahead of time compilation (depending on the benchmark and the quality of the implementations compared).But to stop nitpicking and answer the question you meant to ask: Practically (read: using a somewhat popular and mature implementation), Python is . Not compiled to machine code ahead of time (i.e. \"compiled\" by the restricted and wrong, but alas common definition), \"only\" compiled to , but it's still compilation with at least some of the benefits. For example, the statement  is compiled to a byte stream which, when \"disassembled\", looks somewhat like . This is a simplification, it's actually less readable and a bit more low-level -  you can experiment with the standard library  and see what the real deal looks like. Interpreting this is faster than interpreting from a higher-level representation.That bytecode is either interpreted (note that there's a difference, both in theory and in practical performance, between interpreting directly and first compiling to some intermediate representation and interpret that), as with the reference implementation (CPython), or both interpreted and compiled to optimized machine code at runtime, as with .The CPU can only understand machine code indeed. For interpreted program, the ultimate goal of an interpreter is to \"interpret\" the program code into machine code. However, usually a modern interpreted language does not interpret human code directly because it is too inefficient.The Python interpreter first read the human code and optimize it to some immediate code before interpreting it into machine code. That's why you always need another program to run a Python script unlike in C++ you can run the executable directly. For example c:\\Python27\\python.exe or /usr/bin/python.The answer depends on what implementation of python is being used. If you are using lets say  (The Standard implementation of python) or  (Targeted for integration with java programming language)it is first translated into , and depending on the implementation of python you are using  this .  (Python Virtual Machine) for CPython and  (Java Virtual Machine) for Jython.But lets say you are using  which is another standard CPython implementation. It would use a . Almost, we can say Python is interpreted language. But we are using some part of one time compilation process in python to convert complete source code into byte-code like java language.According to python.org it is an interpreter....... ..."},
{"body": "In shells like the interactive python shell, you can usually use the arrow keys to move around in the current line or get previous commands (with arrow-up) etc.But after I ssh into another machine and start  there, I get sessions like:where the last character comes from arrow-up. Or, using arrow-left:How can I fix this?In the regular bash, arrow keys work fine. The weird behavior is just in the interactive python (or perl etc.) shell.Looks like readline is not enabled. Check if  variable is defined, for me it points to  and that file is executed by the python process before going interactive, which setups readline/history handling.Thanks to @chown here is the docs on this: I've solved this issue by installing  package:On OS X, Xcode updates sometimes break . Solution:If the problem still persists, try to remove  using  and install it using :On OS X, I have different problem.When I using system python shell, the keys is no problem, but problem in virtualenv. I'd try to reinstall/upgrade virtualenv/readline and nothing fixed.While I try to  in problem python shell, get this error message:Cause there is  but not , so I make a symbol link:Problem has been solved!Here are the steps which worked for me in ubuntu 12.04 for python 3.3.1) open teminal and write 2) download the source file of python 3.3.2 from 3) extract it and navigate to the Python-3.3.2/ directory in a shell4) execute the following command:On CentOS, I fix this by and then recompile python 3.4.On OpenSUSE, I fix this byfollowing Valerio Crini's answer.Perhaps \"pip3 install readline\" is a general solution. Haven't tried on my CentOS.I fixed this by doing the following:After that I managed to run  successfully which solved the escape characters in my python shell.FYI, I'm using RedHatHave you tried using a different SSH client? Some SSH clients have special, built-in keymappings for different remote processes. I ran into this one a lot with emacs.What client are you using? I'd recommend trying Putty and SecureCRT to compare their behavior.Did you call ssh with the  parameter to tell ssh to allocate a virtual terminal for you?From the man page:Additionally you may also have to set the  environment variable on the server correctly as suggested in another post.How's your env variable $TERM set [a] when things work fine and [b] when they don't?  Env settings are often the key to such problems.Try getting a key code library running on the server. If that does not work try to download a library with read-key ability.I was trying build Python 2.7 on Ubuntu 14.0. You will need libreadline-dev. However, if you get it from apt-get, the current version is 6.3, which is incompatible with Python 2.7 (not sure about Python 3). For example, the data type \"Function\" and \"CPPFunction\", which were defined in previous versions of readline has been removed in 6.3, as reported here:That is to say you need to get the source code of an earlier version of readline. I installed libreadline 5.2 from apt-get for the library, and get the source code of 5.2 for the header files. Put them in /usr/include. Finally the issue has been resolved. If you use Anaconda Python, you can fix this by running:Worked for me!On OS X, using python 3.5 and virtualenvIn the interpreter do: Now arrow keys should work properly. Additional information...Note that as of Oct 1, 2015 - readline has been  (source )Use  instead (see: )If I install readline instead of gnureadline using python 3.5, I receive errors after attempt to import in the interpreter: "},
{"body": "What I want is this behavior:of course, what really happens when I print is:clearly they are sharing the data in class . how do I get separate instances to achieve the behavior I desire?You want this:Declaring the variables inside the class declaration makes them \"class\" members and not instance members. Declaring them inside the  method makes sure that a new instance of the members is created alongside every new instance of the object, which is the behavior you're looking for.You declared \"list\" as a \"class level property\" and not \"instance level property\".  In order to have properties scoped at the instance level, you need to initialize them through referencing with the \"self\" parameter in the  method (or elsewhere depending on the situation).You don't strictly have to initialize the instance properties in the  method but it makes for easier understanding.The accepted answer works but a little more explanation does not hurt. Class attributes do not become instance attributes when an instance is created. They become instance attributes when a value is assigned to them.In the original code no value is assigned to  attribute after instantiation; so it remains a class attribute. Defining list inside  works because  is called after instantiation. Alternatively, this code would also produce the desired output:However, the confusing scenario in the question will never happen to immutable objects such as numbers and strings, because their value cannot be changed without assignment. For example a code similar to the original with string attribute type works without any problem:So to summarize: . This is a good thing because this way you can have static attributes if you never assign a value to an attribute after instantiation.Yes you must declare in the \"constructor\" if you want that the list becomes an object property and not a class property.So nearly every response here seems to miss a particular point.  Class variables  become instance variables as demonstrated by the code below.  By utilizing a metaclass to intercept variable assignment at the class level, we can see that when a.myattr is reassigned, the field assignment magic method on the class is not called.  This is because the assignment .  This behavior has  to do with the class variable as demonstrated by the second class which has no class variables and yet still allows field assignment. Class variables have NOTHING to do with instance variables. They just happen to be in the scope for lookups on instances. Class variables are in fact  on the class object itself.  You can also have  if you want as well because metaclasses themselves are objects too.  Everything is an object whether it is used to create other objects or not, so do not get bound up in the semantics of other languages usage of the word class.  In python, a class is really just an object that is used to determine how to create other objects and what their behaviors will be.  Metaclasses are classes that create classes, just to further illustrate this point. Although the accepted anwer is spot on, I would like to add a bit description.Let's do a small exercise first of all define a class as follows:So what do we have here?Pretty straight forward so far yeah? Now let's start playing around with this class. Let's initialize  this class first:Now do the following:Well,  worked as expected but how the hell did  work? Well it worked because temp is a class attribute. Everything in python is an object. Here A is also an object of class . Thus the attribute temp is an attribute held by the A class and if you change the value of temp through A (and not through an instance of a), the changed value is going to be reflected in all the instance of A class.\nLet's go ahead and do that:Interesting isn't it? And Any Python object is automatically given a  attribute, which contains its list of attributes. Let's investigate what this dictionary contains for our example objects:So how come that we get a defined value of  if it is not even listed for the instance a. Well that's the magic of  method. In Python the dotted syntax automatically invokes this method so when we write , Python executes a.('temp'). That method performs the attribute lookup action, i.e. finds the value of the attribute by looking in different places.The standard implementation of  searches first the internal dictionary () of an object, then the type of the object itself. In this case  executes first  and then Okay now let's use our  method:Well now that we have used self,  gives us a different value from . I think the answers provided are misleading. A property defined inside a class becomes an instance property when the object is instantiated, . So copies of  are made, and  and  are different copies. The reason they seem to be the same is that they are both aliases to the same list. But that is a consequence of the way lists work, not of the way classes work. If you were to do the same thing with numbers instead of lists (or just using += instead of append, which would create a new list) you would see that changing  doesn't affect changing .Defining  inside  works, because the function is called twice, once for each time the object is instantiated, and so, two different lists are created."},
{"body": "While optimising my code I realised the following:and also:I assume it has to do with the way python is implemented in C, but I wonder if anybody would care to explain why is so?The (somewhat unexpected) reason for your results is that Python seems to fold constant expressions involving floating-point multiplication and exponentiation, but not division.  is a different beast altogether since there's no bytecode for it and it involves a function call.On Python 2.6.5, the following code:compiles to the following bytecodes:As you can see, multiplication and exponentiation take no time at all since they're done when the code is compiled. Division takes longer since it happens at runtime. Square root is not only the most computationally expensive operation of the four, it also incurs various  overheads that the others do not (attribute lookup, function call etc).If you eliminate the effect of constant folding, there's little to separate multiplication and division: is actually a little bit faster than , presumably because it's a special case of the latter and can therefore be done more efficiently, in spite of the overheads: Constant expression folding is done by Python's peephole optimizer. The source code () contains the following comment that explains why constant division isn't folded:The  flag enables \"true division\" defined in ."},
{"body": "I'm having trouble understanding . From what I understand,  returns the absolute path from which the module was loaded. I'm having problem producing this: I have a  with one statement , running from   returns . running from  returns . Any reasons why?From the  linked by @kindall in a comment to the question:For the rest of this, consider  not to include .So, if you aren't inside the part of  that contains the module, you'll get an . If you are inside the part of  that contains the module, you'll get a .If you load a module in the current directory, and the current directory  in , you'll get an absolute path.If you load a module in the current directory, and the current directory  in , you'll get a relative path. is always absolute . Not sure if it resolves symlinks though.: Except for script that has been executed directly using a relative path. Like this:Late simple example:Under Python-2.*, the second call incorrectly determines the  based on the current directory:As noted by @techtonik, in Python 3.4+, this will work fine since  returns an absolute path.With the help of the of Guido mail provided by @kindall, we can understand the standard import process as trying to find the module in each member of , and file as the result of this lookup (more details in .). So if the module is located in an absolute path in  the result is absolute, but if it is located in a relative path in  the result is relative.Now the  startup file takes care of delivering only absolute path in , except the initial , so if you don't change it by other means than setting the PYTHONPATH (whose path are also made absolute, before prefixing ), you will get always an absolute path, but when the module is accessed through the current directory.Now if you trick sys.path in a funny way you can get anything.As example if you have a sample module  in  with the code:If you go in /tmp you get:When in  in , if you add  your  you get:Even if you add , it will be normalized and the result is the same.But if instead of using  you use directly some funny path\nyou get a result as funny as the cause.Guido explains in the above cited thread, why python do not try to transform all entries in absolute paths:So your path is used ."},
{"body": "What's a correct and good way to implement ?I am talking about the function that returns a hashcode that is then used to insert objects into hashtables aka dictionaries.As  returns an integer and is used for \"binning\" objects into hashtables I assume that the values of the returned integer should be uniformly distributed for common data (to minimize collisions).\nWhat's a good practice to get such values? Are collisions a problem?\nIn my case I have a small class which acts as a container class holding some ints, some floats and a string.An easy, correct way to implement  is to use a key tuple. It won't be as fast as a specialized hash, but if you need that then you should probably implement the type in C.Here's an example of using a key for hash and equality:Also, the  has more information, that may be valuable in some particular circumstances.Paul Larson of Microsoft Research studied a wide variety of hash functions. He told me that worked surprisingly well for a wide variety of strings. I've found that similar polynomial techniques work well for computing a hash of disparate subfields.John Millikin proposed a solution similar to this:The problem with this solution is that the . In other words, the hash collides with that of the tuple of its key members. Maybe this does not matter very often in practice?The  suggests to combine the hashes of the sub-components using something like XOR, which gives us this:Bonus: more robust  thrown in there for good measure.Update: as Blckknght points out, changing the order of a, b, and c could cause problems. I added an additional  to capture the order of the values being hashed. This final  can be removed if the values being combined cannot be rearranged (for example, if they have different types and therefore the value of  will never be assigned to  or , etc.).I can try to answer the second part of your question.The collisions will probably result not from the hash code itself, but from mapping the hash code to an index in a collection. So for example your hash function could return random values from 1 to 10000, but if your hash table only has 32 entries you'll get collisions on insertion.In addition, I would think that collisions would be resolved by the collection internally, and there are many methods to resolve collisions. The simplest (and worst) is, given an entry to insert at index i, add 1 to i until you find an empty spot and insert there. Retrieval then works the same way. This results in inefficient retrievals for some entries, as you could have an entry that requires traversing the entire collection to find!Other collision resolution methods reduce the retrieval time by moving entries in the hash table when an item is inserted to spread things out. This increases the insertion time but assumes you read more than you insert. There are also methods that try and branch different colliding entries out so that entries to cluster in one particular spot. Also, if you need to resize the collection you will need to rehash everything or use a dynamic hashing method. In short, depending on what you're using the hash code for you may have to implement your own collision resolution method. If you're not storing them in a collection, you can probably get away with a hash function that just generates hash codes in a very large range. If so, you can make sure your container is bigger than it needs to be (the bigger the better of course) depending on your memory concerns. Here are some links if you're interested more: Wikipedia also has a  of various collision resolution methods: Also, \"\" by Tharp covers alot of collision resolution methods extensively. IMO it's a great reference for hashing algorithms. Depends on the size of the hash value you return. It's simple logic that if you need to return a 32bit int based on the hash of four 32bit ints, you're gonna get collisions.I would favor bit operations. Like, the following C pseudo code:Such a system could work for floats too, if you simply took them as their bit value rather than actually representing a floating-point value, maybe better.For strings, I've got little/no idea."},
{"body": "For example I can point the  to  with my template filename in the context but I think need to send more context details.I need to  know exactly what context to add for each of the password reset and change views.If you take a look at the sources for  you'll see that it uses . The upshot is, you can use Context Processors to modify the context which may allow you to inject the information that you need.The b-list has a good .Edit (I seem to have been confused about what the actual question was):You'll notice that  takes a named parameter called :Check  for more information.... thus, with a urls.py like: will be called for URLs matching  with the keyword argument .Otherwise, you don't need to provide any context as the  view takes care of itself. If you want to see what context you have available, you can trigger a  error and look through the stack trace find the frame with a local variable named . If you want to modify the context then what I said above about context processors is probably the way to go.In summary: what do you need to do to use your own template? Provide a  keyword argument to the view when it is called. You can supply keyword arguments to views by including a dictionary as the third member of a URL pattern tuple.Strongly recommend this article.I just plugged it in and it workedYou just need to wrap the existing functions and pass in the template you want. For example:To see this just have a look at the function declartion of the built in views:You can do the following:Explanation:When the templates are loaded, they are searched in your INSTALLED_APPS variable in settings.py .\nThe order is dictated by the definition's rank in INSTALLED_APPS, so since your app come before 'django.contrib.auth' your template were loaded (reference: ).Motivation of approach:The  says that there only one context variable, .If you're having trouble with login (which is common), the  says there are three context variables:I was using this two lines in the url and the template from the admin what i was changing to my needAnother, perhaps simpler, solution is to add your override template directory to the DIRS entry of the TEMPLATES setting in settings.py.  (I think this setting is new in Django 1.8.  It may have been called TEMPLATE_DIRS in previous Django versions.)Like so:Then put your override template files under .  So the overridden password reset template would be "},
{"body": "I've recently come across a problem which requires at least a basic degree of image processing, can I do this in Python, and if so, with what?The best-known library is .  However if you are simply doing basic manipulation, you are probably better off with the Python bindings for , which will be a good deal more efficient than writing the transforms in Python.Depending on what you mean by \"image processing\", a better choice might be in the numpy based libraries: , , or . All of these work based on numpy arrays, so you can mix and match functions from one library and another.I started the website  which has more information on these.You also have an approach to image processing based on \"standard\" scientific modules:  has a whole package dedicated to image processing: .  Scipy is in effect the standard general numerical calculations package; it is based on the de facto standard array-manipulation module : images can also be manipulated as array of numbers.  As for image display,  (also part of the \"science trilogy\") makes displaying images .SciPy is still actively maintained, so it's a good investment for the future.  Furthermore, SciPy currently runs with Python 3 too, while the Python Imaging Library (PIL) does not.To complete the list: opencv\nThere's also , which might be more suitable depending on your needs.There is actually a wonderful .  It gives you the ability to alter existing images, including anti-aliasing capabilities, and create new images with text and such.  You can also find a  in the PIL handbook provided on the aforementioned site.If you are creating a custom image processing effect, you may find PythonPixels useful.\n\nIt is intended for writing and experimenting with image processing.VIPS should be fast and uses multiple CPUs:\n"},
{"body": "I can't seem to figure out how to setup a \"default\" logger for my Django installation. I would like to use Django 1.3's new  setting in .I've looked at the , but it looks to me like they only setup handlers which will do logging for particular loggers. In the case of their example they setup handler for the loggers named 'django','django.request', and 'myproject.custom'.All I want to do is setup a default  which will handle all loggers by default. i.e., if I make a new module somewhere in my project and it is denoted by something like: , I should be able to do this and have all logging goto the rotating file logs.Figured it out...You set the 'catch all' logger by referencing it with the empty string: .As an example, in the following setup I have the all log events getting saved to , with the exception of  log events which will be saved to . Because  is set to  for my  logger, the log event will never reach the the 'catch all' logger.As you said in , Chris, one option to define a default logger is to use the empty string as its key.However, I think the intended way is to define a special logger under the  key of the logging configuration dictionary. I found this in the :Here's the configuration from your answer changed to use the  key:To be fair, I can't see any difference in behaviour between the two configurations. It appears that defining a logger with an empty string key will modify the root logger, because  will return the root logger.The only reason I prefer  over  is that it is explicit about modifying the root logger. In case you were curious,  overrides  if you define both, just because the root entry is processed last.after add:we may change format to: or "},
{"body": "Is there syntax that allows you to expand a list into the arguments of a function call?Example:It exists, but it's hard to search for. I think most people call it the \"\" operator.It's in the documentation as \"\".You'd use it like this: . There's also one for dictionaries:You should use the * operator, like  Read the Python doc .Also, do read this: Try the following:This can be found in the Python docs as .That can be done with:"},
{"body": "I'm trying to create a directory if the path doesn't exist, but the ! (not) operator doesn't work. I'm not sure how to negate in Python... What's the correct way to do this?The negation operator in Python is . Therefore just replace your  with .For your example, do this:For your specific example (as Neil said in the comments), you don't have to use the  module, you can simply use  to get the result you need, with added exception handling goodness.Example:Python prefers English keywords to punctuation. Use , i.e. . The same thing goes for  and  which are  and  in Python.try instead:Combining the input from everyone else (use not, no parens, use ) you'd get..."},
{"body": "What's the easiest way to play a sound file (.wav) in Python? By easiest I mean both most platform independent and requiring the least dependencies. pygame is certainly an option, but it seems overkill for just sound. can play wav, au and mp3 files.  For Windows, you can use winsound. It's built inYou should be able to use ossaudiodev for linux:(Credit for ossaudiodev: Bill Dandreta )Definitely use  for this. It's kind of a large package, but it is pure python with no extension modules. That will definitely be the easiest for deployment. It's also got great format and codec support.This seems ridiculous and far fetched but you could always use Windows (or whatever OS you prefer) to manage the sound for you!Simple, no extensions, somewhat slow and junky, but working.After the play() command add a delay of say 10 secs or so, it'll workThis also plays .mp3 files.pyMedia's sound example does . This should be all you need.wxPython has support for playing wav files on Windows and Unix - I am not sure if this includes Macs.  However it only support wav files as far as I can tell - it does not support other common formats such as mp3 or ogg.I like pygame, and the command below should work:but it doesn't on either of my computers, and there is limited help on the subject out there. edit: I figured out why the pygame sound isn't working for me, it's not loading most sounds correctly, the 'length' attribute is ~0.0002 when I load them. maybe loading them using something other than mygame will get it morking more generally.with pyglet I'm getting a resource not found error Using the above example, wigh both relative and full paths to the files.using  instead of  lets me load the files.but  only plays the first fraction of a second of the file, unless I run  which blocks everything else...I just released a simple python wrapper around sox that will play a sound with Python.  It's very easy to install as you need Python 2.6 or greater, sox (easy to get binaries for most architectures) and the wrapper (  ).  If you don't have sox, go here: You would play audio with it by:Keep in mind, the only parts actually involved in playing audio are just these:For Linux user, if low level pcm data manipulation is needed, try  module. There is a playwav.py example inside the package too. "},
{"body": "I am attempting to filter users by a custom field in each users profile called profile. This field is called level and is an integer between 0-3.If I filter using equals, I get a list of users with the chosen level as expected:When I try to filter using less than:I get the error:Is there away to filter by < or >, or am I barking up the wrong tree.Less than or equal:Greater than or equal:Likewise,  for less than and  for greater than. You can find them all ."},
{"body": "I'm using the  library and I got this weird error and I don't know what is mean.Anybody has an idea?You need to include the protocol scheme:Without the  part,  has no idea how to connect to the remote server.One more reason, maybe your url include some hiden characters, such as '\\n'.If you define your url like below, this exception will raise:because there are '\\n' hide in the string. The url in fact become:"},
{"body": "I've been trying to find a more pythonic way of generating random string in python that can scale as well. Typically, I see something similar toIt sucks if you want to generate long string.I've been thinking about random.getrandombits for a while, and figuring out how to convert that to an array of bits, then hex encode that. Using python 2.6 I came across the bitarray object, which isn't documented.  Somehow I got it to work, and it seems really fast.It generates a 50mil random string on my notebook in just about 3 seconds.heikogerlach pointed out that it was an odd number of characters causing the issue. New code added to make sure it always sent fromhex an even number of hex digits.Still curious if there's a better way of doing this that's just as fast.and if you need url safe string :(note random_string length is greatest than string_length in that case)Sometimes a uuid is short enough and if you don't like the dashes you can always.replace('-', '') themIf you want it a specific length without dashesTaken from the  bug report at Python.org:Also, see the issues  and It seems the  method expects an even number of hex digits. Your string is 75 characters long.\nBe aware that   the last element! Just use .Regarding the last example, the following fix to make sure the line is even length, whatever the junk_len value:"},
{"body": "I am in the process of migrating an application from django 1.2 To 1.4.I have a daily task object which contains a time of day that task should be completed:In order to check if a task is still required to be completed today, I have the following code:This worked fine under 1.2, But under 1.4 I get the error:due to the lineand both comparison clauses throw this error.I have tried making timeDue timezone aware by adding pytz.UTC as an argument, but this still raises the same error.I've read some of the docs on timezones but am confused as to whether I just need to make timeDue timezone aware, or whether I need to make a fundamental change to my db and existing data.Check  for detail info.Normally, use  to make an offset-aware current datetimeAnd  to make an offset-aware datetimeYou could then compare both offset-aware datetimes w/o trouble. Furthermore, you could convert offset-awared datetime to offset-naive datetime by stripping off timezone info, then it could be compared w/ normal , under utc. is  'by default' (actually it's  by default, but the  file generated by  set it to ), then if your DB supports timezone-aware times, values of time-related model fields would be timezone-aware. you could disable it by setting (or simply remove ) in settings. "},
{"body": "I am new to Python. I need to write some data from my program to a spreadsheet. I've searched online and there seems to be many packages available (xlwt, XlsXcessive, openpyxl). Others suggest to write to a csv file (never used csv & don't really understand what it is).The program is very simple. I have two lists (float) and three variables (strings). I don't know the lengths of the two lists and they probably won't be the same length.I want the layout to be as in the picture below:The pink column will have the values of the first list and the green column will have the values of the second list.So what's the best way to do this? Thanks.P.S. I am running Windows 7 but I won't necessarily have Office installed on the computers running this program.I wrote this using all your suggestions. It gets the job done but it can be slightly improved. How do I format the cells created in the for loop (list1 values) as scientific or number? I do not want to truncate the values. The actual values used in the program would have around 10 digits after the decimal.for more explanation:\n    Use  from . Pandas allows you to represent your data in functionally rich datastructures and will let you  excel files as well.You will first have to convert your data into a DataFrame and then save it into an excel file like so:and the excel file that comes out looks like this:Note that both lists need to be of equal length else pandas will complain. To solve this, replace all missing values with .CSV stands for comma separated values. CSV is like a text file and can be created simply by adding the for example write this code:you can open this file with excel.Try taking a look at the following libraries too: - for getting data into and out of a spreadsheet from Python, as well as manipulating workbooks and charts - an Excel add-in for writing user-defined functions (UDFs) and macros in Python instead of VBAI surveyed a few Excel modules for Python, and found  to be the best.The free book Automate the Boring Stuff with Python  with more details or you can check the  site. You won't need Office or Excel installed in order to use openpyxl.Your program would look something like this:The easiest way to import the exact numbers is to add a decimal after the numbers in your l1 and l2. Python interprets this decimal point as instructions from you to include the exact number. If you need to restrict it to some decimal place, you should be able to create a print command that limits the output, something simple like:print variable_example[:13]Would restrict it to the tenth decimal place, assuming your data has two integers left of the decimal. "},
{"body": "Half of my Flask routes requires a variable say,  or . How do I create links to those locations? takes one argument for the function to route to but I can't add arguments?It takes keyword arguments for the variables:Refer to Other sample snippets of usage for linking js or css to your template are below. in Flask is used for creating a URL to prevent the overhead of having to change URLs throughout an application (including in templates). Without , if there is a change in the root URL of your app then you have to change it in every page where the link is present.Syntax: It can be used as:Now if you have a link the index page:you can use this:You can do a lot o stuff with it, for example:For the above we can use:Like this you can simply pass the parameters!"},
{"body": "When you have a model field with a choices option you tend to have some magic values associated with human readable names. Is there in Django a convenient way to set these fields by the human readable name instead of the value?Consider this model:At some point we have a Thing instance and we want to set its priority. Obviously you could do,But that forces you to memorize the Value-Name mapping of PRIORITIES. This doesn't work:Currently I have this silly workaround:but that's clunky. Given how common this scenario could be I was wondering if anyone had a better solution. Is there some field method for setting fields by choice name which I totally overlooked?Do as . Then you can use a word that represents the proper integer.Like so:Then they are still integers in the DB.Usage would be I'd probably set up the reverse-lookup dict once and for all, but if I hadn't I'd just use:which seems simpler than building the dict on the fly just to toss it away again;-).Here's a field type I wrote a few minutes ago that I think does what you want. Its constructor requires an argument 'choices', which may be either a tuple of 2-tuples in the same format as the choices option to IntegerField, or instead a simple list of names (ie ChoiceField(('Low', 'Normal', 'High'), default='Low') ). The class takes care of the mapping from string to int for you, you never see the int.Use it like this:I appreciate the constant defining way but I believe  type is far best for this task. They can represent integer and a string for an item in the same time, while keeping your code more readable.Enums were introduced to Python in version 3.4. If you are using any lower (such as v2.x) you can still have it by installing the : .This is pretty much all. You can inherit the  to create your own definitions and use them in a model definition like:Querying is icing on the cake as you may guess:Representing them in string is also made easy:Simply replace your numbers with the human readable values you would like. As such:This makes it human readable, however, you'd have to define your own ordering."},
{"body": "Given an arbitrary python object, what's the best way to determine whether it is a number? Here  is defined as .For example, say you are writing a vector class. If given another vector, you want to find the dot product. If given a scalar, you want to scale the whole vector.Checking if something is , , ,  is annoying and doesn't cover user-defined objects that might act like numbers. But, checking for , for example, isn't good enough because the vector class I just described would define , but it wouldn't be the kind of number I want.Use  from the  module to test  (available since 2.6).This is, of course, contrary to duck typing.  If you are more concerned about how an object  rather than what it , perform your operations as if you have a number and use exceptions to tell you otherwise.You want to check if some objectIf you're using Python 2.5 or older, the only real way is to check some of those \"certain circumstances\" and see.In 2.6 or better, you can use  with  -- an abstract base class (ABC) that exists exactly for this purpose (lots more ABCs exist in the  module for various forms of collections/containers, again starting with 2.6; and, also only in those releases, you can easily add your own abstract base classes if you need to).Bach to 2.5 and earlier,\n\"can be added to  and is not iterable\" could be a good definition in some cases.  But,\nyou really need to ask yourself, what it is that you're asking that what you want to consider \"a number\" must definitely be able to , and what it must absolutely be  to do -- and check.This may also be needed in 2.6 or later, perhaps for the purpose of making your own registrations to add types you care about that haven't already be registered onto  -- if you want to  some types that claim they're numbers but you just can't handle, that takes even more care, as ABCs have no  method [[for example you could make your own ABC  and register there all such weird-for-you types, then first check for  thereof to bail out before you proceed to checking for  of the normal  to continue successfully.BTW, if and when you need to check if  can or cannot do something, you generally have to try something like:The presence of  per se tells you nothing useful, since e.g all sequences have it for the purpose of concatenation with other sequences.  This check is equivalent to the definition \"a number is something such that a sequence of such things is a valid single argument to the builtin function \", for example.  Totally weird types (e.g. ones that raise the \"wrong\" exception when summed to 0, such as, say, a  or  &c) will propagate exception, but that's OK, let the user know ASAP that such crazy types are just not acceptable in good company;-); but, a \"vector\" that's summable to a scalar (Python's standard library doesn't have one, but of course they're popular as third party extensions) would also give the wrong result here, so (e.g.) this check should come  the \"not allowed to be iterable\" one (e.g., check that  raises , or for the presence of special method  -- if you're in 2.5 or earlier and thus need your own checks).A brief glimpse at such complications may be sufficient to motivate you to rely instead on abstract base classes whenever feasible...;-).This is a good example where exceptions really shine. Just do what you would do with the numeric types and catch the  from everything else.But obviously, this only checks if a operation , not whether it ! The only real solution for that is to never mix types and always know exactly what typeclass your values belong to.To rephrase your question, you are trying to determine whether something is a collection or a single value. Trying to compare whether something is a vector or a number is comparing apples to oranges - I can have a vector of strings or numbers, and I can have a single string or single number. , not what type you actually have.my solution for this problem is to check whether the input is a single value or a collection by checking the presence of . For example:Or, for the duck-typing approach, you can try iterating on  first:Ultimately, it is easier to test whether something is vector-like than to test whether something is scalar-like. If you have values of different type (i.e. string, numeric, etc.) coming through, then the logic of your program may need some work - how did you end up trying to multiply a string by a numeric vector in the first place?Probably it's better to just do it the other way around: You check if it's a vector. If it is, you do a dot product and in all other cases you attempt scalar multiplication.Checking for the vector is easy, since it should of your vector class type (or inherited from it). You could also just try first to do a dot-product, and if that fails (= it wasn't really a vector), then fall back to scalar multiplication.Multiply the object by zero.  Any number times zero is zero.  Any other result means that the object is not a number (including exceptions)Using isNumber thusly will give the following output:Output:There probably are some non-number objects in the world that define  to return zero when multiplied by zero but that is an extreme exception. This solution should cover all  and  code that you generate/encouter.For the hypothetical vector class:Suppose  is a vector, and we are multiplying it by . If it makes sense to multiply each component of  by , we probably meant that, so try that first. If not, maybe we can dot? Otherwise it's a type error. -- the below code doesn't work, because  instead of raising a . I leave it because it was commented-upon.Just to add upon.\nPerhaps we can use a combination of isinstance and isdigit as follows to find whether a value is a number (int, float, etc)if isinstance(num1, int) or isinstance(num1 , float) or num1.isdigit():I had a similar issue, when implementing a sort of vector class. One way to check for a number is to just convert to one, i.e. by usingThis should reject cases where x cannot be converted to a number; but may also reject other kinds of number-like structures that could be valid, for example complex numbers."},
{"body": "I know that fetching a url is as simple as  and I can get at the raw response body and save it to a file, but for large files, is there a way to stream directly to a file? Like if I'm downloading a movie with it or something?Oddly enough, requests doesn't have anything simple for this. You'll have to iterate over the response and write those chunks to a file:I usually just use . It works, but if you need to use a session or some sort of authentication, the above code works as well."},
{"body": "Is there a reference for the memory size of Python data stucture on 32- and 64-bit platforms?If not, this would be nice to have it on SO. The more exhaustive the better! So how many bytes are used by the following Python structures (depending on the  and the content type when relevant)?(For containers that keep only references to other objects, we obviously do not want to count the size of the item themselves, since it might be shared.)Furthermore, is there a way to get the memory used by an object at runtime (recursively or not)?The recommendation from  on this was to use , quoting:You could take this approach:2012-09-30python 2.7 (linux, 32-bit):python 3.3 (linux, 32-bit)2016-08-01OSX, Python 2.7.10 (default, Oct 23 2015, 19:19:21) [GCC 4.2.1 Compatible Apple LLVM 7.0.0 (clang-700.0.59.5)] on darwinI've been happily using  for such tasks. It's compatible with many versions of Python -- the  module in particular goes back to 2.2!For example, using hughdbrown's example but with  at the start and  at the end, I see (system Python 2.5 on MacOSX 10.5):Clearly there is some approximation here, but I've found it very useful for footprint analysis and tuning.Also you can use  module.And:Try memory profiler.\nThese answers all collect shallow size information. I suspect that visitors to this question will end up here looking to answer the question, \"How big is this complex object in memory?\"There's a great answer here: The punchline:Used like so:If you want to know Python's memory model more deeply, there's a great article here that has a similar \"total size\" snippet of code as part of a longer explanation: When you use the dir([object]) built-in function, you can the  built-in function."},
{"body": "I have used matplotlib to plot lines on a figure. Now I would now like to set the style, specifically the marker, for individual points on the line. How do I do this?Edit:\nto clarify my question, which was answered, I want to be able to set the style for individual markers on a line, not every marker on said line.Specify the keyword args  and/or  in your call to .For example, using a dashed line and blue circle markers: A shortcut call for the same thing:Here is a list of the possible line and marker styles:   with an example of marking an arbitrary subset of points, as requested in the comments:This last example using the  kwarg is possible in since 1.4+, due to the merge of .  If you are stuck on an older version of matplotlib, you can still achieve the result by overlaying a scatterplot on the line plot.  See the  for more details.  For future reference - the  artist returned by  also has a  method which allows you to only set markers on certain points - see "},
{"body": "I'm trying to transfer a function across a network connection (using asyncore). Is there an easy way to serialize a python function (one that, in this case at least, will have no side affects) for transfer like this?I would ideally like to have a pair of functions similar to these:You could serialise the function bytecode and then reconstruct it on the caller.  The  module can be used to serialise code objects, which can then be reassembled into a function.  ie:Then in the remote process (after transferring code_string):A few caveats:Check out , which extends Python's pickle library to support a greater variety of types, including functions:It also supports references to objects in the function's closure: is able to .The most simple way is probably  (see the ) which returns a String with the source code for a function or a method.It all depends on whether you generate the function at runtime or not:If you do -  won't work for dynamically generated functions as it gets object's source from  file, so only functions defined before execution can be retrieved as source.And if your functions are placed in files anyway, why not give receiver access to them and only pass around module and function names.The only solution for dynamically created functions that I can think of is to construct function as a string before transmission, transmit source, and then  it on the receiver side.Edit: the  solution looks also pretty smart, didn't know you can serialize something other thatn built-insThe  package (pip install cloud) can pickle arbitrary code, including dependencies.  See .The basic functions used for this module covers your query, plus you get the best compression over the wire; see the instructive source code:y_serial.py module :: warehouse Python objects with SQLite\"Serialization + persistance :: in a few lines of code, compress and annotate Python objects into SQLite; then later retrieve them chronologically by keywords without any SQL. Most useful \"standard\" module for a database to store schema-less data.\""},
{"body": "For example I have simple DF:Can I select values from 'A' for which corresponding values for 'B' will be greater than 50, and for 'C' - not equal 900, using methods and idioms of Pandas?Sure!  Setup:We can apply column operations and get boolean Series objects:[Update, to switch to new-style ]:And then we can use these to index into the object.  For read access, you can chain indices:but you can get yourself into trouble because of the difference between a view and a copy doing this for write access.  You can use  instead:Note that I accidentally did  and not , or , but I'm too lazy to fix it. Exercise for the reader. :^)"},
{"body": "I want to copy already indented Python code / whole functions and classes into IPython. Everytime I try the indentation is screwed up and I get following error message:You can't copy to IPython directly. This are the steps:A clarification on the steps:For example:As of  you don't need any magic command, just paste itMore on this To upgrade  to the latest version  requires . If you  are in  ubuntu, you can  install it by Then  restart ipython and use  to paste  from your  clipboard.One of the useful answers was lost in the comments, so wanted to restate it along with adding a reference for another useful IPython magic function.First to restate what @EOL said, one way to solve OP's problem is to turn off auto-indentation by first running  and doing the paste (not needed if you are using , of course).Now to add more information to what is already there here, one more useful mode in IPython is  which allows you to copy paste example and test snippets from doc strings. This is also useful to execute interactive python session output that you could find in documentation and online forums, without having to first strip out the prompt strings.For ubuntu users who are on Python 3.The  is for Python 2.To make  work on Python 3, install the  package:"},
{"body": "Why does this NOT return the  tags and stuff in between? It returns nothing. And I know for a fact it exists because I'm staring right at it from  also does not work. There is no answer to this post - how do I delete it? I found that BeautifulSoup is not parsing correctly, which probably actually means the page I'm trying to parse isn't properly formatted in SGML or whatever. You should post your example document, because the code works fine:Finding s inside s works as well:To find an element by its :I think there is a problem when the 'div' tags are too much nested. I am trying to parse some contacts from a facebook html file, and the Beautifulsoup is not able to find tags \"div\" with class \"fcontent\".This happens with other classes as well. When I search for divs in general, it turns only those that are not so much nested.The html source code can be any page from facebook of the friends list of a friend of you (not the one of your friends). If someone can test it and give some advice I would really appreciate it.This is my code, where I just try to print the number of tags \"div\" with class \"fcontent\":Most probably because of the default beautifulsoup parser has problem. Change a different parser, like 'lxml' and try again.In the beautifulsoup source this line allows divs to be nested within divs; so your concern in lukas' comment wouldn't be valid.What I think you need to do is to specify the attrs you want such asHappened to me also while trying to scrape Google.\nI ended up using pyquery.\nInstall:Use:have you tried ?sounds crazy, but if you're scraping stuff from the wild, you can't rule out multiple divs...I used:As my syntax for find/findall; that said, unless there are other optional parameters between the tag and attribute list, this shouldn't be different.Here is a code fragment As you can see I find all  tags and then I find all  tags with class=\"article\" insideBeautiful Soup 4 supports most  with the , therefore you can use an  such as:If you need to specify the element's type, you can add a  before the  selector:The  method will return a collection of elements, which means that it would return the same results as the following  example:If you only want to select a single element, then you could just use the :"},
{"body": "I cannot figure out why this is giving me the error, The actual string I am trying to insert is 74 chars long, it's: \"/gifs/epic-fail-photos-there-i-fixed-it-aww-man-the-tire-pressures-low.gif\"I've tried to str(array[cnt]) before inserting it, but the same issue is happening, the database only has one column, which is a TEXT value.I've been at it for hours and I cannot figure out what is going on.You need to pass in a sequence, but you forgot the comma to make your parameters a tuple:Without the comma,  is just a grouped expression, not a tuple, and thus the  string is treated as the input sequence. If that string is 74 characters long, then Python sees that as 74 separate bind values, each one character long.If you find it easier to read, you can also use a list literal:"},
{"body": "How can I simply SSH to a remote server from a local Python (3.0) script, supply a login/password, execute a command and print the output to the Python console?I would rather not use any large external library or install anything on the remote server.I haven't tried it, but this  module might help, which in turn uses paramiko. I believe everything is client-side.The interesting command is probably  which executes an arbitrary command on the remote machine. (The module also features  and  methods which allude more to its FTP character).UPDATE:I've re-written the answer after the blog post I originally linked to is not available anymore. Some of the comments that refer to the old version of this answer will now look weird.You can code it yourself using Paramiko, as suggested above. Alternatively, you can look into Fabric, a python application for doing all the things you asked about:I think this fits your needs. It is also not a large library and requires no server installation, although it does have dependencies on paramiko and pycrypt that require installation on the client.The app used to be . It can now be found .There are several good articles on it, though you should be careful because it has changed in the last six months:Later: Fabric no longer requires paramiko to install:This is mostly cosmetic, however: ssh is a fork of paramiko, the maintainer for both libraries is the same (Jeff Forcier, also the author of Fabric), and . (This correction via .)If you want to avoid any extra modules, you can use the subprocess module to runand capture the output. Try something like:To deal with usernames and passwords, you can use subprocess to interact with the ssh process, or you could install a public key on the server to avoid the password prompt.I have written . Libssh2 is a client-side library implementing the SSH2 protocol.Your definition of \"simplest\" is important here - simple code means using a module (though \"large external library\" is an exaggeration).I believe the most up-to-date (actively developed) module is . It comes with demo scripts in the download, and has detailed online API documentation. You could also try , which is contained in . There's a short sample along with the documentation at the first link. Again with respect to simplicity, note that good error-detection is always going to make your code look more complex, but you should be able to reuse a lot of code from the sample scripts then forget about it.Like hughdbrown, I like Fabric. Please notice that while it implement its own declarative scripting (for making deploys and the such) it can also be imported as a Python module and used on your programs without having to write a Fabric script.Fabric has a new maintainer and is in the process of being rewriten; that means that most tutorials you'll (currently) find on the web will not work with the current version. Also, Google still shows the old Fabric page as the first result. For up to date documentation you can check: I found paramiko to be a bit too low-level, and Fabric not especially well-suited to being used as a library, so I put together my own library called  that uses paramiko to implement a slightly nicer interface:You can also choose to print the output of the program as it's running, which is useful if you want to see the output of long-running commands before it exits:This worked for me"},
{"body": "I want to use R in Python, as provided by the module Rpy2. I notice that R has very convenient  operations by which you can extract the specific columns or lines. How could I achieve such a function by Python scripts?My idea is to create an R vector and add those wanted elements into this vector so that the final vector is the same as that in R. I created a , but it seems that it has an initial digit 1, so the final result would always start with the digit 1, which is not what I want. So, is there a better way to do this?See also vector helpI pre-allocate a vector withYou can then use [] to insert values into it.You can create an empty vector like soAnd then add elements using c()However as romunov says, it's much better to pre-allocate a vector and then populate it (as this avoids reallocating a new copy of your vector every time you add elements)I've also seenNow  you can concatenate or bind  a vector of any dimension to To create an empty vector use:Please note, I am not making any assumptions about the type of vector you require, e.g. numeric.Once the vector has been created you can add elements to it as follows:For example, to add the numeric value 1:or, to add a string value \"a\"As pointed out by Brani, vector() is a solution, e.g.will return a vector named \"newVector\" with 50 \"0\"'s as initial values. It is also fairly common to just add the new scalar to an existing vector to arrive at an expanded vector, e.g.In rpy2, the way to get the very same operator as \"[\" with R is to use \".rx\".\nSee the documentation about For creating vectors, if you know your way around with Python there should not be any issue.\nSee the documentation about "},
{"body": "I've got list of objects. I want to find one (first or whatever) object in this list that has attribute (or method result - whatever) equal to . What's is the best way to find it?Here's test case:I think using generators and  won't make any difference because it still would be iterating through list.ps.: Equation to  is just an example. Of course we want to get element which meets any condition.This gets the first item from the list that matches the condition, and returns  if no item matches. It's my preferred single-expression form.However,The naive loop-break version, is perfectly Pythonic -- it's concise, clear, and efficient. To make it match the behavior of the one-liner:This will assign  to  if you don't  out of the loop.I just ran into a similar problem and devised a small optimization for the case where no object in the list meets the requirement.(for my use-case this resulted in major performance improvement):Along with the list test_list, I keep an additional set test_value_set which consists of values of the list that I need to filter on. So here the else part of agf's solution becomes very-fast."},
{"body": "I'm trying to understand how the  and  Python built-in functions work. I'm trying to compare the tuples so that if any value is different then it will return  and if they are all the same it will return . How are they working in this case to return [False, False, False]? is a .To my knowledge, this should output since (1,1) are the same, (5,6) are different, and (0,0) are the same.  You can roughly think of  and  as series of logical  and  operators, respectively. will return  when  is Truthy. Read about  will return  only when  are Truthy.  The empty iterable case is explained in the official documentation, like thisSince none of the elements is true, it returns  in this case. Since none of the elements is false, it returns  in this case. Another important thing to know about  and  is, it will short-circuit the execution, the moment they know the result. The advantage is, entire iterable need not be consumed. For example,Here,  is a generator expression which returns  if the current number within 1 and 9 is a multiple of 6.  iterates the  and when it meets , it finds a Truthy value, so it immediately returns , and rest of the  is not iterated. That is what we see when we print , the result of ,  and .This excellent thing is used very cleverly in .With this basic understanding, if we look at your code, you do which makes sure that, atleast one of the values is Truthy but not all of them. That is why it is returning . If you really wanted to check if both the numbers are not the same, and  take iterables and return  if any and all (respectively) of the elements are . If the iterables are empty,  returns , and  returns .I was demonstrating  and  for students in class today. They were mostly confused about the return values for empty iterables. Explaining it this way caused a lot of lightbulbs to turn on.They both look for a condition that allows them to stop evaluating. The first examples I gave required them to evaluate the entire list. checks for elements to be  (so it can return ), then it returns  if none of them were .The way  works is that it checks for elements to be  (so it can return FalseTrue`.I think if you keep in mind the short-cutting behavior, you will intuitively understand how they work without having to reference a Truth Table.The code in question you're asking about comes from my answer given .  It was intended to solve the problem of comparing multiple bit arrays - i.e. collections of  and .   and  are useful when you can rely on the \"truthiness\" of values - i.e. their value in a boolean context.  1 is  and 0 is , a convenience which that answer leveraged.  5 happens to also be , so when you mix that into your possible inputs... well.  Doesn't work.You could instead do something like this:It lacks the aesthetics of the previous answer (I  liked the look of ), but it gets the job done."},
{"body": "I need to download it for Python 2.7, but can't seem to find it... 'pywin32' is its canonical name.There is a a new option as well: get it via pip!  There is a package  with wheels available, so you can just install with: !I've found that UC Irvine has a great collection of python modules, pywin32 (win32api) being one of many listed there.  I'm not sure how they do with keeping up with the latest versions of these modules but it hasn't let me down yet.UC Irvine Python Extension Repository - pywin32 module -  - 3rd .exe down"},
{"body": "Can you simply delete the directory from your python installation, or are there any lingering files that you must delete?It varies based on the options that you pass to  and the contents of the  on the system/in the package. I don't believe that any files are modified outside of directories specified in these ways.Notably,  at this time. It's also noteworthy that deleting a package/egg can cause dependency issues -- utilities like  attempt to alleviate such problems.The three things that get installed that you will need to delete are:Now on my linux system these live in:But on a windows system they are more likely to be entirely within the Python distribution directory. I have no idea about OSX except it is more likey to follow the linux pattern.Another time stamp based hack:Yes, it is safe to simply delete anything that distutils installed. That goes for installed folders or .egg files. Naturally anything that depends on that code will no longer work. If you want to make it work again, simply re-install. By the way, if you are using distutils also consider using the multi-version feature. It allows you to have multiple versions of any single package installed. That means you do not need to delete an old version of a package if you simply want to install a newer version.If this is for testing and/or development purposes,  has a  command that updates every time you make a change (so you don't have to uninstall and reinstall every time you make a change).  And you can uninstall the package using this command as well.If you do use this, anything that you declare as a  will be left behind as a lingering file.In ubuntu 12.04, I have found that the only place you need to look by default is underAnd simply remove the associated folder and file, if there is one!I just uninstalled a python package, and even though I'm not  I did so perfectly, I'm reasonably confident.  I started by getting a list of  python-related files, ordered by date, on the assumption that all of the files in my package will have more or less the same timestamp, and no other files will.Luckily, I've got python installed under ; if I had been using the Python that comes with my Linux distro, I'd have had to scour all of , which would have taken a long time.Then I just examined that list, and noted with relief that all the stuff that I wanted to nuke consisted of one directory, , and one file, .So I just deleted those.Here's how I got the date-sorted list of files:(I think that's got to be GNU \"find\", by the way; the flavor you get on OS X doesn't know about \"-printf '%T@'\")I use that  the time.For Windows 7,, then choose the python package to remove.removes all files and but leaves empty directories behind.That is not ideal, it should be enough to avoid package conflicts.And then you can finish the job manually if you want by reading , or be braver and automate empty directory removal as well.A safe helper would be:Tested in Python 2.7.6, Ubuntu 14.04.for Python in Windows:"},
{"body": "I would like to create a copy of an object. I want the new object to possess all properties of the old object (values of the fields). But I want to have independent objects. So, if I change values of the fields of the new object, the old object should not be affected by that.Have a look at the  function.  It should do exactly what you need.  Also have a look at  to a related question for a nice explanation."},
{"body": "Module  includes  at its top. However under test conditions I'd like to   in  (mock ) and completely refrain from importing . In fact,  isn't installed in the test environment on purpose.A is the unit under test. I have to import A with all its functionality. B is the module I need to mock. But how can I mock B within A and stop A from importing the real B, if the first thing A does is import B?(The reason B isn't installed is that I use pypy for quick testing and unfortunately B isn't compatible with pypy yet.)How could this be done?You can assign to  before importing  to get what you want:::Note B.py does not exist, but when running  no error is returned and  prints . You still have to create a  where you mock B's actual functions/variables/etc. Or you can just assign a Mock() directly::The builtin  can be mocked with the 'mock' library for more control:Say  looks like: returns  which can be mocked also.I realize I'm a bit late to the party here, but here's a somewhat insane way to automate this with the  library:(here's an example usage)The reason this is so ridiculously complicated is when an import occurs python basically does this (take for example )There are some downsides to this hacked together solution: If something else relies on other stuff in the module path this kind of screws it over.  Also this  works for stuff that is being imported inline such asorIf you do an  you are really calling the builtin method  as:You could overwrite this method by importing the  module and make a wrapper around the method. Or you could play with the  hook from the  module. Catching the exception and Mock your module/class in the -block.Pointer to the relevant docs:I hope this helps. Be  adviced that you step into the more arcane perimeters of python programming and that a) solid understanding what you really want to achieve and b)thorough understanding of the implications is important.Easy, just mock the library in sys.modules before it gets imported:and then, so long as  doesn't rely on specific types of data being returned from B's objects:should just work.This works even if you have submodules, but you'll want to mock each module. Say you have this:To mock, simply do the below before the module that contains the above is imported: (My experience: I had a dependency that works on one platform, Windows, but didn't work on Linux, where we run our daily tests. \nSo I needed to mock the dependency for our tests. Luckily it was a black box, so I didn't need to set up a lot of interaction.)Addendum: Actually, I needed to simulate a side-effect that took some time. So I needed an object's method to sleep for a second. That would work like this:And then the code takes some time to run, just like the real method.You can use patch and MagicMock"},
{"body": "Title says it all.I notice that However, there should be some differences, since after all they are two different functions.np.average takes an optional weight parameter.  If it is not supplied they are equivalent.  Take a look at the source code.np.mean:np.average: always computes an arithmetic mean, and has some additional options for input and output (e.g. what datatypes to use, where to place the result). can compute a weighted average if the  parameter is supplied.There is another imporant difference that you must be aware: do not take in account masks, so compute the average over the whole set of data. takes in account masks, so compute the mean only over unmasked values.In your invocation, the two functions are the same. can compute a weighted average though.Doc links:  and "},
{"body": "I have questions about egg files in Python.I have much Python code organized by package and I'm trying to create egg files.\nI'm following , but they are very common.According to that, it seems I need to have a setup.py file.You are reading the wrong documentation. You want this: For #4, the closest thing to starting java with a jar file for your app is a new feature in Python 2.6, .Where myapp.zip is a zip containing a  file which is executed as the script file to be executed. Your package dependencies can also be included in the file:You can also execute an egg, but the incantation is not as nice: This puts the myapp.egg on the Python path and uses the -m argument to run a module. Your myapp.egg will likely look something like:And python will run  (you should check that  in your app for command line use).Egg files are just zip files so you might be able to add  to your egg with a zip tool and make it executable in python 2.6 and run it like  instead of the above incantation where the PYTHONPATH environment variable is set.More information on executable zip files including how to make them directly executable with a  can be found on .I think you should use  for distribution instead of egg now."},
{"body": "I've been trying to wrap my head around how threads work in Python, and it's hard to find good information on how they operate. I may just be missing a link or something, but it seems like the official documentation isn't very thorough on the subject, and I haven't been able to find a good write-up.From what I can tell, only one thread can be running at once, and the active thread switches every 10 instructions or so?Where is there a good explanation, or can you provide one? It would also be very nice to be aware of common problems that you run into while using threads with Python.Yes, because of the Global Interpreter Lock (GIL) there can only run one thread at a time. Here are some links with some insights about this:From the last link an interesting quote:If you want to use multi core,  defines an process based API to do real parallelization. The  also includes some interesting benchmarks.Python's a fairly easy language to thread in, but there are caveats.  The biggest thing you need to know about is the Global Interpreter Lock.  This allows only one thread to access the interpreter.  This means two things:  1)  you rarely ever find yourself using a lock statement in python and 2) if you want to take advantage of multi-processor systems, you have to use separate processes.  EDIT:  I should also point out that you can put some of the code in C/C++ if you want to get around the GIL as well.Thus, you need to re-consider why you want to use threads.  If you want to parallelize your app to take advantage of dual-core architecture, you need to consider breaking your app up into multiple processes.If you want to improve responsiveness, you should CONSIDER using threads.  There are other alternatives though, namely .  There are also some frameworks that you should look into:Below is a basic threading sample. It will spawn 20 threads; each thread will output its thread number. Run it and observe the order in which they print.As you have hinted at Python threads are implemented through time-slicing. This is how they get the \"parallel\" effect. In my example my Foo class extends thread, I then implement the  method, which is where the code that you would like to run in a thread goes. To start the thread you call  on the thread object, which will automatically invoke the  method...Of course, this is just the very basics. You will eventually want to learn about semaphores, mutexes, and locks for thread synchronization and message passing.Use threads in python if the individual workers are doing I/O bound operations. If you are trying to scale across multiple cores on a machine either find a good  framework for python or pick a different language.One easy solution to the GIL is the  module. It can be used as a drop in replacement to the threading module but uses multiple Interpreter processes instead of threads. Because of this there is a little more overhead than plain threading for simple things but it gives you the advantage of real parallelization if you need it.\nIt also easily scales to multiple physical machines.If you need truly large scale parallelization than I would look further but if you just want to scale to all the cores of one computer or a few different ones without all the work that would go into implementing a more comprehensive framework, than this is for you.Try to remember that the GIL is set to poll around every so often in order to do show the appearance of multiple tasks. This setting can be fine tuned, but I offer the suggestion that there should be work that the threads are doing or lots of context switches are going to cause problems.I would go so far as to suggest multiple parents on processors and try to keep like jobs on the same core(s)."},
{"body": "Besides the fact that node.js is written in JS and Tornado in Python, what are some of the differences between the two?  They're both non-blocking asynchronous web servers, right?  Why choose one over the other besides the language?The main advantage of node.js is that  so you don't have to worry much about blocking. There are async libraries for mysql, postgres, redis, etc. All is async by default.Python have a library for anything - but most of these libraries are not asynchronous. In order to take advantage of tornado (and not to block the process) special libraries for are necessary (e.g. you can't just 'pip install redis' and use it, you'll need something like ), and there are much less tornado libraries than node.js libraries. There is no async mysql tornado driver available at the moment, for example (or at least I'm not aware of it).But you can still use many python libraries with tornado (ones that doesn't do i/o), and tornado community is raising and filling the gaps. It is easier to write an app using node.js than using tornado in my experience. I personally switched to tornado from node.js because it fits into existing infrastructure of my python project better (integration between django site serving html pages and tornado server providing realtime features was quite painless). As Rich Bradshaw points out  is written in JS, which means you can keep the front end and the back end in the same language and possibly share some codebase. To me that is a huge potential benefit of .\nNode also comes with more asynchronous libraries out of the box it seems.V8 should make JS faster than Python , but it may not matter much, because both  and  (and most other web frameworks for that matter) use wrappers for native libraries. A lot of the Python standard library is written in C or can be replaced by a faster alternative, which mitigates potential differences even more.Web services are usually I/O bound, so that means we're spending the time waiting for the data store and not processing the data. That makes the synthetic speed difference between JS and Python irrelevant in many applications.node.js uses V8 which compiles into assembly code, tornado doesn't do that yet.Other than that (which doesn't actually seem to make much difference to the speed), it's the ecosystem. Do you prefer the event model of JS, or the way Python works? Are you happier using Python or JS libraries?Nodejs also has a seamless integration / implementation of websockets called Socket.io. It handles browsers supporting sockets - events and also has backward polling compatibility for older browsers. It is quite quick on development requiring a notification framework or some similar event based programming.I would suggested you go with NodeJS, if there is no personal pref to python. I like Python a lot, but for async I choose Tornado over node, and later had to struggle finding way to do a thing, or libraries with async support (like Cassandra has async in tests, but nowhere could I find way to use cqlengine with async. Had to choose Mongo since I already surpassed the deadline).\nIn terms of performance and async, Node far better than tornado."},
{"body": "I just switched to Pycharm and I am very happy about all the warnings and hints it provides me to improve my code. Except for this one which I don't understand:I know it is bad practice to access variable from the outer scope but what is the problem with shadowing the outer scope?Here is one example, where Pycharm gives me the warning message:No big deal in your above snippet, but imagine a function with a few more arguments and quite a few more lines of code. Then you decide to rename your  argument as  but miss one of the places it is used in the function's body... Now  refers to the global, and you start having weird behaviour - where you would have a much more obvious  if you didn't have a global name . Also remember that in Python everything is an object (including modules, classes and functions) so there's no distinct namespaces for functions, modules or classes. Another scenario is that you import function  at the top of your module, and use it somewhere in your function body. Then you add a new argument to your function and named it - bad luck - . Finally, built-in functions and types also live in the same namespace and can be shadowed the same way.None of this is much of a problem if you have short functions, good naming and a decent unittest coverage, but well, sometimes you have to maintain less than perfect code and being warned about such possible issues might help. and most answers here miss the point.It doesn't matter how long your function is, or how you name your variable descriptively (to hopefully minimize the chance of potential name collision).The fact that your function's local variable or its parameter happens to share a name in the global scope is completely irrelevant. And in fact, no matter how carefully you choose you local variable name, your function can never foresee \"whether my cool name  will also be used as a global variable in future?\". The solution? Simply don't worry about that! The correct mindset is to design your function to consume input from and only from its parameters in signature, that way you don't need to care what is (or will be) in global scope, and then shadowing becomes not an issue at all.In other words, shadowing problem only matters when your function need to use the same name local variable AND the global variable. But you should avoid such design in the first place. The OP's code does NOT really have such design problem. It is just that PyCharm is not smart enough and it gives out a warning just in case. So, just to make PyCharm happy, and also make our code clean, see this solution quoting from  to remove the global variable completely.This is the proper way to \"solve\" this problem, by fixing/removing your global thing, not adjusting your current local function.It depends how long the function is. The longer the function, the more chance that someone modifying it in future will write  thinking that it means the global. In fact it means the local but because the function is so long it's not obvious to them that there exists a local with that name.For your example function, I think that shadowing the global is not bad at all.A good workaround in some cases may be to move the vars + code to another function:"},
{"body": "I want to add \u2014the Python Debugger\u2014to my toolbox. What's the best way to get started?Here's a list of resources to get started with the Python debugger:Synopsis:Now run your script:"},
{"body": "I need to verify if a list is a subset of another - a boolean return is all I seek.\nIs testing equality on the smaller list after an intersection the fastest way to do this?\nPerformance is of utmost importance given the amount of datasets that need to be compared.\nAdding further facts based on discussions:  What would be the optimum solution given the scenario?The performant function Python provides for this is . It does have a few restrictions that make it unclear if it's the answer to your question, however.A list may contain items multiple times and has a specific order. A set does not. To achieve high performance sets work on  objects only. Are you asking about subset or subsequence (which means you'll want a string search algorithm)? Will either of the lists be the same for many tests? What are the datatypes contained in the list? And for that matter, does it need to be a list? Your other post  made the types clearer and did get a recommendation to use dictionary key views for their set-like funcitonality. In that case it was known to work because dictionary keys behave like a set (so much so that before we had sets in Python we used dictionaries). One wonders how the issue got less specific in three hours. Explanation: Generator creating booleans by looping through list  checking if that item is in list .   returns  if every item is truthy, else . There is also an advantage that  return False on the first instance of a missing element rather than having to process every item. Assuming the items are hashableIf you don't care about duplicate items eg.  and  then just use: will be the fastest way to do it. Checking the length before testing  will not improve speed because you still have O(N + M) items to iterate through and check. Try bitwise ANDHaven't profiled it yet.If list1 is in list 2:If you are asking if one list is \"contained\" in another list then:If you are asking if each element in listA has an equal number of matching elements in listB try:To check if one  is subset of ,  has  and . It works on  only and works great  the complexity of internal implementation is unknown. Reference: I came up with an algorithm to check if  is a subset of  with following remarks."},
{"body": "Suppose you have the following situationAs you can see, makeSpeak is a routine that accepts a generic Animal object. In this case, Animal is quite similar to a Java interface, as it contains only a pure virtual method. makeSpeak does not know the nature of the Animal it gets passed. It just sends it the signal \u201cspeak\u201d and leaves the late binding to take care of which method to call: either Cat::speak() or Dog::speak(). This means that, as far as makeSpeak is concerned, the knowledge of which subclass is actually passed is irrelevant.But what about Python? Let\u2019s see the code for the same case in Python. Please note that I try to be as similar as possible to the C++ case for a moment:Now, in this example you see the same strategy. You use inheritance to leverage the hierarchical concept of both Dogs and Cats being Animals. \nBut in Python, there\u2019s no need for this hierarchy. This works equally wellIn Python you can send the signal \u201cspeak\u201d to any object you want. If the object is able to deal with it, it will be executed, otherwise it will raise an exception. Suppose you add a class Airplane to both codes, and submit an Airplane object to makeSpeak. In the C++ case, it won\u2019t compile, as Airplane is not a derived class of Animal. In the Python case, it will raise an exception at runtime, which could even be an expected behavior.On the other side, suppose you add a MouthOfTruth class with a method speak(). In the C++ case, either you will have to refactor your hierarchy, or you will have to define a different makeSpeak method to accept MouthOfTruth objects, or in java you could extract the behavior into a CanSpeakIface and implement the interface for each. There are many solutions...What I\u2019d like to point out is that I haven\u2019t found a single reason yet to use inheritance in Python (apart of frameworks and trees of exceptions, but I guess that alternative strategies exist). you don\u2019t need to implement a base-derived hierarchy to perform polymorphically. If you want to use inheritance to reuse implementation, you can accomplish the same through containment and delegation, with the added benefit that you can alter it at runtime, and you clearly define the interface of the contained, without risking unintended side effects.So, in the end, the question stands: what's the point of inheritance in Python?: thanks for the very interesting answers. Indeed you can use it for code reuse, but I am always careful when reusing implementation. In general, I tend to do very shallow inheritance trees or no tree at all, and if a functionality is common I refactor it out as a common module routine and then call it from each object. I do see the advantage of having one single point of change (eg. instead of adding to Dog, Cat, Moose and so on, I just add to Animal, which is the basic advantage of inheritance), but you can achieve the same with a delegation chain (eg. a la JavaScript). I'm not claiming it's better though, just another way.I also found  on this regard.You are referring to the run-time duck-typing as \"overriding\" inheritance, however I believe inheritance has its own merits as a design and implementation approach, being an integral part of object oriented design. In my humble opinion, the question of whether you can achieve something otherwise is not very relevant, because actually you could code Python without classes, functions and more, but the question is how well-designed, robust and readable your code will be.I can give two examples for where inheritance is the right approach in my opinion, I'm sure there are more. First, if you code wisely, your makeSpeak function may want to validate that its input is indeed an Animal, and not only that \"it can speak\", in which case the most elegant method would be to use inheritance. Again, you can do it in other ways, but that's the beauty of object oriented design with inheritance - your code will \"really\" check whether the input is an \"animal\".Second, and clearly more straightforward, is Encapsulation - another integral part of object oriented design. This becomes relevant when the ancestor has data members and/or non-abstract methods. Take the following silly example, in which the ancestor has a function (speak_twice) that invokes a then-abstract function:Assuming  is an important feature, you don't want to code it in both Dog and Cat, and I'm sure you can extrapolate this example. Sure, you could implement a Python stand-alone function that will accept some duck-typed object, check whether it has a speak function and invoke it twice, but that's both non-elegant and misses point number 1 (validate it's an Animal). Even worse, and to strengthen the Encapsulation example, what if a member function in the descendant class wanted to use ?It gets even clearer if the ancestor class has a data member, for example  that is used by non-abstract methods in the ancestor like , but is initiated in the descendant class' constructor (e.g. Dog would initialize it with 4 whereas Snake would initialize it with 0). Again, I'm sure there are endless more examples, but basically every (large enough) software that is based on solid object oriented design will require inheritance.Inheritance in Python is all about code reuse.  Factorize common functionality into a base class, and implement different functionality in the derived classes.Inheritance in Python is more of a convenience than anything else.  I find that it's best used to provide a class with \"default behavior.\"Indeed, there is a significant community of Python devs who argue against using inheritance at all.  Whatever you do, don't just don't overdo it.  Having an overly complicated class hierarchy is a sure way to get labeled a \"Java programmer\", and you just can't have that.  :-)I think the point of inheritance in Python is not to make the code compile, it is for the real reason of inheritance which is extending the class into another child class, and to override the logic in the base class. However the duck typing in Python makes the \"interface\" concept useless, because you can just check if the method exist before invokation with no need to use an interface to limit the class structure.I think that it is very difficult to give a meaningful, concrete answer with such abstract examples...To simplify, there are two types of inheritance: interface and implementation. If you need to inherit the implementation, then python is not so different than statically typed OO languages like C++. Inheritance of interface is where there is a big difference, with fundamental consequences for the design of your software in my experience. Languages like Python does not force you to use inheritance in that case, and avoiding inheritance is a good point in most cases, because it is very hard to fix a wrong design choice there later. That's a well known point raised in any good OOP book.There are cases where using inheritance for interfaces is advisable in Python, for example for plug-ins, etc... For those cases, Python 2.5 and below lacks a \"built-in\" elegant approach, and several big frameworks designed their own solutions (zope, trac, twister). Python 2.6 and above has .In C++/Java/etc, polymorphism is caused by inheritance. Abandon that misbegotten belief, and dynamic languages open up to you.Essentially, in Python there is no interface so much as \"the understanding that certain methods are callable\".  Pretty hand-wavy and academic-sounding, no? It means that because you call \"speak\" you clearly expect that the object should have a \"speak\" method.  Simple, huh?  This is very Liskov-ian in that the users of a class define its interface, a good design concept that leads you into healthier TDD.So what is left is, as another poster politely managed to avoid saying, a code sharing trick.  You could write the same behavior into each \"child\" class, but that would be redundant.  Easier to inherit or mix-in functionality that is invariant across the inheritance hierarchy.  Smaller, DRY-er code is better in general.  It's not inheritance that duck-typing makes pointless, it's interfaces \u2014 like the one you chose in creating an all abstract animal class.If you had used an animal class that introduce some real behavior for its descendants to make use of, then dog and cat classes that introduced some additional behavior there would be a reason for both classes.  It's only in the case of the ancestor class contributing no actual code to the descendant classes that your argument is correct.Because Python can directly know the capabilities of any object, and because those capabilities are mutable beyond the class definition, the idea of using a pure abstract interface to \"tell\" the program what methods can be called is somewhat pointless.  But that's not the sole, or even the main, point of inheritance.You can get around inheritance in Python and pretty much any other language.  It's all about code reuse and code simplification though.  Just a semantic trick, but after building your classes and base classes, you don't even have to know what's possible with your object to see if you can do it.  Say you have d which is a Dog that subclassed Animal.If whatever the user typed in is available, the code will run the proper method.  Using this you can create whatever combination of Mammal/Reptile/Bird hybrid monstrosity you want, and now you can make it say 'Bark!' while flying and sticking out its forked tongue and it will handle it properly!  Have fun with it!I don't see much point in inheritance.Every time I have ever used inheritance in real systems, I got burned because it led to a tangled web of dependencies, or I simply realised in time that I would be a lot better off without it. Now, I avoid it as much as possible. I simply never have a use for it.James Gosling was once asked at a press conference a question along the lines: \"If you could go back and do Java differently, what would you leave out?\". His response was \"Classes\", to which there was laughter. However, he was serious and explained that really, it was not classes that were the problem but inheritance.I kind of view it like a drug dependency - it gives you a quick fix that feels good, but in the end, it messes you up. By that I mean that it is a convenient way to reuse code, but it forces an unhealthy coupling between child and parent class. Changes to the parent may break the child. The child is dependant on the parent for certain functionality and cannot alter that functionality. Therefore the functionality provided by the child is also tied to the parent - you can only have both.Better is to provide one single client facing class for an interface which implements the interface, using the functionality of other objects which are composed at construction time. Doing this via properly designed interfaces, all coupling can be eliminated and we provide a highly composable API (This is nothing new - most programmers already do this, just not enough). Note that the implementing class must not simply expose functionality, otherwise the client should just use the composed classes directly - it  do something new by combining that functionality.There is the argument from the inheritance camp that pure delegation implementations suffer because they require lots of 'glue' methods which simply pass along values through a delegation 'chain'. However, this is simply reinventing an inheritance-like design using delegation. Programmers with too many years of exposure to inheritance-based designs are particularly vulnerable to falling into this trap, as, without realising it, they will think of how they would implement something using inheritance and then convert that to delegation.Proper separation of concerns like the above code doesn't require glue methods, as each step is actually , so they are not really 'glue' methods at all (if they don't add value, the design is flawed).It boils down to this:Another small point is that op's 3'rd example, you can't call isinstance(). For example passing your 3'rd example to another object that takes and \"Animal\" type an calls speak on it. If you do it don't you would have to check for dog type, cat type, and so on. Not sure if instance checking is really \"Pythonic\", because of late binding. But then you would have to implement some way that the AnimalControl doesn't try to throw Cheeseburger types in the truck, becuase Cheeseburgers don't speak.Classes in Python are basically just ways of grouping a bunch of functions and data.. They are different to classes in C++ and such..I've mostly seen inheritance used for overriding methods of the super-class. For example, perhaps a more Python'ish use of inheritance would be..Of course cats aren't a type of dog, but I have this (third party)  class which works perfectly,  the  method which I want to override - this saves re-implementing  the entire class, just so it meows. Again, while  isn't a type of , but a cat does inherit a lot of attributes..A much better (practical) example of overriding a method or attribute is how you change the user-agent for urllib. You basically subclass  and change the version attribute ():Another manner exceptions are used is for Exceptions, when inheritance is used in a more \"proper\" way:..you can then catch  to catch all exceptions which inherit from it, or a specific one like  "},
{"body": "Assume I have a  object and I want to write it out as a CSV file. How can I do this?I know that I can write the  like this:But how can I include the fieldnames?Edit:\nIn 2.7 / 3.2 there is . Also, John Machin's answer provides a simpler method of writing the header row.\nSimple example of using the  method now available in 2.7 / 3.2:Instantiating DictWriter requires a fieldnames argument.\nFrom :  Put another way:  The Fieldnames argument is required because Python dicts are inherently unordered.\nBelow is an example of how you'd write the header and data to a file.\nNote:  statement was added in 2.6. If using 2.5: As @FM mentions in a comment, you can condense header-writing to a one-liner, e.g.:A few options:(1) Laboriously make an identity-mapping (i.e. do-nothing) dict out of your fieldnames so that csv.DictWriter can convert it back to a list and pass it to a csv.writer instance. (2) The documentation mentions \"the underlying  instance\" ... so just use it (example at the end).(3) Avoid the csv.Dictwriter overhead and do it yourself with csv.writerWriting data:orInstead of the  \"functionality\", I'd prefer to code it myself; that way you can report ALL \"extras\" with the keys and values, not just the first extra key. What is a real nuisance with DictWriter is that if you've verified the keys yourself as each dict was being built, you need to remember to use extrasaction='ignore' otherwise it's going to SLOWLY (fieldnames is a list) repeat the check:============Another way to do this would be to add before adding lines in your output, the following line :The zip would return a list of doublet containing the same value. This list could be used to initiate a dictionary."},
{"body": "In a text file, there is a string \"I don't like this\".However, when I read it into a string, it becomes \"I don\\xe2\\x80\\x98t like this\". I understand that \\u2018 is the unicode representation of \"'\". I use command to do the reading.Now, is it possible to read the string in such a way that when it is read into the string, it is \"I don't like this\", instead of \"I don\\xe2\\x80\\x98t like this like this\"?Second edit: I have seen some people use mapping to solve this problem, but really, is there no built-in conversion that does this kind of ANSI to unicode ( and vice versa) conversion?Ref: : I'm assuming that your intended goal is just to be able to read the file properly into a string in Python. If you're trying to convert to an ASCII string from Unicode, then there's really no direct way to do so, since the Unicode characters won't necessarily exist in ASCII.If you're trying to convert to an ASCII string, try one of the following: There are a few points to consider.A \\u2018 character may appear only as a fragment of representation of a unicode string in Python, e.g. if you write:Now if you simply want to print the unicode string prettily, just use unicode's  method:To make sure that every line from any file would be read as unicode, you'd better use the  function instead of just , which allows you to specify file's encoding:But it really is \"I don\\u2018t like this\" and not \"I don't like this\". The character u'\\u2018' is a completely different character than \"'\" (and, visually, should correspond more to '`').If you're trying to convert encoded unicode into plain ASCII, you could perhaps keep a mapping of unicode punctuation that you would like to translate into ASCII.There are an awful lot of , however, but I suppose you can count on only a few of them actually being used by whatever application is creating the documents you're reading.Leaving aside the fact that your text file is broken (U+2018 is a left quotation mark, not an apostrophe): iconv can be used to transliterate unicode characters to ascii.You'll have to google for \"iconvcodec\", since the module seems not to be supported anymore and I can't find a canonical home page for it.Alternatively you can use the  command line utility to clean up your file:There is a possibility that somehow you have a non-unicode string with unicode escape characters, e.g.:This actually happened to me once before. You can use a  codec to decode the string to unicode and then encode it to any format you want:Actually, U+2018 is the Unicode representation of the special character \u2018 . If you want, you can convert instances of that character to U+0027 with this code:In addition, what are you using to write the file?  should return a string that looks like this:If it's returning  string, the file is being written incorrectly:This is Pythons way do show you unicode encoded strings. But i think you should be able to print the string on the screen or write it into a new file without any problems."},
{"body": "I am new to Python and am working on a Linux machine (Ubuntu 10.10). It is running python 2.6, but I'd like to run 2.7 as it has features I want to use. I have been urged to not install 2.7 and set that as my default python. My question is, how can I install 2.7 and run it side by side with 2.6?I did it with  on my Ubuntu 10.10 machine..I recently backported Python 2.7 to Debian squeeze. Since Ubuntu 10.10 is newer than Debian squeeze, if you can do it on squeeze, you can certainly do it on Ubuntu. I don't have access to a Ubuntu 10.10 system. If I set one up, I'll test on it, and update this answer. So, here instead is a brief sketch of what I did on Debian.First, a general and obvious comment, but something that is easily overlooked. One should not take the listed build dependencies of a Debian package too seriously. They may be far more specific than needed. For example, software like Python, which is designed to be portable and run over a wide array of systems, is unlikely to build depend on very specific versions of software. The runtime dependencies can be adjusted as well, but this should be done with more caution. However, runtime dependencies are mostly generated dynamically based on software that is already on this system, so usually that is not a big issue.Selecting the testing version we getLooking at , we see the following build dependency lines.Build-Depends: debhelper (>= 5), quilt, autoconf, libreadline-dev, libtinfo-dev, libncursesw5-dev (>= 5.3), tk8.5-dev, zlib1g-dev, blt-dev (>= 2.4z), libssl-dev, libexpat1-dev, sharutils, libbz2-dev, libbluetooth-dev [linux-any], locales [!armel !avr32 !hppa !ia64 !mipsel], libsqlite3-dev, libffi-dev (>= 3.0.5), mime-support, libgpm2 [linux-any], netbase, lsb-release, bzip2, libdb4.8-dev, gdb, python, help2man\nBuild-Depends-Indep: python-sphinx\nBuild-Conflicts: tcl8.3-dev, tk8.3-dev, tcl8.4-dev, tk8.4-dev, python2.7-xml, python-xml, autoconf2.13, libncurses5-devMost of this is easily satisfied on squeeze. With the handy utility  we get on my machineWe see that everything except  is available in squeeze. I do have the squeeze backport versions of  and , but both of these are also available for debian squeeze in versions satisfying the build requirements.Observe also that I have libncurses5-dev installedBoth of these packages correspond to the source package . Observe that  in fact replaces .One would not expect python 2.7 to develop on such a specific version of curses, and in fact it doesn't. However, if you try to build the packages without satisfying the dependency you getSo, it is necessary to edit . Note that you also need to similarly edit the file , otherwise the \nfile will be incorrectly regenerated from . The simplest thing to do is just remove \nfrom the  line and  from the  line, and then run \nagain. If you are going to have this package installed alongside the standard default Python 2.6 packages on Debian squeeze, you\nalso need to remove the two linesThose lines are there because 2.7 includes the  functionality.\nIf 2.7 is the default python, then  is no longer necessary. \nHowever, if one is installing 2.7 as a non-default Python, that reasoning does not\napply, and  is still needed by 2.6.This should build successfully, and result in the following list of binary packages.Finally, one can install the binary packages withSometimes  can be a little difficult about satisfying dependencies when they are all installed at once, so you might have to run  afterwards if you get dependency errors, or alternatively install the packages in smaller groups.Well if the only thing you need is argparse (saw that in one of your comments!) you could just do :This is not exactly an answer to the exact question :-) , but indeed if you are only missing a few feature, many 2.7 features actually come from independent projects and/or some compatibility packages can be found, eg:The list of  is admittedly longer, but most of the other new features are probably not a big miss, and in exchange you do not mess around with multiple python installations on your box. Otherwise go with pythonbrew :-)ubuntu 12.04Install dependencies:Add the repo:Update the repo index:Install Python 3.3:ubuntu 12.04 > moreInstalling the dependencies:Download and compile python:Some nice touches to install a py command by creating a symlink:Just download Python 2.7 from  and build it.You can use  to create distinct Python environments. Just being newsy, but what does Python 2.7 have that you need?Another option is to install  if you do not want to compile things yourself. It includes a  as well."},
{"body": "The title pretty much summarizes what I'd like to have happen.Here is what I have, and while the program doesn't blow up on a nonpositive integer, I want the user to be informed that a nonpositive integer is basically nonsense.And the output:Output with a negative:Now, obviously I could just add an if to determine  is negative, but I was curious if there was a way to trap it at the  level, so as to take advantage of the automatic usage printing.Ideally, it would print something similar to this:Like so:For now I'm doing this, and I guess I'm happy:This should be possible utilizing . You'll still need to define an actual method that decides this for you:This is basically just an adapted example from the  function in the  on . would be the recommended option to handle conditions/checks, as in Yuushi's answer.In your specific case, you can also use the  parameter if your upper limit is also known:The quick and dirty way, if you have a predictable max as well as min for your arg, is use  with a rangeA simpler alternative, especially if subclassing , is to initiate the validation from inside the  method.Inside such a subclass:This technique may not be as cool as a custom callable, but it does the job.About :Credit: "},
{"body": "In Python, is there any difference between creating a generator object through a  versus using the  statement?Using :Using :Both functions return generator objects, which produce tuples, e.g. (0,0), (0,1) etc.Any advantages of one or the other? Thoughts?Thanks everybody! There is a lot of great information and further references in these answers!There are only slight differences in the two.  You can use the  module to examine this sort of thing for yourself.   My first version decompiled the generator expression created at module-scope in the interactive prompt.  That's slightly different from the OP's version with it used inside a function.  I've modified this to match the actual case in the question.As you can see below, the \"yield\" generator (first case) has three extra instructions in the setup, but from the first  they differ in only one respect:  the \"yield\" approach uses a  in place of a  inside the loop.  The  is  than , so it makes the \"yield\" version slightly faster than the generator expression for large enough values of  (the outer loop) because the value of  is loaded slightly faster on each pass.  For smaller values of  it would be slightly slower because of the extra overhead of the setup code.It might also be worth pointing out that the generator expression would usually be used inline in the code, rather than wrapping it with the function like that.  That would remove a bit of the setup overhead and keep the generator expression slightly faster for smaller loop values even if  gave the \"yield\" version an advantage otherwise.In neither case would the performance difference be enough to justify deciding between one or the other.  Readability counts far more, so use whichever feels most readable for the situation at hand.  In this example, not really. But  can be used for more complex constructs -  it can accept values from the caller as well and modify the flow as a result. Read  for more details (it's an interesting technique worth knowing).Anyway, the best advice is .P.S. Here's a simple coroutine example from :There is no difference for the kind of simple loops that you can fit into a generator expression.  However yield can be used to create generators that do much more complex processing.  Here is a simple example for generating the fibonacci sequence:Using  is nice if the expression is more complicated than just nested loops. Among other things you can return a special first or special last value. Consider:In usage, note a distinction between a generator object vs a generator function.A generator object is use-once-only, in contrast to a generator function, which can be reused each time you call it again, because it returns a fresh generator object.Generator expressions are in practice usually used \"raw\", without wrapping them in a function, and they return a generator object.E.g.:which outputs:Compare with a slightly different usage:which outputs:And compare with a generator expression:which also outputs:When thinking about iterators, the  module:For performance, consider Yes there is a difference.For the generator expression ,  is called when the expression is .When using  and  to create a generator, as in: is not yet called.  It will be called only when iterating on  (and might not be called at all).Taking this iterator as an example:This code:while:Since most iterators do not do a lot of stuff in , it is easy to miss this behavior.  A real world example would be Django's , which  and  might take a lot of time, while  followed by  would return immediately.For more info and the formal definition refer to .There is a difference that could be important in some contexts that hasn't been pointed out yet. Using  prevents you from using  for something else than .This means this code is ill-formed (and feeding it to an interpreter will give you an ):On the other hand, this code works like a charm:"},
{"body": "It's more about python list comprehension syntax. I've got a list comprehension that produces list of odd numbers of a given range:This makes a filter - I've got a source list, where I remove even numbers (). I'd like to use something like if-then-else here. Following code fails:There is a python expression like if-else:How to use it inside a list comprehension? is the syntax for the expression you're returning for each element. Thus you need:The confusion arises from the fact you're using a  in the first example, but not in the second. In the second example you're only  each value to another, using a ternary-operator expression.With a filter, you need:Without a filter you need:and in your second example, the expression is a \"complex\" one, which happens to involve an .Just another solution, hope some one may like it :Using: [False, True][Expression]You can do that with list comprehension too:I was able to do this"},
{"body": "Is there an easy way to sort the letters in a string alphabetically in Python?So for:I would like to return:You can do: returns a list, so you can make it a string again using :which joins the items of  together with an empty string  in between each item.Sorted() solution can give you some unexpected results with other strings. List of other solutions:If you want to get rid of the space in the result, add strip() function in any of those mentioned cases:"},
{"body": "When using setuptools/distribute, I can not get the installer to pull in any  files. Everything I've read says that the following is the correct way to do it. Can someone please advise?where  is the location of the data files.I realize that this is an old question...but for people finding there way here via Google:   is a low-down, .  It is only used when building  packages () but  when building source packages ().  This is, of course, ridiculous -- one would expect that building a source distribution would result in a collection of files that could be sent to someone else to built the binary distribution.In any case, using  will work  for binary and for source distributions.I just had this same issue. The solution, was simply to remove .After , I realized that  aims to include files from , as opposed to merely \"include package data\" as the name implies. From the docs:Taking that argument out fixed it, which is coincidentally why it also worked when you switched to distutils, since it doesn't take that argument.Following @Joe 's recommendation to remove the  line also worked for me. To elaborate a bit more, I have   file. I use Git and not CVS.Repository takes this kind of shape::I run  for a source distrib (haven't tried binary).And when inside of a brand new virtual environment, I have a , file, \nand I useAnd other than everything getting installed to my virtual environment's , those special data files get installed to  and . worked for me.If you use git, remember to include  in . Far less boring than having a  or including all path in  ( in my case it's a django app with all kind of statics )( pasted the comment I made, as  mentioned it's actually helpful as is )I solved this by switching to distutils. Looks like distribute is deprecated and/or broken.Moving the folder containing the package data into to module folder solved the problem for me.See this question: Ancient question and yet... package management of python really leaves a lot to be desired. So I had the use case of installing using pip locally to a specified directory and was surprised both package_data and data_files paths did not work out. I was not keen on adding yet another file to the repo so I ended up leveraging data_files and setup.py option --install-data; something like this"},
{"body": "I have a transparent png image \"foo.png\"\nand I've opened another image with now what i need is to merge foo.png with foo2.png.( foo.png contains some text and I want to print that text on foo2.png )First parameter to  is the image to paste. Second are coordinates, and the secret sauce is the third parameter. It indicates a  that will be used to paste the image. If you pass a image with transparency, then the alpha channel is used as mask.Check the . does not work as expected when the background image also contains transparency. You need to use real .Pillow 2.0 contains an  function that does this. EDIT: Both images need to be of the type RGBA. So you need to call  if they are paletted, etc.. If the background does not have an alpha channel, then you can use the regular paste method (which should be faster). As  already pointed out,  doesn't work properly, when source  destination both contain alpha.Consider the following scenario:Two test images, both contain alpha:\nCompositing image using  like so:produces the following image (the alpha part of the overlayed red pixels is completely taken from the 2nd layer. The pixels are not blended correctly):Compositing image using  like so:produces the following (correct) image:"},
{"body": "Is it possible to have a one line command in python to do a simple ftp server? I'd like to be able to do this as quick and temporary way to transfer files to a linux box without having to install a ftp server. Preferably a way using built in python libraries so there's nothing extra to install.Obligatory  example:And probably useful:Check out  from Giampaolo Rodola.  It is one of the very best ftp servers out there for python.  It's used in google's chromium (their browser) and bazaar (a version control system).  It is the most complete implementation on Python for  (aka: FTP server implementation spec).From the commandline:Alternatively 'my_server.py':There's more examples on the website if you want something more complicated.To get a list of command line options:Note, if you want to override or use a standard ftp port, you'll need admin privileges (e.g. sudo).Why don't you instead use a one-line  server?will serve the contents of the current working directory over HTTP on port 8000.If you use Python 3, you should instead writeSee the  module docs for 2.x and the  docs for 3.x.By the way, in both cases the port parameter is optional.The answers above were all assuming your Python distribution would have some third-party libraries in order to achieve the \"one liner python ftpd\" goal, but that is not the case of what @zio was asking, also, SimpleHTTPServer involves web broswer for downloading files, it's not quick enough.  Python can't do ftpd by itself, but you can use :   (netcat) is basically a built-in tool from any UNIX-like systems (even embedded systems), so it's prefect for \"\".step-1, on the receiver side, run:\n\nwill listen on port 12345, waiting for data.  step-2, on the sender side:\n  You can also put  in the middle to monitor the progress of transferring:\n  After the transferring finished, both sides of  will quit automatically,  and job done.For pyftpdlib users. I found this on the pyftpdlib website. This creates anonymous ftp with write access to your filesystem so please use with due care. More features are available under the hood for better security so just go look:Might be helpful for those that tried using the deprecated method above. Install:Then the code:Get deeper:Good list of tools atI've used woof myself on a number of occasions. Very nice.I dont know about a one-line FTP server, but if you doIt'll run an HTTP server on 0.0.0.0:8000, serving files out of the current directory. If you're looking for a way to quickly get files off a linux box with a web browser, you cant beat it."},
{"body": "Given a list of strings, I want to sort it alphabetically and remove duplicates. I know I can do this:but I don't know how to retrieve the list members from the hash in alphabetical order.I'm not married to the hash, so any way to accomplish this will work. Also, performance is not an issue, so I'd prefer a solution that is expressed in code clearly to a fast but more opaque one.A list can be sorted and deduplicated using built-in functions:If your input is already sorted, then there may be a simpler way to do it:If you want to keep order of the original list, just use OrderedDict with  as values.In Python2:In Python3 it's even simpler:If you don't like iterators (zip and repeat) you can use a generator (works both in 2 & 3):If it's clarity you're after, rather than speed, I think this is very clear:It's O(n^2) though, with the repeated use of not in for each element of the input list.> but I don't know how to retrieve the list members from the hash in alphabetical order.Not really your main question, but for future reference Rod's answer using  can be used for traversing a 's keys in sorted order:and also because 's are ordered by the first member of the tuple, you can do the same with :For the string data"},
{"body": "I'm trying to remove the last 3 characters from a string in python, I don't know what these characters are so I can't use , I also need to remove any white space and convert to upper-casean example would be:This works and gives me BS12 which is what I want, however if the last 4th & 3rd characters are the same I loose both eg if  I just get examples of  could be:The string could be 6 or 7 characters and I need to drop the last 3 (assuming no white space)Any tips?It doesn't work as you expect because strip is character based. You need to do this instead:Removing any and all whitespace:Removing last three characters:Converting to capital letters:All of that code in one line:You might have misunderstood rstrip slightly, it strips not a string but any character in the string you specify.Like this:So instead, just use (after replacing whitespace with nothing)I try to avoid regular expressions, but this appears to work:What's wrong with this?Aren't you performing the operations in the wrong order? You requirement seems to be It some what depends on your definition of whitespace. I would generally call whitespace to be spaces, tabs, line breaks and carriage returns. If this is your definition you want to use a regex with \\s to replace all whitespace charactors:"},
{"body": " is an extremely useful library, and from using it I've found that it's capable of handling matrices which are quite large (10000 x 10000) easily, but begins to struggle with anything much larger (trying to create a matrix of 50000 x 50000 fails). Obviously, this is because of the massive memory requirements.Is there is a way to create huge matrices natively in NumPy (say 1 million by 1 million) in some way (without having several terrabytes of RAM)?PyTables and NumPy are the way to go.PyTables will store the data on disk in HDF format, with optional compression. My datasets often get 10x compression, which is handy when dealing with tens or hundreds of millions of rows. It's also very fast; my 5 year old laptop can crunch through data doing SQL-like GROUP BY aggregation at 1,000,000 rows/second. Not bad for a Python-based solution!Accessing the data as a NumPy recarray again is as simple as:The HDF library takes care of reading in the relevant chunks of data and converting to NumPy.s are meant to live in memory.  If you want to work with matrices larger than your RAM, you have to work around that.  There are at least two approaches you can follow:You should be able to use numpy.memmap to memory map a file on disk.  With newer python and 64-bit machine, you should have the necessary address space, without loading everything into memory.  The OS should handle only keep part of the file in memory.To handle sparse matrices, you need the  package that sits on top of  -- see  for more details about the sparse-matrix options that  gives you.Stefano Borini's  got me to look into how far along this sort of thing already is.    It appears to do basically what you want.  HDF5 will let you store very large datasets, and then access and use them in the same ways NumPy does.  Make sure you're using a 64-bit operating system and a 64-bit version of Python/NumPy. Note that on 32-bit architectures you can address typically 3GB of memory (with about 1GB lost to memory mapped I/O and such). With 64-bit and things arrays larger than the available RAM you can get away with virtual memory, though things will get slower if you have to swap. Also, memory maps (see numpy.memmap) are a way to work with huge files on disk without loading them into memory, but again, you need to have a 64-bit address space to work with for this to be of much use. PyTables will do most of this for you as well.It's a bit alpha, but  seems to be working on solving this. Are you asking how to handle a 2,500,000,000 element matrix without terabytes of RAM?  The way to handle 2 billion items without 8 billion bytes of RAM is by not keeping the matrix in memory.That means much more sophisticated algorithms to fetch it from the file system in pieces.Usually when we deal with large matrices we implement them as .I don't know if numpy supports sparse matrices but I found  instead.As far as I know about numpy, no, but I could be wrong. I can propose you this alternative solution: write the matrix on the disk and access it in chunks. I suggest you the HDF5 file format. If you need it transparently, you can reimplement the ndarray interface to paginate your disk-stored matrix into memory. Be careful if you modify the data to sync them back on the disk. Sometimes one simple solution is using a custom type for your matrix items. Based on the range of numbers you need, you can use a manual  and specially smaller for your items. Because Numpy considers the largest type for object by default this might be a helpful idea in many cases. Here is an example:And with custom type:"},
{"body": "I've tried lots of solution that posted on the net, they don't work.So the system can find the _imaging but still can't use truetype fontRaises this error:Your installed PIL was compiled without libfreetype.You can get precompiled installer of PIL (compiled with libfreetype) here (and many other precompiled Python C Modules):On Ubuntu, you need to have libfreetype-dev installed before compiling PIL.i.e.PS! Running pip install as sudo will usually install packages to /usr/local/lib on most Ubuntu versions. You may consider to install Pil in a virtual environment (virtualenv or venv) in a path owned by the user instead.You may also consider installing pillow instead of pil, which I believe is API compatible: .The following worked for me on Ubuntu 14.04.1 64 bit:Then, in the virtualenv:solution for CentOS 6 (and probably other rpm based):In OS X, I did this to solve the problem:Basically, you need to install freetype before installing PIL.If you're using  on OS X it's just a matter of:Worked for Ubuntu 12.10:For OS X (I'm running 10.6 but should work for others) I was able to get around this error using the advice from . Basically you need to install a couple of the dependencies then reinstall PIL.For me none of the solutions posted here so far has worked. I found another solution here: First install the dev packages:Then create some symlinks:Afterwards PIL should compile just fine:The followed works on ubuntu 12.04:when your see \"-- JPEG support avaliable\" that means it works.But, if it still doesn't work when your edit your jpeg image, check the python path!!\nMy python path missed , so I edit the  add the following code to this file:then, finally, it works!!I used homebrew to install freetype and I have the following in /usr/local/lib:libfreetype.6.dylib\nlibfreetype.a\nlibfreetype.dylibBut the usual:Does not work for me, so I used:Ubuntu 11.10 installs zlib and freetype2 libraries following the multi-arch spec (e.g. ). You may use PIL setup environment variables so it can find them. However it only works on PIL versions beyond the pil-117 tag.Since your multi-arch path may be different (x86-64), it's preferable to install the  packages and use  to retrieve the correct path.Another way given by Barry on Pillow's setup.py is to use  to obtain the proper library directory suffix.See In my Mac, the following steps in terminal works:hopes it works for you. Good luck!\u3010solved\u3011\nIn my ubuntu12.04, after I installed python-imaging using apt-get, it works."},
{"body": "I have a long Hex string that represents a series of values of different types. I wish to convert this Hex String into a byte array so that I can shift each value out and convert it into its proper data type. Suppose your hex string is something likeor since Python 2.6:However, it's possible that by \u201chex string\u201d you just mean a string with unprintable characters, such as In that case, use the options above without the  part.There is a built-in function in bytearray that does what you intend.It returns a bytearray and it reads hex strings with or without space separator.provided I understood correctly, you should look for binascii.unhexlifyYou should be able to build a string holding the binary data using something like:This is probably not the fastest way (many string appends), but quite simple using only core Python.Assuming you have a byte string like soand you know the amount of bytes and their type you can also use this approachAs I specified little endian (using the '<' char) at the start of the format string the function returned the decimal equivalent.0x12 = 180x45 = 690xAB00 = 43776B is equal to one byte (8 bit) unsignedH is equal to two bytes (16 bit) unsignedYou can specify more than one byte and the endian of the valuesYou really need to know the type and length of data your dealing withA good one liner is:This will iterate over each char in the string and run it through the ord() function.  Only tested on python 2.6, not too sure about 3.0+.-JoshFor back conversion use \nbinascii.b2a_hex(obj)"},
{"body": "Sometimes it is useful to \"clone\" a row or column vector to a matrix. By cloning I mean converting a row vector such asInto a matrixor a column vector such asintoIn matlab or octave this is done pretty easily:I want to repeat this in numpy, but unsuccessfullyWhy wasn't the first method (In[16]) working? Is there a way to achieve this task in python in a more elegant way?Here's an elegant, Pythonic way to do it:the problem with  seems to be that the transpose has no effect for an array. you're probably wanting a matrix instead:Use :or for repeating columns:First note that with numpy's  operations it's usually not necessary to duplicate rows and columns.  See  and  for descriptions.But to do this,  and  are probably the best wayThis example is for a row vector, but applying this to a column vector is hopefully obvious.  repeat seems to spell this well, but you can also do it via multiplication as in your exampleI think using the broadcast in numpy is the best, and fasterI did a compare as followingabout 15 times faster using broadcastYou can use tile will generate the reps of the vector and reshape will give it the shape you want"},
{"body": "At some point in the last few days, Matplotlib stopped working for me on OS X. Here's the error I get when trying to :The only system change I can think of was the Apple-forced NTP update and maybe some permission changes I did in /usr/local to get Brew working again.I tried reinstalling both Matplotlib and Python-dateutil via Pip, but this did not help.  Also tried a reboot.  I'm running Python 2.7.6, which is located in /usr/bin/python.  I'm running Yosemite (OS X 10.10.1).I had the same error message this afternoon as well, although I did recently upgrade to Yosemite. I'm not totally sure I understand why reverting dateutil to a previous version works for me, but since running the above I'm having no trouble (I generally use pyplot inline in an ipython notebook).This problem is fixed in the latest  and  versions. However, in OS X, even if you update your  to the latest version, you might not actually update it correctly. This is what happened to me:After doing a , the new  module was installed in . However, when I loaded  in a python 2.7 terminal, and checked its path, this is what I got:So, python was using an old version of , which I removed by typing:This fixed this issue for me.It is possible that you have a perfectly installed version of any packages you have installed, but the version used by default is not the one you want. You can see the list of paths that python search from in order to find its packages as follows:In order to let python search first the most updated version of certain package, instead of removing the system version, what can be done is to set the system variable  in the ~/.bash_profile (or ~/.bashrc if linux) config file to the path where the new packages are installed: An alternative is to modify the python path inside your python script by adding the path at the beginning of the path list:This needs to be done for every script you need a certain package version. You might want for some reason use an older version that you have installed.\nBTW all my installations with easy_install, or pip, or from sources go to /Library/Python/2.7/site-packages\nThis worked en EL Capitan, and now also in macOS Sierra (10.12.2)Installing the  did not work for me.But a quick-and-dirty workaround did work! I replace  in python 2.7 with the  from python 3.4 (virtualenv). Since, I have the problem in 2.7 but not 3.4.I had the same problem again after reinstalling python (and after upgrading to El Capitan). Un-obvious thing is that this error occurs only in the  shell and notebook (when I do ) but works fine from a Python shell. So a better solution (that did work in my case) without a dirty work-around is to force install both  and . Here is what I did to have this fixed :"},
{"body": "I have a program that reads an xml document from a socket. I have the xml document stored in a string which I would like to convert directly to a Python dictionary, the same way it is done in Django's  library.Take as an example:Then  would look like Thanks in advance,\nZe MariaThis is a great module that someone created. I've used it several times. \nHere is the code from the website just in case the link goes bad. Example usage://Or, if you want to use an XML string: (full disclosure: I wrote it) does exactly that:The following XML-to-Python-dict snippet parses entities as well as attributes following . It is the most general solution handling all cases of XML.It is used:The output of this example (as per above-linked \"specification\") should be:Not necessarily pretty, but it is unambiguous, and simpler XML inputs result in simpler JSON. :)If you want to do the , emit an , you can use:This lightweight version, while not configurable, is pretty easy to tailor as needed, and works in old pythons.  Also it is rigid - meaning the results are the same regardless of the existence of attributes.So:Results in:The most recent versions of the PicklingTools libraries (1.3.0 and 1.3.1) support tools for converting from XML to a Python dict.The download is available here:  There is quite a bit of documentation for the converters : the documentation describes in detail all of the decisions and issues that will arise when converting between XML and Python dictionaries (there are a number of edge cases: attributes, lists, anonymous lists, anonymous dicts, eval, etc. that most converters don't handle).  In general, though,\nthe converters are easy to use.  If an 'example.xml' contains:Then to convert it to a dictionary:There are tools for converting in both C++ and Python: the C++ and Python do indentical conversion, but the C++ is about 60x fasterDisclaimer:\nThis modified XML parser was inspired by  \nThe original XML parser works for most of simple cases. However, it didn't work for some complicated XML files. I debugged the code line by line and finally fixed some issues. If you find some bugs, please let me know. I am glad to fix it.The easiest to use XML parser for Python is ElementTree (as of 2.5x and above it is in the standard library xml.etree.ElementTree). I don't think there is anything that does exactly what you want out of the box. It would be pretty trivial to write something to do what you want using ElementTree, but why convert to a dictionary, and why not just use ElementTree directly.Here's a link to an  - and the code in case it disappears again.The code from  works well, but if there are multiple elements that are the same at a given place in the hierarchy it just overrides them.I added a shim between that looks to see if the element already exists before self.update().  If so, pops the existing entry and creates a lists out of the existing and the new.  Any subsequent duplicates are added to the list.Not sure if this can be handled more gracefully, but it works:From @K3---rnc  (the best for me) I've added a small modifications to get an OrderedDict from an XML text (some times order matters):Following @K3---rnc example, you can use it:Hope it helps ;)At one point I had to parse and write XML that only consisted of elements without attributes so a 1:1 mapping from XML to dict was possible easily. This is what I came up with in case someone else also doesnt need attributes:@dibrovsd: Solution will not work if the xml have more than one tag with same nameOn your line of thought, I have modified the code a bit and written it for general node instead of root:You can do this quite easily with lxml. First install it:Here is a recursive function I wrote that does the heavy lifting for you:The below variant preserves the parent key / element:If you want to only return a subtree and convert it to dict, you can use  to get the subtree and then convert it: See the lxml docs . I hope this helps!I have a recursive method to get a dictionary from a lxml element"},
{"body": "Let's take an exampleI wanted to append value in list 'b' but the value of list 'a' have also changed.\n\nI think I have little idea why its like this (python passes lists by reference).\n\nMy question is \"how can I pass it by value so that appending 'b' does't change values in 'a' ?\"As answered in the :To copy a list you can use  or . In both cases a new object is created.\nThese two methods, however, have limitations with collections of mutable objects as inner objects keep their references intact:If you want a full copy of your objects you need Also, you can do:This will work for any sequence, even those that don't support indexers and slices...In terms of performance my favorite answer would be:Check how the related alternatives compare with each other in terms of performance:To create a copy of a list do this:When you do  you simply create another pointer to the same memory of ,\nthat's why when you append to  ,  changes too.You need to create  of  and that's done like this:If you want to copy a one-dimensional list, useHowever, if a is a 2 dimensional list, this is not going to work for you. That is, any changes in a will also be reflected in b. In that case, useI found that we can use extend() to implement the function of copy()As mentioned by phihag in his answer,will work for your case since slicing a list creates a new memory id of the list (meaning you are no longer referencing the same object in your memory and the changes you make to one will not be reflected in the other.)However, there is a slight problem.  If your list is multidimensional, as in lists within lists, simply slicing will not solve this problem.  Changes made in the higher dimensions, i.e. the lists within the original list, will be shared between the two.Do not fret, there is a solution.  The module copy has a nifty copying technique that takes care of this issue.will copy a list with a new memory id no matter how many levels of lists it contains!See ."},
{"body": "Today I was bitten again by mutable default arguments after many years. I usually don't use mutable default arguments unless needed, but I think with time I forgot about that.  Today in the application I added tocElements=[] in a PDF generation function's argument list and now \"Table of Contents\" gets longer and longer after each invocation of \"generate pdf\". :)What else should I add to my list of things to MUST avoid?Don't :Do : will automate most iteration operations for you.Use  if you really need both the index and the element. Pythonistas usually say \"It's easier to ask for forgiveness than permission\".Don't :Do :Or even better with python 2.6 / 3:It is much better because it's much more generical. You can apply \"try / except\" to almost anything. You don't need to care about what to do to prevent it, just about the error you are risking. Python is dynamically typed, therefore checking for type makes you lose flexibility. Instead, use duck typing by checking behavior. E.G, you expect a string in a function, then use str() to convert any object in a string. You expect a list, use list() to convert any iterable in a list.Don't :Do :Using the last way, foo will accept any object. Bar will accept strings, tuples, sets, lists and much more. Cheap DRY :-)Just don't. You would cry.This is tricky, but it will bite you as your program grows. There are old and new classes in Python 2.x. The old ones are, well, old. They lack some features, and can have awkward behavior with inheritance. To be usable, any of your class must be of the \"new style\". To do so, make it inherit from \"object\" :Don't :Do :In Python 3.x all classes are new style so you don't need to do that.People coming from other languages find it tempting because that what you do the job in Java or PHP. You write the class name, then list your attributs and give them a default value. It seems to work in Python, however, this doesn't work the way you think.Doing that will setup class attributes (static attributes), then when you will try to get the object attribute, it will gives you its value unless it's empty. In that case it will return the class attributes. It implies two big hazards :Don't (unless you want static) :Do : When you need a population of arrays you might be tempted to type something like this:And sure enough it will give you what you expect when you look at itBut don't expect the elements of your population to be seperate objects:Unless this is what you need...It is worth mentioning a workaround:Python Language Gotchas -- things that fail in very obscure waysPython Design GotchasMutating a default argument:The default value is evaluated only once, and not every time the function is called. Repeated calls to  would return , , , ...If you want to mutate bar do something like this:Or, if you like arguments to be final:I don\u00b4t know whether this is a common mistake, but while python doesn\u00b4t has increment and decrement operators, double signs are allowed, soandis syntactically correct code, but doesn\u00b4t do anything.Rolling your own code before looking in the standard library. For example, writing this:When you could just use this:Examples of frequently overlooked modules (besides ) include:The another one could be to avoid using keywords as your own identifiers. Also it's always good to not use BTW, wouldn't it be better to post it to community wiki?Surprised that nobody said thisMix tab and spaces when indenting.really, it's a killer. believe me.  if it runs.If you're coming from C++, realize that variables declared in a class definition are static.  You can initialize nonstatic members in the  method.Example:Not using functional tools.  This isn't just a mistake from a style standpoint, it's a mistake from a speed standpoint because a lot of the functional tools are optimized in C.This is the most common example:The correct way to do it:The just as correct way to do it:And if you only need the processed items available one at a time, rather than all at once, you can save memory and improve speed by using the iterable equivalentsImporting  and using the full regular expression approach to string matching/transformation, when perfectly good  exist for every common operation (e.g. capitalisation, simple matching/searching).Last link is the original one, this SO question is an duplicate.Normal copying (assigning) is done by reference, so filling a container by adapting the same object and inserting, ends up with a container with references to the last added object.\nUse copy.deepcopy instead.Using the  formatter in error messages. In almost every circumstance,  should be used.For example, imagine code like this:Printed this error:It's impossible to tell if the  variable is the string , the unicode string  or an instance of the  class (which has  defined as ). Whereas, if  was used, there would be three different error messages:Would produce the much more helpful errors:Another good reason for this is that paths are a whole lot easier to copy/paste. Imagine:If  is , the error message will be:Which is hard to visually parse and hard to copy/paste into a shell.Whereas, if  is used, the error would be:Easy to visually parse, easy to copy-paste, all around better.I would stop using deprecated methods in 2.6 so your app/script will be ready and easier to convert to python 3.A bad habit I had to train myself out of was using  for inline logic.Unless you can 100% always guarantee that  will be a true value, even when your code changes in 18 months time, you set yourself up for some unexpected behaviour.Thankfully, in later versions you can use .some personal opinions, but I find it best NOT to: (edited : best to --> NOT to of course, duh.)Beautiful is better than ugly.\nExplicit is better than implicit.\nSimple is better than complex.\nComplex is better than complicated.\nFlat is better than nested.\nSparse is better than dense.\nReadability counts.\nSpecial cases aren't special enough to break the rules.\nAlthough practicality beats purity.\nErrors should never pass silently.\nUnless explicitly silenced.\nIn the face of ambiguity, refuse the temptation to guess.\nThere should be one-- and preferably only one --obvious way to do it.\nAlthough that way may not be obvious at first unless you're Dutch.\nNow is better than never.\nAlthough never is often better than  now.\nIf the implementation is hard to explain, it's a bad idea.\nIf the implementation is easy to explain, it may be a good idea.\nNamespaces are one honking great idea -- let's do more of those!  Write ugly code.\nWrite implicit code.\nWrite complex code.\nWrite nested code.\nWrite dense code.\nWrite unreadable code.\nWrite special cases.\nStrive for purity.\nIgnore errors and exceptions.\nWrite optimal code before releasing.\nEvery implementation needs a flowchart.\nDon't use namespaces.  I've started learning Python as well and one of the bigest mistakes I made is constantly using C++/C# indexed \"for\" loop. Python have for(i ; i < length ; i++) type loop and for a good reason - most of the time there are better ways to do there same thing.Example:\nI had a method that iterated over a list and returned the indexes of selected items:Instead Python has list comprehension that solves the same problem in a more elegant and easy to read way:Never assume that having a multi-threaded Python application and a SMP capable machine (for instance one equipped with a multi-core CPU) will give you the benefit of introducing true parallelism into your application. Most likely it will not because of GIL (Global Interpreter Lock) which synchronizes your application on the byte-code interpreter level. There are some workarounds like taking advantage of SMP by putting the concurrent code in  C API calls or using multiple processes (instead of threads) via wrappers (for instance like the one available at ) but if one needs true multi-threading in Python one should look at things like Jython, IronPython etc. (GIL is a feature of the CPython interpreter so other implementations are not affected).According to Python 3000 FAQ (available at Artima) the above still stands even for the latest Python versions.++n and --n may not work as expected by people coming from C/Java background++n is positive of a positive number which is positive =  n--n is negative of a negative number which is positive =  nSomewhat related to the default mutable argument, how one checks for the \"missing\" case results in differences when an empty list is passed:Here's the output:The very first mistake before you even start: Don't be afraid of whitespace.When you show someone a piece of Python code, they are impressed until you tell them that they  to indent correctly. For some reason, most people feel that a language shouldn't force a certain style on them while all of them will indent the code nonetheless.Similar to mutable default arguments is the mutable class attribute.Python won't warn you in any way that on the second assignment you misspelled the variable name and created a new one.You've mentioned default arguments... One that's almost as bad as mutable default arguments: default values which aren't .Consider a function which will cook some food:Because it specifies a default value for , it is impossible for some other function to say \"cook your default breakfast\" without a special-case:However, this could be avoided if  used  as a default value:A good example of this is Django bug . Django's caching module had a \"save to cache\" function which looked like this:But, for the memcached backend, a timeout of  means \"never timeout\"\u2026 Which, as you can see, would be impossible to specify.Don't modify a list while iterating over it.One common suggestion to work around this problem is to iterate over the list in reverse:But even better is to use a list comprehension to build a new list to replace the old:Common pitfall: Default arguments are evaluated once:prints:i.e. you always get the same list.Similar to mutable default arguments is the mutable class attribute.Not what you expect."},
{"body": "I am learning how to use the  and the  modules in Python to run certain operations in parallel and speed up my code.I am finding this hard (maybe because I don't have any theoretical background about it) to understand what the difference is between a  object and a  one.Also, it is not entirely clear to me how to instantiate a queue of jobs and having only 4 (for example) of them running in parallel, while the other wait for resources to free before being executed.I find the examples in the documentation clear, but not very exhaustive; as soon as I try to complicate things a bit, I receive a lot of weird errors (like a method that can't be pickled, and so on).So, when should I use the  and  modules?Can you link me to some resources that explain the concepts behind these two modules and how to use them properly for complex tasks? is true for multithreading vs. multiprocessing .However, Python has an added issue: There's a Global Interpreter Lock that prevents two threads in the same process from running Python code at the same time. This means that if you have 8 cores, and change your code to use 8 threads, it won't be able to use 800% CPU and run 8x faster; it'll use the same 100% CPU and run at the same speed. (In reality, it'll run a little slower, because there's extra overhead from threading, even if you don't have any shared data, but ignore that for now.)There are exceptions to this. If your code's heavy computation doesn't actually happen in Python, but in some library with custom C code that does proper GIL handling, like a numpy app, you will get the expected performance benefit from threading. The same is true if the heavy computation is done by some subprocess that you run and wait on.More importantly, there are cases where this doesn't matter. For example, a network server spends most of its time reading packets off the network, and a GUI app spends most of its time waiting for user events. One reason to use threads in a network server or GUI app is to allow you to do long-running \"background tasks\" without stopping the main thread from continuing to service network packets or GUI events. And that works just fine with Python threads. (In technical terms, this means Python threads give you concurrency, even though they don't give you core-parallelism.)But if you're writing a CPU-bound program in pure Python, using more threads is generally not helpful.Using separate processes has no such problems with the GIL, because each process has its own separate GIL. Of course you still have all the same tradeoffs between threads and processes as in any other languages\u2014it's more difficult and more expensive to share data between processes than between threads, it can be costly to run a huge number of processes or to create and destroy them frequently, etc. But the GIL weighs heavily on the balance toward processes, in a way that isn't true for, say, C or Java. So, you will find yourself using multiprocessing a lot more often in Python than you would in C or Java.Meanwhile, Python's \"batteries included\" philosophy brings some good news: It's very easy to write code that can be switched back and forth between threads and processes with a one-liner change.If you design your code in terms of self-contained \"jobs\" that don't share anything with other jobs (or the main program) except input and output, you can use the  library to write your code around a thread pool like this:You can even get the results of those jobs and pass them on to further jobs, wait for things in order of execution or in order of completion, etc.; read the section on  objects for details.Now, if it turns out that your program is constantly using 100% CPU, and adding more threads just makes it slower, then you're running into the GIL problem, so you need to switch to processes. All you have to do is change that first line:The only real caveat is that your jobs' arguments and return values have to be pickleable (and not take too much time or memory to pickle) to be usable cross-process. Usually this isn't a problem, but sometimes it is.But what if your jobs can't be self-contained? If you can design your code in terms of jobs that  from one to another, it's still pretty easy. You may have to use  or  instead of relying on pools. And you will have to create  or  objects explicitly. (There are plenty of other options\u2014pipes, sockets, files with flocks, \u2026 but the point is, you have to do  manually if the automatic magic of an Executor is insufficient.)But what if you can't even rely on message passing? What if you need two jobs to both mutate the same structure, and see each others' changes? In that case, you will need to do manual synchronization (locks, semaphores, conditions, etc.) and, if you want to use processes, explicit shared-memory objects to boot. This is when multithreading (or multiprocessing) gets difficult. If you can avoid it, great; if you can't, you will need to read more than someone can put into an SO answer.From a comment, you wanted to know what's different between threads and processes in Python. Really, if you read Giulio Franco's answer and mine and all of our links, that should cover everything\u2026\u00a0but a summary would definitely be useful, so here goes:Multiple threads can exist in a single process.\nThe threads that belong to the same process share the same memory area (can read from and write to the very same variables, and can interfere with one another).\nOn the contrary, different processes live in different memory areas, and each of them has its own variables. In order to communicate, processes have to use other channels (files, pipes or sockets).If you want to parallelize a computation, you're probably going to need multithreading, because you probably want the threads to cooperate on the same memory.Speaking about performance, threads are faster to create and manage than processes (because the OS doesn't need to allocate a whole new virtual memory area), and inter-thread communication is usually faster than inter-process communication. But threads are harder to program. Threads can interfere with one another, and can write to each other's memory, but the way this happens is not always obvious (due to several factors, mainly instruction reordering and memory caching), and so you are going to need synchronization primitives to control access to your variables.I believe  answers your question in an elegant way.To be short, if one of your sub-problems has to wait while another finishes, multithreading is good (in I/O heavy operations, for example); by contrast, if your sub-problems could really happen at the same time, multiprocessing is suggested. However, you won't create more processes than your number of cores.Here's some performance data for python 2.6.x that calls to question the  notion that threading is more performant that multiprocessing in IO-bound scenarios. These results are from a 40-processor IBM System x3650 M4 BD.IO-Bound Processing : Process Pool performed better than Thread PoolCPU-Bound Processing : Process Pool performed better than Thread PoolThese aren't rigorous tests, but they tell me that multiprocessing isn't entirely unperformant in comparison to threading.Code used in the interactive python console for the above testsWell, most of the question is answered by Giulio Franco. I will further elaborate on the consumer-producer problem, which I suppose will put you on the right track for your solution to using a multithreaded app.You could read more on the synchronization primitives from:The pseudocode is above. I suppose you should search the producer-consumer-problem to get more references."},
{"body": "I'm following the tutorial on  in a Windows 7 environment. My settings file is:I got the  from  the template  from within the default Django admin template directory in the source code of Django itself (django/contrib/admin/templates) into an admin subdirectory of myapp directory as the tutorial instructed. It doesn't seem to take affect for some reason. Any clue of what might be the problem? Do I have to do a sync db?I know this isn't in the Django tutorial, and shame on them, but it's better to set up relative paths for your path variables. You can set it up like so:This way you can move your Django project and your path roots will update automatically. This is useful when you're setting up your production server.Second, there's something suspect to your TEMPLATE_DIRS path. It should point to the root of your template directory. Also, it should also end in a trailing .I'm just going to guess here that the  directory is not your template root. If you still want to write absolute paths you should take out the reference to the admin template directory.With that being said, the template loaders by default should be set up to recursively traverse into your app directories to locate template files.You shouldn't need to copy over the admin templates unless if you specifically want to overwrite something.You will have to run a syncdb if you haven't run it yet. You'll also need to statically server your media files if you're hosting django through runserver.If using Django settings as installed, then why not just use its baked-in, predefined BASE_DIR and TEMPLATES? In the pip installed Django(v1.8), I get: I also had issues with this part of the tutorial (used tutorial for version 1.7).My mistake was that I only edited the 'Django administration' string, and did not pay enough attention to the manual.This is the line from :But after some time and frustration it became clear that there was the 'site_header or default:_' statement, which should be removed. So after removing the statement (like the example in the manual everything worked like expected).Example manual:For Django 1.6.6:Also static and media for debug and production mode:Into urls.py you must add:In Django 1.8 you can set template paths, backend and other parameters for templates in one dictionary ():"},
{"body": "I can do such thing in python:This will check if 'some word' exists in the list. But can I do reverse thing?I have to check whether some words from array are in the string. I can do this using cycle but this way has more lines of code.If your list of words is of substantial length, and you need to do this test many times, it may be worth converting the list to a set and using set intersection to test (with the added benefit that you wil get the actual words that are in both lists):Here are a couple of alternative ways of doing it, that may be faster or more suitable than KennyTM's answer, depending on the context.1) use a regular expression:2) You could use sets if you want to match whole words, e.g. you do not want to find the word \"the\" in the phrase \"them theorems are theoretical\":Of course you can also do whole word matches with regex using the \"\\b\" token.The performance of these and Kenny's solution are going to depend on several factors, such as how long the word list and phrase string are, and how often they change.  If performance is not an issue then go for the simplest, which is probably Kenny's."},
{"body": "I have two times, a start and a stop time, in the format of 10:33:26 (HH:MM:SS).  I need the difference between the two times.  I've been looking through documentation for Python and searching online and I would imagine it would have something to do with the datetime and/or time modules.  I can't get it to work properly and keep finding only how to do this when a date is involved.Ultimately, I need to calculate the averages of multiple time durations. I got the time differences to work and I'm storing them in a list. I now need to calculate the average. I'm using regular expressions to parse out the original times and then doing the differences.  For the averaging, should I convert to seconds and then average?Yes, definitely  is what you need here. Specifically, the  function, which parses a string into a time object.That gets you a  object that contains the difference between the two times. You can do whatever you want with that, e.g. converting it to seconds or adding it to another .This will return a negative result if the end time is earlier than the start time, for example  and . If you want the code to assume the interval crosses midnight in this case (i.e. it should assume the end time is never earlier than the start time), you can add the following lines to the above code:(of course you need to include  somewhere). Thanks to J.F. Sebastian for pointing out this use case.Try this -- it's efficient for timing short-term events.  If something takes more than an hour, then the final display probably will want some friendly formatting.Structure that represent time difference in Python is called . If you have  and  as  types you can calculate the difference using  operator like:you should do this before converting to particualr string format (eg. before start_time.strftime(...)). In case you have already string representation you need to convert it back to time/datetime by using  method.Here's a solution that supports finding the difference even if the end time is less than the start time (over midnight interval) such as  (a half an hour duration): returns a timedelta object that you can pass (as a part of the sequence) to a  function directly e.g.:The  result is also  object that you can convert to seconds ( method (since Python 2.7)), hours ( (Python 3)), etc. site says to try: uses time.mktime()Both  and  have a date component.Normally if you are just dealing with the time part you'd supply a default date. If you are just interested in the difference and know that both times are on the same day then construct a  for each with the day set to today and subtract the start from the stop time to get the interval ().Take a look at the datetime module and the timedelta objects.  You should end up constructing a datetime object for the start and stop times, and when you subtract them, you get a timedelta.I like how this guy does it \u2014 .\nNot sure if it has some cons.But looks neat for me :)Regarding to the question you still need to use  as others said earlier.Try thisOUTPUT :"},
{"body": "I'm starting a new application and looking at using an ORM -- in particular, SQLAlchemy.Say I've got a column 'foo' in my database and I want to increment it.  In straight sqlite, this is easy:I figured out the SQLAlchemy SQL-builder equivalent:This is slightly slower, but there's not much in it.Here's my best guess for a SQLAlchemy ORM approach:This does the right thing, but it takes just under fifty times as long as the other two approaches.  I presume that's because it has to bring all the data into memory before it can work with it.Is there any way to generate the efficient SQL using SQLAlchemy's ORM?  Or using any other python ORM?  Or should I just go back to writing the SQL by hand?SQLAlchemy's ORM is meant to be used together with the SQL layer, not hide it. But you do have to keep one or two things in mind when using the ORM and plain SQL in the same transaction. Basically, from one side, ORM data modifications will only hit the database when you flush the changes from your session. From the other side, SQL data manipulation statements don't affect the objects that are in your session.So if you sayit will do what it says, go fetch all the objects from the database, modify all the objects and then when it's time to flush the changes to the database, update the rows one by one.Instead you should do this:This will execute as one query as you would expect, and because atleast the default session configuration expires all data in the session on commit you don't have any stale data issues.In the almost-released 0.5 series you could also use this method for updating:That will basically run the same SQL statement as the previous snippet, but also select the changed rows and expire any stale data in the session. If you know you aren't using any session data after the update you could also add synchronize_session=False to the update statement and get rid of that select.Try this =)There are several ways to UPDATE using sqlalchemyWithough testing, I'd try:(IIRC, commit() works without flush()).I've found that at times doing a large query and then iterating in python can be up to 2 orders of magnitude faster than lots of queries.  I assume that iterating over the query object is less efficient than iterating over a list generated by the all() method of the query object.[Please note comment below - this did not speed things up at all].If it is because of the overhead in terms of creating objects, then it probably can't be sped up at all with SA.If it is because it is loading up related objects, then you might be able to do something with lazy loading.  Are there lots of objects being created due to references?  (IE, getting a Company object also gets all of the related People objects).Here's an example of how to solve the same problem without having to map the fields manually:So to update a Media instance, you can do something like this:"},
{"body": "I have generated an image using . How can I save it to a string in memory?\nThe  method requires a file.I'd like to have several such images stored in dictionary.You can probably use the  class to get a wrapper around strings that behaves like a file. The StringIO object provides the same interface as a file, but saves the contents just in memory:This might lead to a  if PIL tries to automatically detect the output format.  To avoid this problem you can specify the format manually:sth's solution didn't work for me\nbecause in ...It was trying to detect the format from the extension in the filename , which doesn't exist in StringIO case  You can bypass the format detection by setting the format yourself in a parameter  save() can take a file-like object as well as a path, so you can use an in-memory buffer like a StringIO:For Python3 it is required to use BytesIO: When you say \"I'd like to have number of such images stored in dictionary\", it's not clear if this is an in-memory structure or not.You don't need to do any of this to meek an image in memory.  Just keep the  object in your dictionary.If you're going to write your dictionary to a file, you might want to look at   method and the  functionThe \"format\" (.jpeg, .png, etc.) only matters on disk when you are exchanging the files.  If you're not exchanging files, format doesn't matter."},
{"body": "When I install PIL using easy_install or buildout it installs in such way, that I must do 'import Image', not 'from PIL import Image'.However, if I do \"apt-get install python-imaging\" or use \"pip -E test_pil install PIL\", all work fine.Here are examples of how I trying to install PIL using virtualenv:I see, that easy_install pack PIL into the Egg, and PIP does not. Same thing with buildbot, it uses eggs.How could I install PIL properly, using easy_install or buildout?The PIL version packaged on pypi (by the author) is incompatible with setuptools and thus not easy_installable. People have created easy_installable versions elsewhere. Currently, you need to specify a find-links URL and use  get a good package:By using  with the  you avoid running the risk of finding the PyPI (non-fixed) original of PIL. If you were to use , you must use a direct link to the source tarball of a corrected version; easy_install stubbornly still uses the PyPI link over the find-links URL:To include PIL in a buildout, either specify the egg with the same version pin or use a versions section:Edit March 2011: Fixes to address the packaging issues have been merged into  now, so this workaround may soon be obsolete.Edit February 2013: Just use  and be done with it. :-) Clearly waiting for the original package to be fixed has not paid off.Use  :-) It offers:If PIL ever does exactly what Pillow does, then the fork will die. Until that happens, we have Pillow.: I am the fork author, and Pillow was created mainly to make my job easier (although it's great to see other folks using it too).: Pillow 2.0.0 was released on March 15, 2013. It offers Python 3 support and many bug fixes/enhancements. While we still attempt to track changes with upstream PIL, (unfortunately or fortunately depending on how you look at it) Pillow has begun to drift away from PIL.For Ubuntu  I found I needed to to install the C headers package for my python version (2.7)Afterwards,  worked. On Windows, I installed PIL in a virtualenv as follows:Install PIL in your global python site-packages by executing the .exe from:\nThen, as a \"do it yourself-er\", copy the PIL.pth file and PIL directory in C:\\Python25\\Lib\\site-packages to your virtualenv site-packages directory.   Yeah, python is still a \"get your hands dirty\" environment..."},
{"body": "I have a situation with some code where  came up as a possible solution.  Now I have never had\nto use  before but, I have come across plenty of information about the potential\ndanger it can cause.  That said, I'm very wary about using it.My situation is that I have input being given by a user:Where  needs to be a dictionary.  I searched around and found that  could work this out.\nI thought that I might be able to check the type of the input before trying to use the data and that\nwould be a viable security precaution.I read through the docs and I am still unclear if this would be safe or not.  Does eval evaluate the data as soon as its entered or after the  variable is called?Is the  module's  the only safe option? means that you actually evaluate the code  you deem it to be unsafe or not. It evaluates the code as soon as the function is called. See also . raises an exception if the input isn't a valid Python datatype, so the code won't be executed if it's not.Use  whenever you need . You shouldn't usually evaluate literal Python statements. only considers a small subset of Python's syntax to be valid:Passing  into  will raise an error, but  will happily wipe your drive.Since it looks like you're only letting the user input a plain dictionary, use . It safely does what you want and nothing more.Python's  in its evaluation, so  will evaluate the user's input as soon as it hits the , regardless of what you do with the data afterwards. Therefore, , especially when you  user input.Use .As an example, entering this at the prompt will be very, very bad for you:\nThis is very powerful, but is also very dangerous if you accept strings to evaluate from untrusted input. Suppose the string being evaluated is \"os.system('rm -rf /')\" ? It will really start deleting all the files on your computer.\n\nSafely evaluate an expression node or a string containing a Python literal or container display. The string or node provided may only consist of the following Python literal structures: strings, bytes, numbers, tuples, lists, dicts, sets, booleans, None, bytes and sets.\n"},
{"body": "Suppose you have a dictionary like:How would you go about flattening that into something like:Basically the same way you would flatten a nested list, you just have to do the extra work for iterating the dict by key/value, creating new keys for your new dictionary and creating the dictionary at final step.There are two big considerations that the original poster needs to consider:(Performance is not likely an issue, but I'll elaborate on the second point in case anyone else cares: In implementing this, there are numerous dangerous choices. If you do this recursively and yield and re-yield, or  equivalent which touches nodes more than once (which is quite easy to accidentally do), you are doing potentially O(N^2) work rather than O(N). This is because maybe you are calculating a key  then  then ..., and then calculating  then  then ..., but really you shouldn't have to calculate  again. Even if you aren't recalculating it, re-yielding it (a 'level-by-level' approach) is just as bad. A good example is to think about the performance on )Below is a function I wrote  which can be adapted to many purposes and can do what you want. Sadly it is fairly hard to make a lazy version of this function without incurring the above performance penalties (many python builtins like chain.from_iterable aren't actually efficient, which I only realized after extensive testing of three different versions of this code before settling on this one).To better understand what's going on, below is a diagram for those unfamiliar with (left), otherwise known as \"fold left\". Sometimes it is drawn with an initial value in place of k0 (not part of the list, passed into the function). Here,  is our  function. We preprocess each k with .This is in fact the same as , but where our function does this to all key-paths of the tree.Demonstration (which I'd otherwise put in docstring):Performance:... sigh, don't think that one is my fault...[unimportant historical note due to moderation issues]That question's solution can be implemented in terms of this one by doing . The reverse is not possible: while it is true that the  of  can be recovered from the alleged duplicate by mapping a higher-order accumulator, one cannot recover the keys. (edit: Also it turns out that the alleged duplicate owner's question is completely different, in that it only deals with dictionaries exactly 2-level deep, though one of the answers on that page gives a general solution.)Here is a kind of a \"functional\", \"one-liner\" implementation. It is recursive, and based on a conditional expression and a dict comprehension.Test:Code:Results:I am using python3.2, update for your version of python.This is not restricted to dictionaries, but every mapping type. Further ist faster as it avoides an if condition. Nevertheless credits go to Imran:This is similar to both imran's and ralu's answer. It does not use a generator, but instead employs recursion with a closure:Davoud's solution is very nice but doesn't give satisfactory results when the nested dict also contains lists of dicts, but his code be adapted for that case:Here's an algorithm for elegant, in-place replacement. Tested with Python 2.7 and Python 3.5. Using the dot character as a separator.Example:Output:I published this code  along with the matching  function.My Python 3.3 Solution using generators:Using generators:The answers above work really well. Just thought I'd add the unflatten function that I wrote:Note: This doesn't account for '_' already present in keys, much like the flatten counterparts.Or if you are already using pandas, You can do it with  like so:Output:If you don't want  separator, you can hack it like so:Output:How about a  and performant solution in Python3?In use:I tried to flatten following json:The best option to flatten that file, with . as seperation mark was this one: \nIt only left out the \"pids_stats\": {}, option:In terms of unflattening the file, with . as option, I choose  - because it worked recursively and perfectly. Note that \"pids_stats\": {} is missing as it was lost in the previous step.Simple function to flatten nested dictionaries:The idea/requirement was:\nGet flat dictionaries with no keeping parent keys.Example of usage:Keeping parent keys is simple as well."},
{"body": "I receive as input a list of strings and need to return a list with these same strings but in randomized order. I must allow for duplicates - same string may appear once or more in the input and must appear the same number of times in the output.I see several \"brute force\" ways of doing that (using loops, god forbid), one of which I'm currently using. However, knowing Python there's probably a cool one-liner do get the job done, right?Looks like this is the simplest way, if not the most truly random ( more fully explains the limitations): Given a string , here is a one-liner:  You'll have to read the strings into an array and then use a shuffling algorithm. I recommend "},
{"body": "I'm building an admin for Flask and SQLAlchemy, and I want to pass the HTML for the different inputs to my view using . The templating framework seems to escape the html automatically, so all <\"'> are converted to html entities. How can I disable that so that the HTML renders correctly?the ideal way is tothan completely turning off auto escaping. You can also declare it HTML safe from the code:Then pass that value to the templates and they don't have to  it.Example Check this In the secction HTML Escaping"},
{"body": "I have a simple line plot and need to move the y-axis ticks from the (default) left side of the plot to the right side. Any thoughts on how to do this?Use for example:For right labels use , i.e.:joaquin's answer works, but has the side effect of removing ticks from the left side of the axes. To fix this, follow up  with a call to . A revised example:The result is a plot with ticks on both sides, but tick labels on the right.Just is case somebody asks (like I did), this is also possible when one uses subplot2grid. For example:It will show this:"},
{"body": "In my django app, I need to get the domain name from the referer in  along with its protocol so that from URLs like:I should get:I looked over other related questions and found about urlparse, but that didn't do the trick sinceBesides I didn't find a way to get the protocol and maybe do a simple concatenation :\\Thanks in advance for any helpYou should be able to do it with:UPDATE:Python docs on urlparse (, ) are also quite good, so you may want to check them out :)EDIT:Incorporated a cleaner way of formatting the  string as suggested by @J.F.Sebastian (thanks!)This is a more verbose version of urlparse.  It detects domains and subdomains for you.From their documentation: is a namedtuple, so it's simple to access the parts you want.Pure string operations :):That's all, folks.Is there anything wrong with pure string operations:If you prefer having a trailing slash appended, extend this script a bit like so:That can probably be optimized a bit ...This is a bit obtuse, but uses  in both directions:that odd  bit is because urlparse expects a sequence of   = 6"},
{"body": "I need to insert multiple rows with one query (number of rows is not constant), so I need to execute query like this one:The only way I know isbut I want some simpler way.I built a program that inserts multiple lines to a server that was located in another city.  I found out that using this method was about 10 times faster than . In my case  is a tuple containing about 2000 rows. It took about 10 seconds when using this method:and 2 minutes when using this method:New  in Psycopg 2.7:The pythonic way of doing it in Psycopg 2.6:Explanation: If the data to be inserted is given as a list of tuples like inthen it is already in the exact required format as The only necessary work is to provide a records list template to be filled by psycopgand place it in the  queryPrinting the  outputsNow to the usual  arguments substitutionOr just testing what will be sent to the serverOutput:A snippet from Psycopg2's tutorial page at :It doesn't save much code, but it definitively looks better. is the fastest solution I've found for bulk inserts by far.  I made containing a class named IteratorFile which allows an iterator yielding strings to be read like a file. We can convert each input record to a string using a generator expression. So the solution would beFor this trivial size of args it won't make much of a speed difference, but I see big speedups when dealing with thousands+ of rows. It will also be more memory efficient than building a giant query string. An iterator would only ever hold one input record in memory at a time, where at some point you'll run out of memory in your Python process or in Postgres by building the query string.All of these techniques are called 'Extended Inserts\" in Postgres terminology, and as of the 24th of November 2016, it's still a ton faster than psychopg2's executemany() and all the other methods listed in this thread (which i tried before coming to this answer).Here's some code which doesnt use cur.mogrify and is nice and simply to get your head around:But it should be noted that if you can use copy_from(), you should use copy_from ;)[Update with psycopg2 2.7]The classic  is about 60 times slower than @ant32 's implementation (called \"folded\") as explained in this thread: This implementation was added to psycopg2 in version 2.7 and is called :[Previous Answer]To insert multiple rows, using the multirow  syntax with  is about 10x faster than using psycopg2 . Indeed,  just runs many individual  statements.@ant32 's code works perfectly in Python 2. But in Python 3,  returns bytes,  takes either bytes or strings, and  expects  instance.So in Python 3 you may need to modify @ant32 's code, by adding :Or by using bytes (with  or ) only:   - The snippet below works perfectly fineAnother nice and efficient approach - is to pass rows for insertion as 1 argument, \nwhich is array of json objects.E.g. you passing argument:It is array, which may contain any amount of objects inside.\nThen your SQL looks like:Notice: Your postgress must be new enough, to support jsonIf you're using SQLAlchemy, you don't need to mess with hand-crafting the string because SQLAlchemy :If you want to insert multiple rows within one insert statemens (assuming you are not using ORM) the easiest way so far for me would be to use list of dictionaries. Here is an example:As you can see only one query will be executed:"},
{"body": "Given an ip address (say 192.168.0.1), how do I check if it's in a network (say 192.168.0.0/24) in Python?Are there general tools in Python for ip address manipulation? Stuff like host lookups, ip adddress to int, network address with netmask to int and so on? Hopefully in the standard Python library for 2.5. shows you can do it with  and  modules without too much extra effort.  I added a little to the article as follows:This outputs:If you just want a single function that takes strings it would look like this:I like to use  for that:As arno_v pointed out in the comments, new version of netaddr does it like this:Using  (, ):If you want to evaluate a  of IP addresses this way, you'll probably want to calculate the netmask upfront, likeThen, for each address, calculate the binary representation with one ofFinally, you can simply check:This code is working for me on Linux x86.  I haven't really given any thought to endianess issues, but I have tested it against the \"ipaddr\" module using over 200K IP addresses tested against 8 different network strings, and the results of ipaddr are the same as this code.Example:I tried Dave Webb's solution but hit some problems:Most fundamentally - a match should be checked by ANDing the IP address with the mask, then checking the result matched the Network address exactly. Not ANDing the IP address with the Network address as was done.I also noticed that just ignoring the Endian behaviour assuming that consistency will save you will only work for masks on octet boundaries (/24, /16). In order to get other masks (/23, /21) working correctly I added a \"greater than\" to the struct commands and changed the code for creating the binary mask to start with all \"1\" and shift left by (32-mask). Finally, I added a simple check that the network address is valid for the mask and just print a warning if it is not.Here's the result:I'm not a fan of using modules when they are not needed.  This job only requires simple math, so here is my simple function to do the job:Then to use it:That's it, this is much faster than the solutions above with the included modules.The accepted answer doesn't work ... which is making me angry. Mask is backwards and doesn't work with any bits that are not a simple 8 bit block (eg /24). I adapted the answer, and it works nicely.here is a function that returns a dotted binary string to help visualize the masking.. kind of like  output.eg:Marc's code is nearly correct. A complete version of the code is -Obviously from the same sources as above...A very Important note is that the first code has a small glitch - The IP address 255.255.255.255 also shows up as a Valid IP for any subnet. I had a heck of time getting this code to work and thanks to Marc for the correct answer.Not in the Standard library for 2.5, but ipaddr makes this very easy. I believe it is in 3.3 under the name ipaddress. Here is the usage for this method:Basically you provide an ip address as the first argument and a list of cidrs as the second argument.  A list of hits are returned.Here is a class I wrote for longest prefix matching:And here is a test program:I don't know of anything in the standard library, but  is a Python library that will do subnet matching.Thank you for your script!\nI have work quite a long on it to make everything working... So I'm sharing it hereSo my new addressInNetwork function looks-like:And now, answer is right!!I hope that it will help other people, saving time for them!Relating to all of the above, I think socket.inet_aton() returns bytes in network order, so the correct way to unpack them is probablybugfixed code:The choosen answer has a bug.Following is the correct code:Note:  instead of . I also replace  with , as the latter seems more understandable.There is an API that's called SubnetTree available in python that do this job very well. \nThis is a simple example : $ python check-subnet.py\nFalse\nTrue\nFalseFrom various sources above, and from my own research, this is how I got subnet and address calculation working.  These pieces are enough to solve the question and other related questions.Here is my code"},
{"body": "Is there any rule about which built-in and standard library classes are not subclassable (\"final\")?As of Python 3.3, here are a few examples:I found a  which deals with the implementation of \"final\" classes, both in C and pure Python.I would like to understand what reasons may explain why a class is chosen to be \"final\" in the first place. There seems to be two reasons for a class to be \"final\" in Python.Classes that follow Singleton pattern have an invariant that there's a limited (pre-determined) number of instances. Any violation of this invariant in a subclass will be inconsistent with the class' intent, and would not work correctly. Examples:There may be cases other than the Singleton pattern in this category but I'm not aware of any.A class implemented in C requires additional work to allow subclassing (at least in CPython). Doing such work without a convincing use case is not very attractive, so volunteers are less likely to come forward. Examples:Note 1: I originally thought there were valid use cases, but simply insufficient interest, in subclassing of  and . Thanks to @agf for pointing out that the use cases offered  and  are not convincing (see @agf comments to the question).Note 2:My concern is that another Python implementation might accidentally allow subclassing a class that's final in CPython. This may result in non-portable code (a use case may be weak, but someone might still write code that subclasses  if their Python supports it). This can be resolved by marking in Python documentation all built-in and standard library classes that cannot be subclassed, and requiring that all implementations follow CPython behavior in that respect.Note 3:The message produced by CPython in all the above cases is:It is quite cryptic, as numerous questions on this subject show. I'll submit a suggestion to add a paragraph to the documentation that explains final classes, and maybe even change the error message to:"},
{"body": "I'm used to bringing data in and out of Python using .csv files, but there are obvious challenges to this.  Any advice on simple ways to store a dictionary (or sets of dictionaries) in a json or pck file?  For example:I would like to know both how to save this, and then how to load it back in.Supply extra arguments like  or  to get a pretty result. The argument  will sort the keys alphabetically and  will indent your data structure with  spaces.Minimal example, writing directly to a file:or safely opening / closing:If you want to save it in a string instead of a file:If you're after serialization but won't need the data in other programs I strongly recommend the  module.  Think of it as a persistent dictionary.Also see the speeded-up package ujson.\nIf you want an alternative to  or , you can use .With , if you had used , the dictionary would have been written to  as a pickled dictionary instead of with clear text.You can get  here:  is probably a better choice for pickling then  itself, as  can serialize almost anything in python.   also can use .You can get  here: The additional mumbo-jumbo on the first few lines are because  can be configured to store dictionaries to a file, to a directory context, or to a SQL database.  The API is the same for whatever you choose as the backend archive.  It gives you an \"archivable\" dictionary with which you can use  and  to interact with the archive.To write to a file:To read from a file: is the file object for the file that you stored the dict in."},
{"body": "I am using the following command to run a python script in the background:But it appears that nohup is not writing anything to the log file. cmd.log is created but is always empty. In the python script, I am using  instead of  to print to standard output. Am I doing anything wrong?It looks like you need to flush stdout periodically (e.g. ). In my testing Python doesn't automatically do this even with  until the program exits.You can run Python with the  flag to avoid output buffering:or"},
{"body": "How do you find the median of a list in Python? The list can be of any size and the numbers are not guaranteed to be in any particular order.If the list contains an even number of elements, the function should return the average of the middle two.Here are some examples (sorted for display purposes):Python 3.4 has :Usage:It's pretty careful with types, too:Use  to make a one-line function:This runs as:Or, to :Which runs as:The sorted() function is very helpful for this. Use the sorted function\nto order the list, then simply return the middle value (or average the two middle\nvalues if the list contains an even amount of elements).Here's a cleaner solution:Note: Answer changed to incorporate suggestion in comments.You can try the  algorithm if faster average-case running times are needed. Quickselect has average (and best) case performance , although it can end up  on a bad day.Here's an implementation with a randomly chosen pivot:You can trivially turn this into a method to find medians:This is very unoptimised, but it's not likely that even an optimised version will outperform Tim Sort (CPython's built-in ) because that's . I've tried before and I lost.You can use the  to avoid creating new lists with  and sort the lists in place.Also you should not use  as a variable name as it shadows  python's own .Here what I came up with during this exercise in Codecademy:I defined a median function for a list of numbers asI posted my solution at  , which is a little bit faster than using sort().  My solution uses 15 numbers per column, for a speed ~5N which is faster than the speed ~10N of using 5 numbers per column.  The optimal speed is ~4N, but I could be wrong about it.Per Tom's request in his comment, I added my code here, for reference.  I believe the critical part for speed is using 15 numbers per column, instead of 5.I had some problems with lists of float values. I ended up using a code snippet from the python3  and is working perfect with float values without imports. Here's the tedious way to find median without using the  function: "},
{"body": "I have a small Django project I received from a friend. The code works perfectly on his system. However, on my system I get the following error message when running the server:The problem is with a line in an html file:This exact same code works on his system with no errors. What could that be?I would suggest the following:If everything else fails, check this link:\nI had this problem and fixed it by adding a blank  file in my appname/templatetags/ directory.Possibilities are many: has solved the issue for me. They must have mentioned it in the documentation.suppose you have the following structure:-- Application_Name-------templatetags--------------.py--------------templates_extras.py-------.py-------settings.py-- manage.pyYou have to make sure of the following:\"custom tags\" is not a valid tag library error, more often is occurred because the custom tags are not loaded into the app.place an empty .py inside the same folder where your \"custom template tag\" is placed and run the below code on the terminal to load the custom template tagsI was getting the same error but for a different reason so I'll tell you (in case someone else comes the same problem).I had everything right but I had my custom tag inside a folder named  and after a long search I found out that it had to be , and now it works. So .Please ensure your templatetags folder is initialized with python, if you are in doubt, just take bakup of all files,Remove all files,\nInside templatetags folder create .py file only,\nthen restart your server,Now your folder is under Python, then do your stuff.This works for me...Make sure the  statement is correct. It should be the name of the file, not the name of the app. For instance, if you have this app:Then you should put this in your template:Of course, you should check the other answers too.After you have created the template tag and it should be within the 'templatetags' package within an app installed in the settings.INSTALLED_APPS, make sure you restart your dev-server.For me, it was the mistake of putting quotes around the library name in  tag.WRONG: CORRECT: Refer to other answers too. I solved a couple of those problems too before landing here.All of the advice listed here didn't help me. So in my specific case the problem was that the templatetag had to be loaded from a third-party app, and I manually copied source folder with that app into  folder in my virtualenv. Then I ran  inside that folder. After that django could not load this module. Then I removed the source and installed folder of this app and installed it using  after adding a relevant line into  file. It was downloaded into the  folder, installed and everything began working properly. Hope this helps someone.Maybe someone will find this useful: somehow I had named the directory  instead of , that is -- with a space at the end. Took hours to finally realize."},
{"body": "Ok, Tornado is non-blocking and quite fast and it can handle a lot of standing requests easily.But I guess it's not a silver bullet and if we just blindly run Django-based or any other site with Tornado it won't give any performance boost.I couldn't find comprehensive explanation of this, so I'm asking it here:This distinction is a bit blurry. Only if you are serving static pages, you would use one of the fast server like lighthttpd. Other wise, most servers provides a varying complexity of framework to develop web applications. Tornado is a good web framework. Twisted is even more capable and is considered a good networking framework. It has support for lot of protocols. Tornado and Twisted are frameworks that provide support non-blocking, asynchronous web / networking application development. By it's very nature, Async / Non-Blocking I/O works great when it is I/O intensive and not computation intensive. Most web / networking applications suits well for this model. If your application demands certain computational intensive task to be done then it has to be delegated to some other service that can handle it better. While Tornado / Twisted can do the job of web server, responding to web requests.Performance is usually a characteristic of complete web application architecture. You can bring down the performance with most web frameworks, if the application is not designed properly. Think about caching, load balancing etc. Tornado and Twisted provides reasonable performance and is good for building a very performant web application. You can check out the testimonials for both twisted and tornado to see what they are capable of. I'm sorry for answering an old question, but I came across this one and wondered why it didn't have more answers. To answer Bart J's question:Well that depends on what kind of parsing you're doing and on what hardware :) Long time is long time, so if your app takes more than say half a second to respond, it'll seem sluggish -  profile your app.The key to fast systems is great architecture, not so much the specifics as for instance which framework you're using (Twisted, Tornado, Apache+PHP). Tornado has an asynchronous processing style and that's really what a lot of it comes down to in my opinion. Node.js, Twisted and Yaws are examples of other asynchronous web servers that scale very well because of a lightweight approach and asynchronous processing style.So:Tornado is good for handling a lot of connections, since it can respond to an incoming client, dispatch a request handler and don't think about that client until the result-callback is pushed on the event queue. So for that specific quality Tornado should be used when you want to scale well when handling a lot of requests. \nThe async processing facilitates functional decoupling and shared-nothing data access. That swings really well with stateless design like  or other s. You also don't have to deal with spawning threads or processes with the inherent overhead so much and you can save some of the locking/IPC trouble.Tornado won't make much of a difference, on the other hand, if your backend and/or data store takes a long time to process the requests. It helps doing concurrent designs and Web services in particular. The concurrent architecture makes it easier to scale your design and keep the coupling low. That's my experience with Tornado at least. "},
{"body": "OK I love Python's  function. Use it all the time, it's brilliant. Every now and again I want to do the opposite of , think \"I used to know how to do that\", then google python unzip, then remember that one uses this magical  to unzip a zipped list of tuples. Like this:What on earth is going on? What is that magical asterisk doing? Where else can it be applied and what other amazing awesome things in Python are so mysterious and hard to google?The asterisk in Python is documented in the Python tutorial, under .The asterisk performs  (as it's known in Lisp and Scheme). Basically, it takes your list, and calls the function with that list's contents as arguments.It's also useful for multiple args:And, you can use double asterisk for keyword arguments and dictionaries:And of course, you can combine these:Pretty neat and useful stuff.It doesn't always work:Oops! I think it needs a skull to scare it into working:In python3 I think you needsince zip now returns a generator function which is not False-y.I'm extremely new to Python so this just recently tripped me up, but it had to do more with how the example was presented and what was emphasized.What gave me problems with understanding the zip example was the asymmetry in the handling of the zip call return value(s). That is, when zip is called the first time, the return value is assigned to a single variable, thereby creating a list reference (containing the created tuple list). In the second call, it's leveraging Python's ability to automatically unpack a list (or collection?) return value into multiple variable references, each reference being the individual tuple. If someone isn't familiar with how that works in Python, it makes it easier to get lost as to what's actually happening.Addendum to @bcherry's answer:So it works not just with keyword arguments (in ), but with named arguments too (aka  arguments)."},
{"body": "I'm using Vim and editing Python scripts.Autoindent works pretty well in general, but when I start a new line and type '#' to type a comment, Vim unindents that line for me. For example, if haveand I press enter, Vim will indent properlybut, if instead of typing , I type , it unindents automaticallymy .vimrc file follows.  anyone know why this is happening?Setting  on makes Vim behave like you describe for me, whereas with  (which is what I tend to use) it behaves like you'd prefer it to.Update: From the docs on :That seems to be it.Update: Probably no need to bother with the following... I'll leave it here for the added informational value. ;-)If setting  doesn't help, perhaps you could use the  command -- with no parameters -- to obtain the list of all settings in effect in your Vim session, then paste it somewhere (on  perhaps). There's a few other options which affect automatic indentation, as far as I remember.While Micha\u0142's post explains what smartindent does, you can do a lot better than just turning it off. You could configure it more to your liking, or better yet, let Vim pick better indentation for you. With the following in your vimrc instead of other indent settings:Vim will automatically use the proper indent plugin for python. This is way better than just not de-indenting a # line - pretty much everything should be properly indented.You can try an option only for python files:For some unkown reason the above behavior was caused when i had  on. Turning it off fixed it for me. None of the other fixes above helped.If you install this script, you will get  python (pep8) indenting:"},
{"body": "I've installed Anaconda and created two extra environments: py3k (which holds Python 3.3) and py34 (which holds Python 3.4). Besides those, I have a default environment named 'root' which the Anaconda installer created by default and which holds Python 2.7. This last one is the default, whenever I launch 'ipython' from the terminal it gives me version 2.7. In order to work with Python 3.4, I need to issue the commands (in the shell)which change the default environment to Python 3.4. This works fine, but it's annoying since most of the time I work on Python 3.4, instead of Python 2.7 (which I hold for teaching purposes, it's a rather long story). Anyway, I'll like to know how to change the default environment to Python 3.4, bearing in mind that I don't want to reinstall everything from scratch.  First, make sure you have the latest version of conda by runningThen runThis will attempt to update all your packages in your root environment to Python 3 versions. If it is not possible (e.g., because some package is not built for Python 3), it will give you an error message indicating which package(s) caused the issue.If you installed packages with pip, you'll have to reinstall them. Under Linux there is an easier way to set the default environment by modifying  or \nAt the end you'll find something likeReplace it with and thats all there is to it.\nSome people have multiple Anaconda environments with different versions of python for compatibility reasons.  In this case, you should have a script that sets your default environment.  With this method you can preserve the versions of python you use in your environments.The following assumes  is the name of your environment  \nEdit your bash profile so that the last line is \"source activate \".  In Mac OSX this is ~/.bash_profile, in other environments this may be ~/.bashrc \nHere's how i did it on Mac OSXThe result shows that i'm using my p3.5 environment by default.\nCreate a command file (.cmd) with \"activate \" and follow these instructions to have it execute whenever you open a command promptfrom this answer: I wasn't satisfied with any of the answers presented here, since activating an environment takes a few seconds on my platform (for whatever reason)I modified my path variable so that the environment I want as default has priority over the actual default. In my case I used the following commands to accomplish that for the environment \"py35\":to find out where your environment is stored, activate it and enter .\nI'm not sure yet if this approach has any downsides. Since it also changes the default path of the conda executable. If that should be the case, please comment."},
{"body": "In numpy one can use the 'newaxis' object in the slicing syntax to create an axis of length one, e.g.:The  that one can also use  instead of , the effect is exactly the same.Is there any reason to choose one over the other? Is there any general preference or style guide? My impression is that  is more popular, probably because it is more explicit. So is there any reason why  is allowed? is allowed because  is merely an alias for .The authors probably chose it because they needed a convenient constant, and  was available.As for why you should prefer  over : mainly it's because it's more explicit, and partly because someday the  authors might change it to something other than .  (They're not planning to, and probably won't, but there's no good reason to prefer .)"},
{"body": "Consider the following code:From , how do I call ? I would normally use  or even the base class name directly if this is a normal object method, but apparently I can't find a way to call the classmethod in the base class. In the above example,  prints  class instead of  class.: Oh, wait a minute... it's not clear exactly what you're asking. This is how you would invoke the code in the base class's version of the method, from the derived class.this has been a while, but I think I may have found an answer. When you decorate a method to become a classmethod the original unbound method is stored in a property named 'im_func':This works for me:"},
{"body": "Sorry for the simple question, but I'm having a hard time finding the answer.When I compare 2 lists, I want to know if they are \"equal\" in that they have the same contents, but in different order.Ex:I want  to evaluate to .You can simply check whether the multisets with the elements of x and y are equal:This requires the elements to be hashable; runtime will be in , where  is the size of the lists.If the elements are also unique, you can also convert to sets (same asymptotic runtime, may be a little bit faster in practice):If the elements are not hashable, but sortable, another alternative (runtime in ) isIf the elements are neither hashable nor sortable you can use the following helper function. Note that it will be quite slow () and should generally  be used outside of the esoteric case of unhashable and unsortable elements.Inferring from your example:that the elements of the lists won't be repeated (they are unique) as well as hashable (which strings and other certain immutable python objects are),  uses Python's builtin sets, (which are semantically like mathematical sets you may have learned about in school). In the case that the elements are hashable, but non-unique, the  also works semantically as a multiset, but :Prefer to use :if the elements are orderable. This would account for non-unique or non-hashable circumstances, but this could be much slower than using sets.An empirical experiment concludes that one should prefer , then . Only opt for  if you need other things like counts or further usage as a multiset.First setup:And testing:So we see that comparing sets is the fastest solution, and comparing sorted lists is second fastest.This seems to work, though possibly cumbersome for large lists.However, if each list  contain all the elements of other then the above code is problematic. The problem arises when  and, in this example, . To avoid this, you can add one more statement.One more thing, I benchmarked my solution with timeit.repeat, under the same conditions used by Aaron Hall in his post. As suspected, the results are disappointing. My method is the last one.  it is.As mentioned in comments above, the general case is a pain. It is fairly easy if all items are hashable or all items are sortable. However I have recently had to try solve the general case. Here is my solution. I realised after posting that this is a duplicate to a solution above that I missed on the first pass. Anyway, if you use slices rather than list.remove() you can compare immutable sequences."},
{"body": "I just discovered a logical bug in my code which was causing all sorts of problems. I was inadvertently doing a  instead of a .I changed the code from:TO:To my surprise, I got the rather cryptic error message:Why was a similar error not emitted when I use a bitwise operation - and how do I fix this? is a numpy (rec)array. So  is also a (boolean)\narray. For numpy arrays the  operation returns the bitwise-and of the two\nboolean arrays.The NumPy developers felt there was no one commonly understood way to evaluate\nan array in boolean context: it could mean  if  element is\n, or it could mean  if  elements are , or  if the array has non-zero length, just to name three possibilities.Since different users might have different needs and different assumptions, the\nNumPy developers refused to guess and instead decided to raise a ValueError\nwhenever one tries to evaluate an array in boolean context. Applying  to\ntwo numpy arrays causes the two arrays to be evaluated in boolean context (by\ncalling  in Python3 or  in Python2).Your original code looks correct. However, if you do want , then instead of  use  or .I had the same problem (i.e. indexing with multi-conditions, here it's finding data in a certain date range). The  or  seem not working, at least for me. Alternatively I found another solution which works perfectly for my desired functionality ).Instead of using suggested code above, simply using a  would work. Here you may want to rewrite the code as The reason for the exception is that  implicitly calls . First on the left operand and (if the left operand is ) then on the right operand. So  is equivalent to . However the  on a  (if it contains more than one element) will throw the exception you have seen:The  call is implicit in , but also in , , , so any of the following examples will also fail:There are more functions and statements in Python that hide  calls, for example  is just another way of writing . And the  will call : .The  equivalent for  would be the  function, similarly you could use  as equivalent for . For boolean arrays - and comparisons like , , , ,  and  on NumPy arrays return boolean NumPy arrays - you can also use the  functions (and operators):  ( operator)and  ( operator):A complete list of logical and binary functions can be found in the NumPy documentation:"},
{"body": "I'm having trouble understanding the purpose of 'distributed task queues'. For example, python's . I know that in celery, the python framework, you can set timed windows for functions to get executed. However, that can also be easily done in a linux crontab directed at a python script. And as far as I know, and shown from my own django-celery webapps, celery consumes much more RAM memory than just setting up a raw crontab. Few hundred MB difference for a relatively small app.Can someone please help me with this distinction? Perhaps a high level explanation of how task queues / crontabs work in general would be nice also.Thank you.It depends what you want your tasks to be doing, if you need to distribute them, and how you want to manage them.A crontab is capable of executing a script every N intervals. It runs, and then returns. Essentially you get a single execution each interval. You could just direct a crontab to execute a django management command and get access to the entire django environment, so celery doesn't really help you there.What celery brings to the table, with the help of a message queue, is distributed tasks. Many servers can join the pool of workers and each receive a work item without fear of double handling. It's also possible to execute a task as soon as it is ready. With cron, you're limited to a minimum of one minute.As an example, imagine you've just launched a new web application and you're receiving hundreds of sign ups that require an email to be sent to each user. Sending an email may take a long time (comparatively) so you decide that you'll handle activation emails via tasks.If you were using cron, you'd need to ensure that every minute cron is able to process all of the emails that need to be sent. If you have several servers you now need to make sure that you aren't sending multiple activation emails to the same user - you need some kind of synchronization.With celery, you add a task to the queue. You may have several workers per server so you've already scaled ahead of a cronjob. You may also have several servers allowing you to scale even more. Synchronization is handled as part of the 'queue'.You  use celery as a cron replacement but that's not really its primary use. It is used for farming out asynchronous tasks across a distributed cluster.And of course, celery has a  that cron does not."},
{"body": "Recently began branching out from my safe place (R) into Python and and am a bit confused by the cell localization/selection in . I've read the documentation but I'm struggling to understand the practical implications of the various localization/selection options. Is there a reason why I should ever use  or  over the most general option ? I understand that , , , and  may provide some guaranteed correctness that  can't offer, but I've also read where  tends to be the fastest solution across the board. Can someone please explain the real world, best practices reasoning behind utilizing anything other than ? only work on index\n work on position\n You can get data from dataframe without it being in the index\n get scalar values. It's a very fast loc\n Get scalar values. It's a very fast ilocUpdated for   given that  is deprecated.  This demonstrates not only how to use , , , , , but how to accomplish, mixed positional/label based indexing. - \nAllows you to pass 1-D arrays as indexers.  Arrays can be either slices (subsets) of the index or column, or they can be boolean arrays which are equal in length to the index or columns.   when a scalar indexer is passed,  can assign a new index or column value that didn't exist before. - \nSimilar to  except with positions rather that index values.  However, you  assign new columns or indices. - \nWorks very similar to  for scalar indexers.   operate on array indexers.   assign new indices and columns.   over  is that this is faster.\n is that you can't use arrays for indexers. - \nWorks similarly to .   work in array indexers.   assign new indices and columns. over  is that this is faster.\n is that you can't use arrays for indexers. - \nWorks very similar to  for scalar indexers.   operate on array indexers.   assign new indices and columns Super fast, because there is very little overhead!\n There is very little overhead because  is not doing a bunch of safety checks.  .  Also, this is not intended for public use. - \nWorks similarly to .   work in array indexers.   assign new indices and columns. Super fast, because there is very little overhead!\n There is very little overhead because  is not doing a bunch of safety checks.  .  Also, this is not intended for public use."},
{"body": "I have encountered this weird behavior and failed to explain it. These are the benchmarks:How come comparison with variable assignment is faster than using a one liner with temporary variables by more than 27%?By the Python docs, garbage collection is disabled during timeit so it can't be that. Is it some sort of an optimization?The results may also be reproduced in Python 2.x though to lesser extent.Running Windows 7, CPython 3.5.1, Intel i7 3.40 GHz, 64 bit both OS and Python. Seems like a different machine I've tried running at Intel i7 3.60 GHz with Python 3.5.0 does not reproduce the results.Running using the same Python process with  @ 10000 loops produced 0.703 and 0.804 respectively. Still shows although to lesser extent. (~12.5%)My results were similar to yours: the code using variables was pretty consistently 10-20 % faster. However when I used IPython on the very same Python 3.4, I got these results:Notably, I never managed to get even close to the 74.2 \u00b5s for the former when I used  from the command line.So I decided to run the command with  and indeed there is something fishy going on:Now that is a good reason for the difference. The code that does not use variables causes the  system call be called almost 1000x more than the one that uses intermediate variables.The  is full of / for a 256k region; these same lines are repeated over and over again:The  call seems to be coming from the function  from ; the  also contains the macro , which is d to be  (that is ); similarly the  matches the  from . says thatThus these heuristics and the fact that Python object allocator releases these free arenas as soon as they're emptied lead to  triggering pathological behaviour where one 256 kiB memory area is re-allocated and released repeatedly; and this allocation happens with /, which is comparatively costly as they're system calls - furthermore,  with  requires that the newly mapped pages must be zeroed - even though Python wouldn't care.The behaviour does not seem to be present with the code that uses an intermediate variable, possibly because it is using slightly  memory and never actually freeing all the objects from the last memory arena. Most notably it cannot be guaranteed that the code using intermediate variables is always faster - indeed in some setups it might be that using intermediate variables will result in extra  calls, whereas the code that compares return values directly might be fine.The first question here has to be, is it reproducable? For some of us at least it definitely is though other people say they aren't seeing the effect.\nThis on Fedora, with the equality test changed to  as actually doing a comparison seems irrelevant to the result, and the range pushed up to 200,000 as that seems to maximise the effect:I note that variations between the runs, and the order in which the expressions are run make very little difference to the result.Adding assignments to  and  into the slow version doesn't speed it up. In fact as we might expect assigning to local variables has negligible effect. The only thing that does speed it up is splitting the expression entirely in two. The only difference this should be making is that it reduces the maximum stack depth used by Python while evaluating the expression (from 4 to 3).That gives us the clue that the effect is related to stack depth, perhaps the extra level pushes the stack across into another memory page. If so we should see that making other changes that affect the stack will change (most likely kill the effect), and in fact that is what we see:So, I think the effect is entirely due to how much Python stack is consumed during the timing process. It is still weird though."},
{"body": "I want to send a POST request, but one of the fields should be a list of numbers. How can I do that ? (JSON?)If your server is expecting the POST request to be json, then you would need to add a header, and also serialize the data for your request...If you don't specify the header, it will be the default  type.I recommend using the incredible  module.for python 3.4.2 I found the following will work:You have to add header,or you will get http 400 error.\nThe code works well on python2.6,centos5.4code:This works perfect for Python Version 3.5, If the URL contains Query String / Parameter value,Request URL = Parameter value = 21f6bb43-98a1-419d-8f0c-8133669e40ca"},
{"body": "I'm trying to install pycurl via:It downloaded fine, but when when it runs setup.py I get the following traceback:Any idea why this is happening and how to get around it On Debian I needed the following package to fix this.Similarly with  package managerin my case this fixed the problem:as explained I encountered the same problem whilst trying to get Shinken 2.0.3 to fire up on Ubuntu. Eventually I did a full uninstall then reinstalled Shinken with . As it cleaned up, it mentioned:Installed that with , and all the brokers fired up as expected :-)That solved my problem on Ubuntu 14.04:On OpenSUSE:"},
{"body": "I'm running a set of tests with py.test. They pass. Yippie! But I'm getting this message:How should I go about tracking down the source of that? (I'm not using threading directly, but am using gevent.)I observed a similar issue and decided to see what's going on exactly - let me describe my findings. I hope someone will find it useful.It is indeed related to monkey-patching the  module. In fact, I can easily trigger the exception by importing the threading module before monkey-patching threads. The following 2 lines are enough:When executed it spits the message about ignored :If you swap the import lines, the problem is gone. I could stop my debugging here, but I decided it's worth to understand the exact cause of the problem.First step was to find the code that prints the message about ignored exception. It was a little hard for me to find it (grepping for  yielded nothing), but grepping around CPython source code I've eventually found a function called  in , with a very interesting comment:I decided to check who's calling it, with a little help from , just to get the following C-level stack trace:Now we can clearly see that the exception is thrown while  executes - this call is responsible for shutting down the Python interpreter, freeing allocated memory, etc. It's called just before exitting.Next step was to look at  code (it's in ). The very first call it makes is  - worth looking at, as we know the problem is related to threading. This function in turn calls  callable in the  module. Good, we can go back to python code now. Looking at  I've found the following interesting parts:Clearly, the responsibility of  call is to join all non-daemon threads and delete main thread (whatever that means exactly). I decided to patch  a bit - wrap the whole  body with / and print the stack trace with  module. This gave the following trace:Now we know the exact place where the exception is thrown - inside  method. The rest of the story is obvious after reading  for a while. The  dictionary maps thread IDs (as returned by ) to  instances, for all threads created. When  module is loaded, an instance of  class is always created and added to  (even if no other threads are explicitly created). The problem is that one of the methods patched by 's monkey-patching is  - original one maps to , monkey-patching replaces it with . Obviously both calls return different IDs for main thread.Now, if  module is loaded before monkey-patching,  call returns one value when  instance is created and added to , and another value at the time  is called - hence  in .On the contrary, if monkey-patching is done before  is loaded, all is fine - at the time  instance is being added to ,  is already patched, and the same thread ID is returned at cleanup time. That's it!To make sure I import modules in the right order, I added the following snippet to my code, just before monkey-patching call:I hope you find my debugging story useful :)You could use this:I had a similar problem with a gevent prototype script.The Greenlet callback was executing fine and I was synchronizing back to the main thread via g.join(). For my problem, I had to call gevent.shutdown() to shutdown (what I assume is) the Hub. After I manually shutdown the event loop, the program terminates properly without that error. "},
{"body": "Is there anyway to get tuple operations in Python to work like this:instead of:I know it works like that because the  and  methods are defined to work like that. So the only way would be to redefine them?Using all built-ins..This solution doesn't require an import:Sort of combined the first two answers, with a tweak to ironfroggy's code so that it returns a tuple:Note: using  instead of  to ease subclassing.gives .See All generator solution. Not sure on performance (itertools is fast, though)simple solution without class definition that returns tupleYes. But you can't redefine built-in types. You have to subclass them:Generator comprehension could be used instead of map. Built-in map function is not obsolete but it's less readable for most people than list/generator/dict comprehension, so I'd recommend not to use map function in general.even simpler and without using map, you can do that"},
{"body": "The greatest common divisor (GCD) of a and b is the largest number that divides both of them with no remainder.One way to find the GCD of two numbers is Euclid\u2019s algorithm, which is based on the observation that if  is the remainder when  is divided by , then . As a base case, we can use .Write a function called gcd that takes parameters  and  and returns their greatest common divisor.It's .Source code from the  module in Python 2.7:As of Python 3.5,  ; the one in  is deprecated. Moreover,  no longer returns explanatory source code for either method.The algorithms with m-n can runs awfully long.This one performs much better:This version of code utilizes Euclid's Algorithm for finding GCD.I think another way is to use recursion. Here is my code:A different approach based on euclid's algorithm.The value swapping didn't work well for me. So I just set up a mirror-like situation for numbers that are entered in either a < b OR a > b: in python with recursion:greatest_common_devisor(A)"},
{"body": "I'm looking for the same effect as  in JavaScript.I wrote a simple web-based interpreter this afternoon using Twisted.web. You basically submit a block of Python code through a form, and the client comes and grabs it and executes it. I want to be able to make a simple popup message, without having to re-write a whole bunch of boilerplate wxPython or TkInter code every time (since the code gets submitted through a form and then disappears).I've tried tkMessageBox:but this opens another window in the background with a tk icon. I don't want this. I was looking for some simple wxPython code but it always required setting up a class and entering an app loop etc. Is there no simple, catch-free way of making a message box in Python?You could use an import and single line code like this:Or define a function (Mbox) like so:Note the styles are as follows:Have fun!Note: edited to use  instead of Have you looked at ?Also you can position the other window before withdrawing it so that you position your messageThe code you presented is fine! You just need to explicitly create the \"other window in the background\" and hide it, with this code:Right before your messagebox.On Mac, the python standard library has a module called . There is also a (ctypes based) windows version at If it matters to you: it uses native dialogs and doesn't depend on Tkinter like the already mentioned , but it might not have as much features.In Windows, you can use :The PyMsgBox module does exactly this. It has message box functions that follow the naming conventions of JavaScript: alert(), confirm(), prompt() and password() (which is prompt() but uses * when you type). These function calls block until the user clicks an OK/Cancel button. It's a cross-platform, pure Python module with no dependencies.Install with: Sample usage:Full documentation at UseThe master window has to be created before. This is for Python 3. This is not fot wxPython, but for tkinter.Not the best, here is my basic Message box using only tkinter.check out my python module: pip install quickgui  (Requires wxPython, but requires no knowledge of wxPython)\nCan create any numbers of inputs,(ratio, checkbox, inputbox), auto arrange them on a single gui. "},
{"body": "How would I generate a random date that has to be between two other given dates? \nThe functions signature should something like this-and would return a date such as-\n\"2/4/2008 7:20 PM\"Convert both strings to timestamps (in your chosen resolution, e.g. milliseconds, seconds, hours, days, whatever), subtract the earlier from the later, multiply your random number (assuming it is distributed in the range [0, 1]) with that difference, and add again to the earlier one.  Convert the timestamp back to date string and you have a random time in that range.Python example (output is almost in the format you specified, other than 0 padding - blame the American time format conventions):The precision is seconds. You can increase precision up to microseconds, or decrease to, say, half-hours, if you want. For that just change the last lines calculation.example run:output:A tiny version.Note that both  and  arguments should be  objects. If\nyou've got strings instead, it's fairly easy to convert. The other answers point\nto some ways to do so.It's very simple using radar$ pip install radarThis is a different approach - that sort of works..To chip in a pandas-based solution I use:I like it, because of the nice  features that allow me to throw different stuff and formats at it. Consider the following few examples...Your signature.Random position.Different format.Passing pandas/datetime objects directly.The easiest way of doing this is to convert both numbers to timestamps, then set these as the minimum and maximum bounds on a random number generator.A quick PHP example would be:This function makes use of  to convert a datetime description into a Unix timestamp, and  to make a valid date out of the random timestamp which has been generated.Here is an answer to the literal meaning of the title rather than the body of this question:This code is based loosely on the accepted answer.Since Python 3  supports multiplication with floats, so now you can do:given that  and  are of the type . For example, to generate a random datetime within the next day:You can Use ,and,Many algorithms for converting date to and from numbers are already available in many operating systems.What do you need the random number for? Usually (depending on the language) you can get the number of seconds/milliseconds from the Epoch from a date. So for a randomd date between startDate and endDate you could do:Conceptually it's quite simple.  Depending on which language you're using you will be able to convert those dates into some reference 32 or 64 bit integer, typically representing seconds since epoch (1 January 1970) otherwise known as \"Unix time\" or milliseconds since some other arbitrary date.  Simply generate a random 32 or 64 bit integer between those two values.  This should be a one liner in any language.On some platforms you can generate a time as a double (date is the integer part, time is the fractional part is one implementation).  The same principle applies except you're dealing with single or double precision floating point numbers (\"floats\" or \"doubles\" in C, Java and other languages).  Subtract the difference, multiply by random number (0 <= r <= 1), add to start time and done.In python:(need python  library \u2013 )Use ApacheCommonUtils to generate a random long within a given range,\nand then create Date out of that long. Example:import org.apache.commons.math.random.RandomData;import org.apache.commons.math.random.RandomDataImpl;public Date nextDate(Date min, Date max) {}I made this for another project using random and time. I used a general format from time you can view the documentation  for the first argument in strftime(). The second part is a random.randrange function. It returns an integer between the arguments. Change it to the ranges that match the strings you would like. You must have nice arguments in the tuple of the second arugment.Just to add another one:The day handling needs some considerations. With 28 you are on the secure site.Pandas + numpy solutiondts is the difference between timestamps in seconds (float). It is then used to create a pandas timedelta between 0 and dts, that is added to the start timestamp.Based on the answer by mouviciel, here is a vectorized solution using numpy. Convert the start and end dates to ints, generate an array of random numbers between them, and convert the whole array back to dates. "},
{"body": "How do I limit  to only return files in the directory I provide it?Use the  function.It works just like , but you can pass it a  parameter that indicates how deep the recursion will go.Don't use os.walk.Example:I think the solution is actually very simple.use to only do first iteration of the for loop, there must be a more elegant way.The first time you call os.walk, it returns tulips for the current directory, then on next loop the contents of the next directory.  Take original script and just add a .The suggestion to use  is a good one.  The direct answer to your question is If you have more complex requirements than just the top directory (eg ignore VCS dirs etc), you can also modify the list of directories to prevent os.walk recursing through them.ie:Note - be careful to mutate the list, rather than just rebind it.  Obviously os.walk doesn't know about the external rebinding.You could use  which returns a list of names (for both files and directories) in a given directory. If you need to distinguish between files and directories, call  on each name.The same idea with , but shorter:In Python 3, I was able to do this:Felt like throwing my 2 pence in.You could also do the following:This is how I solved itThere is a catch when using listdir.  The os.path.isdir(identifier) must be an absolute path.  To pick subdirectories you do:The alternative is to change to the directory to do the testing without the os.path.join().You can use this snippet"},
{"body": "Is there an efficient way to know how many elements are in an iterator in Python, in general, without iterating through each and counting?No. It's not possible.Example:Length of  is unknown until you iterate through it.This code should work:Although it does iterate through each item and count them, it is the fastest way to do so.No, any method will require you to resolve every result. You can do but running that on an infinite iterator will of course never return. It also will consume the iterator and it will need to be reset if you want to use the contents.Telling us what real problem you're trying to solve might help us find you a better way to accomplish your actual goal.Edit: Using  will read the whole iterable into memory at once, which may be undesirable. Another way is to doas another person posted. That will avoid keeping it in memory.You cannot (except the type of a particular iterator implements some specific methods that make it possible).Generally, you may count iterator items only by consuming the iterator. One of probably the most efficient ways:(For Python 3.x replace  with ).Kinda. You  check the  method, but be warned that (at least up to Python 3.4, as gsnedders helpfully points out) it's a  (), that could very well vanish or summon nasal demons instead.Otherwise, no. Iterators are just an object that only expose the  method. You can call it as many times as required and they may or may not eventually raise . Luckily, this behaviour is most of the time transparent to the coder. :)An iterator is just an object which has a pointer to the next object to be read by some kind of buffer or stream, it's like a LinkedList where you don't know how many things you have until you iterate through them. Iterators are meant to be efficient because all they do is tell you what is next by references instead of using indexing (but as you saw you lose the ability to see how many entries are next).Regarding your original question, the answer is still that there is no way in general to know the length of an iterator in Python.Given that you question is motivated by an application of the pysam library, I can give a more specific answer: I'm a contributer to PySAM and the definitive answer is that SAM/BAM files do not provide an exact count of aligned reads.  Nor is this information easily available from a BAM index file.  The best one can do is to estimate the approximate number of alignments by using the location of the file pointer after reading a number of alignments and extrapolating based on the total size of the file.  This is enough to implement a progress bar, but not a method of counting alignments in constant time.I like the  package for this, it is very lightweight and tries to use the fastest possible implementation available depending on the iterable.Usage:The actual  implementation is as follows:There are two ways to get the length of \"something\" on a computer.The first way is to store a count - this requires anything that touches the file/data to modify it (or a class that only exposes interfaces -- but it boils down to the same thing).The other way is to iterate over it and count how big it is.It's common practice to put this type of information in the file header, and for pysam to give you access to this.  I don't know the format, but have you checked the API?As others have said, you can't know the length from the iterator.This is against the very definition of an iterator, which is a pointer to an object, plus information about how to get to the next object.An iterator does not know how many more times it will be able to iterate until terminating.  This could be infinite, so infinity might be your answer.A quick benchmark:The results:I.e. the simple count_sum is the way to go."},
{"body": "I am writing a Python module that includes Cython extensions and uses  (and ).  I am open to using either  or , or some kind of  or  solution if necessary.  What is important is that I am able to call  and  routines from Cython in tight loops without Python call overhead.I've found one example .  However, that example depends on SAGE.  I want my module to be installable without installing SAGE, since my users are not likely to want or need SAGE for anything else.  My users are likely to have packages like numpy, scipy, pandas, and scikit learn installed, so those would be reasonable dependencies.  What is the best combination of interfaces to use, and what would the minimal setup.py file look like that could fetch the necessary information (from numpy, scipy, etc.) for compilation?\nHere is what I ended up doing.  It works on my macbook, but I have no idea how portable it is.  Surely there's a better way.This works because, on my macbook, the  header file is in the same directory as .  I can then do this in my pyx file:If I have understood the question correctly, you could make use of SciPy's Cython wrappers for BLAS and LAPACK routines. These wrappers are documented here:As the documentation states, you are responsible for checking that any arrays that you pass to these functions are aligned correctly for the Fortran routines. You can simply import and use these functions as needed in your .pyx file. For instance:Given that this is well-tested, widely-used code that runs on different platforms, I'd argue that it is a good candidate for reliably distributing Cython extensions that directly call BLAS and LAPACK routines.If you do not want your code to have a dependency on the entirety of SciPy, you can find many of the relevant files for these wrapper functions in SciPy's  directory . A useful reference is  which list the source and header files. Note that a Fortran compiler is required!In  it should be possible to isolate only the source files here that are needed to compile the BLAS and LAPACK Cython wrappers and then bundle them as an independent extension with your module. In  this is very fiddly to do. The build process for the linalg submodule requires some Python functions to aid the compilation on different platforms (e.g. from ). Building also relies upon other C and Fortran source files (), the paths of which are hard-coded into these Python functions. Clearly a lot of work has gone into making sure that SciPy compiles properly on different operating systems and architectures.I'm sure it is possible to do, but after shuffling files about and tweaking paths, I have not yet found the right way to build this part of the linalg submodule independently from the rest of SciPy. Should I find the correct way, I'll be sure to update this answer."},
{"body": "I am trying to load the MNIST dataset linked  in Python 3.2 using this program:Unfortunately, it gives me the error:I then tried to decode the pickled file in Python 2.7, and re-encode it.  So, I ran this program in Python 2.7:It ran without error, so I reran this program in Python 3.2:However, it gave me the same error as before.  How do I get this to work?This seems like some sort of incompatibility. It's trying to load a \"binstring\" object,  which is assumed to be ASCII, while in this case it is binary data. If this is a bug in the Python 3 unpickler, or a \"misuse\" of the pickler by numpy, I don't know.Here is something of a workaround, but I don't know how meaningful the data is at this point:Unpickling it in Python 2 and then repickling it is only going to create the same problem again, so you need to save it in another format.If you are getting this error in python3, then, it could be an incompatibility issue between python 2 and python 3, for me the solution was to  with  encoding:It looks like  in pickle between 2.x and 3.x due to the move to unicode. Your file appears to be pickled with python 2.x and decoding it in 3.x could be troublesome.I'd suggest unpickling it with python 2.x and saving to a format that plays more nicely across the two versions you're using.It appears to be an incompatibility issue between Python 2 and Python 3.  I tried loading the MNIST dataset withand it worked for Python 3.5.2"},
{"body": "When implementing a class with multiple properties (like in the toy example below), what is the best way to handle hashing?I guess that the  and  should be consistent, but how to implement a proper hash function that is capable of handling all the properties?I read on  that tuples are hashable, so I was wondering if something like the example above was sensible. Is it? should return the same value for objects that are equal. It also shouldn't change over the lifetime of the object; generally you only implement it for immutable objects.A trivial implementation would be to just . This is always correct, but performs badly.Your solution, returning the hash of a tuple of properties, is good. But note that you don't need to list all properties that you compare in  in the tuple. If some property usually has the same value for inequal objects, just leave it out. Don't make the hash computation any more expensive than it needs to be.Edit: I would recommend against using xor to mix hashes in general. When two different properties have the same value, they will have the same hash, and with xor these will cancel eachother out. Tuples use a more complex calculation to mix hashes, see  in .It's dangerous to writebecause if your rhs (i.e., ) object evaluates to boolean False, it will never compare as equal to anything!In addition, you might want to double check if  belongs to the class or subclass of . If it doesn't, you'll either get exception  or a false positive (if the other class happens to have the same-named attributes with matching values). So I would recommend to rewrite  as:If by any chance you want an unusually flexible comparison, which compares across unrelated classes as long as attributes match by name, you'd still want to at least avoid  and check that  doesn't have any additional attributes. How you do it depends on the situation (since there's no standard way to find all attributes of an object).Documentation for "},
{"body": "I'd like to point to a function that does nothing:my use case is something like thisOf course, I could use the  defined above, but a built-in would certainly run faster (and avoid bugs introduced by my own).Apparently,  and  use  for the identity, but this is specific to their implementations.Doing some more research, there is none, a feature was asked in  And from :So a better way to do it is actually (a lambda avoids naming the function):ORNo, there isn't a built-in  function, but writing one that behaves appropriately for both single and multiple inputs is not difficult:And in use:While you might argue that the second result is a , and you didn't pass a  into the  function, you should also realize that s are how Python passes groups of objects around as a single entity, and in use is what you would want:rds' versions will either:rds' first version (no name, assigned to ):rds' second version (also no name, also assigned to ):yours will work fine. When the number of parameters is fix you can use an anonymous function like this:No, there isn't.Note that your :So, you may want to use  if you want a true identity function.The thread is pretty old. But still wanted to post this.It is possible to build an identity method for both arguments and objects. In the example below, ObjOut is an identity for ObjIn. All other examples above haven't dealt with dict **kwargs."},
{"body": "Does anybody know how Python manage internally int and long types? How should I understand the code below?Update:  and  were \"unified\" . Before that it was possible to overflow an int through math ops.3.x has further advanced this by eliminating int altogether and only having long.Python 2:  contains the maximum value a Python int can hold.Python 3:  contains the maximum value a Python int can hold. This  should help. Bottom line is that you really shouldn't have to worry about it in python versions > 2.4On my machine:Python uses ints (32 bit signed integers, I don't know if they are C ints under the hood or not) for values that fit into 32 bit, but automatically switches to longs (arbitrarily large number of bits - i.e. bignums)  for anything larger. I'm guessing this speeds things up for smaller values while avoiding any overflows with a seamless transition to bignums.Interesting.  On my 64-bit (i7 Ubuntu) box:Guess it steps up to 64 bit ints on a larger machine.Python 2.7.9 auto promotes numbers.\nFor a case where one is unsure to use int() or long().It manages them because  and  are sibling class definitions.  They have appropriate methods for +, -, *, /, etc., that will produce results of the appropriate class.For exampleIn this case, the class  has a  method (the one that implements *) which creates a  result when required.From python 3.x, the unified integer libries are even more smarter than older versions. On my (i7 Ubuntu) box I got the following,For implementation details refer  files. The last file is a dynamic module (compiled to an so file). The code is well commented to follow."},
{"body": "How do you dynamically set local variable in Python?(where the variable name is dynamic) I'm aware this isn't good practice, and the remarks are legit, but this doesn't make it a bad question, just a more theoretical one - I don't see why this justifies downvotes.Contrary to other answers already posted you cannot modify  directly and expect it to work.Modifying  is undefined. Outside a function when  and  are the same it will work; inside a function is will  not work.Use a dictionary, or set an attribute on an object:or if you prefer, use a class::\nAccess to variables in namespaces that aren't functions (so modules, class definitions, instances) are usually done by dictionary lookups (as Sven points out in the comments there are exceptions, for example classes that define ). Function locals can be optimised for speed because the compiler (usually) knows all the names in advance, so there isn't a dictionary until you call .In the C implementation of Python  (called from inside a function) creates an ordinary dictionary initialised from the current values of the local variables. Within each function any number of calls to  will return the same dictionary, but every call to  will update it with the current values of the local variables. This can give the impression that assignment to elements of the dictionary are ignored (I originally wrote that this was the case). Modifications to existing keys within the dictionary returned from  therefore only last until the next call to  in the same scope.In IronPython things work a bit differently. Any function that calls  inside it uses a dictionary for its local variables so assignments to local variables change the dictionary and assignments to the dictionary change the variables  that's only if you explicitly call  under that name. If you bind a different name to the  function in IronPython then calling it gives you the local variables for the scope where the name was bound and there's no way to access the function locals through it:This could all change at any time. The only thing guaranteed is that you cannot depend on the results of assigning to the dictionary returned by .Others have suggested assigning to . This won't work inside a function, where locals are accessed using the  opcode,  you have an  statement somewhere in the function. To support this statement, which could create new variables that are not known at compile time, Python is then forced to access local variables by name within the function, so writing to  works. The  can be out of the code path that is executed.Note: This only works in Python 2.x. They did away with this foolishness in Python 3, and other implementations (Jython, IronPython, etc.) may not support it either.This is a bad idea, though. How will you access the variables if you don't know their name? By  probably. So why not just use your own dictionary rather than polluting  (and taking the chance of overwriting a variable your function actually needs)?I've spent the last... couple hours, I guess, trying to hack around the lack of function closures, and I came up with this, which might help:It's a pretty heavy handed / contrived example, but if there are a lot of locals or you're still in the process of prototyping this pattern becomes useful.  Mostly I was just bitter about all the data stores being replicated or moved in order to handle callback delegates, etc.You can use a local dictionary and put all the dynamic bindings as items into the dictionary. Then knowing the name of such a \"dynamic variable\" you can use the name as the key to get its value.(Just a quick note for others googlin')Ok, so modifying   ( while modifying  ). In the meantime,  , but it's painfully slow, so, as with regular expressions, we may want to  it first:Let's say We have the below dictionary:I want to create new dictionaries as below:a oneliner:would be outputted to:But to precisely declaring variables we could go with:As can be seen the first 3  variables:As mentioned by others, if you want to put it in a function you should add it to the :\nBut a better way would be to have some dict that holds all your dynamic variable names as dictionary keys:"},
{"body": "Do child processes spawned via  share objects created earlier in the program?I have the following setup:I'm loading some big object into memory, then creating a pool of workers that need to make use of that big object. The big object is accessed read-only, I don't need to pass modifications of it between processes.My question is: is the big object loaded into shared memory, as it would be if I spawned a process in unix/c, or does each process load its own copy of the big object? Update: to clarify further - big_lookup_object is a shared lookup object. I don't need to split that up and process it separately. I need to keep a single copy of it. The work that I need to split it is reading lots of other large files and looking up the items in those large files against the lookup object.Further update: database is a fine solution, memcached might be a better solution, and file on disk (shelve or dbm) might be even better. In this question I was particularly interested in an in memory solution. For the final solution I'll be using hadoop, but I wanted to see if I can have a local in-memory version as well.\"Do child processes spawned via multiprocessing share objects created earlier in the program?\"No.  Processes have independent memory space.To make best use of a large structure with lots of workers, do this.Each process reads, does work and writes.This is remarkably efficient since all processes are running concurrently.  The writes and reads pass directly through shared buffers between the processes.In some cases, you have a more complex structure -- often a \"fan-out\" structure.  In this case you have a parent with multiple children.The child parts are pleasant to write because each child simply reads .  The parent has a little bit of fancy footwork in spawning all the children and retaining the pipes properly, but it's not too bad.Fan-in is the opposite structure.  A number of independently running processes need to interleave their inputs into a common process.  The collector is not as easy to write, since it has to read from many sources.  Reading from many named pipes is often done using the  module to see which pipes have pending input.Shared lookup is the definition of a database.  Solution 3A -- load a database.  Let the workers process the data in the database.Solution 3B -- create a very simple server using  (or similar) to provide WSGI applications that respond to HTTP GET so the workers can query the server.Shared filesystem object.  Unix OS offers shared memory objects.  These are just files that are mapped to memory so that swapping I/O is done instead of more convention buffered reads.You can do this from a Python context in several waysIt depends. For global read-only variables it can be often considered so (apart from the memory consumed) else it should not. 's documentation says:On Windows (single CPU):With :Without : is correct. Python's multiprocessing shortcuts effectively give you a separate, duplicated chunk of memory.On most *nix systems, using a lower-level call to  will, in fact, give you copy-on-write memory, which might be what you're thinking. AFAIK, in theory, in the most simplistic of programs possible, you could read from that data without having it duplicated.However, things aren't quite that simple in the Python interpreter. Object data and meta-data are stored in the same memory segment, so even if the object never changes,  something like a reference counter for that object being incremented will cause a memory write, and therefore a copy. Almost any Python program that is doing more than \"print 'hello'\" will cause reference count increments, so you will likely never realize the benefit of copy-on-write.Even if someone did manage to hack a shared-memory solution in Python, trying to coordinate garbage collection across processes would probably be pretty painful.If you're running under Unix, they may share the same object, due to  (i.e., the child processes have separate memory but it's copy-on-write, so it may be shared as long as nobody modifies it).  I tried the following:and got the following output:Of course this doesn't  that a copy hasn't been made, but you should be able to verify that in your situation by looking at the output of  to see how much real memory each subprocess is using.Different processes have different address space. Like running different instances of the interpreter. That's what IPC (interprocess communication) is for.You can use either queues or pipes for this purpose. You can also use rpc over tcp if you want to distribute the processes over a network later.Not directly related to multiprocessing per se, but from your example, it would seem you could just use the  module or something like that. Does the \"big_lookup_object\" really have to be completely in memory?For Linux/Unix/MacOS platform, forkmap is a quick-and-dirty solution. "},
{"body": "This is ugly. What's a more Pythonic way to do it?Generally, you can use the  syntax. You can even pass a part of the tuple, which seems like what you're trying to do here:This is called  a tuple, and can be used for other iterables (such as lists) too. Here's another example (from the ):Refer "},
{"body": "Is there a Bash equivalent to the Python's  statement?You can use  for this. is a command that successfully does nothing.( would, in a way, be the opposite: it doesn't do anything, but claims that a failure occurred.)"},
{"body": "I wish to write to a file based on whether that file already exists or not, only writing if it doesn't already exist (in practice, I wish to keep trying files until I find one that doesn't exist).The following code shows a way in which a potentially attacker could insert a symlink, as suggested in  in between a test for the file and the file being written. If the code is run with high enough permissions, this could overwrite an arbitrary file.Is there any way to solve this problem?: See also : from Python 3.3, you can use the  flag to  to provide this function.Yes, but not using Python's standard  call. You'll need to use  instead, which allows you to specify flags to the underlying C code.In particular, you want to use . From the man page for  under  on my Unix system:So it's not perfect, but AFAIK it's the closest you can get to avoiding this race condition.Edit: the other rules of using  instead of  still apply. In particular, if you want use the returned file descriptor for reading or writing, you'll need one of the ,  or  flags as well.All the  flags are in Python's  module, so you'll need to  and use  etc.For reference, Python 3.3 implements a new  mode in the  function to cover this use-case (create only, fail if file exists).This code will easily create a FILE if one does not exists."},
{"body": "Today, I saw one statement which didn't throw an exception. Can anyone explain the theory behind it?In Python, every  can be unpacked:Moreover, because iterating over a dictionary returns only its keys:unpacking a dictionary (which iterates over it) likewise unpacks only its keys.Actually, I should say that every iterable can be unpacked  the names to unpack into equals the length of the iterable:But this is only the case for Python 2.x.  In Python 3.x, you have , which allows you to unpack an iterable of any (finite) size into just the names you need:Iterating a  iterates over the keys. Since your dict literal has exactly two keys, you can unpack it into a 2-tuple.This is probably not a good practice in general, since dicts are unordered and  and  would be a prefectly legal outcome of that code.when you iterate over a dictionary, you get its keysUnpacking is nothing else than iterating over the object and put the elements in the given variables:No rocket science behind it.  is an iterable, which return the keys in each iteration.  can receive any iterable as argument (as long as they are finite), so:Seeing this, is easy to infer that unpacking will work as shown. Moreover, any  iterable can be unpacked:When in iterable context, dicts are treated as an (unordered) collection of keys, which is what you get when you do , which is the same as calling  on the dict:However, you can also do more.You can unpack both a 's both keys  values if you turn it into a list of pairs first:or if you just want the pairsor, say, just the keys:etc.But of course since dictionaries \u2014 and I mean in general, not only in Python \u2014 contain their items in an un-ordered manner,  (in Python) will also return them in a seemingly arbitrary order, and thus there is no way to know which key will be stored in which variable:As you see, the order of the pairs returned by  was reversed in comparison to their definition order."},
{"body": "After updating to new version 2016.2, I am getting'tests' is a package inside my main app package, and I receive these warnings when I try to execute unit tests inside this folder. This issue only came up after updating to 2016.2. Besides the warnings, the remaining code works fine.Edit: This is a known issue - . They are suggesting to replace utrunner.py in PyCharm installation folder.This is a known issue introduced with the 2016.2 release. Progress can be followed on the JetBrains website . According to this page it is due to be fixed in the 2017.1 release. You can follow the utrunner.py workaround that others have mentioned in the meantime - a copy of this file is attached to the linked ticket.The latest recommendation  is to put this line at the top of your unit test script:On OS X I've fixed this by replacing with an older version that can be found at \n    On  Bobby's solution also works:Just replace your local  file at by the one from the Jetbrains website:  On Windows 10 Bobby's solution also works:Just replace your local utrunner.py file atwith the one from the Jetbrains website: "},
{"body": "How to make a multi-thread python program response to Ctrl+C key event? The code is like this:I tried to remove join() on all threads but it still doesn't work. Is it because the lock segment inside each thread's run() procedure? The above code is supposed to work but it always interrupted when current variable was in 5,000-6,000 range and through out the errors as belowMake every thread except the main one a daemon ( in 2.6 or better,  in 2.6 or less, for every thread object  before you start it).  That way, when the main thread receives the KeyboardInterrupt, if it doesn't catch it or catches it but decided to terminate anyway, the whole process will terminate.  See .: having just seen the OP's code (not originally posted) and the claim that \"it doesn't work\", it appears I have to add...:Of course, if you want your main thread to stay responsive (e.g. to control-C), don't mire it into blocking calls, such as ing another thread -- especially not totally  blocking calls, such as ing  threads.  For example, just change the final loop in the main thread from the current (utterless and damaging):to something more sensible like:if your main has nothing better to do than either for all threads to terminate on their own, or for a control-C (or other signal) to be received.Of course, there are many other usable patterns if you'd rather have your threads not terminate abruptly (as daemonic threads may) -- unless , too, are mired forever in unconditionally-blocking calls, deadlocks, and the like;-).There're two main ways, one clean and one easy.The clean way is to catch KeyboardInterrupt in your main thread, and set a flag your background threads can check so they know to exit; here's a simple/slightly-messy version using a global:The messy but easy way is to catch KeyboardInterrupt and call os._exit(), which terminates all threads immediately.A  might be helpful for you:I would rather go with the code proposed in :What I have changed is the  from  to . The actual number of seconds does not matter, unless you specify a timeout number, the main thread will stay responsive to Ctrl+C. The except on  makes the signal handling more explicit.If you spawn a Thread like so -  - and then do . When CTRL-C is initiated, the main thread doesn't exit because it is waiting on that blocking  call. To fix this, simply put in a timeout on the .join() call. The timeout can be as long as you wish. If you want it to wait indefinitely, just put in a really long timeout, like 99999. It's also good practice to do  so all the threads exit when the main thread(non-daemon) exits.You can always set your threads to \"daemon\" threads like:And whenever the main thread dies all threads will die with it.When you want to kill the thread just use:"},
{"body": "This may sound like a stupid question, since the very purpose of  is to this exactly: Installing some specific version of a package (in this case Django) inside the virtual environment. But it's exactly what I want to do, and I can't figure it out.I'm on Windows XP, and I created the virtual environment successfully, and I'm able to run it, but how am I supposed to install the Django version I want into it? I mean, I know to use the newly-created  script, but how do I make it install Django 1.0.7? If I do , it will install the latest version. I tried putting the version number  into this command in various ways, but nothing worked.How do I do this?There was never a Django 1.0.7. The 1.0 series only went up to 1.0.4. You can see all the releases in the .However to answer your question, don't use , use . (If it's not already installed, do , then never touch easy_install again). Now you can do:+1 on the previous poster's reply: use  if you can. But, in a pinch, the easiest way is to install an older version would be to download the tarball from the  page or, if you have subversion installed, do an  of the release you want (they are all tagged ). Once you have the version of Django you want, just run the following command inside the django directory:This will install that version of Django in your virtualenv. "},
{"body": "Is there a way to automatically create the db file if it doesn't already exist when I connect to it?thanksThe code you give  create  if it doesn't exist.If it isn't created automatically, make sure that you have the directory permissions correctPretty sure .connect will create the file if it doesn't exist.As it was already mentioned, your code should work if you have permissions to write for this path. However, it is important that . If you make call for non-existing folder:It will not work, you will have The same is for relative paths:First will always work, because you are working in already existing directory and second will not work if you do not create te folder beforehand..connect should create a new database file on the fly,  given sub-directories do exist, and you have adequate permissioning."},
{"body": "I had a string which is stored in a variable . I want to split it with delimiter like we do using  in PHP.What is the equivalent in Python?Choose one you need: and The alternative for explode in php is .The first parameter is the delimiter, the second parameter the maximum number splits. The parts are returned without the delimiter present (except possibly the last part). When the delimiter is None, all whitespace is matched. This is the default."},
{"body": "When is it better to use  instead of ?When you know you'll want the full list of items constructed (for instance, for passing to a function that would modify that list in-place). Or when you want to force the arguments you're passing to  to be completely evaluated at that specific point. computes all the list at once,  computes the elements only when requested.One important difference is that 'zip' returns an actual list, 'izip' returns an 'izip object', which is not a list and does not support list-specific features (such as indexing):So, if you need a list (an not a list-like object), just use 'zip'.Apart from this, 'izip' can be useful for saving memory or cycles.E.g. the following code may exit after few cycles, so there is no need to compute all items of combined list:using  would have computed   couples before entering the cycle.Moreover, if  and  are very large (e.g. millions of records),  will build a third list with double space.But if you have small lists, maybe  is faster.In 2.x, when you  a list instead of an iterator.The itertools library provides \"iterators\" for common Python functions. From the itertools docs, \"Like zip() except that it returns an iterator instead of a list.\" The I in izip() means \"iterator\".Python iterators are a \"lazy loaded\" sequence that saves memory over regular in-memory list. So, you would use itertools.izip(a, b) when the two inputs a, b are too big to keep in memory at one time.Look up the Python concepts related to efficient sequential processing:"},
{"body": "I'd really like to be able to print out valid SQL for my application, including values, rather than bind parameters, but it's not obvious how to do this in SQLAlchemy (by design, I'm fairly sure). Has anyone solved this problem in a general way?In the vast majority of cases, the \"stringification\" of a SQLAlchemy statement or query is as simple as:This applies both to an ORM  as well as any  or other statement.: the following detailed answer is being maintained on the .To get the statement as compiled to a specific dialect or engine, if the statement itself is not already bound to one you can pass this in to :or without an engine:When given an ORM  object, in order to get at the  method we only need access the  accessor first:with regards to the original stipulation that bound parameters are to be \"inlined\" into the final string, the challenge here is that SQLAlchemy normally is not tasked with this, as this is handled appropriately by the Python DBAPI, not to mention bypassing bound parameters is probably the most widely exploited security holes in modern web applications.   SQLAlchemy has limited ability to do this stringification in certain circumstances such as that of emitting DDL.  In order to access this functionality one can use the 'literal_binds' flag, passed to :the above approach has the caveats that it is only supported for basic\ntypes, such as ints and strings, and furthermore if a \nwithout a pre-set value is used directly, it won't be able to\nstringify that either.To support inline literal rendering for types not supported, implement\na  for the target type which includes a\n method:producing output like:This works in python 2 and 3 and is a bit cleaner than before, but requires SA>=1.0.Demo:Gives this output: (tested in python 2.7 and 3.4)This code is based on brilliant  from @bukzor. I just added custom render for  type into Oracle's . Feel free to update code to suit your database:So building on @zzzeek's comments on @bukzor's code I came up with this to easily get a \"pretty-printable\" query:I personally have a hard time reading code which is not indented so I've used  to reindent the SQL. It can be installed with .I would like to point out that the solutions given above do not \"just work\" with non-trivial queries. One issue I came across were more complicated types, such as pgsql ARRAYs causing issues. I did find a solution that for me, did just work even with pgsql ARRAYs:borrowed from:\nThe linked code seems to be based on an older version of SQLAlchemy. You'll get an error saying that the attribute _mapper_zero_or_none doesn't exist. Here's an updated version that will work with a newer version, you simply replace _mapper_zero_or_none with bind. Additionally, this has support for pgsql arrays:Tested to two levels of nested arrays."},
{"body": "How do I install SciPy on my system?For the NumPy part (that SciPy depends on) there is actually an installer for 64 bit Windows:  (is direct download URL, 2310144 bytes).Running the SciPy superpack installer results in this\nmessage in a dialog box:I already have Python 2.6.2 installed (and a working Django installation\nin it), but I don't know about any Registry story.The registry entries seem to already exist:What I have done so far:Downloaded the NumPy superpack installer\nnumpy-1.3.0rc2-win32-superpack-python2.6.exe\n(, 4782592 bytes). Running this installer\nresulted in the same message, \"Cannot install. Python\nversion 2.6 required, which was not found in the registry.\".\n: there is actually an installer for NumPy that works - see beginning of the question.Tried to install NumPy in another way. Downloaded the zip\npackage numpy-1.3.0rc2.zip (, 2404011 bytes),\nextracted the zip file in a normal way to a temporary\ndirectory, D:\\temp7\\numpy-1.3.0rc2 (where setup.py and\nREADME.txt is). I then opened a command line window and:This ran for a long time and also included use of cl.exe\n(part of Visual Studio). Here is a nearly 5000 lines long\n (230 KB).This seemed to work. I can now do this in Python:with this result:Downloaded the SciPy superpack installer, scipy-0.7.1rc3-\nwin32-superpack-python2.6.exe (, 45597175\nbytes). Running this installer resulted in the message\nlisted in the beginningTried to install SciPy in another way. Downloaded the zip\npackage scipy-0.7.1rc3.zip (, 5506562\nbytes), extracted the zip file in a normal way to a\ntemporary directory, D:\\temp7\\scipy-0.7.1 (where setup.py\nand README.txt is). I then opened a command line window and:This did not achieve much - here is a  (about 95\nlines).And it fails:Platform: Python 2.6.2 installed in directory D:\\Python262,\nWindows XP 64 bit SP2, 8 GB RAM, Visual Studio 2008\nProfessional Edition installed.The startup screen of the installed Python is:Value of PATH, result from SET in a command line window:I haven't tried it, but you may want to download  of . It comes with Scipy-0.7.0b1 running on Python 2.5.4. Unofficial 64-bit installers for  and  are available at You'll want to ;  From a CMD prompt with administrator privileges for a system-wide (aka. Program Files) install: include the  flag to install to the current user's application folder (Typically  on Windows) from a  CMD prompt:Then do the same for SciPy:Short answer: windows 64 support is still work in progress at this time. The superpack will certainly not work on a 64 bits python (but it should work fine on a 32 bits python, even on windows 64).The main issue with windows 64 is that building with mingw-w64 is not stable at this point: it may be our's (numpy devs) fault, python's fault or mingw-w64. Most likely a combination of all those :). So you have to use proprietary compilers: anything other than MS compiler crashes numpy randomly; for the fortran compiler, ifort is the one to use. As of today, both numpy and scipy source code can be compiled with VS 2008 and ifort (all tests passing), but building it is still quite a pain, and not well supported by numpy build infrastructure.As the transcript for SciPy told you, SciPy isn't really supposed to work on Win64:So I would suggest to install the 32-bit version of Python, and stop attempting to build SciPy yourself. If you still want to try anyway, you first need to compile BLAS and LAPACK, as PiotrLegnica says. See the transcript for the places where it was looking for compiled versions of these libraries. is an open-source distribution that has 64-bit numpy and scipy.Another alternative: Free and includes lots of stuff meant to work together smoothly. says Though I'm not quite sure what that means.This appears to be dead. I use  now, which has 32-bit or 64-bit installers.For completeness:   has a Python distribution which includes SciPy; however, it's not free.  Caveat:  I've never used it.Update: This answer had been long forgotten until an upvote brought me back to it.  At this time, I'll second  of , which is free.Try to install Python 2.6.3 over your 2.6.2 (this should also add correct Registry entry), or to register your existing installation . Installer should work after that.Building SciPy requires a Fortran compiler and libraries -  and .It is terrible to install such Python data science packages independently on Windows. Try  (one installer, 400 more Python packages, py2 & py3 support). Anaconda really helps me a lot!I have a 32bit python3.5 on a 64bit win8.1 machine, I just tried almost every way I can find on Stackoverflow and no one works !Then on : I found it says:then I installed mklthen I install scipy:It worked~ yeah :)A tip: You can just google \"whl_file_name.whl\" to know where to download it~ :)After all these steps you will find that you still can not use scipy in python3, if you print \"import scipy\" you will find there are error messages, but don't worry, there is only one more thing to do. \n\u2014\u2014just comment out that line ,simple and useful.I promise that it is the last thing to do :)PS: \nBefore all these steps, you better install numpy first, that's very simple using command: I was getting this same error on a 32-bit machine.  I fixed it by registering my Python installation, using the script at: It's possible that the script would also make the 64-bit superpack installers work.Install Python distribution .Download and Install Anaconda Python Distribution.Make Anaconda Python distribution link to py3.3, if you want Numpy, Scipy or Matplotlib to work in py3.3 or just use it like that to have only py2.7 and older functionality.The link below provides more detail about Anaconda:\n.You can either download a scientific python distribution. One of the ones mentioned :  Or pip install from whl file  if the above is not an option for you.Okay a lot has been said but just in-case nothing above works, you can try;According to them;\nOkey, Here I am going to share what I have done to install  on my windows-pc without command. "},
{"body": "I have a problem with Python threading and sending a string in the arguments..Where dRecieved is the string of one line read by a connection. It calls a simple function which as of right now has only one job of printing \"hello\".However I get the following error232 is the length of the string that I am trying to pass, so I guess its breaking it up into each character and trying to pass the arguments like that. It works fine if I just call the function normally but I would really like to set it up as a separate thread.You're trying to create a tuple, but you're just parenthesizing a string :)Add an extra ',': Or use brackets to make a list:If you notice, from the stack trace: The  turns your string into a list of characters, passing them to the \nfunction.  If you pass it a one element list, it will pass that element as the first argument - in your case, the string."},
{"body": "This function doesn't work and raises an error. Do I need to change any arguments or parameters?  If the file does not exists,  will fail.You can use , which creates the file if the file does not exist, but it will truncate the existing file.Alternatively, you can use ; this will create the file if the file does not exist, but will not truncate the existing file.instead of using try-except blocks, you could use, if elsethis will not execute if the file is non-existent,\n    open(name,'r+')'w' creates a file if its non-exisfollowing script will use to create any kind of file, with user input as extensionThis works just fine, but instead of you should usealong withYou can os.system function for simplicity :This invokes system terminal to accomplish the task.this will work promise :)You can use However, when you enter filename, use inverted commas on both sides, otherwise cannot be added to filename "},
{"body": "I am trying to take one string, and append it to every string contained in a list, and then have a new list with the completed strings. Example:I tried for loops, and an attempt at list comprehension, but it was garbage. As always, any help, much appreciated. The simplest way to do this is with a list comprehension:Notice that I avoided using builtin names like  because that shadows or hides the builtin names, which is very much not good.Also, if you do not actually need a list, but just need an iterator, a generator expression can be more efficient (although it does not likely matter on short lists):These are very powerful, flexible, and concise.  Every good python programmer should learn to wield them.This will print:I havent found a way to comment on answers till now. So here it is.\nI support Ignacio Vazquez-Abrams's answer of .Others answers with  would work for most of times, but if you accept a more general solution for the simplest case you're - IMHO - following Python Design Principles. There should be preferably one obvious way to do it.  works all the time. seems like the right tool for the job to me.See  on functional programming tools for more examples of .And don't use  as a name; it shadows the built-in type.Using names such as \"list\" for your variable names is bad since it will overwrite/override the builtins.you can use lambda inside map in python. wrote a gray codes generator.\n\n        # your code goes here\n        '''\n        the n-1 bit code, with 0 prepended to each word, followed by\n        the n-1 bit code in reverse order, with 1 prepended to each word.\n        '''Running the following experiment the pythonic way:seems to be ~35% faster than the obvious use of a for loop like this:"},
{"body": "what I'm trying to do is this:Any suggestions?What aboutIn Django 1.4 and newer you can order by providing multiple fields.\nReference: By default, results returned by a  are ordered by the ordering tuple given by the  option in the model\u2019s Meta. You can override this on a per-QuerySet basis by using the  method.The result above will be ordered by  descending, then by  ascending. The negative sign in front of  indicates descending order. Ascending order is implied.I just wanted to illustrate that the built-in solutions (SQL-only) are not always the best ones.  At first I thought that because Django's  method accepts multiple arguments, you could easily chain them:But, it does not work as you would expect.  Case in point, first is a list of presidents sorted by score (selecting top 5 for easier reading):Using Alex Martelli's solution which accurately provides the top 5 people sorted by :And now the combined  call:As you can see it is the same result as the first one, meaning it doesn't work as you would expect.  Here's a way that allows for ties for the cut-off score.You may get more than 30 authors in top_authors this way and the  is there incase you have fewer than 30 authors."},
{"body": "I am learning the concept of filters in Python. I am running a simple code like this.But instead of getting a list, I am getting some message like this.What does this mean? Does it means that my filtered object i.e list to come out is stored at that memory location? How do I get the list which I need?It looks like you're using python 3.x.  In python3, , , , etc return an object which is iterable, but not a list.  In other words,is equivalent to:I think it was changed because you (often) want to do the filtering in a lazy sense -- You don't need to consume all of the memory to create a list up front, as long as the iterator returns the same thing a list would during iteration.  If you're familiar with list comprehensions and generator expressions, the above filter is now (almost) equivalent to the following in python3.x:As opposed to:in python 2.xIt's an  returned by the  function.If you want a list, just doNonetheless, you can just iterate over this object with a  loop."},
{"body": "Suppose I have a model:Currently I am using the default admin to create/edit objects of this type.\nHow do I remove the field  from the admin so that each object  be created with a value, and rather will receive a default value of ?Set  to  and  to your default value.Also, your  field is unnecessary.  Django will add it automatically.You can set the default like this:and then you can hide the field with your model's Admin class like this:You can also use a callable in the default field, such as:And then define the callable:"},
{"body": "Assume that I have a such set of pair datas where index 0 is the value and the index 1 is the type:I want to group them by their type(by the 1st indexed string) as such:How can I achieve this in an efficient way?ThanksDo it in 2 steps. First, create a dictionary.Then, convert that dictionary into the expected format.It is also possible with itertools.groupby but it requires the input to be sorted first.Note both of these do not respect the original order of the keys. You need an OrderedDict if you need to keep the order.Python's built-in  module actually has a  function that you could use, but the elements to be grouped must first be sorted such that the elements to be grouped are contiguous in the list:Now input looks like: returns a sequence of 2-tuples, of the form .  What we want is to turn this into a list of dicts where the 'type' is the key, and 'items' is a list of the 0'th elements of the tuples returned by the values_iterator.  Like this:Now  contains your desired dict, as stated in your question.You might consider, though, just making a single dict out of this, keyed by type, and each value containing the list of values.  In your current form, to find the values for a particular type, you'll have to iterate over the list to find the dict containing the matching 'type' key, and then get the 'items' element from it.  If you use a single dict instead of a list of 1-item dicts, you can find the items for a particular type with a single keyed lookup into the master dict.  Using , this would look like: now contains this dict (this is similar to the intermediate  defaultdict in @KennyTM's answer):(If you want to reduce this to a one-liner, you can:or using the newfangled dict-comprehension form:The following function will quickly ( required) group tuples of any length by a key having any index:In the case of your question, the index of key you want to group by is 1, therefore:giveswhich is not exactly the output you asked for, but might as well suit your needs.I also liked pandas simple . it's powerful, simple and most adequate for large data set"},
{"body": "I have the following recursion code, at each node I call sql query to get the nodes belong to the parent node. here is the error: Method that I call to get sql results:I actually don't have any issue with the above method but I put it anyways to give proper overview of the question.Recursion Code:Calling the recursive function Code to print the dictionary, If the recursion is too deep I should be getting the error when I call my recursion function, but when I get this error when I print the dictionary.You can increment the stack depth allowed - with this, deeper recursive calls will be possible, like this:... But I'd advise you to first try to optimize your code, for instance, using iteration instead of recursion."},
{"body": "I want to plot data, then create a new figure and plot data2, and finally come back to the original plot and plot data3, kinda like this:FYI  does something similar, but not quite! It doesn't let me get access to that original plot.If you find yourself doing things like this regularly it may be worth investigating the object-oriented interface to matplotlib. In your case:It is a little more verbose but it's much clearer and easier to keep track of, especially with several figures each with multiple subplots.When you call , simply number the plot.Edit: Note that you can number the plots however you want (here, starting from ) but if you don't provide figure with a number at all when you create a new one, the automatic numbering will start at  (\"Matlab Style\" according to the docs).However, numbering starts at , so:Also, if you have multiple axes on a figure, such as subplots, use the  command where  is the handle of the desired axes object to focus on that axes.(don't have comment privileges yet, sorry for new answer!)"},
{"body": "Is there an equivalent to 'intellisense' for Python?Perhaps i shouldn't admit it but I find having intellisense really speeds up the 'discovery phase' of learning a new language.  For instance switching from VB.net to C# was a breeze due to snippets and intellisense helping me along. blog entry explains setting Vim up as a Python IDE, he covers Intellisense-like functionality:This is standard in Vim 7. There are a number of other very useful plugins for python development in Vim, such as  which checks code on the fly and  which provides functionality for manipulating python indentation & code blocks.The  environment for Eclipse has intellisense-like functionality for Python. Keeping an interactive console open, along with the  function is very helpful.Have a look at , they provide code completion (a.k.a  intellisense), debugging etc ...Below is a screenshot of the interactive shell for python showing code completion.The dynamic nature of the language tends to make autocomplete type analysis difficult, so the quality of the various completion facilities menitoned above varies wildly.While it's not exactly what you asked for, the ipython shell is very good for exploratory work. When I'm working with a new module, I tend to pull it into ipython and poke at it. Having tried most of the solutions mentioned above (though it's been years since Wing), ipython's completion facilities are consistently more reliable. The two main tools for exploration are tab complete and appending a question mark to the module/function name to get the help text, e.g.:I strongly recommend . In Pydev you can put the module you are using in the , mostly the code-completion will work better than in other IDEs like KOMODO EDIT.Also I think  is very helpful. Since it is 'run-time' in IPython, the code-completion in IPython won't miss anything.The  that comes with Python has an intellisense feature that auto-discovers imported modules, functions, classes and attributes.Wingware for example implements auto-completion, see  .I'd recommend .  However, I should point something out:  you're not going to get anything quite as good as what you're used to with Visual Studio's C# intellisense.  Python's dynamic nature can make it difficult to do these kinds of features. is the best Python IDE with IntelliSense support.ctags + vim works ok, too, although it is not as powerful as intellisense. Using this with ipython, you can get online help, automatic name completion, etc... But that's obviously command-line oriented.Eclipse + pydev can do it as well, but I have no experience with it: Well, I think the most dynamic way to learn Python is to use .You got autocompletion when using tab, dynamic behaviour because it's a shell and you can get the full documentation of any object / method typing :When developping, I agree that PyDev is cool. But it's heavy, so while learning, a text editor + iPython is really nice. has the best intellisense i have meet :)For emacs and VI there's also .I would recommend , it's perfect to me, try it and you won't regret.IronPython is the way to go. Visual Studio has the best intellisense support and you can utilize that using IronPythonTry visual Studio Code. It has very powerful Python and Django support and thousands of plugins for other languages used in a Python project such as CSS, HTML and Django templates. Best of all, it is free:\n"},
{"body": "Basically, I'm asking the user to input a string of text into the console, but the string is very long and includes many line breaks.  How would I take the user's string and delete all line breaks to make it a single line of text.  My method for acquiring the string is very simple.Is there a different way I should be grabbing the string from the user?  I'm running Python 2.7.4 on a Mac.P.S. Clearly I'm a noob, so even if a solution isn't the most efficient, the one that uses the most simple syntax would be appreciated.How do you enter line breaks with ? But, once you have a string with some characters in it you want to get rid of, just  them.In the example above, I replaced all spaces. The string  represents newlines. And  represents carriage returns (if you're on windows, you might be getting these and a second  will handle them for you!).basically:Note also, that it is a bad idea to call your variable , as this shadows the module . Another name I'd avoid but would love to use sometimes: . For the same reason.You can try using string replace:updated based on  comment:read more You can split the string with no separator arg, which will treat consecutive whitespace as a single separator (including newlines and tabs). Then join using a space:A method taking into considerationit takes such a multi-line string which may be messy e.g.and produces nice one-line string"},
{"body": "I understand what  does, but of what \"type\" is that language element? I think it's a function, but why does this fail?Isn't  a function? Shouldn't it print something like this?In 2.7 and down,  is a statement. In python 3,  is a function. To use the print function in Python 2.6 or 2.7, you can do See  from the Python Language Reference, as well as  for why it changed.In Python 3,  is a built-in function (object)Before this,  was a . Demonstration... is a mistake that has been rectified in Python 3. In Python 3 it is a function. In Python 1.x and 2.x it is not a function, it is a special form like  or , but unlike those two it is not a control structure.So, I guess the most accurate thing to call it is a statement.In Python all statements (except assignment) are expressed with reserved words, not addressible objects.  That is why you cannot simply  and you get a  for trying.  It's a reserved word, not an object.Confusingly, you  have a variable named .  You can't address it in the normal way, but you can  and then .Other reserved words that might be desirable as variable names but are nonetheless verboten:In Python 2,  is a statement, which is a whole different kind of thing from a variable or function. Statements are not Python objects that can be passed to ; they're just part of the language itself, even more so than built-in functions. For example, you could do  (even though you shouldn't), but you can't do  or  because  and  are statements.In Python 3, the  statement was replaced with the  function. So if you do , it'll return .In Python 2.6+, you can put  at the top of your script (as the first line of code), and the  statement will be replaced with the  function."},
{"body": "What's the best strategy for managing third-party Python libraries with Google App Engine?Say I want to use Flask, a webapp framework.  says to do this, which doesn't seem right:There must be a better way to manage third-party code, especially if I want to track versions, test upgrades or if two libraries share a subdirectory. I know that Python can import modules from zipfiles and that  can work with a wonderful REQUIREMENTS file, and I've seen that  has a  command for use with GAE.(Note: There's a handful of similar questions \u2014 , , , ,  \u2014 but they're case-specific and don't really answer my question.)What about simply:Create/edit :Google updated their sample to , like:Note: Even though their example has  ignoring  directory you still need to keep that directory under source control if you use  deployment method.Here's how I do it:The  directory is the top level directory where the virtualenv sits. I get the virtualenv using the following commands:The  directory is where all your code goes. When you deploy your code to GAE,  deploy those in the src directory and nothing else. The  will resolve the symlinks and copy the library files to GAE for you.I don't install my libraries as zip files mainly for convenience in case I need to read the source code, which I happen to do a lot just out of curiosity. However, if you really want to zip the libraries, put the following code snippet into your main.pyAfter this you can import your zipped up packages as usual.One thing to watch out for is setuptools' . I copied that directly into my  directory so my other symlinked packages can use it. Watch out for anything that uses s. In my case I'm using Toscawidgets2 and I had to dig into the source code to manually wire up the pieces. It can become annoying if you had a lot of libraries that rely on .I prefer .You set up dependencies in setup.py in your project or buildout.cfg, pin the versions in buildout.cfg, and specify which packages are not available on GAE and should be included in packages.zip. rod.recipe.appengine will copy required packages into packages.zip, and as long as you insert packages.zip into the sys.path, they can be imported anywhere.You can also use forks from github if the package you need is not on pypiand all of these settings and dependencies are versioned in git.Instead of wondering which copy of Flask is currently included in your source tree and perhaps copied into your version control (or requiring new developers to manually unpack and upgrade), you simply check the version in buildout.cfg. If you want a new version, change buildout.cfg and rerun buildout.You can also use it to insert variables into config file templates, like setting the appspot id and version in app.yaml if you have staging server with staging.cfg and so on.I recently created a tool for this called gaenv. It follows a requirements.txt format, but doesn't install it, you can install with pip install -r requirements.txt then run the command line tool gaenv.This creates symlinks automatically, you could install gaenv in your virtualenv too and run the binary from there.\nHere is a blog post about it: also on github is the closest to current practice in the , which I've already improved by changing the  call to  in order to allow for  by processing their attendant  files (which are important for frameworks like Pyramid).So far so good, but that  the directory to the path, and so loses the opportunity to override the included libraries (like WebOb and requests) with newer versions.What is needed then in  (and I am trying to get this change accepted into the official repos as well) is the following:The final version of this code may end up hidden away in a  module and called like  or some other variation, as you can see , but the logic is more or less how it will work regardless, to allow a simple  to work for all packages including namespace ones, and to still allow overriding the included libraries with new versions, as I have so far been .Note: this answer is specific for Flask on Google App Engine.See the flask-appengine-template project for an example of how to get Flask extensions to work on App Engine.\nDrop the extension into the namespace package folder at src/packages/flaskext and you're all set.\nNon-Flask packages can be dropped into the src/packages folder as zip files, eggs, or unzipped packages, as the project template includes the sys.path.insert() snippet posted above."},
{"body": "I am usingbut I get a request.exceptions.SSLError.\nThe website has an expired certficate, but I am not sending sensitive data, so it doesn't matter to me.\nI would imagine there is an argument like 'verifiy=False' that I could use, but I can't seem to find it.From :If you're using a third-party module and want to disable the checks, here's a context manager that monkey patches  and changes it so that  is the default.And an example of how to use it:Use requests.packages.urllib3.disable_warnings().If you want to send exactly post request with verify=False option, fastest way is to use this code:"},
{"body": "I have this python script where I need to run but I get an exception on this line:The  is invalid syntax. I am curious as to why, and what the author probably meant to do. I'm new to python if you haven't already guessed.I think the root cause of the problem is that these imports are failing\nand therefore one must contain this import Are you sure you are using Python 3.x? The syntax isn't available in Python 2.x because  is still a statement.in Python 2.x is identical toori.e. as a call to print with a tuple as argument.That's obviously bad syntax (literals don't take keyword arguments). In Python 3.x  is an actual function, so it takes keyword arguments, too.The correct idiom in Python 2.x for  is:(note the final comma, this makes it end the line with a space rather than a linebreak)If you want more control over the output, consider using  directly. This won't do any special magic with the output.Of course in somewhat recent versions of Python 2.x (2.5 should have it, not sure about 2.4), you can use the  module to enable it in your script file:The same goes with  and some other nice things (, for example). This won't work in really old versions (i.e. created before the feature was introduced) of Python 2.x, though.How about this:This allows you to use the Python 3.0 style  function without having to hand-edit all occurrences of  :)First of all, you're missing a quote at the beginning but this is probably a copy/paste error.In Python 3.x, the  part will place a space after the displayed string instead of a newline. To do the same thing in Python 2.x, you'd put a comma at the end: I think he's using Python 3.0 and you're using Python 2.6.It looks like you're just missing an opening double-quote.  Try:I think the author probably meant:He's missing an initial quote after .Note that as of ,  is a function as opposed to a statement, if you're using older versions of Python the equivalent would be:The  parameter means that the line gets  at the end rather than a newline character. The equivalent in earlier versions of Python is:(thanks Ignacio).USE ::  I had such error , this occured because i have two versions of python installed on my drive namely python2.7 and python3 .\nFollowing was my code :when i run it by the  command  I got the following errorwhen I run it by the command  I executed successfullyThis is just a version thing. Since Python 3.x the print is actually a function, so it now takes arguments like any normal function.The  is just to say that you want a space after the end of the statement instead of a new line character. In Python 2.x you would have to do this by placing a comma at the end of the print statement.For example, when in a Python 3.x environment:Will give the following output:Where as:Will give as output:For python 2.7 I had the same issue \nJust use \"\" without quotes to resolve this issue.This Ensures Python 2.6 and later Python 2.x can use Python 3.x print function.we need to import a header before using , as it is not included in the python's normal runtime.it shall work perfectly now"},
{"body": "I have a Python script I recently wrote that I call using the command line with some options. I now want a very thin web interface to call this script locally on my Mac.I don't want to go through the minor trouble of installing mod_python or mod_wsgi on my Mac, so I was just going to do a system() or popen() from PHP to call the Python script.Any better ideas? Thanks in advance!Depending on what you are doing,  or  may be perfect.  Use system() if the Python script has no output, or if you want the Python script's output to go directly to the browser.  Use popen() if you want to write data to the Python script's standard input, or read data from the Python script's standard output in php.  popen() will only let you read or write, but not both.  If you want both, check out , but with two way communication between programs you need to be careful to avoid deadlocks, where each program is waiting for the other to do something.If you want to pass user supplied data to the Python script, then the big thing to be careful about is command injection.  If you aren't careful, your user could send you data like \"; evilcommand ;\" and make your program execute arbitrary commands against your will. and  can help with this, but personally I like to remove everything that isn't a known good character, using something likeThe backquote operator will also allow you to run python scripts using similar syntax to aboveIn a python file called python.py:In a php file called python.php:There's also a PHP extension: , which I've never tried but had it bookmarked for just such an occasionYou can run a python script via php, and outputs on browser.Basically you have to call the python script this way:Note: if you run any time.sleep() in you python code, it will not outputs the results on browser.For full codes working, visit I do this kind of thing all the time for quick-and-dirty scripts.  It's quite common to have a CGI or PHP script that just uses system/popen to call some external program.Just be extra careful if your web server is open to the internet at large.  Be sure to sanitize your GET/POST input in this case so as to not allow attackers to run arbitrary commands on your machine.Your call_python_file.php should look like this:This executes the python script and outputs the result to the browser.\nWhile in your python script the (sys.argv[1:]) variable will bring in all your arguments. To display the argv as a string for wherever your php is pulling from so if you want to do a text area:If you want to execute your Python script in PHP, it's necessary to do this command in your php script:"},
{"body": "I want to check the operating system (on the computer where the script runs).I know I can use  in Linux, but it gives me a message in the console, and I want to write to a variable.It will be okay if the script can tell if it is Mac, Windows or Linux. How can I check it? You can use :For the valid values, consult .You can get a pretty coarse idea of the OS you're using by checking .Once you have that information you can use it to determine if calling something like  is appropriate to gather more specific information.  You could also use something like  on unix-like OSes, or  for Windows.There's also  if you want to do more in-depth inspection without wanting to care about the OS.More detailed information are available in the .You can use .There seems to be some conflicting information about how Windows identifies. Some sources are saying \"Windows\", and other sources are saying \"win32\".With that in mind...That might be true in Cygwin though. But you can always add a check to make sure \"cy\" isn't in there."},
{"body": "I have a 2D NumPy array and would like to replace all values in it greater than or equal to a threshold T with 255.0. To my knowledge, the most fundamental way would be:This will be part of a window/level adjustment subroutine for MRI scans of the human head. The 2D numpy array is the image pixel data.I think both the fastest and most concise way to do this is to use Numpy's builtin indexing. If you have a  named  you can replace all elements  with a value  as follows:I ran this on my machine with a 500 x 500 random matrix, replacing all values >0.5 with 5, and it took an average of 7.59ms.Since you actually want a different array which is  where , and  otherwise, this can be done simply:More generally, for a lower and/or upper bound:If you just want to access the values over 255, or something more complicated, @mtitan8's answer is more general, but  and  (or ) are nicer and much faster for your case:If you want to do it in-place (i.e., modify  instead of creating ) you can use the  parameter of :or(the  name is optional since the arguments in the same order as the function's definition.)For in-place modification, the boolean indexing speeds up a lot (without having to make and then modify the copy separately), but is still not as fast as :For comparison, if you wanted to restrict your values with a minimum as well as a maximum, without  you would have to do this twice, with something likeor, You can consider using :Here is a performance comparison with the Numpy's builtin indexing:I think you can achieve this the quickest by using the  function:For example looking for items greater than 0.2 in a numpy array and replacing those with 0:"},
{"body": "I am using the standard  in python 2.6 to serialize a list of floats. However, I'm getting results like this:I want the floats to be formated with only two decimal digits. The output should look like this:I have tried defining my own JSON Encoder class:This works for a sole float object:But fails for nested objects:I don't want to have external dependencies, so I prefer to stick with the standard json module.How can I achieve this?Unfortunately, I believe you have to do this by monkey-patching (which, to my opinion, indicates a design defect in the standard library  package). E.g., this code:emits:as you desire. Obviously, there should be an architected way to override  so that EVERY representation of a float is under your control if you wish it to be; but unfortunately that's not how the  package was designed:-(.emitsNo monkeypatching necessary.If you're using Python 2.7, a simple solution is to simply round your floats explicitly to the desired precision.This works because Python 2.7 made . Unfortunately this does not work in Python 2.6: The solutions mentioned above are workarounds for 2.6, but none are entirely adequate. Monkey patching json.encoder.FLOAT_REPR does not work if your Python runtime uses a C version of the JSON module. The PrettyFloat class in Tom Wuttke's answer works, but only if %g encoding works globally for your application. The %.15g is a bit magic, it works because float precision is 17 significant digits and %g does not print trailing zeroes.I spent some time trying to make a PrettyFloat that allowed customization of precision for each number. Ie, a syntax likeIt's not easy to get this right. Inheriting from float is awkward. Inheriting from Object and using a JSONEncoder subclass with its own default() method should work, except the json module seems to assume all custom types should be serialized as strings. Ie: you end up with the Javascript string \"0.33\" in the output, not the number 0.33. There may be a way yet to make this work, but it's harder than it looks.You can do what you need to do, but it isn't documented:If you're stuck with Python 2.5 or earlier versions: The monkey-patch trick does not seem to work with the original simplejson module if the C speedups are installed:Alex Martelli's solution will work for single threaded apps, but may not work for multi-threaded apps that need to control the number of decimal places per thread. Here is a solution that should work in multi threaded apps:You can merely set encoder.thread_local.decimal_places to the number of decimal places you want, and the next call to json.dumps() in that thread will use that number of decimal placesWhen importing the standard json module, it is enough to change the default encoder FLOAT_REPR. There isn't really the need to import or create Encoder instances.Sometimes is also very useful to output as json the best representation python can guess with str. This will make sure signifficant digits are not ignored.Really unfortunate that  doesn't allow you to do anything to floats. However  does. So if you don't mind the extra CPU load, you could throw it through the encoder/decoder/encoder and get the right result:Pros:Cons:If you need to do this in python 2.7 without overriding the global json.encoder.FLOAT_REPR, here's one way.Then, in python 2.7:In python 2.6, it doesn't quite work as Matthew Schinckel points out below:I agree with @Nelson that inheriting from float is awkward, but perhaps a solution that only touches the  function might be forgiveable. I ended up using the  package for this to reformat floats when needed. The upside is that this works in all contexts where  is being called, so also when simply printing lists to stdout for example. Also, the precision is runtime configurable, after the data has been created. Downside is of course that your data needs to be converted to this special float class (as unfortunately you cannot seem to monkey patch ). For that I provide a brief conversion function.The code:Usage example:"},
{"body": "Or is there a better way to quickly output the contents of an array (multidimensional or what not).  Thanks.The python  statement does a good job of formatting multidimesion arrays without requiring the  available in php.As the definition for print states that each object is converted to a string, and as simple arrays print a '[' followed by a comma separated list of object values followed by a ']', this will work for any depth and shape of arrays.For exampleIf you need more advanced formatting than this, s answer suggesting  is probably the way to go.Check out . and  are great for built-in data types or classes which define a sane object representation. If you want a full dump of arbitrary objects, you'll have to roll your own. That is not that hard: simply create a recursive function with the base case being any non-container built-in data type, and the recursive case applying the function to each item of a container or each attribute of the object, which can be gotten using  or the  module.there is print_r for python \nbut it is better to use standard modulesYou were looking for the  bult-in function.\nhere's one you can try:you can install it simply using disclaimer: I wrote it :)if you want to format  as a string you can do:it works regardless the type, and doesn't requiere any imports.If you want to include the contents of an object in a single string together with its type:For simple test views I use HttpResponse. Testing request from form, variable, string, id, not Object"},
{"body": "I'd like to add comments for a few packages in a pip requirements file. (Just to explain why that package is on the list.) Can I do this?I'm imagining something likeSure, you can, based on :Go ahead!"},
{"body": "Say I have an image of size 3841 x 7195 pixels. I would like to save the contents of the figure to disk, resulting in an image of the  I specify in pixels.No axis, no titles. Just the image. I don't personally care about DPIs, as I only want to specify the size the image takes in the screen in disk .I have read  , and they all seem to do conversions to inches and then specify the dimensions of the figure in inches and adjust dpi's in some way. I would like to avoid dealing with the potential loss of accuracy that could result from pixel-to-inches conversions.I have tried with:with no luck (Python complains that width and height must each be below 32768 (?))From everything I have seen,  requires the figure size to be specified in  and , but I am only interested in  the figure takes in disk. How can I do this?To clarify: I am looking for a way to do this with , and not with other image-saving libraries.Matplotlib doesn't work with pixels directly, but rather physical sizes and DPI. If you want to display a figure with a certain pixel size, you need to know the DPI of your monitor. For example  will detect that for you.If you have an image of 3841x7195 pixels it is unlikely that you monitor will be that large, so you won't be able to show a figure of that size (matplotlib requires the figure to fit in the screen, if you ask for a size too large it will shrink to the screen size). Let's imagine you want an 800x800 pixel image just for an example. Here's how to show an 800x800 pixel image in my monitor ():So you basically just divide the dimensions in inches by your DPI. If you want to save a figure of a specific size, then it is a different matter. Screen DPIs are not so important anymore (unless you ask for a figure that won't fit in the screen). Using the same example of the 800x800 pixel figure, we can save it in different resolutions using the  keyword of . To save it in the same resolution as the screen just use the same dpi:To to save it as an 8000x8000 pixel image, use a dpi 10 times larger:Note that the setting of the DPI is not supported by all backends. Here, the PNG backend is used, but the pdf and ps backends will implement the size differently. Also, changing the DPI and sizes will also affect things like fontsize. A larger DPI will keep the same relative sizes of fonts and elements, but if you want smaller fonts for a larger figure you need to increase the physical size instead of the DPI. Getting back to your example, if you want to save a image with 3841 x 7195 pixels, you could do the following:Note that I used the figure dpi of 100 to fit in most screens, but saved with  to achieve the required resolution. In my system this produces a png with 3840x7190 pixels -- it seems that the DPI saved is always 0.02 pixels/inch smaller than the selected value, which will have a (small) effect on large image sizes. Some more discussion of this .This worked for me, based on your code, generating a 93Mb png image with color noise and the desired dimensions:I am using the last PIP versions of the Python 2.7 libraries in Linux Mint 13.Hope that helps!for manipulating images directly I would use the opencv bindings to python, or else numpy arrays"},
{"body": "I am having a problem with my encoding in Python. I have tried different methods but I can't seem to find the best way to encode my output to UTF-8.This is what I am trying to do: returns the first Google result for .This is the error I get:Does anyone know how I can make Python encode my output in UTF-8 to avoid this error?Looks like  already returns :So what you want is:As a side note, your code expects it to return a  encoded string so what was the point in decoding it (using ) and encoding back (using ) using the same encoding?"},
{"body": "I want to start reading the Python source code.My experience,I know Python and Java very well. I know some other languages at various levels of proficiency, but neither C/C+/ particularly well. I studied C in college, but have never professionally programmed in it.My Reasons for reading this code.1 is more important to me than 2.how should I go about this?Start by learning about the Python C API. It is a large and rich API, and the Python source naturally uses it all over the place.  You won't get very far into the Python source code before you have to understand what is meant by Py_INCREF and so on.I gave a presentation at Pycon explaining the API:  that you might find helpful.  C extensions use the same API as the Python code itself.First, if you're mostly interested in 1, I'd start with reading the Python source of various modules (and not jump straight to the C). Whenever I found myself reading the source of some modules, I've always learned new things about Python programming.Second, if you're trying to learn C better, I'd personally suggest something completely different: program in it. Just statically reading source code is  going to make you understand C better (or at least, it's a limited approach; it might make you a little better, but there's only so much that reading source will get you).After programming at least a moderately sized project in C,  I'd start looking at Python source. That's really the only way to know C better, and I really think that reading the C source of Python without knowing C well won't get you very far.In fact, here's an idea for a C project: write a Python interpreter in C. Obviously it's not going to be even close to complete, and this is a pretty hard project, but if you only focus on some parts of the language, I think its a good idea.Not only will it help you learn C, it will help you understand Python a lot better even  looking at the source: you'll have to have a deeper understanding of a  of stuff in Python, you'll understand the design tradeoffs in how Python works, etc.Then, when you do finally read Python's code, not only will you understand why some things work that way, you'll probably learn a lot of really cool C techniques that solve problems you had.1) First make sure you can build your own Python and run it into a debugger.\nSo you can not only add print expressions but also break at points and follow the code flow.\nIf you have toolsl that let you trace function calls, perfect, you will need it.2) Start with the file that implement the data types. They are very easy to understand and you improve your C language language skills while reading the code.3) Make UML diagrams - simple drawing helper tools like Argo UML or MS Visio can help you here. Write down the code flow.4) Read the startup code for python. See what and how the basic infrastructure is initialized.6) Ty to understand the Python side 100% - even the harder implementation details, what an AST is and what bound and unbound methods are and how you would implement them. When you have a model in mind how you would write an python interpreter then you can go to the final master step.7) Write a debugger extension with the provided fast debugger C API.\nThis helps you to improve your C skills.8) Take the final master step and dive into the heart of the interpreter code. This is even hard to read and understand for a well skilled C programmer. Read how expressions are evaluation and method looksup are cached, frames are setup for scoping rules etc. It's difficult and complex - in terms of complexity and lines of code.9) Start Adobe Photoshop and create a nice looking \"Master of Python\" diploma and put it on your office wall.Download the  from the Python website. Say you unzipped the source into a directory named Python-3.1.1. I suggest you two starting points within Python source code that would help you explore how Python works under the hood:The question is quite broad so I guess the best answer is to just download the  and go nuts. Pick a module or section of python you know well and check whats under the hood."},
{"body": "I have a dataframe with 2 index levels:Which I want to turn into this:How can I best do this?   I need this because I want to aggregate the data , but I can't select my columns like that if they are in use as indices.The  is a pandas DataFrame method that will transfer index values into the DataFrame as columns.  The default setting for the parameter is  (which will keep the index values as columns).All you have to do add  after the name of the DataFrame:"},
{"body": "I am getting an issue in Python 3.3.2 on OSX 10.9 where if I open Python in a terminal window, it exits with \"Segmentation error: 11\" after the second line I enter, regardless of what the two commands are. For example, if I enter:that works fine, but if I enter:then I get the error when I press enter on the second line. I can also run a script with more than 2 lines without any problems.I updated to OSX 10.9 this afternoon, so I suspect that may be it. However, I just recently installed IPython (along with several other packages) and have been using that the past couple of days, so it could be something else I installed recently. I had a couple unsuccessful attempts at installing PyQt where I ran configure.py but then the \"make\" command failed, which I was also suspicious of.I tried reinstalling Python, but it didn't resolve the issue. Both IPython and IDLE work with no problems. I'm just concerned about what could be the underlying issue.Any help is appreciated, thanks in advance.This is a bug in the readline compatibility in python, related to changes introduced in OSX10.9. This weekend, release candidates for Python2.7.6 and Python3.3.3 were released which fix this bug. The download links are below.Here's the issue, quoting from Ned Deily, writing on the .I had this problem after upgrading to OS X 10.9 and used the patch provided on the Python website: To use it, open a terminal session in Terminal.app (or other shell), then enter:then\n    Enter your password, if promptedI had this problem. Changing the  in my csv parser to 100 eliminated the error.I was encountering similar 'segmentation fault 11' errors but for me it was using mercurial(hg) This was trying to use Python 2.7.8 installed via the .mpkg installer and pip install mercurial\nOn OS X 10.9.5I thought updating to 2.7.8 would have resolved this but it seemed that mercurial was still looking for the System/Library/Frameworks/Python.framework/Versions/2.7Even after trying to follow this  Things still weren't working.  I would run would get 'segmentation fault 11'The first couple of lines of the stack trace point to this:In the end my solution seems to have come from (re)installing python with  using that to get the 2.7.8 release (as of Dec 2014)I then reinstalled mercurial with brew install mercurial which seems to have resolved whatever dependencies where causing this.  I wish I understood better what was happening with the Seg fault but couldn't get to the bottom of it./usr/local/bin:usr/local/git/bin:/Library/Frameworks/Python.framework/Versions/2.7/bin:/Library/Frameworks/Python.framework/Versions/3.4/bin:/usr/bin:/bin:/usr/sbin:/sbin:/usr/local/bin:~/Develop:/usr/local/git/bin: No such file or directory"},
{"body": "With python properties, I can make it such that calls a function rather than just returning a value.Is there a way to do this with modules? I have a case where I wantto call a function, rather than just returning the value stored there.Only instances of new-style classes can have properties.  You can make Python believe such an instance is a module by stashing it in .  So, for example, your m.py module file could be:I would do this in order to properly inherit all the attributes of a module, and be correctly identified by isinstance()And then you can insert this into sys.modules:A typical use case is: enriching a (huge) existing module with some (few) dynamic attributes - without turning all module stuff into a class layout.\nUnfortunately a most simple module class patch like  fails with . So module creation needs to be rewired.This approach does it without Python import hooks, just by having some prolog on top of the module code:Usage:Note: Something like  will produce a frozen copy of course - corresponding to "},
{"body": "Is it possible to use Python to write cross-platform apps for both iOS and AndroidFor Android Google provides something called ASE (Android Scripting Environment) which allows scripting languages (Python included) to run on Android. More details For iOS, it might be quite a bit more complicated (and I'd advise to check the latest version of iOS SDK agreement to check the current state of the law - apple tends to allow & disallow such apps periodically). For the technical part you might need to interface between Python & Objective C as well as do some wizardry to statically link all the libraries Python requires; full details are available .\nAnother approach for iOS Python development would be to embed a Python interpreter into you app and distribute your Python script with it (so to play nicely with Apple rules). In this case your Python application would be a Python interpreter which is packaged with your script and runs it automatically.Second option is to use Kivy framework(kivy.org) - it's a framework to write touch-friendly applications in Python and package them for different platforms. It's had Android support for a while, and recently added iOS supportAn option is : a cool cross platform Python framework which works for Android, Win7, Linux, MacOSX and iOS.Update: kivy project is growing daily, now supports also Raspberry PiFor writing a GUI applicaion you can use the  library,then package it with  into a Android and/or iOS app (or Windows, Linux, macOS etc). the project is still in alpha state but under rapid development."},
{"body": "When storing a bool in memcached through python-memcached I noticed that it's returned as an integer. Checking the code of the library showed me that there is a place where  is checked to flag the value as an integer.So I tested it in the python shell and noticed the following:But why exactly is  a subclass of ?It kind of makes sense because a boolean basically is an int which can just take two values but it needs much less operations/space than an actual integer (no arithmetics, only a single bit of storage space)....From a comment on Credit goes to dman13 for this nice explanation.See .  Relevent passage:"},
{"body": "I have a python list, say lI want to write a sql query to get the data for all the elements of the list, say\"select name from studens where id = |IN THE LIST l|\"How do i accomlish this?Answers so far have been templating the values into a plain SQL string. That's absolutely fine for integers, but if we wanted to do it for strings we get the escaping issue.Here's a variant using a parameterised query that would work for both:The SQL you want isIf you want to construct this from the python you could useThe  function will transform the list into a list of strings that can be glued together by commas using the  method.Alternatively:if you prefer  to the map function.UPDATE:  mentions in the comments that the Python SQLite bindings don't support sequences. In that case, you might wantGenerated by  the list values separated by commas, and use the  to form a query string.(Thanks, )Dont complicate it, Solution for this is simple.I hope this helped !!!I like bobince's answer:But I noticed this:Can be replaced with:I find this more direct if less clever and less general. Here  is required to have a length (i.e. refer to an object that defines a  method), which shouldn't be a problem. But placeholder must also be a single character. To support a multi-character placeholder use:For example, if you want the sql query:What about:Solution for @umounted answer, because that broke with a one-element tuple, since (1,) is not valid SQL.:Other solution for sql string:"},
{"body": "In my Python socket program, I sometimes need to interrupt it with ctrl-c. When I do this, it does close the connection using socket.close() however when I try to reopen it I have to wait what seems like a minute before I can connect again. How does one correctly close a socket? Or is this intended?Yes, it is intended. Here you can read . It is possible to override this behavior by setting SO_REUSEADDR option on a socket. For example:If you use a TCPServer, UDPServer or their subclasses in the SocketServer module, you can set this class variable (before instanciating a server):(via  )This causes the  (constructor) to:-> ps -fA | grep python501 81211 12368   0  10:11PM ttys000    0:03.12\npython -m SimpleHTTPServer-> kill 81211Nothing worked for me except running a subprocess with this command, before calling HTTPServer(('', 443), myHandler):kill -9 $(lsof -ti tcp:443)Of course this is only for linux-like OS!"},
{"body": "I just want to see the state of the process, is it possible to attach a console into the process, so I can invoke functions inside the process and see some of the global variables.It's better the process is running without being affected(of course performance can down a little bit)If you have access to the program's source-code, you can add this functionality relatively easily. See : To quote:Another implementation of roughly the same concept is provided by .  From the documentation:This will interrupt your process (unless you start it in a thread), but you can use the  module to start a Python console:This will block until the user exits the interactive console by executing .The  module is available in at least Python v2.6, probably others.I tend to use this approach in combination with signals for my Linux work (for Windows, see below). I slap this at the top of my Python scripts:And then trigger it from a shell with , where  is the process ID. The process then stops whatever it is doing and presents a console:Generally from there I'll load the server-side component of a remote debugger like the excellent .Windows is not a -compliant OS, and so does not provide the same signals as Linux. However,  (triggered by pressing +). This does  interfere with normal + () operation, and so is a handy alternative.Therefore a portable, but slightly ugly, version of the above is:Advantages of this approach:Here's the code I use in my production environment which will load the server-side of WinPDB (if available) and fall back to opening a Python console.Use . I can't believe it works so well, but it does. \"\".This launches the python shell with access to the globals() and locals() variables of that running python process, and other wonderful things.Only tested this personally on Ubuntu but seems to cater for OSX too.Adapted from .Note: The line switching off the  property is only necessary for kernels/systems that have been built with  on. Take care messing with ptrace_scope in sensitive environments because it could introduce certain security vulnerabilities. See  for details.Why not simply using the  module? It allows you to stop a script, inspect elements values, and execute the code line by line. And since it is built upon the Python interpreter, it also provides the features provided by the classic interpreter. To use it, just put these 2 lines in your code, where you wish to stop and inspect it:Another possibility, without adding stuff to the python scripts, is described here:Unfortunately, this solution also requires some forethought, at least to the extent that you need to be using a version of python with debugging symbols in it.Using PyCharm, I was getting a failure to connect to process in Ubuntu.  The fix for this is to disable YAMA.  For more info see "},
{"body": "I was reading a question about the Python  statement (  ) and I was remembering about how often I used this statement when I was a Python beginner (I used  a lot) and how, nowadays, years later, I don't use it at all, ever. I even consider it a bit \"un-pythonic\".Do you use this statement in Python ? Has your usage of it changed with time ?I use 'global' in a context such as this:I use 'global' because it makes sense and is clear to the reader of the function what is happening. I also know there is this pattern, which is equivalent, but places more cognitive load on the reader:I've never had a legit use for the statement in any production code in my 3+ years of professional use of Python and over five years as a Python hobbyist. Any state I need to change resides in classes or, if there is some \"global\" state, it sits in some shared structure like a global cache.I've used it in situations where a function creates or sets variables which will be used globally.  Here are some examples:or In my view, as soon as you feel the need to use global variables in a python code, it's a great time to stop for a bit and work on refactoring of your code.\nPutting the  in the code and delaying the refactoring process might sound promising if your dead-line is close, but, believe me, you're not gonna go back to this and fix unless you really have to - like your code stopped working for some odd reason, you have to debug it, you encounter some of those  variables and all they do is mess things up.So, honestly, even it's allowed, I would as much as I can avoid using it. Even if it means a simple classes-build around your piece of code.Objects are the prefered way of having non-local state, so global is rarely needed. I dont think the upcoming nonlocal modifier is going to be widely used either, I think its mostly there to make lispers stop complaining :-)I use it for global options with command-line scripts and 'optparse':my main() parses the arguments and passes those to whatever function does the work of the script... but writes the supplied options to a global 'opts' dictionary.Shell script options often tweak 'leaf' behavior, and it's inconvenient (and unnecessary) to thread the 'opts' dictionary through every argument list. I avoid it and we even have a  rule that forbids it in our production code. I actually believe it shouldn't even exist at all.Rarely. I've yet to find a use for it at all.It can be useful in threads for sharing state (with locking mechanisms around it).However, I rarely if ever use it.I've used it in quick & dirty, single-use scripts to automate some one-time task. Anything bigger than that, or that needs to be reused, and I'll find a more elegant way.Once or twice. But it was always good starting point to refactor.If I can avoid it, no. And, to my knowledge, there is always a way to avoid it. But I'm not stating that it's totally useless either"},
{"body": "Just about everyone uses them, but many, including me simply take it for granted that they just work.I am looking for high-quality material. Languages I use are: Java, C, C#, Python, C++, so these are of most interest to me.Now, C++ is probably a good place to start since you can throw anything in that language. Also, C is close to assembly. How would one emulate exceptions using pure C constructs and no assembly?Finally, I heard a rumor that Google employees do not use exceptions for some projects due to speed considerations. Is this just a rumor? How can anything substantial be accomplished without them?Thank you.Exceptions are just a specific example of a more general case of advanced non-local flow control constructs. Other examples are:(I'm sure there's many others I missed.)An interesting property of these constructs is that they are all roughly equivalent in expressive power: if you have , you can pretty easily build all the others.So, how you best implement exceptions depends on what other constructs you have available: A very interesting use case, both of the  of exceptions  the  of exceptions is Microsoft Live Lab's Volta Project. (Now defunct.) The goal of Volta was to provide architectural refactoring for Web applications at the push of a button. So, you could turn your one-tier web application into a two- or three-tier application just by putting some  or  attributes on your .NET code and the code would then automagically run on the client or in the DB. In order to do that, the .NET code had to be translated to JavaScript source code, obviously.Now, you  just write an entire VM in JavaScript and run the bytecode unmodified. (Basically, port the CLR from C++ to JavaScript.) There are actually projects that do this (e.g. the HotRuby VM), but this is both inefficient and not very interoperable with other JavaScript code.So, instead, they wrote a compiler which compiles CIL bytecode to JavaScript sourcecode. However, JavaScript lacks certain features that .NET has (generators, threads, also the two exception models aren't 100% compatible), and more importantly it lacks certain features that compiler writers  (either  or continuations) and that could be used to implement the above-mentioned missing features.However, JavaScript  have exceptions. So, they used  to implement  and then they used  to implement ,  and even (!!!)So, to answer your original question:With Exceptions, ironically! At least in this very specific case, anyway.Another great example is some of the exception proposals on the Go mailing list, which implement exceptions using Goroutines (something like a mixture of concurrent coroutines ans CSP processes). Yet another example is Haskell, which uses Monads, lazy evaluation, tail call optimization and higher-order functions to implement exceptions. Some modern CPUs also support basic building blocks for exceptions (for example the Vega-3 CPUs that were specifically designed for the Azul Systems Java Compute Accelerators).Here is a common way C++ exceptions are implemented:\nIt is for the Itanium architecture, but the implementation described here is used in other architectures as well. Note that it is a long document, since C++ exceptions are complicated.Here is a good description on how LLVM implements exceptions:\nSince LLVM is meant to be a common intermediate representation for many runtimes, the mechanisms described can be applied to many languages.In his book ,  D. R. Hanson provides a nice implementation of exceptions in pure C using a set of macros and . He provides TRY/RAISE/EXCEPT/FINALLY macros that can emulate pretty much everything C++ exceptions do and more. The code can be perused  (look at except.h/except.c).P.S. re your question about Google. Their employees are actually allowed to use exceptions in new code, and the official reason for the ban in old code is because it was already written that way and it doesn't make sense to mix styles.Personally, I also think that C++ without exceptions isn't the best idea.C/C++ compilers use the underlying OS facilities for exception handling. Frameworks like .Net or Java also rely, in the VM, on the OS facilities. In Windows for instance, the real heavy lifting is done by SEH, the Structured Exception Handling infrastructure. You should absolutely read the old reference article: .As for the cost of not using exceptions, they are expensive but compared to what? Compared to return error codes? After you factor in the cost of correctness and the quality of code, exceptions will always win for commercial applications. Short of few very critical OS level functions, exceptions are always better overall. An last but not least there is the anti-pattern of using exceptions for flow control. Exceptions should be exceptional and code that abuses exceptions fro flow control will pay the price in performance.The best paper ever written on the  of exceptions (under the hood) is  by Barbara Liskov and Alan Snyder.  I have referred to it every time I've started a new compiler.For a somewhat higher-level view of an implementation in C using  and , I recommend Dave Hanson's  (like Eli Bendersky).The key thing an exception implementation needs to handle is how to return to the exception handler once an exception has been thrown. Since you may have made an arbitrary number of nested function calls since the try statement in C++, it must  searching for the handler. However implemented, this must incur the  of maintaining sufficient information in order to perform this operation (and generally means a table of data for calls that can take exceptions). It also means that the dynamic code  than simply returning from functions calls (which is a fairly inexpensive operation on most platforms). There may be other costs as well depending on the implementation.The relative cost will vary depending on the language used. The higher-level language used, the less likely the code size cost will matter, and the information may be retained regardless of whether exceptions are used.An application where the use of exceptions (and C++ in general) is often avoided for good reasons is embedded firmware. In typical small bare metal or RTOS platforms, you might have 1MB of code space, or 64K, or even smaller. Some platforms are so small, even C is not practical to use. In this kind of environment, the size impact is relevant because of the cost mentioned above. It also impacts the standard library itself. Embedded toolchain vendors will often produce a library without exception capability, which has a huge impact on code size. Highly optimizing compilers may also analyze the callgraph and optimize away needed call frame information for the unwind operation for considerable space reduction. Exceptions also make it more difficult to analyze hard real-time requirements.In more typical environments, the code size cost is almost certainly irrelevant and the performance factor is likely key. Whether you use them will depend on your performance requirements and how you want to use them. Using exceptions in non-exceptional cases can make an elegant design, but at a performance cost that may be unacceptable for high performance systems. Implementations and relative cost will vary by platform and compiler, so the best way to truly understand if exceptions are a problem is to analyze your own code's performance. and  usually.Exception catching does have a non-trivial cost, but for most purposes it's not a big deal.C++ code at Google (save for some Windows-specific cases) don't use exceptions: cfr , short form: \"We do not use C++ exceptions\".  Quoting from the discussion (hit the arrow to expand on the URL):This rule does not apply to Google code in other languages, such as Java and Python.Regarding performance - sparse use of exceptions will probably have negligible effects, but do not abuse them.I have personally seen Java code which performed two orders of magnitude worse than it could have (took about x100 the time) because exceptions were used in an important loop instead of more standard if/returns.Some runtimes like  have zero-cost 64-bit exceptions. What that means is that it doesn't cost anything to enter a try block. However, this is quite costly when the exception is thrown. This follows the paradigm of \"optimize for the average case\" - exceptions are meant to be exceptional, so it is better to make the case when there are no exceptions really fast, even if it comes at the cost of significantly slower exceptions."},
{"body": "Much of my programming background is in Java, and I'm still doing most of my programming in Java. However, I'm starting to learn Python for some side projects at work, and I'd like to learn it as independent of my Java background as possible - i.e. I don't want to just program Java in Python. What are some things I should look out for?A quick example - when looking through the Python tutorial, I came across the fact that defaulted mutable parameters of a function (such as a list) are persisted (remembered from call to call). This was counter-intuitive to me as a Java programmer and hard to get my head around. (See  and  if you don't understand the example.)Someone also provided me with  list, which I found helpful, but short. Anyone have any other examples of how a Java programmer might tend to misuse Python...? Or things a Java programmer would falsely assume or have trouble understanding?: Ok, a brief overview of the reasons addressed by the article I linked to to prevent duplicates in the answers (as suggested by Bill the Lizard). (Please let me know if I make a mistake in phrasing, I've only  started with Python so I may not understand all the concepts fully. And a disclaimer - these are going to be  brief, so if you don't understand what it's getting at check out the link.)(And if you find this question at all interesting, check out the link anyway. :) It's quite good.)Just do this:The referenced article has some good advice that can easily be misquoted and misunderstood.  And some bad advice.Leave Java behind.   Start fresh.  \"do not trust your [Java-based] instincts\".  Saying things are \"counter-intuitive\" is a bad habit in any programming discipline.  When learning a new language, start fresh, and drop your habits.  Your intuition  be wrong.  Languages are .  Otherwise, they'd be the same language with different syntax, and there'd be simple translators.  Because there are not simple translators, there's no simple mapping.  That means that intuition is unhelpful and dangerous.One thing you might be used to in Java that you won't find in Python is strict privacy.  This is not so much something to look out for as it is something  to look for (I am embarrassed by how long I searched for a Python equivalent to 'private' when I started out!).  Instead, Python has much more transparency and easier introspection than Java.  This falls under what is sometimes described as the \"we're all consenting adults here\" philosophy.  There are a few conventions and language mechanisms to help prevent  use of \"unpublic\" methods and so forth, but the whole mindset of information hiding is virtually absent in Python.The biggest one I can think of is not understanding or not fully utilizing duck typing.  In Java you're required to specify very explicit and detailed type information upfront.  In Python typing is both dynamic and largely implicit.  The philosophy is that you should be thinking about your program at a higher level than nominal types.  For example, in Python, you don't use inheritance to model substitutability.  Substitutability comes by default as a result of duck typing.  Inheritance is only a programmer convenience for reusing implementation.Similarly, the Pythonic idiom is \"beg forgiveness, don't ask permission\".  Explicit typing is considered evil.  Don't check whether a parameter  a certain type upfront.  Just try to do whatever you need to do with the parameter.  If it doesn't conform to the proper interface, it will throw a very clear exception and you will be able to find the problem very quickly.  If someone passes a parameter of a type that was nominally unexpected but has the same interface as what you expected, then you've gained flexibility for free.The most important thing, from a Java POV, is that it's perfectly ok to not make classes for everything.  There are many situations where a procedural approach is simpler and shorter.The next most important thing is that you will have to get over the notion that the  of an object controls what it may do; rather, the  controls what objects must be able to support  (this is by virtue of duck-typing).  Oh, and use native lists and dicts (not customized descendants) as far as possible.The way exceptions are treated in Python is different from \nhow they are treated in Java. While in Java the advice\nis to use exceptions only for exceptional conditions this is not\nso with Python. In Python things like Iterator makes use of exception mechanism to signal that there are no more items.But such a design is not considered as good practice in Java.As Alex Martelli puts in his book \nthe exception mechanism with other languages (and applicable to Java) \nis  (Look Before You Leap) : \nis to check in advance, before attempting an operation, for all circumstances that might make the operation invalid. Where as with Python the approach is EAFP (it's easier to Ask for forgiveness than permission)A sensation of ?A corrollary to \"Don't use classes for everything\": callbacks. The Java way for doing callbacks relies on passing objects that implement the callback interface (for example  with its  method). Nothing of this sort is necessary in Python, you can directly pass methods or even locally defined functions:Or even lambdas:"},
{"body": "When I ran   (Mac OS X 10.9.2), I get the following warning message:Therefore, I ran  and followed the steps provided in the installation's caveats output to install Homebrew's version of . Running  confirms that Homebrew's version of it is indeed at the top of my . Output is .Despite all this, when I rerun , I am still getting the . How do I suppress this warning? Do I need to delete the /Library/Frameworks/Python.framework directory from my computer? Am I just supposed to ignore it? Is there a different application on my computer that may be causing this warning to emit?Note that I don't have any applications in particular that are running into errors due to this warning from . Also note that this warning message didn't always print out when I ran , it was something that started to appear recently. Also, I am using Python 2.7 on my computer, trying to stay away from Python 3.I had the same problem. When I upgraded python3 through Homebrew, I started getting this:I had the same conflict with Python somehow being installed in . I just did a  and everything is working fine now. There is some info about what to do with the Python version in the  .I guess you could try deleting that version as the link suggests, just make sure that version isn't being used. When I got into the Python.framework directory I was seeing some EPD version of Python, which I think is Enthought. You could delete it, but I if it isn't causing you any problems besides the unsightly Homebrew warning message, then I think you should just ignore it for now. Update:I did delete the Python.framework directory which, through some poking around inside that directory, I started seeing a few old versions of Python that I didn't install with Homebrew. One was from Enthought, and another was a distribution of Python3.3. I think some of these installs in the Framework directory are user installs. I installed R on my system, and there is also an R.framework directory, so I think most of these are user installs. After I deleted the directory, I just had to call brew prune to remove the old symlinks. I checked both brew versions of python 2.7.6 and 3.3.4, and they seem to be in good working order with all of my installed packages. I guess I leave the decision to remove that directory, or python version, to your discretion. I also received this message. Something, sometime installed on my machine (the folder date was about 4 years old).I've chosen to remove it.Please note that the Apple provided framework lives in per , enter this command:because there are multiple installations of Python on your computer, and this removes the one that may cause additional problems in the future.You can use this solution as I've put belowThat combo fixed it for me, even thought this error usually doesn't cause any major problems its just was annoying me to see them pop up under brew doctorRemoving directories manually can be a nightmare, but fortunately 'brew' can take care of that. Once you are done with the removal, put this:The above command will list the broken system links. In order to get rid of these broken symlinks, put this:Check with 'brew doctor' once more to ensure no links are broken. Your system will then be ready to brew."},
{"body": "I haven't seen anything about Windows compatibility -- is this on the way or currently available somewhere if I put forth some effort? (I have a Mac and an Ubuntu box but the Windows machine is the one with the discrete graphics card that I currently use with theano). Today we released the first release candidate of TensorFlow 0.12, which includes support for Windows. You can install the Python bindings using the following command in a Python shell:...or, if you want GPU support:You can also build TensorFlow yourself using Microsoft Visual C++ and NVCC (for the CUDA parts). The easiest way to build on Windows is currently to use the , and we will soon provide support for . We haven't tried to build TensorFlow on Windows so far: the only supported platforms are Linux (Ubuntu) and Mac OS X, and we've only built binaries for those platforms.For now, on Windows, the easiest way to get started with TensorFlow would be to use Docker: It should become easier to add Windows support when Bazel (the build system we are using) adds support for building on Windows, which is .  You can see .In the meantime, you can follow .As @mrry suggested, it is easier to set up TensorFlow with Docker. Here's how I managed to set it up as well as getting iPython Notebook up and running in my Docker environment (I find it really convenient to use iPython Notebook for all testing purposes as well as documenting my experiments).I assume that you have installed both docker and boot2docker for Windows here.First, run TensorFlow docker on daemon and set it up so Jupyter server (iPython Notebook) can be accessed from your main Windows system's browser:Replace  with a path in your host you wish to mount i.e. where you can keep your iPython files. .  is the location in your TensorFlow docker where your host path's mounted against. basically means \"map port 8888 in docker to 8888 in host directory\". You can change the second part to other ports if you wish.When you got it running, you can access it by running the following code:Where [docker-id] can be found by running:To start your ipython notebook server from within TensorFlow's docker, run the following command:To allow ipython server to listen to all ip so your app may be accessible from host machine.Instead of viewing your app in , you can only view it in . To find  run this in your terminal (not boot2docker terminal):Another way to run it on Windows is to install for example Vmware (a free version if you are not using it commercially), install Ubuntu Linux into that and then install TensorFlow using the Linux instructions. That is what I have been doing, it works well.TensorFlow is not supporting Windows at this point, but Windows has changed. Windows 10 Build 14432 includes bash.You can download the build from After installation, just install/enable bash, and type  in cmd. That's it.\n(The image is from )Then, run this (Python is already installed):Happy tensorflowing in Windows!Initial support for building TensorFlow on Microsoft Windows was added on  2016-10-05 in commit :.TensorFlow is now officially available on Windows!"},
{"body": "In Matplotlib, I make dashed grid lines as follows:however, I can't find out how (or even if it is possible) to make the grid lines be drawn behind other graph elements, such as bars. Changing the order of adding the grid versus adding other elements makes no difference. Is it possible to make it so that the grid lines appear behind everything else?According to this -  - you can use Axis.set_axisbelow()(I am currently installing matplotlib for the first time, so have no idea if that's correct - I just found it by googling \"matplotlib z order grid\" - \"z order\" is typically used to describe this kind of thing (z being the axis \"out of the page\"))To me, it was unclear how to apply andrew cooke's answer, so this is a complete solution based on that:I had the same problem and the following worked:Increase to a higher value if it does not work.If you want to validate the setting for  figures, you may setor It works for Matplotlib>=2.0."},
{"body": "I've got some example Python code that I need to mimic in C++. I do not require any specific solution (such as co-routine based yield solutions, although they would be acceptable answers as well), I simply need to reproduce the semantics in some manner.This is a basic sequence generator, clearly too large to store a materialized version.The goal is to maintain two instances of the sequence above, and iterate over them in semi-lockstep, but in chunks. In the example below the  uses the sequence of pairs to initialize the buffer, and the  regenerates the  and processes the buffer again.The only thing I can find for a solution in C++ is to mimic  with C++ coroutines, but I haven't found any good reference on how to do this. I'm also interested in alternative (non general) solutions for this problem. I do not have enough memory budget to keep a copy of the sequence between passes.Generators exist in C++, just under another name: . For example, reading from  is similar to having a generator of .You simply need to understand what a generator does:In your trivial example, it's easy enough. Conceptually:Of course, we wrap this as a proper class:So hum yeah... might be that C++ is a tad more verbose :)In C++ there are iterators, but implementing an iterator isn't straightforward: one has to consult the  and carefully design the new iterator class to implement them. Thankfully, Boost has an  template which should help implementing the iterators and iterator-compatible generators.Sometimes .P.S. See also  which mentions both a  hack by Christopher M. Kohlhoff and  by Oliver Kowalke. Oliver Kowalke's work  on  by Giovanni P. Deretta.P.S. I think you can also write a kind of generator :Or with a functor:P.S. Here's a generator implemented with the  coroutines:Since  now supports it very well (I found it because I wanted to solve exactly the same  problem), I am posting the C++ code that matches your original intention:In this example,  does not take additional arguments. If it needs to,  or a lambda should be used to generate a function object that takes only one argument (of ), when it is passed to the  constructor.You should probably check generators in std::experimental in Visual Studio 2015 e.g: I think it's exactly what you are looking for. Overall generators should be available in C++17 as this is only experimental Microsoft VC feature.If you only need to do this for a relatively small number of specific generators, you can implement each as a class, where the member data is equivalent to the local variables of the Python generator function. Then you have a next function that returns the next thing the generator would yield, updating the internal state as it does so.This is basically similar to how Python generators are implemented, I believe. The major difference being they can remember an offset into the bytecode for the generator function as part of the \"internal state\", which means the generators can be written as loops containing yields. You would have to instead calculate the next value from the previous. In the case of your , that's pretty trivial. It may not be for complex generators.You also need some way of indicating termination. If what you're returning is \"pointer-like\", and NULL should not be a valid yieldable value you could use a NULL pointer as a termination indicator. Otherwise you need an out-of-band signal.Something like this is very similar:Using the operator() is only a question of what you want to do with this generator, you could also build it as a stream and make sure it adapts to an istream_iterator, for example.Just as a function simulates the concept of a stack, generators simulate the concept of a queue.  The rest is semantics.  As a side note, you can always simulate a queue with a stack by using a stack of operations instead of data.  What that practically means is that you can implement a queue-like behavior by returning a pair, the second value of which either has the next function to be called or indicates that we are out of values.  But this is more general than what yield vs return does.  It allows to simulate a queue of any values rather than homogeneous values that you expect from a generator, but without keeping a full internal queue.More specifically, since C++ does not have a natural abstraction for a queue, you need to use constructs which implement a queue internally.  So the answer which gave the example with iterators is a decent implementation of the concept.What this practically means is that you can implement something with bare-bones queue functionality if you just want something quick and then consume queue's values just as you would consume values yielded from a generator.All answers that involve writing your own iterator are completely wrong. Such answers entirely miss the point of Python generators (one of the language's greatest and unique features). The most important thing about generators is that execution picks up where it left off. This does not happen to iterators. Instead, you must manually store state information such that when operator++ or operator* is called anew, the right information is in place  of the next function call. This is why writing your own C++ iterator is a gigantic pain; whereas, generators are elegant, and easy to read+write.I don't think there is a good analog for Python generators in native C++, at least not yet (there is a rummor that ). You can get something similarish by resorting to third-party (e.g. Yongwei's Boost suggestion), or rolling your own.I would say the closest thing in native C++ is threads. A thread can maintain a suspended set of local variables, and can continue execution where it left off, very much like generators, but you need to roll a little bit of additional infrastructure to support communication between the generator object and its caller. E.g.This solution has several downsides though:Something like :Example use:Will print the numbers from 0 to 99"},
{"body": "I've started to use my mac to install python packages in the same way I do with my Windows PC at work, however on my mac I've come across frequent  errors writing log files & sometimes writing to site-packages.Therefore I thought about running  under  but is that a safe/acceptable use of sudo considering I'm just wanting this to be installed under my current user account.Example traceback from a logfile I/O error;\nI am able to install if I use  Because I had the same problem, I want to stress that actually the first comment by  is the solution to the \"IOError: [Errno 13]\"-problem:If executed in the temp directory (), the IOError does not occur anymore if I run .Use a :You only use  or elevated permissions when you want to install stuff for the global, system-wide Python installation.It is best to use a virtual environment which isolates packages for you.  That way you can play around without polluting the global python install.As a bonus, virtualenv does not need elevated permissions.It's not safe and it's being frowned upon \u2013 see \nTo install Python package in your home directory you don't need root privileges. See  of  option to pip.It looks like your permissions are messed up. Type  in the Terminal and try  again? Let me know if you're sorted.Actually for this particular case (you not wanting to use  for installing python packages) and no need for global package installs you can use the  flag like this :and it will work just fine.Another approach  like in your case you want to do something like :or more generally  I had a problem installing  after successfully installing .My terminal complained after this:So, I unsuccessfully tried this ():Then, I  installed it with this:"},
{"body": "I have been developing a basic app. Now at the deployment stage it has become clear I have need for both a local settings and production settings.It would be great to know the following:My favorite way of doing this is to use the  environment variable and use two (or more) settings files, e.g.  and . You can then use a bootstrap script or a process manager to load the correct settings (by setting the environment). If you're using a virtualenv, you could even hack this into the virtualenv's activate script.You can take advantage of the  variable to store the settings in a completely different location (e.g. on a production server, storing them in  makes sense) \u2014 this allows for easier deployment as you totally separate configuration from application files (which is a good thing).If you're not fond of writing a bootstrap script that sets the environment (and there are very good reasons to feel that way!), I would recommend using a process manager:If using a virtualenv, you append this to your  script:And on your test server:I usually have one settings file per environment, and a shared settings file:Each of my environment files has:This allows me to override shared settings if necessary (by adding the modifications below that stanza).I then select which settings files to use by linking it in to settings.py:Create multiple  files, extrapolating the variables that need to change per environment. Then at the end of your master  file:You keep the separate  files for each stage.At the top of your  file, add this:To import variables that you need to modify.This  has more ideas on how to split your settings.By default use production settings, but create a file called  in the same folder as your  file. Add overrides there, such as .On the computer that will be used for development, add this to your  file:At the bottom of your  file, add the following.(Note that importing  should generally be avoided in Python, but this is a unique circumstance)By default the production servers will not override anything. Done!Compared to the other answers, this one is simpler because it doesn't require updating , or setting  which only allows you to work on one django project at a time.If you want to keep 1 settings file, and your development operating system is different than your production operating system, you can put this at the bottom of your settings.py:Read more: I use the awesome , and all the settings are stored in my :To configure the Django project I just followed the .This is my solution, with different environements for dev, test and prod"},
{"body": "I have a freshly installed Ubuntu on a freshly built computer. I just installed python-pip using apt-get. Now when I try to pip install Numpy and Pandas, it gives the following error.I've seen this error mentioned in quite a few places on SO and Google, but I haven't been able to find a solution. Some people mention it's a bug, some threads are just dead... What's going on?I had this exact problem recently and usedThis adds numpy to your system python interpreter.  I may have had to do the same for matplotlib.  To use in a virtualenv, you have to create your environment using theoptionFor me @Charles Duffy comment solved it.\nPut this in your env:You can add it to your .bashrc with a line like this:But take in care that you'll affect all other programs. So you may want to use it just for the pip run:Try updating pip:I had that problem with matplotlib package.\nI had to execute:For me this was solved by ignoring a (presumably) corrupted cache withas described here: I had a similar error when running  and it was due to a memory shortage.  I increased the memory in my virtual machine to 4G and that fixed things.A combination of and solved my problem. try  .\nIt worked out for me and same can be used for scipy,pandas etc by replacing them in place of numpy. (Y)In 'site-packages' directory, make 'sitecustomize.py' like thisNow you can get the file 'pip.log'If you want the pip version of numpy, you can build the dependencies for the package and then install it using pipThis should install everything needed at system level to install the package. Had a similar problem on a Jetson TK1 with Ubuntu.Works fine with  "},
{"body": "Given a DataFrame with a column \"BoolCol\", we want to find the indexes of the DataFrame in which the values for \"BoolCol\" == TrueI currently have the iterating way to do it, which works perfectly:But this is not the correct panda's way to do it.\nAfter some research, I am currently using this code:This one gives me a list of indexes, but they dont match, when I check them by doing:The result is actually False!!Which would be the correct Pandas way to do this? returns the  row of .  does not refer to the index label,  is a 0-based index.In contrast, , not numeric row-indices:or equivalently,You can see the difference quite clearly by playing with a DataFrame with\nan \"unusual\" index:, :Note that ::Use  to select rows by ordinal index:Can be done using numpy where() function:Though you don't always need index for a match, but incase if you need:"},
{"body": "I have two models like this:I need to do something if the user has Type1 or Type2 profile:But, for users that don't have either type1 or type2 profiles, executing code like that produces the following error:How can I check the type of profile a user has?ThanksTo check if the (OneToOne) relation exists or not, you can use the  function:It's possible to see if a nullable one-to-one relationship is null for a particular model simply by testing the corresponding field on the model for ness, but  if you test on the model where the one-to-one relationship originates. For example, given these two classes\u2026\u2026 to see if a  has a , we can use the following code:To see if a  has a , it's important to understand that referencing the  property on an instance of  raises a  exception if there is no corresponding restaurant. This happens because Django performs a lookup internally using . For example:In this scenario, Occam's razor prevails, and the best approach for making a determination about whether or not a  has a  would be a standard  /  construct as described .While joctee's suggestion to use  works in practice, it really only works by accident since  suppresses  exceptions (including ) as opposed to just s, like it should. As Piet Delport pointed out, this behavior was actually corrected in Python 3.2 per the following ticket: . Furthermore \u2014 and at the risk of sounding opinionated \u2014 I believe the above  /  construct is more representative of how Django works, while using  can cloud the issue for newbies, which may create FUD and spread bad habits.How about using try/except blocks?Then, use like this!I suppose you could use this as a generic function to get any reverse OneToOne instance, given an originating class (here: your profile classes) and a related instance (here: request.user).I like , because it's so simple.Other commenters have raised concerns that it may not work with certain versions of Python or Django, but  shows this technique as one of the options:Of course, the documentation also shows the exception catching technique:I agree with  that catching the exception makes it clearer what's happening, but it just seems messier to me. Perhaps this is a reasonable compromise?This is just querying the  objects by place. It returns  if that place has no restaurant.Here's an executable snippet for you to play with the options. If you have Python, Django, and SQLite3 installed, it should just run. I tested it with Python 2.7, Python 3.4, Django 1.9.2, and SQLite3 3.8.2.Use !"},
{"body": "I know Python doesn't have pointers, but is there a way to have this yield  instead?Here's an example: I want  and  to always have the same value. It's not completely necessary, but I think it would be nice.This is feasible, because it involves decorated names and indexing -- i.e.,  different constructs from the   and  that you're asking about, and for with your request is utterly impossible.  Why ask for something impossible  totally different from the (possible) thing you actually ?!Maybe you don't realize how drastically different barenames and decorated names are.  When you refer to a barename , you're getting exactly the object  was last bound to in this scope (or an exception if it wasn't bound in this scope) -- this is such a deep and fundamental aspect of Python that it can't possibly be subverted.  When you refer to a  name , you're asking an object (the object  refers to) to please supply \"the  attribute\" -- and in response to that request, the object can perform totally arbitrary computations (and indexing is quite similar: it also allows arbitrary computations to be performed in response).Now, your \"actual desiderata\" example is mysterious because in each case two levels of indexing or attribute-getting are involved, so the subtlety you crave could be introduced in many ways.  What other attributes is  suppose to have, for example, besides ?  Without that further  computations, possibilities would include:andThe presence of  suggests picking the first form, plus a kind-of-useless wrapper:If  such  is also supposed to set the entry in , then the wrapper must become more complex indeed, and not all that useless:The latter example is roughly as close as it gets, in Python, to the sense of \"a pointer\" as you seem to want -- but it's crucial to understand that such subtleties can ever only work with  and/or ,  with barenames as you originally asked!It's not a bug, it's a feature :-)When you look at the '=' operator in Python, don't think in terms of assignment. You don't assign things, you bind them. = is a binding operator.So in your code, you are giving the value 1 a name: a. Then, you are giving the value in 'a' a name: b. Then you are binding the value 2 to the name 'a'. The value bound to b doesn't change in this operation.Coming from C-like languages, this can be confusing, but once you become accustomed to it, you find that it helps you to read and reason about your code more clearly: the value which has the name 'b' will not change unless you explicitly change it. And if you do an 'import this', you'll find that the Zen of Python states that Explicit is better than implicit.Note as well that functional languages such as Haskell also use this paradigm, with great value in terms of robustness.There's no way you can do that changing only that line.  You can do: That creates a list, assigns the reference to a, then b also, uses the a reference to set the first element to 2, then accesses using the b reference variable.From one point of view,  is a pointer in Python.  Your example works a lot like the C++ code.(A closer equivalent would use some type of  instead of .)You can do this by overloading  in 's class.Yes! there is a way to use a variable as a pointer in python!I am sorry to say that many of answers were partially wrong. In principle every equal(=) assignation shares the memory address (check the id(obj) function), but in practice it is not such. There are variables whose equal(\"=\") behaviour works in last term as a copy of memory space, mostly in simple objects (e.g. \"int\" object), and others in which not (e.g. \"list\",\"dict\" objects).Here is an example of pointer assignationHere is an example of copy assignationPointer assignation is a pretty useful tool for aliasing without the waste of extra memory, in certain situations for performing comfy code,but one have to be aware of this use in order to prevent code mistakes.To conclude, by default some variables are barenames (simple objects like int, float, str,...), and some are pointers when assigned between them (e.g. dict1 = dict2). How to recognize them? just try this experiment with them. In IDEs with variable explorer panel usually appears to be the memory address (\"@axbbbbbb...\") in the definition of pointer-mechanism objects.I suggest investigate in the topic. There are many people who know much more about this topic for sure. (see \"ctypes\" module). I hope it is helpful. Enjoy the good use of the objects! Regards, Jos\u00e9 Crespo"},
{"body": "I have an existing file on disk (say /folder/file.txt) and a FileField model field in Django.When I do it re-saves the file as  (the next time it's , etc.).I understand why, but I don't want this behavior - I know the file I want the field to be associated with is really there waiting for me, and I just want Django to point to it.How?If you want to do this permanently, you need to create your own FileStorage classNow in your model, you use your modified MyFileStoragejust set  to the path of your filee.g.try this ():It's right to write own storage class. However  is not the right method to override. is called when Django sees a file with same name and tries to get a new available file name. It's not the method that causes the rename. the method caused that is . Comments in  is pretty good and you can easily find it opens file for writing with flag  which will throw an OSError if same file name already exists. Django catches this Error then calls  to get a new name.So I think the correct way is to override  and call os.open() without flag . The modification is quite simple however the method is a little be long so I don't paste it here. Tell me if you need more help :)I had exactly the same problem! then I realize that my Models were causing that. example I hade my models like this:Then, I wanted to have more the one tile referencing the same file in the disk! The way that I found to solve that was change my Model structure to this:Which after I realize that make more sense, because if I want the same file being saved more then one in my DB I have to create another table for it! I guess you can solve your problem like that too, just hoping that you can change the models! Also I guess you can use a different storage, like this for instance: SymlinkOrCopyStorage"},
{"body": "I have a dictionary which looks like this: I would like to apply it to the \"col1\" column of a dataframe similar to:to get:How can I best do this? For some reason googling terms relating to this only shows me links about how to make columns from dicts and vice-versa :-/ You can use .  For example:or directly on the , i.e. .There is a bit of ambiguity in your question. There are at least  two interpretations:Below is a solution for each case.\nIf the keys of  are meant to refer to index values, then you could use the  method:For example,yieldsI've modified the values from your original post so it is clearer what  is doing.\nNote how the keys in  are associated with index values. The order of the index values -- that is, the index  -- does not matter.\nIf the keys in  refer to  values, then @DanAllan and @DSM show how to achieve this with :yieldsNote how in this case the keys in  were changed to match  in .\nIf the keys in  refer to index locations, then you could usesinceyieldsHere, the first and third rows were altered, because the keys in  are  and , which with Python's 0-based indexing refer to the first and third locations.This is an alternative answer that can be much faster when your dictionary has more than a couple of keys.  If your dictionary exhaustively maps all possible values and the column is numeric (not object type), this takes a very simple form:Here are the timings on dataframe with 100,000 rows and 8 dictionary keys (it's about 10x faster).With some additional code, you can extend this method to more general cases as in the question (columns of object type (non-integer, non-float) with partial mappings):That will still be much faster, but obviously is not as simple or elegant as using ."},
{"body": "I am plotting two similar trajectories in matplotlib and I'd like to plot each of the lines with partial transparency so that the red (plotted second) doesn't obscure the blue.: Here's the image with transparent lines.Plain and simple:(I know I add nothing new, but the straightforward answer should be visible).After I plotted all the lines, I was able to set the transparency of all of them as follows: please see Joe's answer in the comments.It really depends on what functions you're using to plot the lines, but try see if the on you're using takes an alpha value and set it to something like 0.5. If that doesn't work, try get the line objects and set their alpha values directly."},
{"body": "Normally Fabric quits as soon as a run() call returns a non-zero exit code. For some calls, however, this is expected. For example, PNGOut returns an error code of 2 when it is unable to compress a file.Currently I can only circumvent this limitation by either using shell logic ( or ), but I'd rather be able to keep my logic in plain Python (as is the Fabric promise).Is there a way to check for an error code and react to it rather than having Fabric panic and die? I still want the default behaviours for other calls, so changing its behaviour by modifying the environment doesn't seem like a good option (and as far as I recall, you can only use that to tell it to warn instead of dying anyway).You can prevent aborting on non-zero exit codes by using the  context manager and the  setting: My answer is outdated. See comments below. Yes, you can. Just change the environment's . For example:The documentation on  is .Apparently messing with the environment  the answer. can be used as a context manager (with ) to apply it to individual statements. The return value of ,  and  calls isn't just the output of the shell command, but also has special properties ( and ) that allow reacting to the errors.I guess I was looking for something closer to the behaviour of  or Python's usual exception handling.try this"},
{"body": "I have a list like below where the first element is the id and the other is a string:I want to create a list of ids only from this list of tuples as below:I'll use this list in  so it needs to be a list of integer values. Please help!Use the zip function to decouple elements.do you mean something like this?What you actually have is a list of  objects, not a list of sets (as your original question implied).  If it is actually a list of sets, then  because sets have no order.Here I've created a flat list because generally that seems more useful than creating a list of 1 element tuples.  However, you can easily create a list of 1 element tuples by just replacing  with .if the tuples are unique then this can workThis is what  is for. The  statement  that returns the index of the element you specify. It's exactly the same as writingBut I find that  is a clearer and . This is handy for making compact sort statements. For example,Those are tuples, not sets. You can do this:when I ran (as suggested above):instead of returning:I received this as the return:I found I had to use list():to successfully return a list using this suggestion. That said, I'm happy with this solution, thanks. (tested/run using Spyder, iPython console, Python v3.6)You can use \"tuple unpacking\":At iteration time each tuple is unpacked and its values are set to the variables  and . "},
{"body": "Ordered dictionaries are extremely useful structures, but unfortunately these are quite recent only working in versions from  and . How can I use an ordered dictionary in older versions?I installed ordereddict on python 2.6 with pipAccording to the , for Python versions 2.4 or later  should be used. There is also some , one of the contributors to the . The code here is claimed to work under 2.6 and 3.0 and was made for the proposal.To import a OrderedDict class for different versions of Python, consider this snippet:Versions older than Python 2.6 will need to install  (using pip or other methods), but newer versions will import from the built-in collections module.Also, you could just program your way around it if your situation allows:You could embed the ordered keys in your Dictionary too, I suppose, as oDict['keyList'] = orderedKeysin python2.6 gave to me:but did.For those who can't depend on the user having pip for some reason, here is a  implementaiton of OrderedDict (it is immutable, has most of the features but none of the performance boost)."},
{"body": "I have output of a command in tabular form. I'm parsing this output from result file and storing it in a string. Each element in 1 row is seperated by one or more space. So I'm using regular expression to match 1 or more spaces and split it, but python is inserting a space between every element:Is there a better way to do this? After each split str2 is appended to list.By using ,, you are capturing the group, if you simply remove them you will not have this problem.However there is no need for regex,  without any delimiter specified will split this by whitespace for you. This would be the best way in this case.If you really wanted regex you can use this ( represents whitespace and it's clearer):or you can find all non-whitespace charactersThe  method will automatically remove all white space between items:Docs are here: When you use  and the split pattern contains capturing groups, the groups are retained in the output.  If you don't want this, use a non-capturing group instead.Its very simple actually. Try this:"},
{"body": "Suppose I have an  statement with a . From the efficiency perspective, should I useorShould I prefer one or another when using a compiled language (C) or a scripted one (Python)?Since the  statement terminates the execution of the current function, the two forms are equivalent (although the second one is arguably more readable than the first).The efficiency of both forms is comparable, the underlying machine code has to perform a jump if the  condition is false anyway.Note that Python supports a syntax that allows you to use only one  statement in your case:From  style guide:Don't use else after return:With any sensible compiler, you should observe no difference; they should be compiled to identical machine code as they're equivalent.Version A is simpler and that's why I would use it.And if you turn on all compiler warnings in Java you will get a warning on the second Version because it is unnecesarry and turns up code complexity.Regarding coding style:Most coding standards no matter language ban multiple return statements from a single function as bad practice. (Although personally I would say there are several cases where multiple return statements do make sense: text/data protocol parsers, functions with extensive error handling etc)The consensus from all those industry coding standards is that the expression should be written as:Regarding efficiency:The above example and the two examples in the question are all  in terms of efficiency. The machine code in all these cases have to compare A > B, then branch to either the A+1 or the A-1 calculation, then store the result of that in a CPU register or on the stack.Sources:I would always use:As it obeys both the good convention of having a single return statement as the last statement in the function (as already mentioned) and the good functional programming paradigm of avoiding imperative style intermediate results. For more complex functions I revert to using a imperative style variable called rval.  I never use multiple return statements, preferring to break the function into multiple sub-functions to avoid this if necessary.  I do not consider the question of efficiency at all, as its the efficiency of coding (i.e. writing error free code) that matters to me. "},
{"body": "How do I create a file-like object (same duck type as File) with the contents of a string?For Python 2.x, use the  module.  For example:I use cStringIO (which is faster), but note that it doesn't .  (You can switch to StringIO by changing  \"from cStringIO\" to \"from StringIO\".)For Python 3.x, use the  module.In Python 3.0:Two good answers. I\u2019d add a little trick \u2014 if you need a real file object (some methods expect one, not just an interface), here is a way to create an adapter:"},
{"body": "I want to save the name of the error and the traceback details into a variable.  Here's is my attempt.P.S. I know this can be done easily using the traceback module, but I want to know usage of sys.exc_info()[2] object here.This is how I do it:You should however take a look at the , as you might find there more suitable methods, depending to how you want to process your variable afterwards...Use  if you want convenient access to module and function names and line numbers.Use  if you just want a string that looks like the  output.Notice that even with  you will get a multi-line string, since the elements of  contain .  See output below.Remember to .Here's the output from .  Formatting added for readability.Here's the output from .  Formatting added for readability.sys.exc_info() returns a tuple with three values (type, value, traceback). For Example, In the following programNow If we print the tuple the values will be this.The above details can also be fetched by simply printing the exception in string format. My  to another question may help illustrate the details - with links!  For canned strings, standard library traceback module seems okay.  If you want to get the details, read the source () for more info."},
{"body": "The folder I want to get to is called python and is on my desktop.I get the following error when I try to get to itYou need to use a  string, double your slashes or use forward slashes instead:In regular python strings, the  character combination signals a extended Unicode codepoint escape.\nThis syntax worked for me.This usually happens in Python 3. One of the common reasons would be that while specifying your file path you need \"\\\\\" instead of \"\\\". As in:For Python 2, just using \"\\\" would work.f = open('C:\\\\Users\\\\Pooja\\\\Desktop\\\\trolldata.csv')...\nUse '\\\\' for python program in python version 3 and above..\nError will be resolved..All the three syntax work very well.Another way is to first writepath = r'C:\\user\\...................' (whatever is the path for you)and then passing it to os.chdir(path)"},
{"body": "I want to start writing unit tests for my Python code, and the  framework sounds like a better bet than Python's bundled . So I added a \"tests\" directory to my project, and added  to it. Now I want to configure PyCharm to run all the tests in my \"tests\" directory.PyCharm allegedly  in its test runner. You're supposed to be able to  to run your tests, and PyCharm allegedly has a . But that's the complete extent of their documentation on the subject, and I can't find this alleged dialog box anywhere.If I right-click the directory in the Project tool window, I'm  to see a \"Create <name>\" menu item, but the only menu item starting with \"Create\" is \"Create Run Configuration\". Okay, maybe the documentation is just wrong, and \"Create Run Configuration\" does sound promising. Unfortunately, the only two items in its submenu are \"Unittests in C:\\mypath...\" and \"Doctests in C:\\mypath...\", neither of which applies -- I'm using neither unittest nor doctest. There is no menu item for py.test.If I open my test_sample.py and right-click in the editor window, I do get the promised \"Create <name>\" menu items: there's \"Create 'Unittests in test_sa...'...\", followed by \"Run 'Unittests in test_sa...'\" and \"Debug 'Unittests in test_sa...'\". So again, it's all specific to the unittest framework; nothing for py.test.If I do try the menu items that say \"unittest\", I get a dialog box with options for \"Name\", \"Type\", a \"Tests\" group box with \"Folder\" and \"Pattern\" and \"Script\" and \"Class\" and \"Function\", etc. This sounds exactly like what's documented as the dialog to add a , and not like the \"Name\" and \"Test to run\" and \"Keywords\" options that are supposed to show up in the  dialog. There's nothing inside the dialog to switch which test framework I'm adding.I'm using PyCharm 1.5.2 on Windows with Python 3.1.3 and pytest 2.0.3. I can successfully run  on my tests from the command line, so it's not something simple like pytest not being installed properly.How do I configure PyCharm to run my py.test tests?Please go to File | Settings | Tools | Python Integrated Tools and change the default test runner to py.test. Then you'll get the py.test option to create tests instead of the unittest one.I think you need to use the Run/Debug Configuration item on the toolbar. Click it and 'Edit Configurations' (or alternatively use the menu item Run->Edit Configurations). In the 'Defaults' section in the left pane there is a 'py.test' item which I think is what you want.I also found that the manual didn't match up to the UI for this. Hope I've understood the problem correctly and that helps.It's poorly documented to be sure.   Once you get add a new configuration from defaults, you will be in the realm of running the \"/Applications/PyCharm CE.app/Contents/helpers/pycharm/pytestrunner.py\" script.   It's not documented and has its own ideas of command line arguments.You can:Oddly, you will find it hard to find any discussion as JetBrains does a good job of bombing Google algorithms with its own pages.With a special Conda python setup which included the pip install for py.test plus usage of the Specs addin (option --spec) (for Rspec like nice test summary language), I had to do ; 1.Edit the default py.test to  include option= --spec , which means use the plugin: 2.Create new test configuration, using py.test. Change its python interpreter to use ~/anaconda/envs/ your choice of interpreters, eg py27  for my namings.3.Delete the 'unittests' test configuration.4.Now the default test config is py.test with my lovely Rspec style outputs. I love it! Thank you everyone!p.s. Jetbrains' doc on run/debug configs is here: "},
{"body": "I have a file called test_web.py containing a class TestWeb and many methods named like  test_something().I can run every test in the class like so:But I can\u2019t seem to run individual tests. These give me \u201cNo such test\u201d errors when run in the same PWD:What could be wrong here?You must specify it like so: , orSee You can also specify a module:Specifying names on the command line like the other answers suggest does work and is useful. However, when I'm in the midst of writing tests, I often find that I want to run just the test I'm working on, and the names that I would have to write on the command line get pretty long and cumbersome to write. In such case, I prefer to use a custom decorator and flag.I define  (\"work in progress decorator\") like this:This defines a decorator  which will set the  attribute on objects it decorates. For instance:Then  can be used at the command line to narrow the execution of the test to the ones marked with .I'm using the name  for the decorator rather than  to avoid this kind of problem:The  will make the  decorator a member , and  tests in the class will be selected. The  plugin checks the parent class of a test method to see if the attribute selected exists there too, and the attributes that are created and tested by  do not exist in a segregated space. So if you test with  and your class contains , then all tests in the class will be selected by the plugin.To run multiple specific tests, you can just add them to the command line, separated by space."},
{"body": "Take a look at this:I ran a , but could not find the answer - what should I be using instead of ? is deprecated because of .Now you can  your changes using . This transforms your model changes into python code to make them deployable to another databases.After you created the migrations you have to  them: .So instead of using  you should use  and then .You should definitely use . Which lets you track changes in your , and create migrations for the database. The migration system uses the commands  to create migrations and  to migrate the database.If for whatever reason you need to create a database the same way  did it there is command flag that causes  to work the same way. You should only do this if you  need it and you know what you are doing. For example to create an empty database on for a continuous integration system of your choice.Tested on Django 1.9.1.You should use the  and  commands that were introduced in django 1.7 has some problem with db migration. so, after django 1.7  and  have been introduced. \nNow in django 1.9  has been deprecated.\ntry\n1.  which detects changes in db and creates one  file as inside migrations folder\n2.  will apply the migrations to the database"},
{"body": "Is there any way to get SQLAlchemy to do a bulk insert rather than inserting each individual object. i.e.,doing:rather than:I've just converted some code to use sqlalchemy rather than raw sql and although it is now much nicer to work with it seems to be slower now (up to a factor of 10), I'm wondering if this is the reason.May be I could improve the situation using sessions more efficiently. At the moment I have  and do a  after I've added some stuff. Although this seems to cause the data to go stale if the DB is changed elsewhere, like even if I do a new query I still get old results back?Thanks for your help!SQLAlchemy introduced that in version :With these operations, you can now do bulk inserts or updates!For instance, you can do:Here, a bulk insert will be made.As far as I know, there is no way to get the ORM to issue bulk inserts. I believe the underlying reason is that SQLAlchemy needs to keep track of each object's identity (i.e., new primary keys), and bulk inserts interfere with that. For example, assuming your  table contains an  column and is mapped to a  class:Since SQLAlchemy picked up the value for  without issuing another query, we can infer that it got the value directly from the  statement. If you don't need subsequent access to the created objects via the  instances, you can skip the ORM layer for your insert:SQLAlchemy can't match these new rows with any existing objects, so you'll have to query them anew to for any subsequent operations.As far as stale data is concerned, it's helpful to remember that the session has no built-in way to know when the database is changed outside of the session. In order to access externally modified data through existing instances, the instances must be marked as . This happens by default on , but can be done manually by calling  or . An example (SQL omitted): expires , so the first print statement implicitly opens a new transaction and re-queries 's attributes. If you comment out the first print statement, you'll notice that the second one now picks up the correct value, because the new query isn't emitted until after the update.This makes sense from the point of view of transactional isolation - you should only pick up external modifications between transactions. If this is causing you trouble, I'd suggest clarifying or  re-thinking your application's transaction boundaries instead of immediately reaching for .The sqlalchemy docs have a great writeup on the performance of various techniques that can be used for bulk inserts:Direct support was added to SQLAlchemy as of version 0.8As per the ,  should do the trick. (Note that this is  the same as  which results in many individual row inserts via a call to ). On anything but a local connection the difference in performance can be enormous.This is a way:This will insert like this:Reference: The SQLAlchemy  includes benchmarks for various commit methods.I usually do it using .SQLAlchemy introduced that in version :With these operations, you can now do bulk inserts or updates!For instance (if you want the lowest overhead for simple table INSERTs), you can use :Or, if you want, skip the  tuples and write the dictionaries directly into  (but I find it easier to leave all the wordiness out of the data and load up a list of dictionaries in a loop).Piere's answer is correct but one issue is that  by default does not return the primary keys of the objects, if that is of concern to you. Set  to  to get this behavior.The documentation is ."},
{"body": "When do I use each ?Also...is the NLTK lemmatization dependent upon Parts of Speech?\nWouldn't it be more accurate if it was?Short and dense: From the NLTK docs:: As MYYN pointed out, stemming is the process of removing inflectional and sometimes derivational affixes to a base form that all of the original words are probably related to.  Lemmatization is concerned with obtaining the single word that allows you to group together a bunch of inflected forms.  This is harder than stemming because it requires taking the context into account (and thus the meaning of the word), while stemming ignores context.As for when you would use one or the other, it's a matter of how much your application depends on getting the meaning of a word in context correct.  If you're doing machine translation, you probably want lemmatization to avoid mistranslating a word.  If you're doing information retrieval over a billion documents with 99% of your queries ranging from 1-3 words, you can settle for stemming.As for NLTK, the WordNetLemmatizer does use the part of speech, though you have to provide it (otherwise it defaults to nouns).  Passing it \"dove\" and \"v\" yields \"dive\" while \"dove\" and \"n\" yields \"dove\".The purpose of both stemming and lemmatization is to reduce morphological variation. This is in contrast to the the more general \"term conflation\" procedures, which may also address lexico-semantic, syntactic, or orthographic variations. The real difference between stemming and lemmatization is threefold:Lemmatization may also be backed up by a part-of-speech tagger in order to disambiguate homonyms.There are two aspects to show their differences:Reference ianacl\nbut i think Stemming is a rough hack people use to get all the different forms of the same word down to a base form which need not be a legit word on its own\nSomething like the Porter Stemmer can uses simple regexes to eliminate common word suffixesLemmatization brings a word down to its actual base form which, in the case of irregular verbs, might look nothing like the input word\nSomething like Morpha which uses FSTs to bring nouns and verbs to their base formAn example-driven explanation on the differenes between lemmatization and stemming: handles    handles  ."},
{"body": "I am looking for a fast way to preserve large numpy arrays. I want to save them to the disk in a binary format, then read them back into memory relatively fastly. cPickle is not fast enough, unfortunately. I found  and . But the weird thing is, numpy.load loads a npy file into \"memory-map\". That means regular manipulating of arrays really slow. For example, something like this would be really slow: more precisely, the first line will be really fast, but the remaining lines that assign the arrays to  are ridiculously slow:Is there any better way of preserving numpy arrays? Ideally, I want to be able to store multiple arrays in one file. I'm a big fan of hdf5 for storing large numpy arrays. There are two options for dealing with hdf5 in python:Both are designed to work with numpy arrays efficiently.I've compared performance (space and time) for a number of ways to store numpy arrays. Few of them support multiple arrays per file, but perhaps it's useful anyway.Npy and binary files are both really fast and small for dense data. If the data is sparse or very structured, you might want to use npz with compression, which'll save a lot of space but cost some load time.If portability is an issue, binary is better than npy. If human readability is important, then you'll have to sacrifice a lot of performance, but it can be achieved fairly well using csv (which is also very portable of course).More details and the code are available at .There is now a HDF5 based clone of  called !savez() save data in a zip file, It may take some time to zip & unzip the file. You can use save() & load() function:To save multiple arrays in one file, you just need to open the file first, and then save or load the arrays in sequence.Another possibility to store numpy arrays efficiently is :and the output for my laptop (a relatively old MacBook Air with a Core2 processor):that means that it can store really fast, i.e. the bottleneck is typically the disk.  However, as the compression ratios are pretty good here, the effective speed is multiplied by the compression ratios.  Here are the sizes for these 76 MB arrays:Please note that the use of the  compressor is fundamental for achieving this.  The same script but using 'clevel' = 0 (i.e. disabling compression):is clearly bottlenecked by the disk performance.The lookup time is slow because when you use  to does not load content of array to memory when you invoke  method. Data is lazy loaded when particular data is needed. \nAnd this happens in lookup in your case. But second lookup won`t be so slow.This is nice feature of  when you have a big array you do not have to load whole data into memory.To solve your can use  you can dump any object you want using  even two or more , see the example"},
{"body": "I'm trying to use Python to download the HTML source code of a website but I'm receiving this error. I'm following the guide here: I'm using Python 3, thanks for the help!This works in Python 2.x.For Python 3 look here:A Python 2+3 compatible solution is:In Python v3 the \"urllib.request\" is a module by itself, therefore \"urllib\" cannot be used here."},
{"body": "I am using ConfigParser to read the runtime configuration of a script.I would like to have the flexibility of not providing a section name (there are scripts which are simple enough; they don't need a 'section'). ConfigParser will throw the  exception, and will not accept the file.How can I make ConfigParser simply retrieve the  tuples of a config file without section names? For instance:I would rather not write to the config file.Alex Martelli  for using  to parse  files (which are apparently section-less config files). is a file-like wrapper that will automagically insert a dummy section heading to satisfy 's requirements.Enlightened by , I come up with this solution:You can do this with a single additional line of code. (Two lines if you count  statements.)In python 3, use  to simulate a section header for .In python 2, prepend a section header line to the data you read from your config file, wrap the result in a  object, and pass it to .With either approach, your config settings will be available in .You could use  in python 3 as well, by importing it from its new home in the  package, but note that  is deprecated in python 3 and should therefore be avoided.You can use the ConfigObj library to do that simply : Updated: Find latest code .If you are under Debian/Ubuntu, you can install this module using your package manager :An example of use:Having ran into this problem myself, I wrote a complete wrapper to ConfigParser (the version in Python 2) that can read and write files without sections transparently, based on Alex Martelli's approach linked on the accepted answer. It should be a drop-in replacement to any usage of ConfigParser. Posting it in case anyone in need of that finds this page. The easiest way to do this is to use python's CSV parser, in my opinion.  Here's a read/write function demonstrating this approach as well as a test driver. This should work provided the values are not allowed to be multi-line.  :)Blueicefield's answer mentioned configobj, but the original lib only supports Python 2. It now has a Python 3+ compatible port: APIs haven't changed, see it's . lib can help in case  "},
{"body": "I want my Python script to copy files on Vista. When I run it from a normal  window, no errors are generated, yet the files are NOT copied. If I run  \"as administator\" and then run my script, it works fine.This makes sense since User Account Control (UAC) normally prevents many file system actions.Is there a way I can, from within a Python script, invoke a UAC elevation request (those dialogs that say something like \"such and such app needs admin access, is this OK?\")If that's not possible, is there a way my script can at least detect that it is not elevated so it can fail gracefully?As of 2017, an easy method to achieve this is the following:Some of the advantages here are:Documentation for the underlying ShellExecute call is . It took me a little while to get dguaraglia's answer working, so in the interest of saving others time, here's what I did to implement this idea:It seems there's no way to elevate the application privileges for a while for you to perform a particular task. Windows needs to know at the start of the program whether the application requires certain privileges, and will ask the user to confirm when the application performs any tasks that  those privileges. There are two ways to do this:This   explain in much more detail how this works.What I'd do, if you don't want to write a nasty ctypes wrapper for the CreateElevatedProcess API, is use the ShellExecuteEx trick explained in the Code Project article (Pywin32 comes with a wrapper for ShellExecute). How? Something like this:When your program starts, it checks if it has Administrator privileges, if it doesn't it runs itself using the ShellExecute trick and exits immediately, if it does, it performs the task at hand.As you describe your program as a \"script\", I suppose that's enough for your needs.Cheers.Recognizing this question was asked years ago, I think a more elegant solution is offered on  by frmdstryr using his module pyminutils: Excerpt:This utilizes the COM interface and automatically indicates that admin privileges are needed with the familiar dialog prompt that you would see if you were copying into a directory where admin privileges are required and also provides the typical file progress dialog during the copy operation.This may not completely answer your question but you could also try using the Elevate Command Powertoy in order to run the script with elevated UAC privileges.I think if you use it it would look like 'elevate python yourscript.py'If your script always requires an Administrator's privileges then:    You can make a shortcut somewhere and as the target use:\npython yourscript.py\nthen under properties and advanced select run as administrator. When the user executes the shortcut it will ask them to elevate the application.A variation on Jorenko's work above allows the elevated process to use the same console (but see my comment below):The following example builds on  excellent work and accepted answer. In particular, two enumerations are introduced. The first allows for easy specification of how an elevated program is to be opened, and the second helps when errors need to be easily identified. Please note that if you want all command line arguments passed to the new process,  should probably be replaced with a function call: ."},
{"body": "I have two dictionaries and I'd like to be able to make them one:Something like this pseudo-Python would be nice:If you're interested in creating a new dict without using intermediary storage: (this is faster, and in my opinion, cleaner than using dict.items())Or if you're happy to use one of the existing dicts:You are looking for the givesNote this doesn't actually return the combined dictionary, it just mutates .Please search the site before asking questions next time: The easiest way to do it is to simply use your example code, but using the items() member of each dictionary. So, the code would be:I tested this in IDLE and it works fine. \nHowever, the previous question on this topic states that this method is slow and chews up memory. There are several other ways recommended there, so please see that if memory usage is important.Here are quite a few ways to add dictionaries.Creates a new dict by adding both items.If your ok to modify If your  ok to modify If all the keys in one dict are ensured to be strings ( in this case, of course args can be swapped)In some cases it may be handy to use dict comprehensions (Python 2.7 or newer),Especially if you want to filter out or transform some keys/values at the same time."},
{"body": "I am asking Python to print the minimum number from a column of CSV data, but the top row is the column number, and I don't want Python to take the top row into account. How can I make sure Python ignores the first line?This is the code so far:Could you also explain what you are doing, not just give the code? I am very very new to Python and would like to make sure I understand everything.You could use themodule's  class to detect whether a header row is present and the built-infunction to skip over it if necessary:You can also skip rows with a  call if you're using Python 2.x.Since  and  are hardcoded in your example, it would be slightly faster to read  this way:To skip the first line just call:Files in Python are iterators over lines.You would normally use  which advances the iterator one row, so you skip the header. The other (say you wanted to skip 30 rows) would be:In a similar use case I had to skip annoying lines before the line with my actual column names. This solution worked nicely. Read the file first, then pass the list to .use csv.DictReader instead of csv.Reader.\nIf the fieldnames parameter is omitted, the values in the first row of the csvfile will be used as field names. you would then be able to access field values using row[\"1\"] etcThe new 'pandas' package might be more relevant than 'csv'. The code below will read a CSV file, by default interpreting the first line as the column header and find the minimum across columns.Well, my  would do the job as well.Meanwhile, if you know what header column index one is, for example \"Column 1\", you can do this instead:just add example below:that works for me in iPythonPython 3.XHandles UTF8 BOM + HEADERIt was quite frustrating that the  module could not easily get the header, there is also a bug with the UTF-8 BOM (first char in file).\nThis works for me using only the  module:I would use  to get rid of the unwanted first line:"},
{"body": "I'm trying to generate a linear regression on a scatter plot I have generated, however my data is in list format, and all of the examples I can find of using  require using .  doesn't accept lists though. I have searched high and low about how to convert a list to an array and nothing seems clear. Am I missing something?Following on, how best can I use my list of integers as inputs to the ?here is the polyfit example I am following:  lists (well, numpy arrays); type  for the details.  You don't need to call it on existing lists.I should add that I tend to use  here rather than write out \"m*x+b\" and the higher-order equivalents, so my version of your code would look something like this:This code:gives out a list with the following:Another quick and dirty answer is that you can just convert your list to an array using: "},
{"body": "I'm new to Python... and coming from a mostly Java background, if that accounts for anything.I'm trying to understand polymorphism in Python. Maybe the problem is that I'm expecting the concepts I already know to project into Python. But I put together the following test code:From the polymorphism I'm used to (e.g. java's ), I would expect both of these statements to print true, as an instance of dog  animal and also  dog. But my output is:What am I missing?The  operator in Python checks that the two arguments refer to the same object in memory; it is not like the  operator in C#.:What you're looking for in this case is .However, idiomatic Python dictates that you (almost) never do type-checking, but instead rely on  for polymorphic behavior.  There's nothing wrong with using  to understand inheritance, but it should generally be avoided in \"production\" code.phimuemue and Mark have answered your question. But this is ALSO an example of polymorphism in Python, but it's not as explicit as your inheritance based example.Try  resp. ."},
{"body": "Given a 3 times 3 numpy arrayTo normalize the rows of the 2-dimensional array I thought ofThere must be a better way, isn't there?Perhaps to clearify: By normalizing I mean, the sum of the entrys per row must be one. But I think that will be clear to most people.Broadcasting is really good for this: reshapes row_sums from being  to being . When you do ,  and  are broadcast against each other.You can learn more about   or even better .Scikit-learn has a normalize function that lets you apply various normalizations. The \"make it sum to 1\" is the L1 norm, and to take that do:Now your rows will sum to 1.I think this should work,it appears that this also worksIn case you are trying to normalize each row such that its magnitude is one (i.e. a row's unit length is one or the sum of the square of each element in a row is one):Verifying:Or using lambda function, likeeach vector of vec will have a unit norm.You could also use matrix transposition:"},
{"body": "I'm just starting to use NLTK and I don't quite understand how to get a list of words from text. If I use , I get a list of words and punctuation. I need only the words instead. How can I get rid of punctuation? Also  doesn't work with multiple sentences: dots are added to the last word.Take a look at the other tokenizing options that nltk provides . For example, you can define a tokenizer that picks out sequences of alphanumeric characters as tokens and drops everything else:Output:You do not really need NLTK to remove punctuation. You can remove it with simple python. For strings:Or for unicode:and then use this string in your tokenizer. string module have some other sets of elements that can be removed (like digits).As noticed in comments start with sent_tokenize(), because word_tokenize() works only on a single sentence. You can filter out punctuation with filter(). And if you have an unicode strings make sure that is a unicode object (not a 'str' encoded with some encoding like 'utf-8'). I just used the following code, which removed all the punctuation:I use this code to remove punctuation:And If you want to check whether a token is a valid English word or not, you may need Tutorial:I think you need some sort of regular expression matching (the following code is in Python 3):Output:Should work well in most cases since it removes punctuation while preserving tokens like \"n't\", which can't be obtained from regex tokenizers such as .Below code will remove all punctuation marks as well as non alphabetic characters. Copied from their book. output"},
{"body": "I have a date string of the form '2009/05/13 19:19:30 -0400'. It seems that previous versions of Python may have supported a %z format tag in strptime for the trailing timezone specification, but 2.6.x seems to have removed that.What's the right way to parse this string into a datetime object?You can use the parse function from dateutil:This way you obtain a datetime object you can then use.As , dateutil2.0 is written for Python 3.0 and does not work with Python 2.x. For Python 2.x dateutil1.5 needs to be used. is supported in Python 3.2+:On earlier versions:where  is a class based on :The problem with using dateutil is that you can't have the same format string for both serialization and deserialization, as dateutil has limited formatting options (only  and ).In my application, I store the format string in .INI file, and each deployment can have its own format. Thus, I really don't like the dateutil approach.Here's an alternative method that uses pytz instead:Here is a fix of the  issue for Python 2.7 and earlierInstead of using:Use the  to account for the timezone, like this:Note that the dates would be converted to , which would allow doing date arithmetic without worrying about time zones.If you are on Linux, then you can use the external  command to dwim:This is of course less portable than dateutil, but slightly more flexible, because  will also accept inputs like \"yesterday\" or \"last year\" :-)"},
{"body": "I need to iterate through the subdirectories of a given directory and search for files. If I get a file I have to open it and change the content and replace it with my own lines.I tried this:but I am getting an error. What am I doing wrong? The actual walk through the directories works as you have coded it. If you replace the contents of the inner loop with a simple  statement you can see that each file is found:If you still get errors when running the above, please provide the error message.Another way of returning all files in subdirectories is to use , introduced in Python 3.4, which provides an object oriented approach to handling filesystem paths (Pathlib is also available on Python 2.7 via ):Since Python 3.5, the  module also supports recursive file finding:The  from either of the above approaches can be iterated over without the need for a nested loop:"},
{"body": "I can give it floating point numbers, such asbut how accurate is it? If i give itwill it really sleep about 50 ms?The accuracy of the time.sleep function depends on the accuracy of your underlying OS's sleep accuracy.  For non-realtime OS's like a stock Windows the smallest interval you can sleep for is about 10-13ms.  I have seen accurate sleeps within several milliseconds of that time when above the minimum 10-13ms.Update:\nLike mentioned in the docs sited below, it's common to do the sleep in a loop that will make sure to go back to sleep if the wakes you up the early.I should also mention that if you are running Ubuntu you can try out a pseudo real-time kernel (with the RT_PREEMPT patch set) by installing the rt kernel package (at least in Ubuntu 10.04 LTS).EDIT: Correction non-realtime Linux kernels have minimum sleep interval much closer to 1ms then 10ms but it varies in a non-deterministic manner.People are quite right about the differences between operating systems and kernels, but I do not see any granularity in Ubuntu and I see a 1 ms granularity in MS7. Suggesting a different implementation of time.sleep, not just a different tick rate. Closer inspection suggests a 1\u03bcs granularity in Ubuntu by the way, but that is due to the time.time function that I use for measuring the accuracy.\nFrom the :And  w.r.t. :Why don't you find out:For the record, I get around 0.1ms error on my HTPC and 2ms on my laptop, both linux machines.Here's my follow-up to Wilbert's answer: the same for Mac OS X Yosemite, since it's not been mentioned much yet.Looks like a lot of the time it sleeps about 1.25 times the time that you request and sometimes sleeps between 1 and 1.25 times the time you request. It almost never (~twice out of 1000 samples) sleeps significantly more than 1.25 times the time you request.  Also (not shown explicitly) the 1.25 relationship seems to hold pretty well until you get below about 0.2 ms, after which it starts get a little fuzzy. Additionally, the actual time seems to settle to about 5 ms longer than you request after the amount of time requested gets above 20 ms.Again, it appears to be a completely different implementation of  in OS X than in Windows or whichever Linux kernal Wilbert was using.You can't really guarantee anything about sleep(), except that it will at least make a best effort to sleep as long as you told it (signals can kill your sleep before the time is up, and lots more things can make it run long).  For sure the minimum you can get on a standard desktop operating system is going to be around 16ms (timer granularity plus time to context switch), but chances are that the % deviation from the provided argument is going to be significant when you're trying to sleep for 10s of milliseconds.  Signals, other threads holding the GIL, kernel scheduling fun, processor speed stepping, etc. can all play havoc with the duration your thread/process actually sleeps."},
{"body": "Note: I'm using virtualenvwrapper.Before activating the virtual environment:After activating the virtual environment:You probably already have lxml installed on your system, perhaps installed due to a system package. Thus, the first attempt ( without an active virtualenv) doesn't fail, but it also doesn't install it; it really doesn't do anything.In a virtualenv, by default, the system packages are ignored. Therefore, pip thinks that lxml is not installed. Therefore, it tries to install it into your virtual environment.lxml contains C modules that need to be compiled in order to install properly. However, the compilation of those C modules rely on your having some \"development libraries\" already installed as well. These development libraries are C libraries, not Python, and as such pip won't be able to automatically fetch them from the internet and install them for you.Therefore, you will need to install these development libraries on your own, most likely using your package manager. In a Debian system (like Ubuntu), this is...This will install the libxml2 and libxslt development libraries to your local system. If you try again to install lxml, the C module compilation step should work because now these development libraries are on your system.The error message you were receiving was due to the fact that these libraries were missing (the  part of the error message).See also: \nwhen getting:DO:If you have  installed at the system level, and want to migrate it into a  that you didn't create with , you can symlink it into your 's  folder.Outside your , in a python shell:In my case, it's found in . There'll be an lxml folder and egg-info file. Wherever your virtualenv is, go into its  folder (you may need to create ), and symlink both the library folder and egg into it."},
{"body": "How does one check whether a task is running in celery (specifically, I'm using celery-django)?I've read the documentation, and I've googled, but I can't see a call like:My use-case is that I have an external (java) service for transcoding. When I send a document to be transcoded, I want to check if the task that runs that service is running, and if not, to (re)start it.I'm using the current stable versions - 2.4, I believe.Return the task_id (which is given from .delay()) and ask the celery instance afterwards about the state:When asking, get a new AsyncResult using this task_id:Every  object has a  property, which contains it  object. Accordingly, the following line gives the state of a Task :Creating an  object from the task id  the way recommended in the  to obtain the task status when the only thing you have is the task id.However, as of Celery 3.x, there are significant caveats that could bite people if they do not pay attention to them. It really depends on the specific use-case scenario.In order for Celery to record that a task is running, you must set  to . Here is a simple task that tests this:When  is , which is the default, the state show is  even though the task has started. If you set  to , then the state will be .An  with the state  does not mean anything more than that Celery does not know the status of the task. This could be because of any number of reasons.For one thing,  can be constructed with invalid task ids. Such \"tasks\" will be deemed pending by Celery:Ok, so nobody is going to feed  invalid ids to . Fair enough, but it also has for effect that  Again,  this can be a problem. Part of the issue hinges on how Celery is configured to keep the results of tasks, because it depends on the availability of the \"tombstones\" in the results backend. (\"Tombstones\" is the term use in the Celery documentation for the data chunks that record how the task ended.) Using  won't work at all if  is . A more vexing problem is that Celery expires the tombstones by default. The  setting by default is set to 24 hours. So if you launch a task, and record the id in long-term storage, and more 24 hours later, you create an  with it, the status will be . All \"real tasks\" start in the  state. So getting  on a task could mean that the task was requested but never progressed further than this (for whatever reason). Or it could mean the task ran but Celery forgot its state.I prefer to keep track of  than keep track of the . I do keep some task information but it is really secondary to keeping track of the goals. The goals are stored in storage independent from Celery. When a request needs to perform a computation depends on some goal having been achieved, it checks whether the goal has already been achieved, if yes, then it uses this cached goal, otherwise it starts the task that will effect the goal, and sends to the client that made the HTTP request a response that indicates it should wait for a result.You can also create custom states and update it's value duting task execution.\nThis example is from docs:Old question but I recently ran into this problem. If you're trying to get the task_id you can do it like this: Now you know exactly what the task_id is and can now use it to get the AsyncResult:Try: this will provide the Celery Task status. If Celery Task is already is under  state it will throw an Exception: for simple tasks, we can use  and  to do the monitoring. and for complicated tasks, say a task which deals with a lot other modules. We recommend manually record the progress and message on the specific task unit."},
{"body": "I am working on Scrapy 0.20 with Python 2.7. I found PyCharm has a good Python debugger. I want to test my Scrapy spiders using it. Anyone knows how to do that please?Actually I tried to run the spider as a scrip. As a result, I built that scrip. Then, I tried to add my Scrapy project to PyCharm as a model like this:But I don't know what else I have to doThe  command is a python script which means you can start it from inside PyCharm.When you examine the scrapy binary () you will notice that this is actually a python script:This means that a command like \n can also be executed like this: Try to find the scrapy.cmdline package.\nIn my case the location was here: Create a run/debug configuration inside PyCharm with that script as script. Fill the script parameters with the scrapy command and spider. In this case . Like this:\nPut your breakpoints anywhere in your crawling code and it should work\u2122.You just need to do this.Create a Python file on crawler folder on your project. I used main.py.Inside your main.py put this code below.And you need to create a \"Run Configuration\" to run your main.py.Doing this, if you put a breakpoint at your code it will stop there.To add a bit to the accepted answer, after almost an hour I found I had to select the correct Run Configuration from the dropdown list (near the center of the icon toolbar), then click the Debug button in order to get it to work. Hope this helps! I am running scrapy in a virtualenv with Python 3.5.0 and setting the \"script\" parameter to  solved the issue for me.I am also using PyCharm, but I am not using its built-in debugging features.For debugging I am using . I set up a keyboard shortcut to insert  on any line I want the break point to happen.Then I can type  to execute the next statement,  to step in a function, type any object name to see its value, alter execution environment, type  to continue execution...This is very flexible, works in environments other than PyCharm, where you don't control the execution environment.Just type in your virtual environment  and place  on a line where you want the execution to pause."},
{"body": "Following this Django by Example tutotrial here: The tutorial says:though, when I run , I get the error:Is this because I am using sqlite3 and not postgresql?The command:  returns the error:So I added 'todo' to my INSTALLED_APPS in settings.py, and ran  again, resulting in this error: has been replaced by  with Django 1.5, see:\nIt looks like the 'flush' answer will work for some, but not all cases.  I needed not just to flush the values in the database, but to recreate the tables properly.  I'm not using migrations yet (early days) so I really needed to drop all the tables.  On Heroku one can drop all the tables with pg:reset:And although manage.py 'reset' was replaced with 'flush', Django Extensions has a way to still do a complete reset:  Similar to LisaD's answer,  has a great reset_db command that totally drops everything, instead of just truncating the tables like \"flush\" does. You have to specify a router, so it may look like:Merely flushing the tables wasn't fixing a persistent error that occurred when I was deleting objects. Doing a reset_db fixed the problem.If you want to clean the whole database, you can use:\n\nIf you want to clean database table of a Django app, you can use:\nFor me this solved the problem.Just a follow up to  answer.\nAs of 2016 (), you need to type:This will give you a fresh new database within Heroku."},
{"body": "I want to redirect the print to a .txt file using python. I have a 'for' loop, which will 'print' the output for each of my .bam file; while I want to redirect ALL these output to one file. So I tried to putat the beginning of my script. However I get nothing in the .txt file.\nMy script is like below:So what's the problem? Any other way besides this sys.stdout?\nthxedit:\nand i need my result look like:The most obvious way to do this would be to print to a file object:However, redirecting stdout also works for me.  It is probably fine for a one-off script such as this:What is the first filename in your script?  I don't see it initialized.My first guess is that glob doesn't find any bamfiles, and therefore the for loop doesn't run.  Check that the folder exists, and print out bamfiles in your script.Also, use  to manipulate paths and filenames.You can redirect print with the  operator.In most cases, you're better off just writing to the file normally.or, if you have several items you want to write with spaces between, like :Since  normally contains  method, all you need to do is to pass a  into its argument.This works perfectly:Now the hello will be written to the test.txt file. Make sure to close the  with a , without it the content will not be save in the fileThe easiest solution isn't through python; its through the shell.  From the first line of your file () I'm guessing you're on a UNIX system.  Just use  statements like you normally would, and don't open the file at all in your script.  When you go to run the file, instead ofto run the file, usewhere you replace  with the name of the file you want the output to go in to.  The  token tells (most) shells to set stdout to the file described by the following token.One important thing that needs to be mentioned here is that \"script.py\" needs to be made executable for  to run.So before running ,execute this command\n(make the script executable for all users)You may not like this answer, but I think it's the RIGHT one. Don't change your stdout destination unless it's absolutely necessary (maybe you're using a library that only outputs to stdout??? clearly not the case here).I think as a good habit you should prepare your data ahead of time as a string, then open your file and write the whole thing at once. This is because input/output operations are the longer you have a file handle open, the more likely an error is to occur with this file (file lock error, i/o error, etc). Just doing it all in one operation leaves no question for when it might have gone wrong.Here's an example:And then when you're all done collecting your \"data lines\" one line per list item, you can join them with some  characters to make the whole thing outputtable; maybe even wrap your output statement in a  block, for additional safety (will automatically close your output handle even if something goes wrong):However if you have lots of data to write, you  write it one piece at a time. I don't think it's relevant to your application but here's the alternative:Changing the value of sys.stdout does change the destination of all calls to print. If you use an alternative way to change the destination of print, you will get the same result.Your bug is somewhere else:"},
{"body": "The ElementTree.parse reads from a file, how can I use this if I already have the XML data in a string?Maybe I am missing something here, but there must be a way to use the ElementTree without writing out the string to a file and reading it again.If you're using  to parse from a file, then you can use  to parse from text.See You can parse the text as a string, which creates an Element, and create an ElementTree using that Element.I just came across this issue and the documentation, while complete, is not very straightforward on the difference in usage between the parse and fromstring methods.You need the xml.etree.ElementTree.fromstring(text)It's on the page you linked. Use fromstring."},
{"body": "I have a CSV file and I want to bulk-import this file into my sqlite3 database using Python. the command is \".import .....\". but it seems that it cannot work like this. Can anyone give me an example of how to do it in sqlite3? I am using windows just in case.\nThanksCreating an sqlite connection to a file on disk is left as an exercise for the reader ... but there is now a two-liner made possible by the pandas libraryThe  command is a feature of the sqlite3 command-line tool. To do it in Python, you should simply load the data using whatever facilities Python has, such as the , and inserting the data as per usual.This way, you also have control over what types are inserted, rather than relying on sqlite3's seemingly undocumented behaviour.Many thanks for bernie's !  Had to tweak it a bit - here's what worked for me:My text file (PC.txt) looks like this:My 2 cents (more generic):You can do this using  &  efficientlyOdo will store the csv file to  (sqlite database) under the schema Or you use  directly, without . Either ways is fine. Read this "},
{"body": "any tips on testing email sending? Other than maybe creating a gmail account, especially for receiving those emails?I would like to maybe store the emails locally, within a folder as they are sent.You can use a  which is a very handy solution for development and testing; emails are not sent but stored in a folder you can specify!Django test framework has some built in helpers to aid you with testing .Example from docs (short version):For any project that doesn't require sending attachments, I use , which has the benefit of all outbound emails ending up in a queue until I trigger their sending, and even after they've been sent, they are then logged - all of which is visible in the Admin, making it easy to quickly check what you emailing code is trying to fire off into the intertubes.Django also has an in-memory email backend. More details in the docs under . This is present in Django 1.6 not sure if it's present in anything earlier.If you are into unit-testing the best solution is to use the  provided by django.Take the case of use it as a  fixtureIn each test, the  is reset with the server, so there are no side effects between tests.Patching SMTPLib for testing purposes can help test sending mails without sending them.Using the file backend works well, but I find it a little a cumbersome to poke around the filesystem to look at emails.  You could use mailcatcher, , to capture emails and display them in a web UI.To use mailcatcher with Django you'll need to add something like the following to your settings.py:Use Also, it has a component called , the , which enables you to test sending emails with various problems happening:Read more about it .(Unlike original mailcatcher, which  and it WASN'T really fixed in the current release, MailHog just works.)Why not start your own really simple SMTP Server by inherit from  and :process_message is called whenever your SMTP Server receive a mail request, you can do whatever you want there.In the testing code, do something like this:Just remember to  the  by calling  to end the asyncore loop(stop the server from listening).Use Maildump.However it requires Python 2.I prefer real email sending via  service:My solution is write content to a html file. This way can help you see how email look like. I leave it here .Other tip: You can use  which can help you edit your email template with zero inline css."},
{"body": "Let's pretend I have the following QueryDict: I'd like to have a dictionary out of this, eg:(I don't care if the unicode symbol  stays or goes.)If I do , as suggested by the , I lose the extra values belonging to , eg:I was thinking of doing this:Is there a better way? This should work: New in Django >= 1.4.This is what I've ended up using:From my usage this seems to get you a list you can send back to e.g. a form constructor. maybe this isn't the best method. It seems if you want to e.g. write  to a file for whatever crazy reason,  is the way to go. To reconstruct the  you simply do .I ran into a similar problem, wanting to save arbitrary values from a form as serialized values.My answer avoids explicitly iterating the dictionary contents: In order to retrieve a dictionary-like value that functions as the original, an inverse function uses  to populate a new  value.  In this case, I don't think the explicit iteration is avoidable.My helper functions look like this:If you do not want the values as Arrays you can do the following:zip is a powerful tool read more about it here Please Note : underscore  in iterlists method of . Django version :1.5.1 returns a weird python dictionary with array wrapped values.where as  returns expected output."},
{"body": "What is the simplest way to get monitor resolution (preferably in a tuple)? On Windows:Based on this In Windows,\nyou can also use ctypes:so that you don't need to install the pywin32 package;\nit doesn't need anything that doesn't come with Python itself.If you're using wxWindows, you can simply do:FYI I created a PyPI module for this reason:The code:Result:. Its goal is to be cross platform; for now it supports Cygwin and X11 but pull requests are totally welcome.Taken directly from an answer to this post: And for completeness, Mac OS Xwill give you a list of tuples containing all screen sizes (if multiple monitors present)Here is a quick little Python program that will display the information about your multi-monitor setup:Here's an example of its output:On Windows 8.1 I am not getting the correct resolution from either ctypes or tk.\nOther people are having this same problem for ctypes: \nTo get the correct full resolution of a high DPI monitor on windows 8.1, one must call SetProcessDPIAware and use the following code:Full Details Below:I found out that this is because windows is reporting a scaled resolution. It appears that python is by default a 'system dpi aware' application. Types of DPI aware applications are listed here:\nBasically, rather than displaying content the full monitor resolution, which would make fonts tiny, the content is scaled up until the fonts are big enough.On my monitor I get:\nPhysical resolution: 2560 x 1440 (220 DPI)\nReported python resolution: 1555 x 875 (158 DPI)Per this windows site: \nThe formula for reported system effective resolution is:\n(reported_px*current_dpi)/(96 dpi) = physical_pxI'm able to get the correct full screen resolution, and current DPI with the below code.\nNote that I call SetProcessDPIAware() to allow the program to see the real resolution.Which returned:I am running windows 8.1 with a 220 DPI capable monitor.\nMy display scaling sets my current DPI to 158.I'll use the 158 to make sure my matplotlib plots are the right size with:\nfrom pylab import rcParams\nrcParams['figure.dpi'] = curr_dpiIf you are using the  toolkit specifically , you can do the following:Using Linux, the simplest way is to execute bash commandand parse its output using regexp. Also you can do it through PyGame: I am using a get_screen_resolution method in one of my projects like the one below, which is basically an import chain. You can modify this according to Your needs by removing those parts that are not needed and move more likely ports upwards in the chain.XWindows version:Try the following code:Try the following code:Using Linux\nInstead of regexp take the first line and take out the current resolution values.Current resolution of display :0 This was done on xrandr 1.3.5, I don't know if the output is different on other versions, but this should make it easy to figure out.To get bits per pixel:parameters in gdi32:Old question but this is missing. \nI'm new to python so please tell me if this is a \"bad\" solution.\nThis solution is supported for windows and mac only and it works just for the main screen - but the os is not mentioned in the question. Measure the size by taking a screenshot. As the screensize should not change this has to be done only once.\nThere are more elegant solutions if you have a gui toolkit like gtk, wx, ... installed. see Try pyautogui:    "},
{"body": "Following the documentation for setting up Sphinx documentation links between packages, I have added to my , but can't seem to get links to any project other than Python itself to work. For examplejust takes me to the index page, without adding the expected  anchor, and I can't even locate the glossary for  or figure out how to determine what s or s are supported by a package.Where can I find instructions on how to specify targets for s and s in , , and ?For that matter, how do I link to Sphinx itself? Addinganddoesn't work.I have a  with a handful of  mappings, which now includes all of ,  and .  You should be able to use these entries directly in , within your . If anyone has suggestions for further entries to be added to this list, please feel free to post requests into the comments of the Gist.For all of these packages, per  I highly recommend using  to decode and inspect the syntax of the  file for each library.  (Full disclosure: I am the author of .) Each line of the (decoded)  gives you all the information you need to construct a working  reference; see the  documentation . Sometimes you need a fully-qualified name, e.g.:Other times (for C functions, for example) you can just reference the function's base name BUT you have to explicitly indicate the domain, e.g.:Yet other times you may have to reference the custom  domain, e.g.:There's really no way to know what the right syntax is without consulting the .\u00a0 Things are further complicated by the introduction of numerous custom domains for the various  subpackages, e.g.:\u00a0 it appears you always have to provide the (quite verbose) fully-specified object name in the reference, e.g.:All of the  code objects seem to reside in the default  domain, however, which simplifies things somewhat.\u00a0 if you're having trouble getting a link to construct properly, the first thing I fall back to is using the generic  role, e.g.:This will construct an  link regardless of the role in which a particular object was defined, though I think you still have to correctly specify any relevant non-default domain. If the reference doesn't work properly with an  role, then there's an error in the object name or the domain somewhere. Check for typos in both places.It is possible to manually specify which inventory to look. For example, if \ndoes not work, you can always download the inventory and manually append it to the mapping (e.g. download from , save the binary file in your docs and append the path to it in the mapping; this will give something like:To verify if a reference exists within the inventory, you can explore the binary with the  python package and check where is the reference you want.This may not be a solution to your problem but can help to debug some things."},
{"body": "My goal is to distribute a Python package that has several other widely used Python packages as dependencies.  My package depends on well written, Pypi-indexed packages like pandas, scipy and numpy, and specifies in the setup.py that certain versions or higher of these are needed, e.g. \"numpy >= 1.5\".I found that it's immensely frustrating and nearly impossible for Unix savvy users who are  experts in Python packaging (even if they know how to write Python) to install a package like mine, even when using what are supposed to be easy to use package managers.  I am wondering if there is an alternative to this painful process that someone can offer, or if my experience just reflects the very difficult current state of Python packaging and distribution.Suppose users download your package onto their system.  Most will try to install it \"naively\", using something like:Since if you google instructions on installing Python packages, this is usually what comes up.  This will fail for the vast majority of users, since most do not have root access on their Unix/Linux servers.  With more searching, they will discover the \"--prefix\" option and try:Since the users are not aware of the intricacies of Python packaging, they will pick an arbitrary directory as an argument to , e.g. .  It will not be a cleanly curated directory where all other Python packages reside, because again, most users are not aware of these details.  If they install another package \"myotherpackage\", they might pass it , and you can imagine how down the road this will lead to frustrating hacking of  and other complications.Continuing with the installation process, the call to  with  will also fail once users try to use the package, even though it appeared to have been installed correctly, since one of the dependencies might be missing (e.g. pandas, scipy or numpy) and a package manager is not used.  They will try to install these packages individually.  Even if successful, the packages will inevitably not be in the  due to the non-standard directories given to  and patient users will dabble with modifications of their  to get the dependencies to be visible.At this stage, users might be told by a Python savvy friend that they should use a package manager like , the mainstream manager, to install the software and have dependencies taken care of.  After installing , which might be difficult, they will try:This too will fail, since users again do not typically have permission to install software globally on production Unix servers.  With more reading, they will learn about the  option, and try:They will get the error:They will be extremely puzzled why their  does not have the  option where there are clearly pages online describing the option.  They might try to upgrade their  to the latest version and find that it still fails.If they continue and consult a Python packaging expert, they will discover that there are  of , both named \" so as to maximize confusion, but one part of \"distribute\" and the other part of \"setuptools\".  It happens to be that only the  of  supports  and the vast majority of servers/sys admins install 's  and so local installation will not be possible.  Keep in mind that these distinctions between  and  are meaningless and hard to understand for people who are not experts in Python package management.At this point, I would have lost 90% of even the most determined, savvy and patient users who try to install my software package -- and rightfully so!  They wanted to install a piece of software that happened to be written in Python, not to become experts in state of the art Python package distribution, and this is far too confusing and complex.  They will give up and be frustrated at the time wasted.The tiny minority of users who continue on and ask more Python experts will be told that they ought to use  instead of .  Installing  and  and figuring out how these tools work and how they are different from the conventional  or  calls is in itself time consuming and difficult, and again too much to ask from users who just wanted to install a simple piece of Python software and use it. Even those who pursue this path will be confused as to whether whatever dependencies they installed with  or  are still usable with  or if everything needs to be reinstalled from scratch.This problem is exacerbated if one or more of the packages in question depends on installing a different version of Python than the one that is the default.  The difficulty of ensuring that your Python package manger is using the Python version you want it to, and that the required dependencies are installed in the relevant Python 2.x directory and not Python 2.y, will be so endlessly frustrating to users that they will certainly give up at that stage.Is there a simpler way to install Python software that doesn't require users to delve into all of these technical details of Python packages, paths and locations?  For example, I am not a big Java user, but I do use some Java tools occasionally, and don't recall ever having to worry about X and Y dependencies of the Java software I was installing, and I have no clue how Java package managing works (and I'm happy that I don't -- I just wanted to use a tool that happened to be written in Java.)  My recollection is that if you download a Jar, you just get it and it tends to work.  Is there an equivalent for Python?  A way to distribute software in a way that doesn't depend on users having to chase down all these dependencies and versions?  A way to perhaps compile all the relevant packages into something self-contained that can just be downloaded and used as a binary?I would like to emphasize that this frustration happens even with the narrow goal of distributing a package to savvy Unix users, which makes the problem simpler by not worrying about cross platform issues, etc.  I assume that the users are Unix savvy, and might even know Python, but just aren't aware (and don't want to be made aware) about the ins and outs of Python packaging and the myriad of internal complications/rivalries of different package managers.  A disturbing feature of this issue is that it happens even when all of your Python package dependencies are well-known, well-written and well-maintained Pypi-available packages like Pandas, Scipy and Numpy.  It's not like I was relying on some obscure dependencies that are not properly formed packages: rather, I was using the most mainstream packages that many might rely on.Any help or advice on this will be greatly appreciated.  I think Python is a great language with great libraries, but I find it virtually impossible to distribute the software I write in it (once it has dependencies) in a way that is easy for people to install locally and just run.  I would like to clarify that the software I'm writing is not a Python library for programmatic use, but software that has executable scripts that users run as individual programs. Thanks.We also develop software projects that depend on numpy, scipy and other PyPI packages. Hands down, the best tool currently available out there for managing remote installations is . It is  easy to use. You download a bootstrapping script from their website and distribute that with your package. You write a \"local deployment\" file, called normally , that explains how to install the package locally. You ship both the  file and  with your package - we use the  file in our python packages to force the embedding of these two files with the zip or tar balls distributed by PyPI. When the user unpackages it, it should execute two commands:The package is compiled and all dependencies are installed , which means that the user installing your package doesn't even need root privileges, which is an added feature. The scripts are (normally) placed under , so the user can just execute them after that.  uses  for interaction with PyPI so everything you expect works out of the box.You can extend  quite easily if all that power is not enough - you create the so-called \"recipes\" that can help the user to create extra configuration files, download other stuff from the net or instantiate custom programs.  website contains a video tutorial that explains in details how to use buildout and how to extend it. Our project  makes extensive use of buildout for distributing packages for scientific usage. If you would like, please  that contains detailed instructions for our developers on how they can setup their python packages so other people can build and install them locally using .We're currently working to make it easier for users to get started installing Python software in a platform independent manner (in particular see  and )For right now, the problem with two competing versions of easy_install has been resolved, with the competing fork \"distribute\" being merged backing into the setuptools main line of development.The best currently available advice on cross-platform distribution and installation of Python software is captured here: "},
{"body": "I'm just wondering what the differences and advantages are for the different CGI's out there. Which one would be best for python scripts, and how would I tell the script what to use?A part answer to your question, including scgi.Lazy and not writing it on my own. From the wikipedia: There's also a good background reader  on CGI, WSGI and other options, in the form of an official python HOWTO: In a project like , you can use a WSGI () server from the .A WSGI server wraps a back-end process using one or more protocol:"},
{"body": "I am trying to use imshow in matplotlib to plot data as a heatmap, but some of the values are NaNs. I'd like the NaNs to be rendered as a special color not found in the colormap.example:The resultant image is unexpectedly all blue (the lowest color in the jet colormap).  However, if I do the plotting like this:--then I get something better, but the NaN values are drawn the same color as vmin... Is there a graceful way that I can set NaNs to be drawn with a special color (eg: gray or transparent)?Hrm, it appears I can use a masked array to do this:This should suffice, though I'm still open to suggestions. :]It did not work for me. I was getting error message, so did workaround:"},
{"body": "I'm looking over the code for Python's  module, and it contains this line:instead ofthe subtle difference being the period before . What does that mean? Why the period?That's the new syntax for explicit . It means import from the current package.The dot in the module name is used for relative module import (see  and ,  section 6.4.2). You can use more than one dot, referring not to the curent package but its parent(s). This should only be used within packages, in the main module one should always use absolute module names."},
{"body": "I've troubles setting VIM (7.1.xxx) for editing python files. \nIndenting seems to be broken (optimal 4 spaces).\nI've followed some tutorials I found via Google. Still no effect :/ \nPlease help.I use this on my macbook:(edited to only show stuff related to indent / tabs)I use:But but I'm going to try Daren's entriesI use the vimrc in the python repo among other things:I also addEnsure you are editing the correct configuration file for VIM. Especially if you are using windows, where the file could be named _vimrc instead of .vimrc as on other platforms.In vim typeand check your path to the _vimrc/.vimrc file withMake sure you are only using one file. If you want to split your configuration into smaller chunks you can source other files from inside your _vimrc file.for more advanced python editing consider installing the  vim plugin. it allows you do advanced code folding using regular expressions. i use it to fold my class and method definitions for faster editing.A simpler option: just uncomment the following part of the configuration (which is originally commented out) in the /etc/vim/vimrc file:"},
{"body": "Either interactively, such as from within an Ipython session, or from within a script, how can you determine which backend is being used by matplotlib?Use the  function to obtain a string denoting which backend is in use:Another way to determine the current backend is to read  dictionary:"},
{"body": "I've googled around for this, but I still have trouble relating to what Django defines as \"apps\". Should I create a new app for each piece of functionality in a site, even though it uses models from the main project? Do you guys have good rule of thumb of when to split off a new app, and when to keep functionality together with the \"main project\" or other apps?James Bennett has a wonderful  on how to organize reusable apps in Django.I prefer to think of Django applications as reusable modules or components than as \"applications\". This helps me encapsulate and decouple certain features from one another, improving re-usability should I decide to share a particular \"app\" with the community at large, and maintainability.My general approach is to bucket up specific features or feature sets into \"apps\" as though I were going to release them publicly. The hard part here is figuring out how big each bucket is. A good trick I use is to imagine how my apps would be used if they were released publicly. This often encourages me to shrink the buckets and more clearly define its \"purpose\".I tend to create new applications for each logically separate set of models. e.g.:Here is the updated presentation on 6 September 2008.The rule I follow is it should be a new app if I want to reuse the functionality in a different project.If it needs deep understanding of the models in your project, it's probably more cohesive to stick it with the models.An 'app' could be many different things, it all really comes down to taste. For example, let's say you are building a blog. Your app could be the entire blog, or you could have an 'admin' app, a 'site' app for all of the public views, an 'rss' app, a 'services' app so developers can interface with the blog in their own ways, etc.I personally would make the blog itself the app, and break out the functionality within it. The blog could then be reused rather easily in other websites.The nice thing about Django is that it will recognize any models.py file within any level of your directory tree as a file containing Django models. So breaking your functionality out into smaller 'sub apps' within an 'app' itself won't make anything more difficult."},
{"body": "I'm working on a webapp where users can login to see their online wine cellar.I've got the Django REST models setup, as well as the front-end design in Angular but I'm having trouble putting the pieces together and my main issue is for user authentication.I've read many posts on here and various tutorials but I can't seem to find a step by step method to implement authentication:From what I understand Angular makes a POST request on a url where DRF verifies that username and password match and returns a token or other auth proof.I feel like I'm close but I need a more general view of how this works to put the pieces together.Thanks in advanceI imagine there are a lot of ways to do this, let me explain what I do, hopefully it is helpful. This is going to be a long post. I would love to hear how others do this, or better ways of implementing the same approach. You can also check out my seed project on Github, .I use token authentication with Witold Szczerba's . The beauty of his approach is that whenever a request is sent from your site without proper credentials, you are redirected to the login screen, but your request is queued to be re-fired on login complete.Here is a login directive used with the login form. It posts to Django's auth token endpoint, sets a cookie with the response token, sets the default header with the token so all requests will be authenticated, and fires the http-auth-interceptor login event.})I use the module .run method to set check for the cookie when a user comes to the site, if they have the cookie set I set the default authorization.Here is my interceptor directive that handles the authService broadcasts. If login is required, I hide everything and show the login form. Otherwise hide the login form and show everything else.To use it all my html was basically like this.Check out django-rest-auth and angular-django-registration-auth alsoWe've tried to solve most common Django auth/registration related things from a backend and angular perspective in these two libraries.Thanks!"},
{"body": "I have a bunch of classes I want to rename.  Some of them have names that are small and that name is reused in other class names, where I don't want that name changed.  Most of this lives in Python code, but we also have some XML code that references class names.Simple search and replace only gets me so far.  In my case, I want to rename AdminAction to AdminActionPlug and AdminActionLogger to AdminActionLoggerPlug, so the first one's search-and-replace would also hit the second, wrongly.Does anyone have experience with Python refactoring tools ? Bonus points if they can fix class names in the XML documents too.In the meantime, I've tried it two tools that have some sort of integration with vim.The first is , a python refactoring library that comes with a Vim (and emacs) plug-in.  I tried it for a few renames, and that definitely worked as expected.  It allowed me to preview the refactoring as a diff, which is nice.  It is a bit text-driven, but that's alright for me, just takes longer to learn.The second is  which I guess wins points on name.  Also plugs into vim and emacs.  Haven't played much with it yet, but I remember trying it a long time ago.Haven't played with both enough yet, or tried more types of refactoring, but I will do some more hacking with them.I would strongly recommend  - not just for refactorings. Since the first PyCharm answer was posted here a few years ago the refactoring support in PyCharm has improved significantly. (last checked 2016/07/27 in PyCharm 2016.2)XML refactorings (I checked in context menu in an XML file):Javascript refactorings:Your IDE can support refactorings !!\nCheck it Eric, Eclipse, WingIDE have build in tools for refactorings (Rename including). And that are very safe refactorings - if something can go wrong IDE wont do ref.Also consider adding few unit test to ensure your code did not suffer during refactorings.WingIDE 4.0 (WingIDE is my python IDE of choice) will support a few refactorings, but I just tried out the latest beta, beta6, and... there's still work to be done.  Retract Method works nicely, but Rename Symbol does not.Update: The 4.0 release has fixed all of the refactoring tools.  They work great now. have some refactoring features.You can use sed to perform this. The trick is to recall that regular expressions can recognize word boundaries. This works on all platforms provided you get the tools, which on Windows is Cygwin, Mac OS may require installing the dev tools, I'm not sure, and Linux has this out of the box. So grep, xargs, and sed should do the trick, after 12 hours of reading man pages and trial and error ;)"},
{"body": "I'm trying to pass a list of arguments to a python script using the argh library.  Something that can take inputs like these:My internal code looks like this:And here's how it behaves:The problem seems pretty straightforward: argh is only accepting the first argument, and treating it as a string.  How do I make it \"expect\" a list of integers instead?I see , but what about the (not-deprecated) argparse?  Or using argh's much nicer decorated syntax?  These seem much more pythonic.With , you just use Example output:Edit: I'm not familiar with , but it seems to be just a wrapper around  and this worked for me:Example output:I order to have access to each parameter value, the following code may be helpful."},
{"body": "I didn't really pay as much attention to Python 3's development as I would have liked, and only just noticed some interesting new syntax changes. Specifically from  function parameter annotation:Not knowing anything about this, I thought it could maybe be used for implementing static typing in Python!After some searching, there seemed to be a lot discussion regarding (entirely optional) static typing in Python, such as that mentioned in , and  (and )..but, I'm not clear how far this has progressed. Are there any implementations of static typing, using the parameter-annotation? Did any of the parameterised-type ideas make it into Python 3?Thanks for reading my code!Indeed, it's not hard to create a generic annotation enforcer in Python. Here's my take:Given this simplicity, it's strange at the first sight that this thing is not mainstream. However, I believe there are good reasons why it's . Generally, type checking helps because if you add integer and dictionary, chances are you made some obvious mistake (and if you meant something reasonable, it's still ).But in real life you often mix quantities of the same  as seen by compiler but clearly different , for example the following snippet contains an obvious mistake:Any human should immediately see a mistake in the above line provided it knows the 'human type' of variables  and  even though it looks to computer as  multiplication of  and .There's more that can be said about possible solutions to this problem, but enforcing 'computer types' is apparently a half-solution, so, at least in my opinion, it's . It's the same reason why  is a terrible idea while  is a great one. There's more at the very informative .Now if somebody was to implement some kind of Pythonic third-party library that would automatically assign to real-world data its  and then took care to transform that type like  and enforce that check with function annotations, I think that would be a type checking people could really use!As mentioned in that PEP, static type checking is one of the possible applications that function annotations can be used for, but they're leaving it up to third-party libraries to decide how to do it. That is, there isn't going to be an official implementation in core python.As far as third-party implementations are concerned, there are some snippets (such as ), which seem to do the job pretty well.EDIT:As a note, I want to mention that checking behavior is preferable to checking type, therefore I think static typechecking is not so great an idea. My answer above is aimed at answering the question, not because I would do typechecking myself in such a way.\"Static typing\" in Python can only be implemented so that the type checking is done in run-time, which means it slows down the application. Therefore you don't want that as a generality. Instead you want some of your methods to check it's inputs. This can be easily done with plain asserts, or with decorators if you (mistakenly) think you need it a lot.There is also an alternative to static type checking, and that is to use an aspect oriented component architecture like The Zope Component Architecture. Instead of checking the type, you adapt it. So instead of:you do this:If theobject already implements IMyClass nothing happens. If it doesn't, an adapter that wraps whatever theobject is to IMyClass will be looked up, and used instead of theobject. If no adapter is found, you get an error.This combined the dynamicism of Python with the desire to have a specific type in a specific way.This is not an answer to question directly, but I found out a Python fork that adds static typing: , of course one can't rely on it as it's still small endeavor, but interesting.Sure, static typing seems a bit \"unpythonic\" and I don't use it all the time. But there are cases (e.g. nested classes, as in domain specific language parsing) where it can really speed up your development. Then I prefer using  explained in this *. It comes with a git repo, tests and an explanation what it can and what it can't do ... and I like the name ;) * Please don't pay attention to Cecil's rant about why Python doesn't come with batteries included in this case."},
{"body": "I am looking for what type of code would I put in  files and what are the best practices related to this. Or, is it a bad practice in general ?Any reference to known documents that explain this is also very much appreciated.Libraries and frameworks usually use initialization code in  files to neatly hide internal structure and provide a uniform interface to the user. Let's take the example of Django forms module. Various functions and classes in forms module are defined in different files based on their classification.Now if you were to create a form, you would have to know in which file each function is defined and your code to create a contact form will have to look something like this (which is incovenient and ugly).Instead, in Django you can just refer to various widgets, forms, fields etc. directly from the forms namespace.How is this possible? To make this possible, Django adds the following statement to  file which import all the widgets, forms, fields etc. into the  namespace.As you can see, this simplifies your life when creating the forms because now you don't have to worry about in where each function/class is defined and just use all of these directly from  namespace. This is just one example but you can see examples like these in other frameworks and libraries.One of the best practices in that area is to import all needed classes from your library (look at , for example). So, a user of your library can do this:instead ofAlso, good practice is include in  version constant"},
{"body": "I've started working on a rather big (multithreaded) Python project, with loads of (unit)tests. The most important problem there is that running the application requires a preset environment, which is implemented by a context manager. So far we made use of a patched version of the unit test runner that would run the tests inside this manager, but that doesn't allow switching context between different test modules.Both nose and pytest do support such a thing because they support fixtures at many granularities, so we're looking into switching to nose or pytest. Both these libraries would also support 'tagging' tests and run only these tagged subsets, which is something we also would like to do.I have been looking through the documentation of both nose and pytest a bit, and as far as I can see the bigger part of those libraries essentially support the same functionality, except that it may be named differently, or require slightly different syntax. Also, I noted some small differences in the available plugins (nose has multiprocess-support, pytest doesn't seem to for instance)So it seems, the devil is in the detail, which means (often at least) in personal taste and we better go with the library that fits our personal taste best.So I'd to ask for a subjective argumentation why I should be going with nose or pytest in order to choose the library/community combo that best fits our needs.I used to use Nose because it was the default with Pylons.  I didn't like it at all.  It had configuration tendrils in multiple places, virtually everything seemed to be done with an underdocumented plugin which made it all even more indirect and confusing, and because it did unittest tests by default, it regularly broke with Unicode tracebacks, hiding the sources of errors.I've been pretty happy with py.test the last couple years.  Being able to just write a test with  out of the box makes me hate writing tests  less, and hacking whatever I need atop the core has been pretty easy.  Rather than a fixed plugin interface it just has piles of hooks, and pretty understandable source code should you need to dig further.  I even wrote an adapter for running Testify tests under py.test, and had more trouble with Testify than with py.test.That said, I hear nose has plugins for classless tests and assert introspection nowadays, so you'll probably do fine with either.  I still feel like I can hit the ground running with py.test, though, and I can understand what's going on when it breaks."},
{"body": "In reading \"High performance MySQL\" from O'Reilly I've stumbled upon the followingI'm a bit confused, because I used to put \"SET NAMES utf8\" on the top of every script to let the db know that my queries are utf8 encoded.Can anyone comment the above quote, or, to put it more formally, what are your suggestions / best practices to ensure that my database workflow is unicode-aware.My target languages are php and python if this is relevant. would be an option - but an option limited to the . For  it is  and for  you need to specify a connection parameter.As using this function results in a MySQL API call, it should be considered much faster than issuing a query.In respect of performance the fastest way to ensure a UTF-8-based communiction between your script and the MySQL server is setting up the MySQL server correctly. As  is  to whereas  internally also executes  you can also set  statically in your .Please be aware of possible problems with other applications running on the same MySQL server instance and requiring some other character set.This answer has an emphasis on php's pdo library because it's so ubiquitous.A brief reminder - mysql is a client-server architecture. This is significant because there's not only the mysql server where the actual database is, but there's also the seperate mysql client driver, which is the thing that talks to the mysql server(they're separate entities). You could kinda sorta say the mysql client and pdo are mixed together.When you use , you issue a standard sql query to mysql. While the sql query does pass through pdo, and then through the mysql client library, and then finally it reaches the mysql server, ONLY the mysql server parses and interprets that sql query. This is significant because the mysql server doesn't send any message back to pdo or the mysql client letting it know the character set and encoding has changed, and so pdo is totally ignorant to fact that it happened.It's important not to do this because the client library cannot properly handle strings if it isn't aware of the current character set. Most common operations will work correctly without the client knowing the correct character set, but one that won't is string escaping, such as . You may think you don't need to worry about such manual primitive string escaping because you use prepared statements, but the truth is the vast majority of pdo:mysql users unknowingly use  because it's been the default setting for the pdo:mysql driver for a very long time now. An emulated prepared statement doesn't use real native mysql prepared statements as provided by the mysql api; instead, php does the equivalent of calling  on all your values, and str_replacing'ing all your placeholders with the quoted values for you.Since you can't properly escape a string unless you know the character set you're using, these emulated prepared statements are vulnerable to sql injection if you've changed to certain character sets via set names. Regardless of the possibility of sql injection, you can still break your strings if you use an escaping scheme intended for a different character set.For the pdo mysql driver, you can specify the character set when you connect, by . The client library will be aware of the character set if you do this.But improper string escaping isn't the only problem. For example, you can also have problems with using  because column names are specified as strings, and so again the encoding matters. An example could be a column name named (note the umlaut), and you switch from  to  via set names, and then you try to  with  being a utf8 encoded string because your php file is utf8 encoded. It won't work, you would need to encode the string as a latin1 variant... and now you have all kinds of crazy going on.Not sure about py, but php has  now, which states that this is the \"preferred way to change the charset [and] using mysql_query() to execute SET NAMES is not recommended.\" Note, that this function was introduced for MySQL 5.0.7, so it won't work with earlier versions.Where $link is a connection created with "},
{"body": "I have been a Python programmer for almost two years, and I am used to writing small scripts to automate some repetitive tasks I had to do at the office. Now, apparently my colleagues noticed this, and they want those scripts too.Some of them have Macs, some Windows; I made these on windows. I investigated the possibility of using py2exe or even py2app to make natives of my script, but they never satisfied me...I came to know that all of them have JVM on their systems, so can I give them one single executable JAR file of my script using something like Jython may be?How feasible is this... I mean, I had no idea how to write scripts for Jython, neither did I care about it when I wrote them... what kind of problems will it give?The best current techniques for distributing your Python files in a jar are detailed in this article on Jython's wiki: For your case, I think you would want to take the jython.jar file that you get when you install Jython and zip the Jython Lib directory into it, then zip your .py files in, and then add a  file with your startup logic (this file is treated specially by Jython and will be the file executed when you call the jar with \"java -jar\").This process is definitely more complicated then in ought to be, and so we (the Jython developers) need to come up with a nice tool that will automate these tasks, but for now these are the best methods.  Below I'm copying the recipe at the bottom of the above article (modified slightly to fit your problem description) to give you a sense of the solution.Create the basic jar:Add other modules to the jar:Add the  module:On MS Windows, that last line, setting the CLASSPATH environment variable, would look something like this:Or, again on MS Windows, use the Control Panel and the System properties to set the CLASSPATH environment variable.Run the application:Or, if you have added your start-up script to the jar, use one of the following:The double -jar is kind of annoying, so if you want to avoid that and get the more pleasing:You'll have to do a bit more work until we get something like this into a future Jython [Update: JarRunner is part of Jython 2.5.1].  Here is some Java code that looks for the  automatically, and runs it.  Note that this is my first try at this class.  Let me know if it needs improvement!I put this code into the org.python.util package, since that's where it would go if we decide to include it in a future Jython.  To compile it, you'll need to put jython.jar (or your myapp.jar) into the classpath like:Then you'll need to add JarRunner.class to your jar (the class file will need to be in org/python/util/JarRunner.class) calling jar on the \"org\" directory will get the whole path into your jar.Add this to a file that you will use to update the manifest, a good name is manifest.txt:Then update the jar's manifest:Now you should be able to run your app like this:I experienced a similar issue in that I want to be able to create simple command line calls for my jython apps, not require that the user go through the jython installation process, and be able to have the jython scripts append library dependencies at runtime to sys.path so as to include core java code.When running the 'jython' launcher explicitly on the command line, on Unix systems, it just runs a big shell script to properly form a java command line call.  This jython launcher seems to have a dependency on reaching back to a core install of jython, and by some way of magic allows the proper handling of .jar files being added to the sys.path at runtime from within my .py scripts.  You can see what the call is and block execution by the following:But it's still just firing up a JVM and running a class file.  So my goal was to be able to make this java call to a standalone jython.jar present in my distribution's lib directory so users would not need to do any additional installation steps to start using my .py scripted utilities.Trouble is that the behavior is enough different that I would get responses like this:Now you might say that I should just add the jar files to the -classpath, which in fact I tried, but I would get the same result.The suggestion of bundling all of your .class files in a jython.jar did not sound appealing to me at all.  It would be a mess and would bind the Java/Python hybrid application too tightly to the jython distribution.  So that idea was not going to fly.  Finally, after lots of searching, I ran across bug #1776 at jython.org, which has been listed as critical for a year and a half, but I don't see that the latest updates to jython incorporate a fix.  Still, if you're having problems with having jython include your separate jar files, you should read this.In there, you will find the temporary workaround for this.  In my case, I took the Apache POI jar file and unjar'ed it into its own separate lib directory and then modified the sys.path entry to point to the directory instead of the jar:Now, when I run jython by way of java, referencing my local jython.jar, the utility runs just peachy.  Now I can create simple scripts or batch files to make a seamless command line experience for my .py utilities, which the user can run without any additional installation steps.The 'jythonc' command should be able to compile your .py source into JVM bytecode, which should make it portable to any Java install.  Or so I read at: For distributing your Python scripts in a way that doesn't require a native Python installation, you could also try , which basically translates your Python code to C++ code, which is then compiled to a true native binary."},
{"body": "I have these models:Now I want to query Bars that are having active Foo's as such:I am getting error such asHow can I achieve this?You cannot query against model methods or properties. Either use the criteria within it in the query, or filter in Python using a list comprehension or genex.You could also use a custom manager. Then you could run something like this:And all you have to do is:Check out the .You can't filter on methods, however if the is_active method on Foo checks an attribute on Foo, you can use the double-underscore syntax like I had similar problem: I am using class-based view  and I had to filter by model's method. (storing the information in database wasn't an option because the property was based on time and I would have to create a cronjob and/or... )My answer is ineffective and I don't know how it's gonna scale on larger data; but, it works:May be you can use simple filter, for this type of conditions."},
{"body": "EDIT: switched to a better example, and clarified why this is a real problem.I'd like to write unit tests in Python that continue executing when an assertion fails, so that I can see multiple failures in a single test. For example:Here, the purpose of the test is to ensure that Car's  sets its fields correctly. I could break it up into four methods (and that's often a great idea), but in this case I think it's more readable to keep it as a single method that tests a single concept (\"the object is initialized correctly\").If we assume that it's best here to not break up the method, then I have a new problem: I can't see all of the errors at once. When I fix the  error and re-run the test, then the  error appears. It would save me time to see both errors when I first run the test.For comparison, Google's C++ unit testing framework  between non-fatal  assertions and fatal  assertions:Is there a way to get -like behavior in Python's ? If not in , then is there another Python unit test framework that does support this behavior?Incidentally, I was curious about how many real-life tests might benefit from non-fatal assertions, so I looked at some  (edited 2014-08-19 to use searchcode instead of Google Code Search, RIP). Out of 10 randomly selected results from the first page, all contained tests that made multiple independent assertions in the same test method. All would benefit from non-fatal assertions.What you'll probably want to do is derive  since that's the class that throws when an assertion fails.  You will have to re-architect your  to not throw (maybe keep a list of failures instead). Re-architecting stuff can cause other issues that you would have to resolve.  For example you may end up needing to derive  to make changes in support of the changes made to your .Another way to have non-fatal assertions is to capture the assertion exception and store the exceptions in a list. Then assert that that list is empty as part of the tearDown.One option is assert on all the values at once as a tuple.For example:The output from this tests would be:This shows that both the model and the wheel count are incorrect.It is considered an anti-pattern to have multiple asserts in a single unit test. A single unit test is expected to test only one thing. Perhaps you are testing too much. Consider splitting this test up into multiple tests. This way you can name each test properly.Sometimes however, it is okay to check multiple things at the same time. For instance when you are asserting properties of the same object. In that case you are in fact asserting whether that object is correct. A way to do this is to write a custom helper method that knows how to assert on that object. You can write that method in such a way that it shows all failing properties or for instance shows the complete state of the expected object and the complete state of the actual object when an assert fails.Do each assert in a separate method.I liked the approach by @Anthony-Batchelor, to capture the AssertionError exception. But a slight variation to this approach using decorators and also a way to report the tests cases with pass/fail.Output from console:I don't think there is a way to do this with PyUnit and wouldn't want to see PyUnit extended in this way.I prefer to stick to one assertion per test function () and would rewrite  as four separate test functions. This would give more useful information on failure, :If you decide that this approach isn't for you, you may find  helpful.It looks like you are testing two concepts with your updated question and I would split these into two unit tests. The first being that the parameters are being stored on the creation of a new object. This would have two assertions, one for  and one for . If the first fails, the that clearly needs to be fixed, whether the second passes or fails is irrelevant at this juncture.The second concept is more questionable... You're testing whether some default values are initialised. ? It would be more useful to test these values at the point that they are actually used (and if they are not used, then why are they there?). Both of these tests fail, and both should. When I am unit-testing, I am far more interested in failure than I am in success as that is where I need to concentrate."},
{"body": "In a module residing inside a package, i have the need to use a function defined within the  of that package. how can i import the package within the module that resides within the package, so i can use that function?Importing  inside the module will not import the package, but instead a module named , leading to two copies of things with different names...Is there a pythonic way to do this?Also, starting in Python 2.5, relative imports are possible. e.g.:Quoting from :Starting with Python 2.5, in addition to the implicit relative imports described above, you can write explicit relative imports with the from module import name form of import statement. These explicit relative imports use leading dots to indicate the current and parent packages involved in the relative import. From the surrounding module for example, you might use:This doesn't exactly answer your question, but I'm going to suggest that you move the function outside of the  file, and into another module inside that package. You can then easily import that function into your other module. If you want, you can have an import statement in the  file that will import that function (when the package is imported) as well.If the package is named  and your init file is therefore  and your module within the package is  then from within  file, you should just be able to say  and use whatever you want that's defined in testmod.In Django, the file manage.py has , but  is not a module. It is a function within the  module of the  directory.I'm not totally sure what the situation is, but this may solve your \"different name\" problem:Or maybe?:"},
{"body": "I have a list of 3-tuples representing a set of points in 3D space. I want to plot a surface that covers all these points. The plot_surface function in the mplot3d package requires as arguments X,Y and Z which are 2d arrays. Is plot_surface the right function to plot surface and how do I transform my data in to the required format ?For surfaces it's a bit different than a list of 3-tuples, you should pass in a grid for the domain in 2d arrays. If all you have is a list of 3d points, rather than some function , then you will have a problem because there are multiple ways to triangulate that 3d point cloud into a surface.  Here's a smooth surface example:I just came across this same problem.  I have evenly spaced data that is in 3 1-D arrays instead of the 2-D arrays that 's  wants.  My data happened to be in a  so here is the  with the modifications to plot 3 1-D arrays.  That is the original example.  Adding this next bit on creates the same plot from 3 1-D arrays.Here are the resulting figures:\ncheck the official example.\nX,Y and Z are indeed 2d arrays, numpy.meshgrid() is a simple way to get 2d x,y mesh out of 1d x and y values.here's pythonic way to convert your 3-tuples to 3 1d arrays.Here's mtaplotlib delaunay triangulation (interpolation), it converts 1d x,y,z into something compliant (?):In Matlab I did something similar using the  function on the ,  coords only (not the ), then plotting with  or , using  as the height.SciPy has the  class, which is based on the same underlying QHull library that the Matlab's  function is, so you should get identical results.From there, it should be a few lines of code to convert this  example into what you wish to achieve, as  gives you the specification of each triangular polygon."},
{"body": "I am trying to iterate the lambda func over a list as in ,  and I want to get the call result of the lambda, not the function object itself. However, the following output really confused me. I modified the variable name when print the call result to  as following, and everything goes well. I am wondering what is all about of that. ?In Python 2 list comprehension 'leaks' the variables to outer scope:Note that the behavior is different on Python 3:When you define lambda it's bound to variable , not its' current value as your second example shows. \nNow when you assign new value to  the lambda will return whatever is the current value:Since the value of  within the loop is the lambda itself you'll get it as a return value:: Example on Python 2.7:Same on Python 3.4:For details about the change regarding the variable scope with list comprehension see Guido's .Closures in Python are , meaning that each lambda function in the list will only evaluate the variable  when invoked, and  when defined. That's why all functions return the same value, i.e. the last value of  (which is 4).To avoid this, one technique is to bind the value of  to a local named parameter:Another option is to create a  and bind the current value of  as its parameter: Sorry, misread the question initially, since these kind of questions are so often about late binding (thanks  for the comment).The second reason for the behavior is list comprehension's variable leaking in Python2 as others have already explained. When using  as the iteration variable in the  loop, each function prints the current value of  (for the reasons stated above), which is simply the function itself.\nWhen using a different name (e.g. ), functions print the last value of  as it was in the list comprehension loop, which is 4. is an anonymous function with no arguments that returns i. So you are generating a list of anonymous functions, which you can later (in the second example) bind to the name  and invoke with . Note you can do the same with non-anonymous functions:@plamut has just answered the implied other part of the question, so I won't."},
{"body": "I have a module that conflicts with a built-in module.  For example, a  module defined in .I can reference  anywhere in my code without issue.  However, I need to reference the built-in email module from my email module.It only finds itself, and therefore raises an , since  doesn't have a  method.   causes the same issue when I try .Is there any native support to do this in Python, or am I stuck with renaming my \"email\" module to something more specific?You will want to read about  which addresses this very problem. Use:Using that, any unadorned package name will always refer to the top level package. You will then need to use relative imports () to access your own package. The above  line needs to be put into any 2.x Python  files above the  lines you're using. In Python 3.x this is the default behavior and so is no longer needed."},
{"body": "What's better practice in a user-defined function in Python: raise an exception or return None?  For example, I have a function that finds the most recent file in a folder.Another option is leave the exception and handle it in the caller code, but I figure it's more clear to deal with a FileNotFoundError than an IndexError.  Or is it bad form to re-raise an exception with a different name?It's really a matter of semantics.  What does  ?Is it perfectly reasonable that there's no latest file?  Then sure, just return None.Are you expecting to always find a latest file?  Raise an exception.  And yes, re-raising a more appropriate exception is fine.If this is just a general function that's supposed to apply to any directory, I'd do the former and return None.  If the directory is, e.g., meant to be a specific data directory that contains an application's known set of files, I'd raise an exception.I would make a couple suggestions before answering your question as it may answer the question for you.As soon as I did this it became clear what it should return..  If there isn't a pdf raise an exception. But wait there more..Hope this helps!I usually prefer to handle exceptions internally (i.e. try/except inside the called function, possibly returning a None) because python is dynamically typed.  In general, I consider it a judgment call one way or the other, but in a dynamically typed language, there are small factors that tip the scales in favor of not passing the exception to the caller:Honestly, though, it's always a judgment call, and the situation you're describing, where the called function receives an error it can't help, is an excellent reason to re-raise an exception that is meaningful.  You have the exact right idea, but unless you're exception is going to provide more meaningful information in a stack trace thanwhich, nine times out of ten, is what the caller will see if you return an unhandled None, don't bother.(All this kind of makes me wish that python exceptions had the  attributes by default, as in java, which lets you pass exceptions into new exceptions so that you can rethrow all you want and never lose the original source of the problem.)In general, I'd say an exception should be thrown if something catastrophic has occured that cannot be recovered from (i.e. your function deals with some internet resource that cannot be connected to), and you should return None if your function should really return something but nothing would be appropriate to return (i.e. \"None\" if your function tries to match a substring in a string for example)."},
{"body": "When I create a fresh virtualenv,  shows that I have a couple of packages installed even though I've not installed anything into the environment. I was expecting  to return empty output until after my first  into the environment.  isn't it, so why does it show up at all? Some extra info:Everytime you create a virtualenv with --no-site-packages it installs  or . And the reason  appears is because python 2.5+ standard library provides egg info to  lib (and  does not know if it stdlib or 3rd party package).It seems to be solved on Python3.3+: To answer a slightly different question: you can exclude  (and any other similarly-problematic  files if you are unfortunate enough to have any for some reason) by doing  instead of . describes this option:"},
{"body": " This is asking for the reverse of the usual tuple-to-array conversion.I have to pass an argument to a (wrapped c++) function as a nested tuple.  For example, the following workswhereas the following Unfortunately, the argument I would like to use comes to me as a numpy array.  That array always has dimensions 2xN for some N, which may be quite large.Is there an easy way to convert that to a tuple?  I know that I could just loop through, creating a new tuple, but would prefer if there's some nice access the numpy array provides.If it's not possible to do this as nicely as I hope, what's the prettiest way to do it by looping, or whatever?Here's a function that'll do it:And an example:Another optionIf you are passing NumPy arrays to C++ functions, you may also wish to look at using Cython or SWIG. I was not satisfied, so I finally used this:I don't know if it's quicker, but it looks more effective ;)"},
{"body": "Why is the  better than the ?I ask because I don't see them resulting in anything different.Borg:Singleton:What I want to display here is that the service object, whether implemented as Borg or Singleton, has a nontrivial internal state (it provides some service based on it) (I mean it has to be something useful it's not a Singleton/Borg just for fun).And this state has to be inited. Here the Singleton implementation is more straightforward, since we treat  as the set-up of the global state. I find it awkward that the Borg object has to query its internal state to see if it should update itself.It becomes worse the more internal state you have. For example, if the object has to listen to the Application's teardown signal to save its register to disk, that registration should only be done once as well, and this is easier with a Singleton.The real reason that borg is different comes down to subclassing.If you subclass a borg, the subclass' objects have the same state as their parents classes objects, unless you explicitly override the shared state in that subclass. Each subclass of the singleton pattern has its own state and therefore will produce different objects.Also in the singleton pattern the objects are actually the same, not just the state (even though the state is the only thing that really matters).In python if you want a unique \"object\" that you can access from anywhere just create a class  that only contains static attributes, s, and s; you could call it the Unique Pattern. Here I implement and compare the 3 patterns:UniqueSingletonBorgTestOutput:In my opinion, Unique implementation is the easiest, then Borg and finally Singleton with an ugly number of two functions needed for its definition.It is not. What is generally not recommended is a pattern like this in python:where you use a class method to get the instance instead of the constructor. Python's metaprogramming allows much better methods, e.g. the one on :It's only better in those few cases where you actually have a difference. Like when you subclass. The Borg pattern is extremely unusual, I've never needed it for real in ten years of Python programming.A class basically describes how you can access (read/write) the internal state of your object.In the singleton pattern you can only have a single class, i.e. all your objects will give you the same access points to the shared state.\nThis means that if you have to provide an extended API, you will need to write a wrapper, wrapping around the singletonIn the borg pattern you are able to extend the base \"borg\" class, and thereby more conveniently extend the API for your taste."},
{"body": "I find myself repeatedly writing the same chunk of code:Is there any way I can write this function quicker/with less code?\nI usually use this in an if statement, like this:Yes, use :As the docs mention, this is pretty much equivalent to your function, but  can take generator expressions instead of just a string and a list, and  short-circuits. Once  is True, the function breaks (you can simply do this with your function if you just change  to . Remember that functions break when it returns a value).You should avoid naming strings  and lists . That will override the built-in types."},
{"body": "I want to find all values in a Pandas dataframe that contain whitespace (any arbitrary amount) and replace those values with NaNs.Any ideas how this can be improved?Basically I want to turn this:Into this:I've managed to do it with the code below, but man is it ugly. It's not Pythonic and I'm sure it's not the most efficient use of pandas either. I loop through each column and do boolean replacement against a column mask generated by applying a function that does a regex search of each value, matching on whitespace.It could be optimized a bit by only iterating through fields that could contain empty strings:But that's not much of an improvementAnd finally, this code sets the target strings to None, which works with Pandas' functions like fillna(), but it would be nice for completeness if I could actually insert a NaN directly instead of None.Help!I think  does the job:Produces:How about:The  function applies a function to every cell of the dataframe.I will did this:orYou can strip all str, then replace empty str with ."},
{"body": "I have a base class with a property which (the get method) I want to overwrite in the subclass. My first thought was something like:This does not work (subclass bar.age returns 11). I found a solution with an lambda expression which works:So is this the right solution for using properties and overwrite them in a subclass, or are there other preferred ways to do this?I simply prefer to repeat the  as well as you will repeat the  decorator when overriding a class method. While this seems very verbose, at least for Python standards, you may notice:1) for read only properties,  can be used as a decorator:2) in Python 2.6,   and  which can be used to apply to general properties the shortcut already available for read-only ones:Another way to do it, without having to create any additional classes. I've added a set method to show what you do if you only override one of the two:This is a pretty contrived example, but you should get the idea.Yes, this is the way to do it; the property declaration executes at the time the parent class' definition is executed, which means it can only \"see\" the versions of the methods which exist on the parent class. So when you redefine one or more of those methods on a child class, you need to re-declare the property using the child class' version of the method(s).I don't agree that the chosen answer is the ideal way to allow for overriding the property methods.  If you expect the getters and setters to be overridden, then you can use lambda to provide access to self, with something like .This works (at least) for Python versions 2.4 to 3.4.If anyone knows a way to do this with by using property as a decorator instead of as a direct property() call, I'd like to hear it!Example:This way, an override can be easily performed:so that:I discovered this on .I agree with your solution, which seems an on-the-fly template method. \n deals with your problem and provides exactly your solution.Something like this will workSame as 's but with decorator.  This way, an override can be easily performed:I ran into problems setting a property in a parent class from a child class. The following workround extends a property of a parent but does so by calling the _set_age method of the parent directly. Wrinkled should always be correct. It is a little javathonic though."},
{"body": "Caught an exception while rendering:  I was able to successfully import  from the shell and it worked so the path should be correct.Here is the urls.py:  This is where the error is being generated:  Which I can't understand because this works fine from the same file:  Here is the function definition:  I don't understand why Django would think that the function would not be able to find the Reverse for that function.I deleted all the  files and restarted Apache.What am I doing wrong?There are 3 things I can think of off the top of my head:Shell calls to  (as mentioned above) are very good to debug these problems, but there are two critical conditions:Yes, it's logical. Yes, it's also confusing because  will only throw the exception and won't give you any further hints.An example of URL pattern:And then what happens in shell:It doesn't work because I supplied no arguments.Now it worked, but...Now it didn't work because  didn't match the regexp (expected numeric, supplied string).You can use  with both positional arguments and keyword arguments. The syntax is:As it comes to the  template tag, there's funny thing about it. Django  gives example of using quoted view name:So I used it in a similar way in my HTML template:But this didn't work for me. But the exception message gave me a hint of what could be wrong - note the double single quotes around view name:It started to work when I  the quotes:And this  confusing.You need single quotes around the view nameinstead ofI had a similar problem and the solution was in the right use of the '$' (end-of-string) character:My main url.py looked like this (notice the $ character):and my url.py for my card_purchases app said: So a simple change worked:Notice the change in the second url! My url.py for my card_purchases app looks like this:Apart from this, I can confirm that quotes around named urls are crucial!I don't think you need the trailing slash in the URL entry. Ie, put this instead:This is assuming you have  enabled, which is the default."},
{"body": "I have a Python file which might have to support Python versions < 3.x and >= 3.x. Is there a way to introspect the Python runtime to know the version which it is running (for example, )?Sure, take a look at  and .For example, to check that you are running Python 3.x, useHere,  is the major version number.  would give you the minor version number.In Python 2.7 and later, the components of  can also be accessed by name, so the major version number is .See also Try this code, this should work:Per  and :Here's some code I use with  to check the Python installation:The best solution depends on how much code is incompatible. If there are a lot of places you need to support Python 2 and 3,  is the compatibility module.  and  are two booleans if you want to check the version.However, a better solution than using a lot of  statements is to use  compatibility functions if possible. Hypothetically, if Python 3000 has a new syntax for , someone could update  so your old code would still work.CheersTo make the scripts compatible with Python2 and 3 i use :"},
{"body": "I like very much the  package and its comfortable way to handle JSON responses. Unfortunately, I did not understand if I can also process XML responses. Has anybody experience how to handle XML responses with the  package? Is it necessary to include another package such as  for the XML decoding? does not handle parsing XML responses, no. XML responses are much more complex in nature than JSON responses, how you'd serialize XML data into Python structures is not nearly as straightforward.Python comes with built-in XML parsers. I recommend you use the :or, if the response is particularly large, use an incremental approach:The external  builds on the same API to give you more features and power still."},
{"body": "I want to host several sites with under the same server which uses Debian 5, say I have ,  and , and assume my ip is :Here is my apache default: And here is my wsgi config for , at :How can I add  and , which are Django-based sites and will be served like ?Your ServerName/ServerAlias directives are wrong. ServerName should be hostname. You probably should just delete ServerAlias.Then just do the obvious and duplicate VirtualHost/Listen directives, just changing the port number and locations of scripts in the file system.Finally, do not set DocumentRoot to be where your Django code is as it makes it easier to accidentally expose your source code to download if you stuff up Apache configuration. So, just remove DocumentRoot directive from VirtualHost for Django sites.I have also add missing Directory directive for allowing access to static files. You should review paths however.Make sure you read:\nfor further information.UPDATE 1BTW, since you are using PHP in same Apache, you would be much better off using mod_wsgi daemon mode and push each Django instance out into its own separate process. That allows those processes to be multithreaded, even though main Apache processes are forced to be single threaded because of PHP. End result will be much much less memory being used than if running multiple Django instances in each process under embedded mode with prefork MPM. Your Django code just needs to be thread safe. Configuration in addition to above would be to add WSGIDaemonProcess/WSGIProcessGroup to each Django VirtualHost, where name of daemon process group is different for each VirtualHost.This also allows you to more easily restart each Django instance without restart whole of Apache. Read:\nPutting all virtualHost configuration in one place works fine, but Debian has its own concept by separating them in a file for each site in /etc/apache2/sites-available, which are activated by symlinking them in ../sites-enabled. \nIn this way a server-admin could also assign separate access rights to the config file for each of the site-admin unix users, scripts can check if a site is active etc. Basically it would be nice to have one central howto for Django-Admin installations, the current multitude of separate docs, links and blog articles is not really helpful for the proliferation of Django. "},
{"body": "How do I get a thread to return a tuple or any value of my choice back to the parent in Python?I suggest you instantiate a  before starting the thread, and pass it as one of the thread's args: before the thread finishes, it s the result on the queue it received as an argument.  The parent can  or  it at will.Queues are generally the best way to arrange thread synchronization and communication in Python: they're intrinsically thread-safe, message-passing vehicles -- the best way to organize multitasking in general!-)If you were calling join() to wait for the thread to complete, you could simply attach the result to the Thread instance itself and then retrieve it from the main thread after the join() returns.  On the other hand, you don't tell us how you intend to discover that the thread is done and that the result is available.  If you already have a way of doing that, it will probably point you (and us, if you were to tell us) to the best way of getting the results out.You should pass a Queue instance as a parameter then you should .put() your return object into the queue. You can gather the return value via queue.get() whatever object you put.Sample:Use for multiple threads:I use this implementation and it works great for me. I wish you do so.I'm surprised nobody mentioned that you could just pass it a mutable:perhaps this has major issues of which I'm unaware.Another approach is to pass a callback function to the thread. This gives a simple, safe and flexible way to return a value to the parent, anytime from the new thread.Use  to wrap your target thread function and pass its return value back to the parent thread using a .  (Your original target function remains unchanged without extra queue parameter.)Sample code:Output:POC:You can use synchronised  module.\nConsider you need to check a user infos from database with a known id:Now you can get your data like this:Well, in the Python threading module, there are condition objects that are associated to locks. One method  will return whatever value is returned from the underlying method. For more information: Based on jcomeau_ictx's suggestion. The simplest one I came across. Requirement here was to get exit status staus from three different processes running on the server and trigger another script if all three are successful. This seems to be working fineThe following wrapper function will wrap an existing function and return an object which points both to the thread (so that you can call ,, etc. on it) as well as access/view its eventual return value.It looks OK, and the  class seems to be easily extended(*) with this kind of functionality, so I'm wondering why it isn't already there. Is there a flaw with the above method?(*) Note that husanu's answer for this question does exactly this, subclassing  resulting in a version where  gives the return value."},
{"body": "How can I make any use of PYTHONPATH? When I try to run a script in the path the file is not\nfound. When I cd to the directory holding the script the script runs. So what good is the\nPYTHONPATH?After cd to the file directory it runs ..Why can I not make any use of the PYTHONPATH?I think you're a little confused. PYTHONPATH sets the search path for  python modules, not for executing them like you're trying.What you're looking for is PATH. However, to run your python script as a program, you also need to set a  for Python in the first line. Something like this should work:And give execution privileges to it:Then you should be able to simply run  from anywhere. only affects  statements, not the top-level Python interpreter's lookup of python files given as arguments.Needing  to be set is not a great idea - as with anything dependent on environment variables, replicating things consistently across different machines gets tricky. Better is to use Python 'packages' which can be installed (using 'pip', or distutils) in system-dependent paths which Python already knows about.Have a read of [  - Broken Link]  - 'The Hitchhiker's Guide to Packaging', and also  - which explains PYTHONPATH and packages at a lower level.You're confusing PATH and PYTHONPATH. You need to do this:PYTHONPATH is used by the python interpreter to determine which modules to load. PATH is used by the shell to determine which executables to run."},
{"body": "I've been using  for a Python program that can ,  or both:The program is meaningless without at least one parameter. How can I configure  to force at least one parameter to be chosen?Following the comments: What's the Pythonic way to parametrize a program with at least one option?If not the 'or both' part (I have initially missed this) you could use something like this:Though, probably it would be a better idea to use  instead.There are also some implicit requirements when living on command line:Try to run it:Show the help:And use it:There can be even shorter variant:Usage looks like this:Note, that instead of boolean values for \"process\" and \"upload\" keys there are counters.It turns out, we cannot prevent duplication of these words:Designing good command line interface can be challenging sometime.There are multiple aspects of command line based program: offers a lot, but restricts possible scenarios and can become very complex.With  things go much shorter while preserving readability and offering high degree of flexibility. If you manage getting parsed arguments from dictionary and do some of conversions (to integer, opening files..) manually (or by other library called ), you may find  good fit for command line parsing.If you require a python program to run with at least one parameter, add an argument that  have the option prefix  (- or -- by default) and set  (Minimum of one argument required). The problem with this method I found is that if you do not specify the argument, argparse will generate a \"too few arguments\" error and not print out the help menu. If you don't need that functionality, here's how to do it in code:I  that when you add an argument with the option prefixes, nargs governs the entire argument parser and not just the option. (What I mean is, if you have an  flag with , then  flag expects at least one argument. If you have  with , it expects at least one argument overall.)I know this is old as dirt, but the way to require one option but forbid more than one (XOR) is like this:Output:  For  I am exploring ways of generalizing the  concept to handle cases like this.With this development ,  \nI am able to write:which produces the following :This accepts inputs like '-u',  '-up', '--proc --up' etc.It ends up running a test similar to , though the error message needs to be clearer:I wonder:Use append_const to a list of actions and then check that the list is populated:You can even specify the methods directly within the constants.The best way to do this is by using python inbuilt module .If you want only one argument to be selected by command line just use required=True as an argument for group "},
{"body": "I came across this example in the matplotlib websiteand I was wondering if it was possible to increase the figure size.I tried withbut it does nothing.If you already have the figure object use:But if you use the .subplots() command (as in the examples you're showing) to create a new figure you can also use:"},
{"body": "I have the following plot:and now I would like to give this plot common x-axis labels and y-axis labels. With \"common\", I mean that there should be one big x-axis label below the whole grid of subplots, and one big y-axis label to the right. I can't find anything about this in the documentation for , and my googlings suggest that I need to make a big  to start with - but how do I then put my 5*2 subplots into that using ?This looks like what you actually want. It applies the same approach of  to your specific case:Without  you get:With it you should get it nicer:But if you want to add additional labels, you should add them only to the edge plots:Adding label for each plot would spoil it (maybe there is a way to automatically detect repeated labels, but I am not aware of one).Since the command:you used returns a tuple consisting of the figure and a list of the axes instances, it is already sufficient to do something like (mind that I've changed to ):If you happen to want to change some details on a specific subplot, you can access it via  where  iterates over your subplots.It might also be very helpful to include aat the end of the file, before the , in order to avoid overlapping labels.I ran into a similar problem while plotting a grid of graphs. The graphs consisted of two parts (top and bottom). The y-label was supposed to be centered over both parts.I did not want to use a solution that depends on knowing the position in the outer figure (like fig.text()), so I manipulated the y-position of the set_ylabel() function. It is usually 0.5, the middle of the plot it is added to. As the padding between the parts (hspace) in my code was zero, I could calculate the middle of the two parts relative to the upper part.Downsides:There is probably a general solution that takes padding between figures into account.I discovered an alternative method; if you know the  and  kwargs that went into a  initialization, or you otherwise know the edges positions of your axes in , you can also specify the ylabel position in  coordinates with some fancy \"transform\" magic. For example:...and you should see that the label  to keep from overlapping with ticklabels, just like normal -- but now it will adjust to be always  the desired subplots.Furthermore, if you don't even use , the ylabel will show up by default . I'm guessing this is because when the label is finally drawn,  uses 0.5 for the -coordinate without checking whether the underlying coordinate transform has changed. "},
{"body": "I'm trying to run a google search query from a python app. Is there any python interface out there that would let me do this? If there isn't does anyone know which Google API will enable me to do this. Thanks.There's a simple example  (peculiarly missing some quotes;-).  Most of what you'll see on the web is Python interfaces to the old, discontinued SOAP API -- the example I'm pointing to uses the newer and supported AJAX API, that's definitely the one you want!-): here's a more complete Python 2.6 example with all the needed quotes &c;-)...:Here is Alex's answer ported to Python3Here's my approach to this: A couple code examples:Note that this code does NOT use the Google API, and is still working to date (January 2012).I am new in python and I was investigating how to do this. None of the provided examples are working properly for me. Some are blocked by google if you make many (few) requests, some are outdated. \nParsing the google search html (adding the header in the request) will work until google changes the html structure again. You can use the same logic to search in any other search engine, looking into the html (view-source). Usage:(Edit 1: Adding a parameter to narrow the google search to a specific site)(Edit 2: When I added this answer I was coding a Python script to search subtitles. I recently uploaded it to Github: )"},
{"body": "Before jumping into python, I had started with some Objective-C / Cocoa books. As I recall, most functions required keyword arguments to be explicitly stated. Until recently I forgot all about this, and just used positional arguments in Python. But lately, I've ran into a few bugs which resulted from improper positions - sneaky little things they were. Got me thinking - generally speaking, unless there is a circumstance that specifically requires non-keyword arguments - is there any good reason NOT to use keyword arguments? Is it considered bad style to always use them, even for simple functions? I feel like as most of my 50-line programs have been scaling to 500 or more lines regularly, if I just get accustomed to always using keyword arguments, the code will be more easily readable and maintainable as it grows. Any reason this might not be so? The general impression I am getting is that its a style preference, with many good arguments that they should generally not be used for very simple arguments, but are otherwise consistent with good style. Before accepting I just want to clarify though - is there any specific non-style problems that arise from this method - for instance, significant performance hits? There isn't any reason not to use keyword arguments apart from the clarity and readability of the code. The choice of whether to use keywords should be based on whether the keyword adds additional useful information when reading the code or not.I follow the following general rule:In most cases, stable mandatory arguments would be positional, and optional arguments would be keyword.There's also a possible difference in performance, because in every implementation the keyword arguments would be slightly slower, but considering this would be generally a premature optimisation and the results from it wouldn't be significant, I don't think it's crucial for the decision.Keyword arguments can do everything that positional arguments can, and if you're defining a new API there are no technical disadvantages apart from possible performance issues. However, you might have little issues if you're combining your code with existing elements.Consider the following:None of these would be a real issue if you design your API well and document the use of keyword arguments, especially if you're not designing something that should be interchangeable with something that already exists.If your consideration is to improve readability of function calls, why not simply declare functions as normal, e.g.And simply call functions by declaring the names explicitly, like so:Which obviously gives you the output:or this exercise would be pointless.This avoids having arguments be optional and needing default values (unless you want them to be, in which case just go ahead with the keyword arguments! :) and gives you all the versatility and improved readability of named arguments that are not limited by order.Well, there are a few reasons why I would not do that.If all your arguments are keyword arguments, it increases noise in the code and it might remove clarity about which arguments are required and which ones are optionnal.Also, if I have to use your code, I might want to kill you !! (Just kidding), but having to type the name of all the parameters everytime... not so fun.I remember reading a very good explanation of \"options\" in UNIX programs: \"Options are meant to be optional, a program should be able to run without any options at all\". The same principle could be applied to  arguments in Python. \nThese kind of arguments should allow a user to \"customize\" the function call, but a function should be able to be called without any implicit keyword-value argument pairs at all. Just to offer a different argument, I think there are some cases in which named parameters might improve readability. For example, imagine a function that creates a user in your system:From that definition, it is not at all clear what these values might mean, even though they are all required, however with named parameters it is always obvious:When Python's built-in  and  functions , the same argument was made in favor of clarity. There appears to be no significant performance hit, if any.Now, if you make your functions  accept keyword arguments (as opposed to passing the positional parameters using keywords when calling them, which is allowed), then yes, it'd be annoying.Sometimes, things should be simple because they are simple.If you always enforce you to use keyword arguments on every function call, soon your code will be unreadable.I don't see the purpose of using keyword arguments when the meaning of the arguments is obviousOne downside I could see is that you'd have to think of a sensible default value for everything, and in many cases there might not be any sensible default value (including ).  Then you would feel obliged to write a whole lot of error handling code for the cases where a kwarg that logically should be a positional arg was left unspecified.  Imagine writing stuff like this every time..Keyword args are good when you have long parameter lists with no well defined order (that you can't easily come up with a clear scheme to remember); however there are many situations where using them is overkill or makes the program less clear.First, sometimes is much easier to remember the order of keywords than the names of keyword arguments, and specifying the names of arguments could make it less clear.  Take  from  with the following docstring:When wanting to generate a random int from [0,10) its clearer to write  than  in my view.  If you need to generate an array with 100 numbers in [0,10) you can probably remember the argument order and write .  However, you may not remember the variable names (e.g., is the first parameter low, lower, start, min, minimum) and once you have to look up the parameter names, you might as well not use them (as you just looked up the proper order).  Also consider variadic functions (ones with variable number of parameters that are anonymous themselves).  E.g., you may want to write something like:that can be applied a bunch of bare parameters ( ).  Sure you could have written the function to take an named keyword iterable  and called it like  but that may be less intuitive, especially when there's no potential confusion about the argument name or its contents."},
{"body": "I would like a general way to generate column labels directly from the selected column names, and recall seeing that python's psycopg2 module supports this feature.From \"Programming Python\" by Mark Lutz:To , you can query the information_schema.columns table.To , you can use the description field of the cursor:  I also used to face similar issue. I use a simple trick to solve this.\nSuppose you have column names in a list likeThen you can do followingI have noticed that you must use  after the query to get the list of columns in  (i.e in )After executing SQL query write following python script written in 2.7"},
{"body": "I am writing a python package with modules that need to open data files in a  subdirectory. Right now I have the paths to the files hardcoded into my classes and functions. I would like to write more robust code that can access the subdirectory regardless of where it is installed on the user's system. I've tried a variety of methods, but so far I have had no luck. It seems that most of the \"current directory\" commands return the directory of the system's python interpreter, and not the directory of the module. This seems like it ought to be a trivial, common problem. Yet I can't seem to figure it out. Part of the problem is that my data files are not  files, so I can't use import functions and the like. Any suggestions? Right now my package directory looks like:I am trying to access  from Thanks!You can use underscore-underscore-file-underscore-underscore () to get the path to the package, like this:The standard way to do this is with setuptools packages and pkg_resources.You can lay out your package according to the following hierarchy, and configure the package setup file to point it your data resources, as per this link:You can then re-find and use those files using pkg_resources, as per this link: To provide a solution working today. Definitely use this API to not reinvent all those wheels.A true filesystem filename is needed. Zipped eggs will be extracted to a cache directory:Return a readable file-like object for the specified resource; it may be an actual file, a StringIO, or some similar object. The stream is in \u201cbinary mode\u201d, in the sense that whatever bytes are in the resource will be read as-is.Package Discovery and Resource Access using pkg_resourcesI think I hunted down an answer. I make a module data_path.py, which I import into my other modules containing:And then I open all my files with You need a name for your whole module, you're given directory tree doesn't list that detail, for me this worked:Notibly setuptools does not appear to resolve files based on a name match with packed data files, soo you're gunna have to include the  prefix pretty much no matter what. You can use  if you need alternate directory separators, Generally I find no compatibility problems with hard-coded unix style directory separators though."},
{"body": "How would you write, in python:I've tried every way I can think of, and am finding it very frustrating.If  isn't an  or  but a ing, you need to convert it to an  first by doingor to a  by doingOtherwise, what you have in your question should work, butorwould be a bit clearer.Here's a boolean thing:but  , \nso both have to be false to equate to true\nis true only if a and be are both false."},
{"body": "When I am writing code in Python, I often need to remove items from a list or other sequence type based on some criteria. I haven't found a solution that is elegant and efficient, as removing items from a list you are currently iterating through is bad.  For example, you can't do this:I usually end up doing something like this:This is innefficient, fairly ugly and possibly buggy (how does it handle multiple 'John Smith' entries?). Does anyone have a more elegant solution, or at least a more efficient one? How about one that works with dictionaries?Two easy ways to accomplish just the filtering are:Note that both cases keep the values for which the predicate function evaluates to , so you have to reverse the logic (i.e. you say \"keep the people who do not have the last name Smith\" instead of \"remove the people who have the last name Smith\"). Funny... two people individually posted both of the answers I suggested as I was posting mine.You can also iterate backwards over the list:This has the advantage that it does not create a new list (like  or a list comprehension) and uses an iterator instead of a list copy (like ).Note that although removing elements while iterating backwards is safe, inserting them is somewhat trickier.The obvious answer is the one that John and a couple other people gave, namely:But that has the disadvantage that it creates a new list object, rather than reusing the original object.  I did some profiling and experimentation, and the most efficient method I came up with is:Assigning to \"names[:]\" basically means \"replace the contents of the names list with the following value\".  It's different from just assigning to names, in that it doesn't create a new list object.  The right hand side of the assignment is a generator expression (note the use of parentheses rather than square brackets).  This will cause Python to iterate across the list.Some quick profiling suggests that this is about 30% faster than the list comprehension approach, and about 40% faster than the filter approach.: while this solution is faster than the obvious solution, it is more obscure, and relies on more advanced Python techniques.  If you do use it, I recommend accompanying it with a comment.  It's probably only worth using in cases where you really care about the performance of this particular operation (which is pretty fast no matter what).  (In the case where I used this, I was doing A* beam search, and used this to remove search points from the search beam.)Using There are times when filtering (either using filter or a list comprehension) doesn't work. This happens when some other object is holding a reference to the list you're modifying and you need to modify the list in place.The only difference from the original code is the use of  instead of  in the for loop. That way the code iterates over a (shallow) copy of the list and the removals work as expected. Since the list copying is shallow, it's fairly quick.filter would be awesome for this. Simple example: Corey's list comprehension is awesome too.Both solutions,  and  requires building a new list. I don't know enough of the Python internals to be sure, but I  that a more traditional (but less elegant) approach could be more efficient:Anyway, for short lists, I stick with either of the two solutions proposed earlier.To answer your question about working with dictionaries, you should note that Python 3.0 will include :In the mean time, you can do a quasi-dict comprehension this way:Or as a more direct answer:If the list should be filtered in-place and the list size is quite big, then algorithms mentioned in the previous answers, which are based on list.remove(), may be unsuitable, because their computational complexity is O(n^2). In this case you can use the following no-so pythonic function:Edit:\nActually, the solution at  is superior to mine solution. It is more pythonic and works faster. So, here is a new filter_inplace() implementation:The filter and list comprehensions are ok for your example, but they have a couple of problems:Your original solution is actually more efficient for very big lists, even if we can agree it's uglier. But if you worry that you can have multiple 'John Smith', it can be fixed by deleting based on position and not on value:We can't pick a solution without considering the size of the list, but for big lists I would prefer your 2-pass solution instead of the filter or lists comprehensions In the case of a set.  Here is my  implementation that can be used to filter items from a list in-place, I came up with this on my own independently before finding this page. It is the same algorithm as what PabloG posted, just made more generic so you can use it to filter lists in place, it is also able to remove from the list based on the  if reversed is set ; a sort-of of reversed filter if you will.Well, this is clearly an issue with the data structure you are using. Use a hashtable for example. Some implementations support multiple entries per key, so one can either pop the newest element off, or remove all of them.But this is, and what you're going to find the solution is, elegance through a different data structure, not algorithm. Maybe you can do better if it's sorted, or something, but iteration on a list is your only method here. one does realize he asked for 'efficiency'... all these suggested methods just iterate over the list, which is the same as what he suggested. "},
{"body": "... the  keyword that can be used for equality in strings.I tried both  and  but they didn't work.Thanks for your help!Testing strings with  only works when the strings are interned. Unless you really know what you're doing and explicitly  the strings you should  use  on strings.  tests for , not . That means Python simply compares the memory address a object resides in.  basically answers the question \"Do I have two names for the same object?\" - overloading that would make no sense.For example,  is . Usually Python writes each string into a different memory location, interning mostly happens for string literals.The  operator is equivalent to comparing  values.  is currently implemented to use pointers as the comparison. So you can't overload  itself, and AFAIK you can't overload  either.So, you can't. Unusual in python, but there it is.The Python  keyword tests object identity.  You should NOT use it to test for string equality.  It may seem to work frequently because Python implementations, like those of many very high level languages, performs \"interning\" of strings.  That is to say that string literals and values are internally kept in a hashed list and those which are identical are rendered as references to the same object.  (This is possible because Python strings are immutable).However, as with any implementation detail, you should not rely on this.  If you want to test for equality use  the == operator.  If you truly want to test for object identity then use  --- and I'd be hard-pressed to come up with a case where you should care about string object identity.  Unfortunately you can't count on whether two strings are somehow \"intentionally\" identical object references because of the aforementioned interning.The  keyword compares objects (or, rather, compares if two references are to the same object).Which is, I think, why there's no mechanism to provide your own implementation.It happens to work sometimes on strings because Python stores strings 'cleverly', such that when you create two identical strings they are stored in one object.You can hopefully see the reference vs data comparison in a simple 'copy' example:If you are not afraid of messing up with bytecode, you can intercept and patch  with  argument to call your hook function on objects being compared. Look at  module documentation for start-in.And don't forget to intercept  too if someone will do  instead of .is fails to compare a string variable to string value and two string variables when the string starts with '-'. My Python version is 2.6.6You can't overload the  operator.  What you want to overload is the  operator.  This can be done by defining a  method in the class.You are using identity comparison.  is probably what you want. The exception to this is when you want to be checking if one item and another are the EXACT same object and in the same memory position. In your examples, the item's aren't the same, since one is of a different type (my_string) than the other (string). Also, there's no such thing as someclass. in python (unless, of course, you put it there yourself). If there was, comparing objects with  wouldn't be reliable to simply compare the memory locations.When I first encountered the  keyword, it confused me as well. I would have thought that  and == were no different. They produced the same output from the interpreter on many objects. This type of assumption is actually EXACTLY what ... is for. It's the python equivalent \"Hey, don't mistake these two objects. they're different.\", which is essentially what [whoever it was that straightened me out] said. Worded much differently, but one point == the other point.the\nfor some helpful examples and some text to help with the sometimes confusing differences\nvisit  written by \"Danny Yoo\"or, if that's offline, use the  I made of it's body.in case they, in some 20 or so blue moons (blue moons are a real event), are both down, I'll quote the code examples'is' compares object identity whereas == compares values.Example:Assertion Errors can easily arise with  keyword while comparing objects. For example, objects  and  might hold same value and share same memory address. Therefore, doing an is going to evaluate to But ifevaluates to you should probably check and These might be different and a reason for failure."},
{"body": "I have 2 csv files. First one is data file and other one is mapping file. Mapping file has 4 columns \nThese are also the columns which are present in data file and need to be worked upon.Data file contains data with Device_Name column populated & rest 3 columns blank. Mapping file contains all columns populated. I want my Python code to open both files and for each device name in data file map its GDN, Device_Type & Device_OS value from mapping file.I know how to use dict when only 2 columns are present (1 is needed to be mapped) but I don't know how to accomplish this when 3 columns need to be mapped.Following is the code using which I tried to accomplish mapping of :It returns the .After some researching, I realized that I need to create a nested dict, but I don't have any idea on how to do this. \nPlease help me in resolving this or nudge me in the right direction to resolve this. A nested dict is a dictionary within a dictionary. A very simple thing.You can also use a  from the  package to facilitate creating nested dictionaries.You can populate that however you want.I would recommend in your code something  the following:According to your :My suggestion would be something  this (without using defaultdict):Please note though, that for parsing csv files there is a .: For an arbitrary length of a nested dictionary, go to .Use the defaultdict function from the collections. High performance: \"if key not in dict\" is very expensive when the data set is large.Low maintenance: make the code more readable and can be easily extended.For arbitrary levels of nestedness:It is important to remember when using defaultdict and similar nested dict modules such as nested_dict, that looking up a non existent key may inadvertently create a new key entry in the dict and cause a lot of havoc. Here is a Python3 example with nested_dict.Output is:"},
{"body": "Given a list of numbers how to find differences between every ()-th and ()-th of its elements? Should one better use lambda or maybe lists comprehension?Example:\nGiven a list  it is to find a list  because , , etc.The other answers are correct but if you're doing numerical work, you might want to consider numpy. Using numpy, the answer is:If you don't want to use  nor , you can use the simple (simplest in my opinion) solution:You can use  and  to efficiently build the result:Or using  instead:You can also avoid using the  module:All these solution work in constant space if you don't need to store all the results.\nThe first and last solution also works with infinite iterables, while the second one requires a finite sequence as input.Here are some micro-benchmarks of the solutions:And the other proposed solutions:Note that:Ok. I think I found the proper solution:My way"},
{"body": "How do you prevent PIP from re-downloading previously downloaded packages? I'm testing the install of matplotlib, an 11MB package that depends on several distro-specific packages. Everytime I run , it re-downloads matplotlib. How do I stop this?You can use a specific environment variable  and make it point to a directory where your packages will be stored. If they are to be installed again, they will be taken from this directory.There seems to be also an additional option for PIP  which ought to do something similar, but I have never tried it myself. For your example, to avoid re-downloading  every time, you would do the following:Does that answer your question?Newer Pip versions by default now cache downloads. See this documentation:Create a configuration file named , and add the following contents:In one command:You could Also, you could manually download the packageThen install it by un-tar and  later.The  works in a similar way w/ extra checking: it firstly search for the latest or specified version of the target package from web, if the search has result and there is cached package in the directory specified by , the cached package will be used instead of downloading. For example, will download pymongo package to current directory:"},
{"body": "I've discovered a new pattern.  Is this pattern well known or what is the opinion about it?Basically, I have a hard time scrubbing up and down source files to figure out what module imports are available and so forth, so now, instead ofI move all my imports into the function where they're actually used., like this:This does a few things.  First, I rarely accidentally pollute my modules with the contents of other modules.  I could set the  variable for the module, but then I'd have to update it as the module evolves, and that doesn't help the namespace pollution for code that actually lives in the module.Second, I rarely end up with a litany of imports at the top of my modules, half or more of which I no longer need because I've refactored it.  Finally, I find this pattern MUCH easier to read, since every referenced name is right there in the function body.The (previously)  to this question is nicely formatted but absolutely wrong about performance. Let me demonstrateAs you can see, it can be  efficient to import the module in the function. The reason for this is simple. It moves the reference from a global reference to a local reference. This means that, for CPython at least, the compiler will emit  instructions instead of  instructions. These are, as the name implies, faster.  The other answerer artificially inflated the performance hit of looking in  by . As a rule, it's best to import at the top but performance is  the reason if you are accessing the module a lot of times. The reasons are that one can keep track of what a module depends on more easily and that doing so is consistent with most of the rest of the Python universe.This does have a few disadvantages.On the off chance you want to test your module through runtime modification, it may make it more difficult. Instead of doingYou'll have to doThis means that you'll have to patch the othermodule globally, as opposed to just change what the reference in mymodule points to.This makes it non-obvious what modules your module depends on. This is especially irritating if you use many third party libraries or are re-organizing code.I had to maintain some legacy code that used imports inline all over the place, it made the code extremely difficult to refactor or repackage.Because of the way python caches modules, there isn't a performance hit. In fact, since the module is in the local namespace, there is a slight performance benefit to importing modules in a function.A few problems with this approach:So... the preferred way is to put all imports at the top of the file. I've found that if my imports get hard to keep track of, it usually means I have too much code that I'd be better off splitting it into two or more files.Some situations where I  found imports inside functions to be useful:Also: putting imports inside each function is actually  appreciably slower than at the top of the file. The first time each module is loaded it is put into , and each subsequent import costs only the time to look up the module, which is fairly fast (it is not reloaded).Another useful thing to note is that the  syntax inside of a function has been removed in Python 3.0.There is a brief mention of it under \"Removed Syntax\" here:I would suggest that you try to avoid  imports. I only use them inside packages, where the splitting into modules is an implementation detail and there won't be many of them anyway.In all other places, where you import a package, just use  and then reference it by the full name . This way you can always tell where a certain element comes from and don't have to maintain the list of imported elements (in reality this will always be outdated and import no longer used elements). If  is a really long name you can simplify it with  and then write . This is still far more convenient and explicit than maintaining all the  imports.People have explained very well why to avoid inline-imports, but not really alternative workflows to address the reasons you want them in the first place.To check for unused imports I use . It does static(ish)-analysis of Python code, and one of the (many) things it checks for is unused imports. For example, the following script....would generate the following message:As for checking available imports, I generally rely on TextMate's (fairly simplistic) completion - when you press Esc, it completes the current word with others in the document. If I have done ,  will expand to , if not I jump to the start of the file and add the import.From a performance point of view, you can see this: In general, I only use local imports in order to break dependency cycles.You might want to take a look at Import  in the python wiki. In short: if the module has already been loaded (look at ) your code will run slower. If your module hasn't been loaded yet, and will  will only get loaded when needed, which can be zero times, then the overall performance will be better.I believe this is a recommended approach in some cases/scenarios. For example in Google App Engine lazy-loading big modules is recommended since it will minimize the warm-up cost of instantiating new Python VMs/interpreters. Have a look at a  presentation describing this. However keep in mind this  mean you should lazy-load all your modules.Consider an environment where all of your Python code is located within a folder only a privileged user has access to. In order to avoid running your whole program as privileged user, you decide to drop privileges to an unprivileged user during execution. As soon as you make use of a function that imports another module, your program will throw an  since the unprivileged user is unable to import the module due to file permissions."},
{"body": "This is my declarative model:However, when I try to import this module, I get this error:If I use an Integer type, I can set a default value. What's going on? doesn't have a default key as an input. The default key should be an input to the  function. Try this:For sanity, you probably want to have all  calculated by your DB server, rather than the application server. Calculating the timestamp in the application can lead to problems because network latency is variable, clients experience slightly different clock drift, and different programming languages occasionally calculate time slightly differently. SQLAlchemy allows you to do this by passing  or  (they are aliases of each other) which tells the DB to calculate the timestamp itself.Additionally, for a default where you're already telling the DB to calculate the value, it's generally better to use  instead of . This tells SQLAlchemy to pass the default value as part of the  statement.For example, if you write an ad hoc script against this table, using  means you won't need to worry about manually adding a timestamp call to your script--the database will set it automatically.SQLAlchemy also supports  so that anytime the row is updated it inserts a new timestamp. Again, best to tell the DB to calculate the timestamp itself: There is a  parameter, but unlike , it doesn't actually set anything serverside. It just tells SQLalchemy that your database will change the column when an update happens (perhaps you created a  ), so SQLAlchemy will ask for the return value so it can update the corresponding object. You might be surprised to notice that if you make a bunch of changes within a single transaction, they all have the same timestamp. That's because the SQL standard specifies that  returns values based on the start of the transaction. PostgreSQL provides the non-SQL-standard  and  which  change within a transaction. Docs here: You can also use sqlalchemy builtin function for default DateTimeThe  keyword parameter should be given to the Column object.Example:The default value can be a callable, which here I defined like the following.[enter link description here][1]You likely want to use  so that UPDATEs also change the  field.[1]: SQLAlchemy has two defaults for python executed functions."},
{"body": "I am trying to learn how an application works. And for this I am inserting debug commands as the first line of each function's body with the goal of logging the function's name as well as the line number (within the code) where I send a message to the log output. Finally, since this application comprises of many files, I want to create a single log file so that I can better understand the control flow of the application.Here is what I know:I would greatly appreciate any help.Thanks!You have a few marginally related questions here.I'll start with the easiest: (3). Using  you can aggregate all calls to a single log file or other output target: they will be in the order they occurred in the process.Next up: (2).  provides a dict of the current scope. Thus, in a method that has no other arguments, you have  in scope, which contains a reference to the current instance. The trick being used that is stumping you is the string formatting using a dict as the RHS of the  operator.  will be replaced by whatever the value of  is.Finally, you can use some introspection tricks, similar to those used by  that can log more info:This will log the message passed in, plus the (original) function name, the filename in which the definition appears, and the line in that file. Have a look at  for more details.As I mentioned in my comment earlier, you can also drop into a  interactive debugging prompt at any time by inserting the line  in, and re-running your program. This enables you to step through the code, inspecting data as you choose.The correct answer for this is to use the already provided  variable Then anywhere you want, just add:Example output from a script I'm working on right now:"},
{"body": "Is Python slower than Java/C#?Here is a project that optimizes CPython:  Don't conflate Language and Run-Time.Python (the language) has many run-time implementations.Note that Python (the language) is not slow.  Some Python run-times (CPython, for example) will be slower than native-code C++.It is not really correct to ask why Python is slower than Java/C#. How fast is Java? Well, naive interpreters are around ten times slower than optimised compilers. I believe there is a Java bytcode interpreter written in JavaScript - that probably isn't very fast. So, the intended question appears to be \"Why is the CPython language system slower than the equivalent Sun, IBM and Oracle JRE and Microsoft .NET runtime?\"I believe the correct answer is non-technical. The fastest Java and .NET runtime are faster because they have large full time technical teams developing them in performance-competitive environment.Dynamic language systems are easy to implement. Any idiot can do it. I have. Static language systems are more complex to design and implement. A simple static system will tend to run much faster than the equivalent just-working dynamic equivalent. However, it is possible for highly optimised dynamic systems to run almost as fast. I understand some Smalltalk implementation were quite good. An often quoted example of a developed dynamic system is the .In addition if the real grunt is being done by library code, then the language system may not matter. Alternatively, the language may encourage (or give time(!)) to develop more efficient algorithms which can easily wipe out constant factor performance differences.As mentioned in the other answers this depends on the run-time system as well as the task at hand. So the standard (C)Python is not necessarily slower than Java or C#. Some of its modules are implemented in C. Thus combining speed of a  implementation with Python's language.We did a small experiment: We compared the execution time of a Factorial computation in different languages. The test was actually intended to evaluate the performance of arbitrary-precision integers implementations.The bar chart shows the results. Python is the clear winner. As far as I know Python uses the  to multiply large integers, which explains the speed.Besides, Python's \"arbitrary-precision integers\"-type is the built-in . Hence you don't even need special type handling which is required for Java's BigInteger-class.As suggested in comments, you should really provide a test case to reason about. Reasons behind performance differences will change depending on the test being executed.However, I'd suggest that the static vs dynamic nature may well have a lot to do with it. For non-virtual calls, the JIT-compiled C#/Java is extremely cheap as it can be determined accurately at JIT-time. Even virtual calls just involve a single level of redirection. When binding becomes dynamic, there's a wider range of things to consider.I don't know enough details about Python to claim to understand its exact runtime behaviour, which I suspect may vary with version and implementation too. There is such a thing as \"python byte code\" which is then executed by a virtual machine - whether this virtual machine actually performs JIT-compilation or not is another matter.It boils down to the fact that the compilation phase has lesser information to work with and hence the runtime needs to do more work in case of duck typed (dynamically typed) languages. Thus if I am making the method invocation foo.bar(), in case of Java or C++ the invocation to bar can be optimized in the compilation process by discovering the type of \"foo\" and then directly invoking the method at the memory location where the compiler knows it will be found. Since a python or any other dynamically typed language compiler does not know what type the object foo belongs to, it has to do a type check at runtime and then look up the address of the bar method and then invoke it. There are other difficulties a python compiler writer struggles with as well, though the one above hopefully adequately gives an indication. So even with the best compiler writers, statically typed languages are likely to perform much better at runtime.Where dynamically typed languages score are typically in the development time. Due to fewer lines of code to write and maintain, and no compile wait times for developers, the development often goes through much faster.Simply - . No matter what interpreter (currently available) you use, it is slower than Java and C. In various benchmarks, its slower than Ruby and PHP.\nDo not depend on other's answers, check and verify yourself. Personally I do not think, there is much serious contribution and development done on getting python faster. Since the productivity is good in python and it solves some of problem straight forward, speed/performance is not taken seriously. There are some architecture issues too preventing Python getting performance tweaks. - This answer probably will hurt Python lovers. I too am Python developer, loves developing webapps in Django/Flask/Pyramid rather than Spring (Java). But I see practically in my work and experience, how Python is slower. The speed is not always my priority. But I do stand with them, who says Python Interpreter should get oiling and greasing or total engine change to at least stand in marathon. It's a mainstream programming language.What you got there is clear example of writing Java in Python:A bit more pythonic:I think it's ultimately that Python doesn't go as far as it can with optimizations.  Most of the optimization techniques that are common are for static languages.  There  optimization techniques for dynamic languages, but the modern ones don't seem to make as much use of them as they could.  Steve Yegge has an excellent .:  I just wanted to point out that I'm not necessarily stating this to be critical of Python.  I prefer simplicity over unnecessary speed any day. It doesn't have anything to do with the languages themselves, it's just the fact that java  and  (JVM) are very high quality, and that lots of resources have been invested in stability, scalability and performance improvements over the years.Contrast that to the fact that CPython implementation just recently implemented eg threaded dispatch in its interpreter which gave it performance boost of up to 20% for certain problems. It's not a good thing as it sounds, it is bad because that kind of basic optimization should be there from the day one.This type of question can't be answered just by qualitative reasoning, you need good benchmarks to back it up.  Here's one set that compare  and find Python to be 3 to 300 times slower.  The Python vs. Java results are similar.  (The usual cautions about interpreting benchmarks apply.)These benchmarks also report the source code size, and Python was significantly more concise than Java and C#.I think opposite. I can do simple program in Python faster than in Java,\nand those Python scripts work really fast.Of course your question without examples is hard to answer. Maybe you have found slow library, bug etc. Give us more details please.I would argue that the ease and simplicity of writing Python code makes it possible to write more complex code; for example, code that takes advantage of multi-core processors.  Since per-core performance has been mostly stagnant for the past 5-10 years, I don't think it's clear that Python programs (whether they're running on CPython or something else) are slower in the long run.Since it's interpreted and not compiled.. it should be slower in execution time. As a table mentioned in  book, page 600, C# equals C++ in  (1:1). And Python is slower above hundred times than C++ in  (>100:1).And Java is slower than C++ by one time and a half (1.5:1). These statistics are on average. I don't know who made this study, but seems interesting.   "},
{"body": "I have two lists of objects. Each list is already sorted by a property of the object that is of the datetime type. I would like to combine the two lists into one sorted list. Is the best way just to do a sort or is there a smarter way to do this in Python?People seem to be over complicating this.. Just combine the two lists, then sort them:..or shorter (and without modifying ):..easy! Plus, it's using only two built-in functions, so assuming the lists are of a reasonable size, it should be quicker than implementing the sorting/merging in a loop. More importantly, the above is much less code, and very readable.If your lists are large (over a few hundred thousand, I would guess), it may be quicker to use an alternative/custom sorting method, but there are likely other optimisations to be made first (e.g not storing millions of  objects)Using the  (which repeats the functions 1000000 times), I loosely benchmarked it against  solution, and  is substantially quicker: took.. took..This hasn't been mentioned, so I'll go ahead - there is a  in the heapq module of python 2.6+. If all you're looking to do is getting things done, this might be a better idea. Of course, if you want to implement your own, the merge of merge-sort is the way to go.Here's the documentation Long story short, unless  use:Description of the figure and source code can be found . The figure was generated by the following command:This is simply merging. Treat each list as if it were a stack, and continuously pop the smaller of the two stack heads, adding the item to the result list, until one of the stacks is empty. Then add all remaining items to the resulting list.There is a slight flaw in  solution, making it O(n**2), rather than O(n).\nThe problem is that this is performing:With linked lists or deques this would be an O(1) operation, so wouldn't affect complexity, but since python lists are implemented as vectors, this copies the rest of the elements of l1 one space left, an O(n) operation.  Since this is done each pass through the list, it turns an O(n) algorithm into an O(n**2) one.  This can be corrected by using a method that doesn't alter the source lists, but just keeps track of the current position.I've tried out benchmarking a corrected algorithm vs a simple sorted(l1+l2) as suggested by I've tested these with lists generated withFor various sizes of list, I get the following timings (repeating 100 times):So in fact, it looks like dbr is right, just using sorted() is preferable unless you're expecting very large lists, though it does have worse algorithmic complexity.  The break even point being at around a million items in each source list (2 million total).One advantage of the merge approach though is that it is trivial to rewrite as a generator, which will use substantially less memory (no need for an intermediate list).\nI've retried this with a situation closer to the question - using a list of objects containing a field \"\" which is a datetime object.\nThe above algorithm was changed to compare against  instead, and the sort method was changed to:This does change things a bit.  The comparison being more expensive means that the number we perform becomes more important, relative to the constant-time speed of the implementation.  This means merge makes up lost ground, surpassing the sort() method at 100,000 items instead.  Comparing based on an even more complex object (large strings or lists for instance) would likely shift this balance even more.[1]: Note: I actually only did 10 repeats for 1,000,000 items and scaled up accordingly as it was pretty slow.The output:I bet this is faster than any of the fancy pure-Python merge algorithms, even for large data. Python 2.6's  is a whole another story.This is simple merging of two sorted lists. Take a look at the sample code below which merges two sorted lists of integers.This should work fine with datetime objects. Hope this helps.Python's sort implementation \"timsort\" is specifically optimized for lists that contain ordered sections.  Plus, it's written in C.  \nAs people have mentioned, it may call the comparison function more times by some constant factor (but maybe call it more times in a shorter period in many cases!).I believe the Python developers are committed to keeping timsort, or at least keeping a sort that's O(n) in this case.Right, sorting in the general case can't be faster than that.  But since O() is an upper bound, timsort being O(n log n) on arbitrary input doesn't contradict its being O(n) given sorted(L1) + sorted(L2).Use the 'merge' step of merge sort, it runs in O(n) time.From  (pseudo-code):Recursive implementation is below. Average performance is O(n).or generator with improved space complexity:If you want to do it in a manner more consistent with learning what goes on in the iteration try thisHave used merge step of the merge sort. But I have used .   Well, the naive approach (combine 2 lists into large one and sort) will be O(N*log(N)) complexity. On the other hand, if you implement the merge manually (i do not know about any ready code in python libs for this, but i'm no expert) the complexity will be O(N), which is clearly faster.\nThe idea is described wery well in post by Barry Kelly.Will sort the list in place. Define your own function for comparing two objects, and pass that function into the built in sort function.Do NOT use bubble sort, it has horrible performance."},
{"body": "I've got a set of models that look like this:and an admin.py that looks like this:My goal is to get an admin interface that lets me edit everything on one page. The end result of this model structure is that things are generated into a view+template that looks more or less like:I know that the inline-in-an-inline trick fails in the Django admin, as I expected. Does anyone know of a way to allow this kind of three level model editing? Thanks in advance.You need to create a custom  and  for the .Something like this should work for the form:(That just came off the top of my head and isn't tested, but it should get you going in the right direction.)Your template just needs to render the form and form.link_formset appropriately.  is built for just this. Usage is simple.My recommendation would actually be to change your model.  Why not have a  in  to ?  Or, if it's not OneToMany, perhaps a  field?  The admin interface will generate that for free.  Of course, I don't recommend this if links don't logically have anything to do with link sections, but maybe they do?  If they don't, please explain what the intended organization is.  (For example, is 3 links per section fixed or arbitrary?)You can create a new class, similar to TabularInline or StackedInline, that is able to use inline fields itself.Alternatively, you can create new admin templates, specifically for your model. But that of course overrules the nifty features of the admin interface."},
{"body": "The other day I came across a Python implementation called Jython.\nWith Jython you can write Java applications with Python and compile them to pure Java.  I was wondering: Android programming is done with Java.\nSo, is it possible to make Android apps with Jython?  Jython doesn't compile to \"pure java\", it compiles to  - ie, to  files. To develop for Android, one further compiles java bytecode to Dalvik bytecode. This means that, yes, Jython  let you use Python for developing Android, subject to you getting it to play nice with the Android SDK (I haven't personally tried this, so I don't know how hard it actually is) - you  need to make sure you don't depend on any Java APIs that Android doesn't provide, and  need to have some of the Android API  files around when you run jython. Aside from these niggles, your core idea should work - Jython does, indeed, let write code in Python that interacts with anything else that runs on the JVM.As long as it compiles to pure java (with some constraints, as some APIs are not available), but I doubt that python will be of much use in  development of android-specific stuff like activities and  UI manipulation code. You also have to take care of application size - that is serious constraint for mobile developement.Yes and no.  With jython you can use java classes to compile for the JVM. But Android use the DVM (Dalvik Virtual Machine) and the compiled code is different.  You have to use tools to convert from JVM code to DVM. brings scripting languages to Android by allowing you to edit and execute scripts and interactive interpreters directly on the Android device.It is possible jython compiles python to java bytecode(THIS IS DONE WITH JSE) then this java bytecode can be treated normally and converted to DVM operable code(.dex) with other xml files it is then packaged into an apk.\nThe process will be as follows-This will use JYTHON and android COMPILER(.apk)I am working on this projectIt's not possible. You can't use jython with android because the DVM doesn't understand it. DVM is not JVM.sadly No.Mobile phones only have Java ME (Micro Edition) but Jython requires Java SE (Standard Edition). There is no Jython port to ME, and there is not enough interest to make it worth the effort."},
{"body": "I am interested in getting an install of Django running on IronPython, has anyone had any success getting this running with some level of success?  If so can you please tell of your experiences, performance, suggest some tips, resources and gotchas?Besides the Jeff Hardy blog post on  mentioned by Tony Meyer, it might be useful to also read Jeff's two other posts in the same series on his struggles with IronPython, easy_install and zlib. The first is  which discusses the absence of zlib for IronPython; hence, no easyinstall. Jeff reimplemented zlib based on ComponentAce's zlib.net. And finally, in  Jeff discusses some final tweaks that are needed before easy_install can be used with IronPython.This was  at last year's PyCon (the  are also available).  More recently, , including suggestions.I don't think Django is yet working on IronPython, but I'm not saying it cannot be done. project is a work currently underway to run Django on IronPython. The project has reported  with sqlite on 2nd of May 2010.  "},
{"body": "Why does ?Is this inconsistent with recommending spaces around every other occurrence of  in Python code?How is:better than:Any links to discussion/explanation by Python's  will be appreciated.Mind, this question is more about kwargs than default values, i just used the phrasing from PEP 8.I'm not soliciting opinions. I'm asking for reasons behind this decision. It's more like asking  would I use  on the same line as  statement in a C program, not  I should use it or not.I guess that it is because a keyword parameter is essentially different than a variable assignment.For example, there is plenty of code like this:As you see, it makes completely sense to assign a variable to a keyword argument named exactly the same, so it improves readability to see them without spaces. It is easier to recognize that we are using keyword arguments and not assigning a variable to itself.Also, parameters tend to go in the same line whereas assignments usually are each one in their own line, so saving space is likely to be an important matter there.I wouldn't use very_long_variable_name as a default argument. So consider this:over this:Also, it doesn't make much sense to use variables as default values. Perhaps some constant variables (which aren't really constants) and in that case I would use names that are all caps, descriptive yet short as possible. So no another_very_...IMO leaving out the spaces for args provides cleaner visual grouping of the arg/value pairs; it looks less cluttered.There are pros and cons.I very much dislike how PEP8 compliant code reads. I don't buy into the argument that  can ever be more human readable than\n.\nThis is not how people read. It's an additional cognitive load, particularly in the absence of syntax highlighting.There is a significant benefit, however. If the spacing rules are adhered to, it makes searching for parameters exclusively  much more effective.I think there are several reasons for this, although I might just be rationalizing:"},
{"body": "How can I use win32 API in Python?\nWhat is the best and easiest way to do it?\nCan you please provide some examples?PyWin32 is the way to go - but how to use it? One approach is to begin with a concrete problem you're having and attempting to solve it. PyWin32 provides bindings for the Win32 API functions for which there are many, and you really have to pick a specific goal first.In my Python 2.5 installation (ActiveState on Windows) the win32 package has a Demos folder packed with sample code of various parts of the library. For example, here's CopyFileEx.py:It shows how to use the CopyFileEx function with a few others (such as GetTempPath and GetTempFileName). From this example you can get a \"general feel\" of how to work with this library. PyWin32, as mentioned by @chaos, is probably the most popular choice; the alternative is  which is part of Python's standard library. For example,  will show the module-handle of the current module (EXE or DLL). A more extensive example of using ctypes to get at win32 APIs is .The important functions that you can to use in win32 Python are the message boxes, this is classical example of OK or Cancel.The collection: "},
{"body": "Why does the  function call wipe out the dupes, but parsing a set literal does not?(Python 2.7.12.  Possibly same root cause as for  similar question)Sets test for equality, and until there are new Python releases, the order in which they do this can differ based on the form you hand the values to the set being constructed, as I'll show below.Since  is true   is true, but  is , the behaviour here is really , as the set assumes that  must be true if the first two tests were true too.If you  the list passed to , then you get the same output as using a literal, because the order of equality tests changes:and the same for reversing the literal:What's happening is that the set  loads the values onto the stack and then the stack values are added to the new set object in reverse order.As long as  is loaded , the other two objects are then tested against  already in the set. The moment one of the other two objects is loaded first, the equality test fails and you get two objects added:That set literals add elements in reverse is a bug present in all versions of Python that support the syntax, all the way until Python 2.7.12 and 3.5.2. It was recently fixed, see  (part of 2.7.13, 3.5.3 and 3.6, none of which have been released yet). If you look at 2.7.12, you can see that  reads the stack from the top down:while the bytecode adds elements to the stack in reverse order (pushing  on the stack first):The fix is to read the elements from the stack in reverse order; the  uses  instead of  (and a  to remove the elements from the stack afterwards):The equality testing issue has the same root cause as the other question; the  class is having some equality issues with  here, which was fixed in Python 3.2 (by making ).It's all down to the order in which the set is constructed, combined with the bug you discovered with .  It appears that the literal is constructed in the opposite order to conversion from a list."},
{"body": "So I'm starting like Python a bit, but I'm having trouble erm...running it. LolI'm using IDLE for now, but its no use whatsoever because you can only run a couple of lines at a time.I'm also using Komodo Edit to create the actual .py files.My question is, how can I run the .py files to test out the actual program? I'm using Windows 7, and Komodo Edit 5 as my IDE. Pressing F5 in Komodo doesn't do anythin at all.I'm very glad you asked! I was just working on explaining this very thing  (which is obviously incomplete).  We're working with Python novices, and had to help a few through exactly what you're asking!   If you get this message:  then  (the  program that can translate Python into 'computer instructions') isn't on your path (see Putting Python in Your Path below).  Then try calling it like this (assuming Python2.6, installed in the usual location):> C:\\Python26\\python.exe first.py(Advanced users:  instead of first.py, you could write out first.py's full path of C:\\Documents and Settings\\Gregg\\Desktop\\pyscripts\\first.py)In order to run programs, your operating system looks in various places,\nand tries to match the name of the program / command you typed with some \nprograms along the way.  In windows:control panel > system >  advanced > |Environmental Variables| > system variables -> Paththis needs to include:  C:\\Python26; (or equivalent).  If you put it at the front,\nit will be the first place looked.  You can also add it at the end, which is possibly saner.Then restart your prompt, and try typing 'python'.  If it all worked, you should\nget a \">>>\" prompt.You can just callIn IDLE press F5You can open your .py file with IDLE and press F5 to run it. You can open that same file with other editor ( like Komodo as you said ) save it and press F5 again; F5 works with IDLE ( even when the editing is done with another tool ).If you want to run it directly from Komodo according to this article:  you have to:Python itself comes with an editor that you can access from the IDLE File > New File menu option.Write the code in that file, save it as [filename].py and then (in that same file editor window) press F5 to execute the code you created in the IDLE Shell window.Note: it's just been the easiest and most straightforward way for me so far.if you dont want call  you can add  to the PATHEXT, that way you will just call If this helps anyone, neither \"python [filename].py\" or \"python.exe [filename.py]\" worked for me, but \"start python [filename].py\" did. If anyone else is experiencing issues with the former two commands, try the latter one."},
{"body": "Suppose I have pandas DataFrame like this:I want to get new DataFrame with top 2 records for each id, like this:I can do it with numbering records within group after group by:But is there more effective/elegant approach to do this? And also is there more elegant approach to number records within each group (like SQL window function ).Thanks in advance.Did you try Ouput generated: (Keep in mind that you might need to order/sort before, depending on your data)EDIT: As mentioned by the questioner, use  to remove the multindex and flatten the results., you can now do  and  on a  object:There's a slight weirdness that you get the original index in there as well, but this might be really useful depending on what your original index .If you're not interested in it, you can do  to get rid of it altogether.(Note:  you'll be able to do this on a DataFrameGroupBy too but for now it only works with  and .)"},
{"body": "I'd like a function, , which behaves like the following:How can I implement this function?I looked at the  attribute but it appears to be used for something else.If I know the first value that needs to be sent into the generator, I can do something like this:However, this seems abhorrent. This only works in Python 3.2+:So, the requested function is:Out of curiosity, I looked into CPython to figure out how it was determining this...  Apparently it looks at  which is the \"index of last attempted instruction in bytecode\".  If it's  then it hasn't started yet.Here's a py2 version:Make a new generator which simply yields from your generator of interest.  .  Afterwards, it can simply use  for the rest of the items.  Use the substitute generator as a drop in replacement for the generator you're interested in monitoring the \"is_just_started\" state.  This technique is non-intrusive, and can be used even on generators for which you have no control over the source code.  You may create a iterator and set the flag as the instance property to iterator class as:And your value check function would be like:Sample run:To know the difference between  and , check "},
{"body": "Consider..I'd like to replace all dict keys with their respective dict values in .Using re:This will match whole words only. If you don't need that, use the pattern:Note that in this case you should sort the words descending by length if some of your dictionary entries are substrings of others.You could use the  function:Solution  (I like its simplicity):one way, without reAlmost the same as ghostdog74, though independently created. One difference,\nusing d.get() in stead of d[] can handle items not in the dict.I used this in a similar situation (my string was all in uppercase):hope that helps in some way... :)"},
{"body": "Using  on Windows 8 with Python 2.7 gives me the error: How can the error be resolved? Running  gives the same error...You could use ol' good  instead. isn't pip but one good aspect of it is the ability to download and install binary packages too, which would free you for the need having VC++ ready. This of course relies of the assumption that the binaries were prepared for your Python version.UPDATE:Yes, Pip can install binaries now!There's a new binary Python archive format () that is supposed to replace \"eggs\". Wheels are already supported by . This means you'll be able to install  with  without compiling it as soon as someone builds the wheel for your platform and uploads it to PyPI.The problem here is the line 292 (Using Python 3.4.3 here) in   which says:This only checks for the MSVC version that your python was built with. Just replacing this line with your actual Visual Studio version, eg.  for will fix the issue.UPDATE: Turns out that there is a good reason why this version is hardcoded. MSVC C runtime is not required to be compatible between major versions. Hence when you use a different VS version you might run into runtime problems. So I advise to use VS 2008 (for Python 2.6 up to 3.2) and VS2010 for (Python 3.3 and later) until this issue is sorted out.Binary compatibility will arrive with VS 2015 () along with Python 3.5 .For Python 2.7 users Microsoft released a special  which can be used without installing the whole VS 2008.If you are getting this error on Python 2.7 you can now get the  as a stand alone download.If you are on 3.3 or later you need to install Visual Studio 2010 express which is available for free here: If you are 3.3 or later and using a 64 bit version of python you need to install the Microsoft SDK 7.1 that ships a 64 bit compiler and follow the directions here First you should look for the file vcvarsall.bat in your system.If it does not exits I recommend you to install . This will create the vcvarsall.bat in \"C:\\Program Files (x86)\\Common Files\\Microsoft\\Visual C++ for Python\\9.0\" if you install it for all users.The problem now is that a function  in the  module is the one looking for vcvarsall.bat. If you follow the function calls you will see is looking in the registry for the directory of the vcvarsall.bat file and it won't never find it because the function is looking in other directories different from where the above mentioned installation placed it, and in my case the registry didn't exits.The easiest way to solve this problem is to remove the body (or just place in the first line) of the function   in the  file with the absolute path to vcvarsall.bat. For example:This solution worked for me.If you already have the vcvarsall.bat file you should check if you have the key  in the registry:Where If you don't have the key just do:To understand the exact behavior check  module starting in the  function.Simply because you don't have c++ compiler installed there in your machine, check the following : 0.9 in the registery directory is the currently installed version of your visual studio, if you running VS 2013, you have to find the path HKEY_LOCAL_MACHINE\\Software\\Wow6432Node\\Microsoft\\VisualStudio\\12.0....You need to have Visual Studio's bin dir in your path. Pip install is trying to compile some C code. I spent hours researching this vcvarsall.bat as well. Most answers on SO focus on Python 2.7 and / or creating workarounds by modifying system paths. None worked for me. This solution worked out of the box for Python 3.5 and (I think) is the \"correct\" way of doing it.Hope this saves someone some time!Thanks to \"msoliman\" for his hint, however his answer doesn't give clear solution for those who doesn't have VS2010\nFor example I have VS2012 and VS2013 and there are no such KEYs in system registry. Solution: \nEdit file: \"/Lib/distutils/msvc9compiler.py\"\nChange on line 224:to:and that should workIf you are trying to install matplotlib in order to work with graphs on python. Try this link.\n.\nThis is a set of scripts to build matplotlib from source on the MS Windows platform.To build & install matplotlib in your Python, do:The build script will auto-detect Python version & 32/64 bit automatically.I appreciate this might not be the answer to resolving on 3.4 but I tried a huge variety of things to fix this on 3.4 and thought this might be useful if someone is time pressed or doesn't have the know-how to correct it (in my case, work demands).With exactly the same setup, I found that my installation problems only happened with Python 3.4.  When I changed to 2.7, all my issues seemed to be resolved.  We have a rather overzealous security setup though so I'm going to try the same on my home version (still 3.4) and see if I have any more joy.  My inclination is that my VS version has somehow been restricted and the answers above should help.  If I find anything more tonight I'll add further detail.This is my first reply, not the most technical I'm afraid!"},
{"body": "I have a Django app that requires a  attribute in the form of:Then hooks their post_save signal to update some other fixed model depending on the  defined.I would like to test this behaviour and tests should work even if this app is the only one in the project (except for its own dependencies, no other wrapper app need to be installed). How can I create and attach/register/activate mock models just for the test database? (or is it possible at all?)Solutions that allow me to use test fixtures would be great.You can put your tests in a  subdirectory of the app (rather than a  file), and include a  with the test-only models.Then provide a test-running script () that includes your  \"app\" in . (This doesn't work when running app tests from a real project, which won't have the tests app in , but I rarely find it useful to run reusable app tests from a project, and Django 1.6+ doesn't by default.)(: The alternative dynamic method described below only works in Django 1.1+ if your test case subclasses  - which slows down your tests significantly - and no longer works at all in Django 1.7+. It's left here only for historical interest; don't use it.)At the beginning of your tests (i.e. in a setUp method, or at the beginning of a set of doctests), you can dynamically add  to the INSTALLED_APPS setting, and then do this:Then at the end of your tests, you should clean up by restoring the old version of INSTALLED_APPS and clearing the app cache again. encapsulates the pattern so it doesn't clutter up your test code quite as much.@paluh's answer requires adding unwanted code to a non-test file and in my experience, @carl's solution does not work with django.test.TestCase which is needed to use fixtures.  If you want to use django.test.TestCase, you need to make sure you call syncdb before the fixtures get loaded.  This requires overriding the _pre_setup method (putting the code in the setUp method is not sufficient).  I use my own version of TestCase that let's me add apps with test models.  It is defined as follows:This solution works only for earlier versions of  (before ). You can check your version easily:Original response:It's quite strange but form me works very simple pattern:Below I've put some code which defines Article model which is needed only for tests (it exists in someapp/tests.py and I can test it just with:  ):Unit tests also working with such model definition.I chose a slightly different, albeit more coupled, approach to dynamically creating models just for testing. I keep all my tests in a  subdirectory that lives in my  app. The  file in the  subdirectory contains my test-only models. The coupled part comes in here, where I need to add the following to my  file:I also set db_table in my test model, because otherwise Django would have created the table with the name , which may have caused a conflict with other test models in another app. Here's my my test model:Quoting from :I've figured out a way for test-only models for django 1.7+.The basic idea is, make your  an app, and add your  to .Here's an example:And I have different  for different purposes(ref: ), namely:And in , you can modify ::And make sure that you have set a proper label for your tests app, namely, , set up proper (ref: ).Then, generate db migration byFinally, you can run your test with param . If you use py.test, you can even drop a  file along with django's .I shared my  that I use in my projects. Maybe it helps someone.Two simple steps to create fake model:1) Define model in any file (I usualy define model in test file near a test case)2) Add decorator  to your  or to test function.This decorator creates table in your database before each test and remove the table after test.Also you may / table manually:  / Here's the pattern that I'm using to do this. I've written this method that I use on a subclassed version of TestCase.  It goes as follows:Then, I create a special test-specific models.py file in something like  that's not included in INSTALLED_APPS. In my setUp method, I call create_models_from_app('myapp.tests') and it creates the proper tables.The only \"gotcha\" with this approach is that you don't really want to create the models ever time  runs, which is why I catch DatabaseError.  I guess the call to this method could go at the top of the test file and that would work a little better. Combining your answers, specially @slacy's, I did this:With this, you don't try to create db tables more than once, and you don't need to change your INSTALLED_APPS.If you are writing a reusable django-app, !add both  and  to the , create your models there and it's good to go!I have gone through all these answers as well as django ticket , and I finally went for a totally different approach. \nI wanted my app (somehow extending queryset.values() ) to be able to be tested in isolation; also, my package does include some models and I wanted a clean distinction between test models and package ones.That's when I realized it was easier to add a very small django project in the package!\nThis also allows a much cleaner separation of code IMHO:In there you can cleanly and without any hack define your models, and you know they will be created when you run your tests from in there!If you are not writing an independent, reusable app you can still go this way: create a  app, and add it to your INSTALLED_APPS only in a separate !"},
{"body": "I just installed a linux system (Kubuntu) and was wondering if there is a program to make python programs executable for linux.Just put this in the first line of your script :Make the file executable withExecute withIf you want to obtain a stand-alone binary application in Python try to use a tool like py2exe or .You can use PyInstaller. It generates a build dist so you can execute it as a single \"binary\" file.Python 3 has the native option of create a build dist also:Putting these lines at the starting of the code will tell your operating systems to look up the binary program needed for the execution of the python script i.e it is the python interpreter.So it depends on your operating system where it keeps the python interpreter. As I have Ubuntu as operating system it keeps the python interpreter in  so I have to write this line at the starting of my python script;After completing and saving your codeAnother way to do it could be by creating an alias.\nFor example in terminal write:Writing  will run hello_world.py, but this is only temporary.\nTo make aliases permanent, you have to add them to bashrc, you can edit it by writing this in the terminal:On the top of your code first write: or \nThen create a new  file called  and write:Therefore removing the issue of not having python and installing any missing packages. To make a desktop shortcut do this:Please, note that if your code isn't made to create a GUI you will have to run your code from terminal with "},
{"body": "Is it possible to have overloaded functions in Python? In C# I would do something likeand then when I call the function it would differentiate between the two based on the number of arguments. Is it possible to do something similar in Python?  For the new single dispatch generic functions in Python 3.4, see You generally don't need to overload functions in Python. Python is , and supports optional arguments to functions.in normal python you can't do what you want.  there are two close approximations:however, if you  want to do this, you can certainly make it work (at the risk of offending the traditionalists ;o).  in short, you would write a  function that checks the number of arguments and delegates as appropriate.  this kind of \"hack\" is usually done via decorators.  in this case you could achieve something like:and you'd implement this by making the   (or constructor) return a callable object where the   does the delegation explained above and the   adds extra functions that can be delegated to.you can see an example of something similar here  but that is overloading methods by type.  it's a very similar approach...UPDATE: and something similar (using argument types) is being added to python 3 - Yes, it's possible. I wrote code below in Python 3.2.1:Usage:Note that the lambda returned by the  functions choose function to call depending on number of  arguments.The solution isn't perfect, but at the moment I can't write anything better.Not possible directly. You can use explicit type checks on the arguments given though, although this is generally frowned upon.Python is dynamic. If you are unsure what an object can do, just try: and call a method on it, then except: errors.If you don't need to overload based on types but just on number of arguments, use keyword arguments.overloading methods is tricky in python. However, there could be usage of passing the dict, list or primitive variables.I have tried something for my use cases, this could help here to understand people to overload the methods.Let's take the example use in one of the stackoverflow thread:a class overload method with call the methods from different class.pass the arguments from remote class:OR  So, handling is being achieved for list, Dictionary or primitive variables from method overloading.try it out for your codes"},
{"body": "I have a non-negative int and I would like to efficiently convert it to a big-endian string containing the same data.  For example, the int 1245427 (which is 0x1300F3) should result in a string of length 3 containing three characters whose byte values are 0x13, 0x00, and 0xf3.My ints are on the scale of 35 (base-10) digits.How do I do this? You can use the  module: is a format string.  means big endian and  means unsigned int. Check the documentation for more format chars.In Python 3.2+, you can use :This is fast and works for small and (arbitrary) large ints:Probably the best way is via the built-in :Alternatively -- and I wouldn't usually recommend this, because it's mistake-prone -- you can do it \"manually\" by shifting and the  function:Out of curiosity, why do you only want three bytes? Usually you'd pack such an integer into a full 32 bits (a C ), and use  but skip the  step?Single-source Python 2/3 compatible version based on :Using the  module:Note though that for this method you need to specify the length in bits of the bitstring you are creating.Internally this is pretty much the same as Alex's answer, but the module has a lot of extra functionality available if you want to do more with your data.The shortest way, I think, is the following:This converts an integer to a byte-swapped integer."},
{"body": "Python has a built in function , which is effectively equivalent to:for all types of parameters except strings. It works for numbers and lists, for example:Why were strings specially left out?I seem to remember discussions in the Python list for the reason, so an explanation or a link to a thread explaining it would be fine.: I am aware that the standard way is to do . My question is why the option of using sum for strings was banned, and no banning was there for, say, lists.: Although I believe this is not needed given all the good answers I got, the question is: Python tries to discourage you from \"summing\" strings. You're supposed to join them:It's a lot faster, and uses much less memory.A quick benchmark:Edit (to answer OP's edit): As to why strings were apparently \"singled out\", I believe it's simply a matter of optimizing for a common case, as well as of enforcing best practice: you can join strings much faster with ''.join, so explicitly forbidding strings on  will point this out to newbies.BTW, this restriction has been in place \"forever\", i.e., since the  was added as a built-in function ()You can in fact use  to concatenate strings, if you use the appropriate starting object! Of course, if you go this far you have already understood enough to use  anyway..From :By making  refuse to operate on strings, Python has encouraged you to use the correct method.Here's the source: In the builtin_sum function we have this bit of code:So.. that's your answer.It's explicitly checked in the code and rejected.Short answer: Efficiency.Long answer: The  function has to create an object for each partial sum.Assume that the amount of time required to create an object is directly proportional to the size of its data.  Let N denote the number of elements in the sequence to sum.s are always the same size, which makes 's running time O(1)\u00d7N = . (formerly known as ) is arbitary-length.  Let M denote the absolute value of the largest sequence element.  Then 's worst-case running time is lg(M) + lg(2M) + lg(3M) + ... + lg(NM) = N\u00d7lg(M) + lg(N!) = .For  (where M = the length of the longest string), the worst-case running time is M + 2M + 3M + ... + NM = M\u00d7(1 + 2 + ... + N) = .Thus, ming strings would be much slower than ming numbers. does not allocate any intermediate objects.  It preallocates a buffer large enough to hold the joined strings, and copies the string data.  It runs in  time, much faster than .@dan04 has an excellent explanation for the costs of using  on large lists of strings.The missing piece as to why  is not allowed for  is that many, many people were trying to use  for strings, and not many use  for lists and tuples and other O(n**2) data structures. The trap is that  works just fine for short lists of strings, but then gets put in production where the lists can be huge, and the performance slows to a crawl. This was such a common trap that the decision was made to ignore duck-typing in this instance, and not allow strings to be used with . Moved the parts about immutability to history.Basically, its a question of preallocation. When you use a statement such as and expect it to work similar to a  statement, the code generated looks something likeIn each of these steps a new string is created, which for one might give some copying overhead as the strings are getting longer and longer. But that\u2019s maybe not the point here. What\u2019s more important, is that every new string on each line must be  to it\u2019s specific size (which. I don\u2019t know it it must allocate in every iteration of the  statement, there might be some obvious heuristics to use and Python might allocate a bit more here and there for reuse \u2013 but at several points the new string will be large enough that this won\u2019t help anymore and Python must allocate again, which is rather expensive.A dedicated method like , however has the job to figure out the real size of the string before it starts and would therefore in theory only allocate once, at the beginning and then just fill that new string, which is much cheaper than the other solution.I dont know why, but this works!"},
{"body": "Very basic question - how to get one value from a generator in Python?So far I found I can get one by writing . I just want to make sure this is the right way?Yes, or  in 2.6+.In Python <= 2.5, use . This will work for all Python 2.x versions, but not Python 3.xIn Python >= 2.6, use . This is a built in function, and is clearer. It will also work in Python 3.Both of these end up calling a specially named function, , which can be overridden by subclassing. In Python 3, however, this function has been renamed to , to be consistent with other special functions.This is the correct way to do it.You can also use .In python 3 you don't have gen.next(), but you still can use next(gen).\nA bit bizarre if you ask me but that's how it is.Use (for python 3)Here is an exampleshould print"},
{"body": "This is the raw request for an API call:This request returns a success (2xx) response.Now I am trying to post this request using :Everything looks fine to me and I am not quite sure what I posting wrong to get a 400 response. is for GET-style URL parameters,  is for POST-style body information. It is perfectly legal to provide  types of information in a request, and your request does so too, but you encoded the URL parameters into the URL already.Your raw post contains  data though, you better use the  module to properly encode that:You could split out the URL parameters too:Then post this with:If you are using  version 2.4.2 or newer, you can have the library do the JSON encoding for you; it'll set the correct Content-Header too; all you need to do is pass in the Python object to be encoded as JSON into the  keyword argument:Set data to this: Assign the response to a value and test the attributes of it. These should tell you something useful."},
{"body": "Jinja2 and Mako are both apparently pretty fast.How do these compare to (the less featured but probably good enough for what I'm doing) string.Template ?Here are the results of the popular template engines for rendering a 10x1000 HTML table.The benchmark is based on  with some added template engines and added iterations to increase accuracy. The list and generator concat at the end are hand coded Python to get a feel for the upper limit of performance achievable by compiling to Python bytecode. The optimized versions use string interpolation in the inner loop.But before you run out to switch your template engine, make sure it matters. You'll need to be doing some pretty heavy caching and really optimized code before the differences between the compiling template engines starts to matter. For most applications good abstraction facilities, compatibility with design tools, familiarity and other things matter much much more.From the , it seems that string.Template is the fastest if that's all you need.If you can throw caching in the mix (like memcached) then choose based on features and ease of use rather than optimization.I use Mako because I like the syntax and features. Fortunately it is one of the fastest as well.In general you will have to do profiling to answer that question, as it depends on how you use the templates and what for.string.Template is the fastest, but so primitive it can hardly be called a template in the same breath as the other templating systems, as it only does string replacements, and has no conditions or loops, making it pretty useless in practice.I think Cheetah might be the fastest, as it's implemented in C."},
{"body": "I have what I think should be a very easy task that I can't seem to solve.How do I write a Python dictionary to a csv file?  All I want is to write the dictionary keys to the top row of the file and the key values to the second line.The closest that I've come is the following (that I got from somebody else's post):The problem is, the above code seems to only be writing the keys to the first line and that's it.  I'm not getting the values written to the second line.Any ideas?You are using  which expects a list of dicts, not a dict. You want  to write a single row.You will also want to use  if you want a header for you csv file.You also might want to check out  for . It's not only more pythonic and readable but handles closing for you, even when exceptions occur.Example with these changes made:Which produces:Your code was  close to working.  Try using a regular  rather than a .  The latter is mainly used for writing a list of dictionaries.Here's some code that writes each key/value pair on a separate row:If instead you want all the keys on one row and all the values on the next, that is also easy::  When developing code like this, set the writer to  so you can more easily see what is being generated.  When the logic is perfected, switch back to .Why use csv for this simple task?I use python in two ways:My answer is only useful for the 2nd purpose. And also considers the pain to remember all these modules and their individual functions :P"},
{"body": "I would like to include image in a jupyter notebook.If I did the following, it works :But I would like to include the images in a markdown cell and the following code gives a 404 error :I also triedBut I still get the same error :You mustn't use quotation marks around the name of the image files in markdown!If you carefully read your error message, you will see the two  parts in the link. That is the html encoded quotation mark. You have to change the lineto There are several ways to post an image in Jupyter notebooks:You retain the ability to use HTML tags to resize, etc...You can also display images stored locally, either via relative or absolute path. use  to disable max-width confinement of the imageas shown by @cristianmtr\nPaying attention not to use either these quotes  or those  around the url.demonstrated by @SebastianAlternatively, you can use a plain HTML , which allows you to change height and width and is still read by the markdown interpreter:Here's how you can do it with Markdown:If you want to use the Jupyter Notebook API (and not the IPython one anymore), I find the  Jupyter's sub-project. You have an  widget. Docstring specifies that you have a  parameter which is a bytes. So you can do:I agree, it's simpler to use the Markdown style. But it shows you the Image display Notebook API. You can also resize the image with the  and  parameters.Here is a Solution for  and :I droped my images in a folder named . \nMy directory is:To show the image I used this expression:Also watch out for  and  I'm surprised no one here has mentioned the html cell magic option.\nfrom the  (IPython, but same for Jupyter)"},
{"body": "I am working on a project using Hadoop and it seems to natively incorporate Java and provide streaming support for Python. Is there is a significant performance impact to choosing one over the other?  I am early enough in the process where I can go either way if there is a significant performance difference one way or the other.Java is less dynamic than Python and more effort has been put into its VM, making it a faster language. Python is also held back by its Global Interpreter Lock, meaning it cannot push threads of a single process onto different core.Whether this makes any significant difference depends on what you intend to do. I suspect both languages will work for you.With Python you'll probably develop faster and with Java will definitely run faster.Google \"benchmarksgame\" if you want to see some very accurate speed comparisons between all popular languages, but if I recall correctly you're talking about 3-5x faster.That said, few things are processor bound these days, so if you feel like you'd develop better with Python, have at it!In reply to comment (how can java be faster than Python):All languages are processed differently.  Java is about the fastest after C & C++ (which can be as fast or up to 5x faster than java, but seems to average around 2x faster).  The rest are from 2-5+ times slower.  Python is one of the faster ones after Java.  I'm guessing that C# is about as fast as Java or maybe faster, but the benchmarksgame only had Mono (which was a tad slower) because they don't run it on windows.Most of these claims are based on the  which tends to be pretty fair because advocates of/experts in each language tweak the test written in their specific language to ensure the code is well-targeted.For example,  shows all tests with Java vs c++ and you can see the speed ranges from about equal to java being 3x slower (first column is between 1 and 3), and java uses much more memory!Now  shows java vs python (from the point of view of Python).  So the speeds range from python being 2x slower than Java to 174x slower, python generally beats java in code size and memory usage though.Another interesting point here--tests that allocated a lot of memory, Java actually performed significantly better than Python in memory size as well.  I'm pretty sure java usually loses memory because of the overhead of the VM, but once that factors out, java is probably more efficient than most (again, except the C's).This is Python 3 by the way, the other python platform tested (Just called Python) faired much worse.If you really wanted to know  it is faster, the VM is amazingly intelligent.  It compiles to machine language AFTER running the code, so it knows what the most likely code paths are and optimizes for them.  Memory allocation is an art--really useful in an OO language. It can perform some amazing run-time optimizations which no non-VM language can do.  It can run in a pretty small memory footprint when forced to, and is a language of choice for embedded devices along with C/C++.I worked on a Signal Analyzer for Agilent (think expensive o-scope) where nearly the entire thing (aside from the sampling) was done in Java.  This includes drawing the screen including the trace (AWT) and interacting with the controls.Currently I'm working on a project for all future cable boxes.  The Guide along with most other apps will be written in Java.  Why wouldn't it be faster than Python? You can write Hadoop mapreduce transformations either as \"streaming\" or as a \"custom jar\". If you use streaming, you can write your code in any language you like, including Python or C++. Your code will just read from STDIN and output to STDOUT.  However, on hadoop versions before 0.21, hadoop streaming used to only stream text - not binary - to your processes.  Therefore your files needed to be text files, unless you do some funky encoding transformations yourself. But now it appears a  has been added that now allows the use of binary formats with hadoop streaming.If you use a \"custom jar\" (i.e. you wrote your mapreduce code in Java or Scala using the hadoop libraries), then you will have access to functions that allow you to input and output binary (serialize in binary) from your streaming processes (and save the results to disk). So future runs will be much faster (depending on how much your binary format is smaller than your text format).So if your hadoop job is going to be I/O bound, then the \"custom jar\" approach will be faster (since both Java is faster as previous posters have shown and reading from disk will also be faster).But you have to ask yourself how valuable is your time. I find myself far more productive with python, and writing map-reduce that reads STDIN and writes to STDOUT is really straightforward. So I personally would recommend going the python route - even if you have to figure the binary encoding stuff out yourself. Since hadoop 0.21 handles non-utf8 byte arrays, and since there is a binary (byte array) alternative to use for python (), which shows the python code only being about 25% slower than the \"custom jar\" java code, I would definitely go the python route."},
{"body": "The simple task of adding a row to a  object seems to be hard to accomplish. There are 3 stackoverflow questions relating to this, none of which give a working answer.Here is what I'm trying to do. I have a DataFrame of which I already know the shape as well as the names of the rows and columns.Now, I have a function to compute the values of the rows iteratively. How can I fill in one of the rows with either a dictionary or a  ? Here are various attempts that have failed:Apparently it tried to add a column instead of a row.Very uninformative error message.Apparently that is only for setting individual values in the dataframe.Well, I don't want to ignore the index, otherwise here is the result:It did align the column names with the values, but lost the row labels.That also failed miserably.So how do you do it ? will set a columnsince you want to set a row, use Note that  is equivalent here, yours failed because you tried to assign a dictionary\nto each element of the row  probably not what you want; converting to a Series tells pandas\nthat you want to align the input (for example you then don't have to to specify all of the elements)More simpler versionMy approach was, but I can't guarantee that this is the fastest solution."},
{"body": "I'm fairly new to actual programming languages, and Python is my first one. I know my way around Linux a bit, enough to get a summer job with it (I'm still in high school), and on the job, I have a lot of free time which I'm using to learn Python.One thing's been getting me though. What exactly is different in Python when you have expressions such asI know what methods do and stuff, and I get what they do, but my question is: How are those double underscore methods above different from their simpler looking equivalents?P.S., I don't mind being lectured on programming history, in fact, I find it very useful to know :) If these are mainly historical aspects of Python, feel free to start rambling.Well, power for the programmer is good, so there should be a way to customize behaviour. Like operator overloading (, , , ...), attribute access (,  (those two are differnt), , ...) etc. In many cases, like operators, the usual syntax maps 1:1 to the respective method. In other cases, there is a special procedure which at some point involves calling the respective method - for example,  is only called if the object doesn't have the requested attribute and  is not implemented or raised AttributeError. And some of them are really advanced topics that get you deeeeep into the object system's guts and are rarely needed. So no need to learn them all, just consult the reference when you need/want to know. Speaking of reference, .Here is the creator of Python :...When you start a method with two underscores (and no trailing underscores), Python's  rules are applied. This is a way to loosely simulate the  keyword from other OO languages such as C++ and Java. (Even so, the method is still technically not private in the way that Java and C++ methods are private, but it is \"harder to get at\" from outside the instance.)Methods with two leading and two trailing underscores are considered to be \"built-in\" methods, that is, they're used by the interpreter and are generally the concrete implementations of overloaded operators or other built-in functionality.They are used to specify that the Python interpreter should use them in specific situations.E.g., the  function allows the  operator to work for custom classes. Otherwise you will get some sort of not defined error when attempting to add. From an historical perspective, leading underscores have often been used as a method for indicating to the programmer that the names are to be considered internal to the package/module/library that defines them.  In languages which do not provide good support for private namespaces, using underscores is a convention to emulate that.  In Python, when you define a method named '__foo__' the maintenance programmer knows from the name that something special is going on which is not happening with a method named 'foo'.  If Python had choosen to use 'add' as the internal method to overload '+', then you could never have a class with a method 'add' without causing much confusion.  The underscores serve as a cue that some magic will happen. A number of other questions are now marked as duplicates of this question, and at least two of them ask what either the  methods are called, or what the convention is called, and none of the existing answers cover that, so:There actually is no official name for either. Many developers unofficially call them \"dunder methods\", for \"Double UNDERscore\". Some people use the term \"magic methods\", but that's somewhat ambiguous between meaning dunder methods, special methods (see below), or something somewhere between the two.There  an official term \"special attributes\", which overlaps closely but not completely with dunder methods. The  chapter in the reference never quite explains what a special attribute is, but the basic idea is that it's at least one of the following:Most special attributes are methods, but not all (e.g.,  isn't). And most use the \"dunder\" convention, but not all (e.g., the  method on iterators in Python 2.x).And meanwhile, most dunder methods are special attributes, but not all\u2014in particular, it's not that uncommon for stdlib or external libraries to want to define their own protocols that work the same way, like the  protocol. Python was influenced by ,  possibly used Algol68 at the  where Algol68 has a similar \"\" called \"Quote stropping\". In Algol68 the   appear in a different typeface (usually ****, or __), in sourcecode files this typeface is achieved with quotes, e.g. 'abs' (quoting similar to quoting in '')In Algol68 these were refered to as , e.g. , but \"under-under-abs\" __abs__ in python.My 2 cents:  \u00a2 So sometimes \u2014 like a ghost \u2014 when you cut and paste python classes into a wiki you will magically reincarnate Algol68's  keywords. \u00a2"},
{"body": "I'm trying to understand how  works.I see in their documentation that they use a pre-populated list of users. I want to play with a database-stored users list.However, I don't understand some things in this  module.This code will be called at every request? This is used to load all the details of my user object?For now, I have this code:When I access /make-login, I want to log in.My user class:Also, I wrote another two functions for authenticate/registerI don't know how to make this  work with MySQL. Also, I don't know if the user are logged-in. How can I get the user ID or the username?Anyone can explain me in some rows how this  works?Flask-login doesn't actually have a user backend, it just handles the session machinery to help you login and logout users. You have to tell it (by decorating methods), what represents a user and it is also up to you to figure out how to know if a user is \"active\" or not (since being \"active\" can mean different things in different applications).You should read the  and be sure what it does . Here I am only going to concentrate on wiring it up with the db backend.To start off with, define a user object; which represents properties for your users. This object can then query databases, or LDAP, or whatever and it is the hook that connects the login mechanism with your database backend.I will be using the  script for this purpose.Once you have the user object created, you need to write a method that loads the user (basically, creates an instance of the  class from above). This method is called with the user id.Once you have these steps, your login method does this:Flask-login will try and load a user BEFORE every request. So yes, your example code below will be called before every request. It is used to check what userid is in the current session and will load the user object for that id.If you look at the Flask-login source code on github, there is a line under function init_app which goes:So before every request, the _load_user function is called. The _load_user functions actually calls another function \"reload_user()\" based on conditions. And finally, reload_user() function calls your callback function that you wrote (load_user() in your example).Also, flask-login only provides the mechanism to login/logout a user. It does not care if you are using mysql database. You might want to use , which combines Flask-Login with SQLAlchemy for database access and automates much of the back-end handling of user records.The  tutorial will get you started. Set app.config['SQLALCHEMY_DATABASE_URI'] to your MySQL database connection string.As per from the 's document a user object must be returned and if the user id is not found it should return None instead of Exception.Here is a Flask example of using login:   You need to use @login_required for every method that requires login.  For example, "},
{"body": "I have a model that I would like to contain a subjects name and their initials.  (The data is somewhat anonymized and tracked by initials.)  Right now, I wrote As indicated by the last line, I would prefer to be able to have the initials actually get stored in the database as a field (independent of name), but that is initialized with a default value based on the name field.  However, I am having issues as django models don't seem to have a 'self'.  If I change the line to , I can do the syncdb, but can't create new subjects.Is this possible in django, having a callable function give a default to some field based on the value of another field?(For the curious, the reason I want to separate my store initials separately is in rare cases where weird last names may have different than the ones I am tracking.  E.g., someone else decided that Subject 1 Named \"John O'Mallory\" initials are \"JM\" rather than \"JO\" and wants to fix edit it as an administrator.)Models certainly do have a \"self\"!  It's just that you're trying to define an attribute of a model class as being dependent upon a model instance; that's not possible, as the instance does not (and cannot) exist before your define the class and its attributes.To get the effect you want, override the save() method of the model class.  Make any changes you want to the instance necessary, then call the superclass's method to do the actual saving.  Here's a quick example.This is covered in  in the documentation.I don't know if there is a better way of doing this, but you can use a  handler:"},
{"body": "I've been successfully using Python properties, but I don't see how they could work. If I dereference a property outside of a class, I just get an object of type :But if I put a property in a class, the behavior is very different:I've noticed that unbound  is still the  object, so class instantiation must be doing the magic, but what magic is that?As others have noted, they use a language feature called descriptors.The reason that the actual property object is returned when you access it via a class  lies in how the property implements the  special method. If a descriptor is accessed on an , then that instance is passed as the appropriate argument, and  is the  of that instance.On the other hand, if it is accessed through the class, then  is None and only  is passed. The  object recognizes this and returns .Besides the , see also the documentation on  and  in the Language Guide.In order for @properties to work properly the class needs to be a subclass of . \nwhen the class is not a subclass of  then the first time you try access the setter it actually makes a new attribute with the shorter name instead of accessing through the setter. The following does  work correctly.The following will work correctlyProperties are , and descriptors behave specially when member of a class instance.  In short, if  is an instance of type , and  is a descriptor, then  is equivalent to .The  object just implements the descriptor protocol: "},
{"body": "I was planning to move from Django to Pylons, but then I bumped into Pyramid.What are the differences between Pylons and Pyramid?I read some text in , which currently covers Pylons 0.9.7, and wonder if it is a to start for Pylons and Pyramid.Pylons isn't being \"cancelled\", and it will continue to receive updates. That said, the \"future\" per se is in Pyramid. On the mailing list is has been referred to as Pylons 2.0. It is fully tested and better documented than Pylons 1.0, so you might as well jump aboard if you're fresh.Pyramid is essentially the merger of Pylons and Repoze.bfg.  Read more about this on the  email list.Especially, in  introductory email. If you have deeper questions, hang out in the #pylons, #pyramid, or #repoze irc (freenode) channels.You want to go with Pyramid most probably. It may seem like it's a more complicated system, but it's actually much cleaner and very intuitive. Furthermore, the documentation for the project is simply awesome and in my opinion, even better then the django project which is well known for it's documentation.That does not make it \"like django\" however. It is still lightly coupled and you can use whatever template, database, form, session management system, etc that you like.It's worth noting that Pyramid is a base framework and Pylons is being built on top of it.If you plan to start new project, migrate or just to learn framework I recommend to use Pyramid.Pylons will stop it's development.\nMeanwhile Pyramid is a continuation of Pylons. Thus, it's code is stable. It includes most features from Pylons, adds some new usefull features."},
{"body": "I was looking through my codebase today and found this:It seems to take a dict of parameters and turn them into a list of parameters for a shell command. It looks like it's using yield inside a generator comprehension, which I thought would be impossible...?How does it work?Since Python 2.5,  is an expression, not a statement.  See .The code is hideously and unnecessarily ugly, but it's legal.  Its central trick is using  inside the generator expression.  Here's a simpler example of how this works:Basically, using  in the generator expression causes it to produce two values for every value in the source iterable.  As the generator expression iterates over the list of strings, on each iteration, the  first yields a string from the list.  The target expression of the genexp is , so for every value in the list, the \"result\" of the generator expression is the value of .  But  just ignores its argument and always returns the option string .  So on every step through the generator, it yields first the key-value string (e.g., ), then .  The outer  just makes a list out of this generator and then reverses it so that the s will come before each option instead of after.However, there is no reason to do it this way.  There are a number of much more readable alternatives.  Perhaps the most explicit is simply:Even if you like terse, \"clever\" code, you could still just doThe  list comprehension itself is a bizarre mix of attempted readability and unreadability.  It is more simply written:You should consider arranging an \"intervention\" for whoever put this in your codebase.Oh god. Basically, it boils down to this,:So when iterated over, thegenerator yields  (a member of ) and then the return value of , which is always , all in one iteration over . Whatever  returns and what gets passed to  is ignored.Equivalents:There are lots of ways to do this much simpler, of course. Even with the original double-yield trick, the entire thing could've been "},
{"body": "Why is numpy giving this result:when I'd expect it to do this:Clearly my understanding of the function is lacking.According to  indicates that the smallest element is at index 2, the next smallest at index 3, then index 1, then index 0.There are  to get the result you are looking for:For example,This checks that they all produce the same result:These IPython  benchmarks suggests for large arrays  is the fastest:For small arrays,  may be faster:Note also that  gives you more control over how to handle elements of equal value. As  says, :That means the first element of the argsort is the index of the element that should be sorted first, the second element is the index of the element that should be second, etc.What you seem to want is the rank order of the values, which is what is provided by .  Note that you need to think about what should happen if there are ties in the ranks."},
{"body": "Is there a ready-to-use English grammar that I can just load it and use in NLTK? I've searched around examples of parsing with NLTK, but it seems like that I have to manually specify grammar before parsing a sentence. Thanks a lot!You can take a look at , a simple python statistical parser that returns NLTK parse Trees. It comes with public treebanks and it generates the grammar model only the first time you instantiate a Parser object (in about 8 seconds). It uses a CKY algorithm and it parses average length sentences (like the one below) in under a second.My library, , provides a high performance dependency parser.Installation:Usage: found spaCy to be the fastest dependency parser available. It processes over 13,000 sentences a second, on a single thread. On the standard WSJ evaluation it scores 92.7%, over 1% more accurate than any of CoreNLP's models.There is a Library called . It is quite fast and easy to use.There are a few grammars in the  distribution. In your Python interpreter, issue .Use the MaltParser, there you have a pretrained english-grammar, and also some other pretrained languages.\nAnd the Maltparser is a dependency parser and not some simple bottom-up, or top-down Parser.Just download the MaltParser from  and use the NLTK like this:I've tried NLTK, PyStatParser, Pattern. IMHO Pattern is best English parser introduced in above article. Because it supports pip install and There is a fancy document on the website (). I couldn't find reasonable document for NLTK (And it gave me inaccurate result for me by its default. And I couldn't find how to tune it). pyStatParser is much slower than described above in my Environment. (About one minute for initialization and It took couple of seconds to parse long sentences. Maybe I didn't use it correctly).  "},
{"body": "As I understand the g variable in Flask, it should provide me with a global place to stash data like holding the current user after login.  Is this correct?I would like my navigation to display my user's name, once logged in, across the site.My views containDuring login, I assign It doesn't seem I can access g.user.  Instead, when my base.html template has the following...I get the error:The login otherwise works fine.  What am I missing? is a  and is per-request (See ).  The  is  a thread local, but in the default context is persisted to a MAC-signed cookie and sent to the client.The problem that you are running into is that  is rebuilt on each request (since it is sent to the client and the client sends it back to us), while data set on  is only available for the lifetime of  request.The  thing to do (note  - if you need secure take a look at ) is to simply add the user's ID to the session and load the user on each request:Minor correction, the g object is bound to the application context now instead of the request context.I would try to get rid of globals all together, think of your applications as a set of functions that perform tasks, each function has inputs and outputs, and should not touch globals. Just fetch your user and pass it around, it makes your code much more testable. Better yet: get rid of flask, flask promotes using globals such as "},
{"body": "I've asked  before about killing a process that uses too much memory, and I've got most of a solution worked out.However, there is one problem: calculating massive numbers seems to be untouched by the method I'm trying to use. This code below is intended to put a 10 second CPU time limit on the process.What I  to see when I run this script (on a Unix machine) is this:Instead, I get no output. The only way I get output is with  + , and I get this if I  +  after 10 seconds:If I  +   10 seconds, then I have to do it twice, and the console output looks like this:In the course of experimenting and trying to figure this out, I've also put  between the print and large number calculation. It doesn't seem to have any effect. If I change  to , then the print and sleep statements work as expected. Adding  to the print statement or  after the print statement don't work either. How can I fix or at least mitigate this?Additional information:Python version: Linux information:  Python precomputes constants in the code. If any very large number is calculated with at least one intermediate step, the process  be CPU time limited.It took quite a bit of searching, but I have discovered evidence that Python 3  precompute constant literals that it finds in the code before evaluating anything. One of them is this webpage: . I've quoted some of it below.The fact that  is used as an example very strongly suggests that not only small constants are being precomputed and cached, but also any constant literals in the code. I also found  on another Stack\u00a0Overflow question ():These are supported by the fact that replacingwith this  hangs, even though I never call the function!Luckily for me, I don't have any such giant literal constants in my code. Any computation of such constants will happen later, which can be and is limited by the CPU time limit. I changedto this,and got this output, as desired! Limiting a process by CPU time or memory consumption (or some other method)  if there is not a large literal constant in the code that Python tries to precompute.Use a function.It does seem that Python tries to precompute integer literals (I only have empirical evidence; if anyone has a source please let me know). This would normally be a helpful optimization, since the vast majority of literals in scripts are probably small enough to not incur noticeable delays when precomputing. To get around this, you need to make your literal be the result of a non-constant computation, like a function call with parameters.Example:This gives the expected result:"},
{"body": "I get this error:when running my code, I don't really see what I'm doing wrong here though:You are calling the wrong class name in your super() call:Essentially what you are resolving to is the  of the object base class which takes no params.Its a bit redundant, I know, to have to specify the class that you are already inside of, which is why in python3 you can just do:  "},
{"body": "I'm trying to replicate (and if possible improve on) Python 2.x's sorting behaviour in 3.x, so that mutually orderable types like ,  etc. are sorted as expected, and mutually unorderable types are grouped within the output.Here's an example of what I'm talking about:My previous attempt at this, using a class for the key parameter to  (see \n) is fundamentally broken, because its approach ofcan lead to intransitive ordering, as explained by .A na\u00efve approach, which I initially rejected without even trying to code it, would be to use a key function that returns a  tuple:However, this doesn't do what I want. In the first place, it breaks the natural ordering of mutually orderable types:Secondly, it raises an exception when the input contains two objects of the same intrinsically unorderable type:... which admittedly is the standard behaviour in both Python 2.x and 3.x \u2013 but ideally I'd like such types to be grouped together (I don't especially care about their ordering, but it would seem in keeping with Python's guarantee of stable sorting that they retain their original order).I can work around the first of these problems for numeric types by special-casing them:... which works as far as it goes:... but doesn't account for the fact that there may be other distinct (possibly user-defined) types which are mutually orderable, and of course still fails with intrinsically unorderable types:Is there another approach which solves  the problem of arbitrary, distinct-but-mutually-orderable types  that of intrinsically unorderable types?Stupid idea: make a first pass to divide all the different items in groups that can be compared between each other, sort the individual groups and finally concatenate them. I assume that an item is comparable to all members of a group, if it is comparable with the first member of a group. Something like this (Python3):This will have quadratic running time in the pathetic case that none of the items are comparable, but I guess the only way to know that for sure is to check all possible combinations. See the quadratic behavior as a deserved punishment for anyone trying to sort a long list of unsortable items, like complex numbers. In a more common case of a mix of some strings and some integers, the speed should be similar to the speed of a normal sort. Quick test:It seems to be a 'stable sort' as well, since the groups are formed in the order the incomparable items are encountered.This answer aims to faithfully re-create the Python 2 sort order, in Python 3, in every detail.The actual Python 2 implementation is quite involved, but  does the final fallback after instances have been given a chance to implement normal comparison rules. This is after individual types have been given a chance to compare (via the  or  hooks).Implementing that function as pure Python in a wrapper, plus emulating the exceptions to the rules ( and complex numbers specifically) gives us the same Python 2 sorting semantics in Python 3:I incorporated , since that'd be supported by the type itself via a  hook. I've stuck to the Python 2 ordering for the keys and values as well, naturally.I've also added special casing for complex numbers, as Python 2 raises an exception when you try sort to these:You may have to add more special cases if you want to emulate Python 2 behaviour exactly.If you wanted to sort complex numbers  you'll need to consistently put them with the non-numbers group; e.g.:Some test cases:Not running Python 3 here, but maybe something like this would work.  Test to see if doing a \"less than\" compare on \"value\" creates an exception and then do \"something\" to handle that case, like convert it to a string.Of course you'd still need more special handling if there are other types in your list that are not the same type but are mutually orderable.To avoid the use of exceptions and going for a type based solution, i came up with this:Note that an additional dictionary to hold the different types in list and a type holding variable (notImpl) is needed. Further note, that floats and ints aren't mixed here.Output:One way for Python 3.2+ is to use .\nWith this you can quickly implement a solution that tries to compare the values and then falls back on comparing the string representation of the types. You can also avoid an error being raised when comparing unordered types and leave the order as in the original case:Examples (input lists taken from ):This has the disadvantage that the three-way compare is always conducted, increasing the time complexity. However, the solution is low overhead, short, clean and I think  was developed for this kind of Python 2 emulation use case.We can solve this problem in the following way.We can get a deterministic and orderable key function from types by using . Note that the 'type hierarchy' here is determined by the repr of the types themselves. A flaw in this method is that if two types have identical  (the types themselves, not the instances), you will 'confuse' types. This can be solved by using a key function that returns a tuple , but I have not implemented that in this solution.The advantage of my method over Bas Swinkel's is a cleaner handling of a group of un-orderable elements. We do not have quadratic behavior; instead, the function gives up after the first attempted ordering during sorted()).My method functions worst in the scenario where there are an extremely large number of different types in the iterable. This is a rare scenario, but I suppose it could come up.I'd like to recommend starting this sort of task (like imitation of another system's behaviour very close to this one) with detailed clarifying of the target system. How should it work with different corner cases. One of the best ways to do it - write a bunch of tests to ensure correct behaviour. Having such tests gives:One can write such test cases:Next one may have such sorting function:Usage is quite simple and is documented in tests:Here is one method of accomplishing this:"},
{"body": "What is the typical underlying data structure used to implement Python's built-in list data type?See also:\nBtw, I find it interesting that the Python tutorial on data structures recommends using pop(0) to simulate a queue but does not mention O(n) or the deque option.CPython:As can be seen on the following line, the list is declared as an array of pointers to .In the , it's an ."},
{"body": "I started learning a bit of python and would now like to toy around a bit with gui-building. Qt seems to be a good choice because of its cross-platformishness.\nNow there seem to be two bindings available: PyQt by Riverbank Computing and PySide, originally developed by Nokia.\nSo which one should I choose? All I can find are two year old feature comparisons, but what differences are there nowadays?\nWhich one is easier to use, has more/better documentation? Are both still in active development?\nLicensing isn't of much concern to me since I don't intend to write commercial applications.Both toolkits are actively maintained, and by now more or less equal in features and quality.  There are only few, rather unimportant differences.Still, I'd recommend PySide for Python 2.  It has a more reasonable API, mainly it doesn't expose Qt types, which have a direct equivalent in Python (e.g. QString, QList, etc.) or which are completely superfluous due to Python's dynamic nature, like QVariant.  This avoids many tedious conversions to and from Qt types, and thus eases programming and avoids many errors.  PyQt also supports this modern API, and uses it by default for Python 3, but not for Python 2 to maintain backwards compatibility.There is also the licensing difference.  PySide is LGPL while PyQt is GPL.  This could make a difference if you don't wish to make your project opensource. Although PyQt always has the propriety version available for a fairly reasonable price.I tend to find the PySide documentation more intuitive.  The API, in my opinion is slightly more Pythonic and the rate of bug fixes is quite impressive at the moment.PyQt has the advantage of Python 3 support and incumbency.  There is a lot more 3rd party documentation/tutorials for it.I recently ported a significant code base (over 8,000 lines of code) from PyQt to PySide.Right now I'd say PyQt is a much more mature, performant and stable project. I hit a number of bugs in PySide, and suspect that any big project will hit issues. Having said that, I reported a bug to the project and it was fixed and in a new release within a few weeks. I'm also having a problem where the app takes about 15 seconds to quit. I've not yet spent the time to find out why. However it's only a matter of time before there will be no reasons for choosing PyQt over PySide.If you do decide to go with PyQt for now, make sure you use API v2 throughout. It is a better API, and will ease any future transition to PySide. Also if you do port, just follow the guidelines on the PySide wiki. Even for an 8+ kloc app consisting of about 20 source files it just took an afternoon.An important fact is that PyQt4 has two versions of its APIs for some things. Version 1 items are such things as using  instead of , and  (basically just a wrapper, I believe - I've never actually done anything which uses it) instead the wrapped. Version 2, which can be enabled in Python 2 and is enabled in Python 3, is much better (though still unpythonic in many places - PySide is too, but it's getting distinctly better. There are still some remaining incompatibilities with them; PyQt4 has , PySide has .For a project of my own, I decided that I wanted to support both with no changes to the code. I prefer PySide, but on Windows I distribute with PyQt4 as at present it's quite a bit smaller for distribution at present. My solution is to check for PySide and if it's there insert an import hook to redirect PyQt4 imports to PySide, or if it's not, fix up PyQt4 to work like it should.The files used:Then you just  and  (as in  in that repository). And after that you can just .Aside: it was also stated a few days ago on the PySide mailing list that they are planning on supporting Python 3 fully within the next few months.Although they might have similar interface for Qt/C++ classes, their interface for Qt/C++ macros such as signal/slot/property are very different.\nPorting one to another is not an easy job. It would be better to make the right decision at the very beginning.Beyond the grammar/license differences, I just want to point out some deficiency of PyQt in language binding, which might be essential to write QML project in Python.\nThese differences finfally push me to PySide from PyQt.I have a 20k line Python app that I unsuccessfully tried to convert to PySide.\nConversion is easy and most of the functionality works.\nThere are several methods that are not implemented because they are 'deprecated', so I had to fix those.  That was OK.\nOn Windows, using PySide-1.1.2, the '==' operator is not implemented for many Qt objects.  One workaround is to say: \"if id(item1) == id(item2):\".\nThe other observation is that PySide seemed noticeably slower.  I did not isolate PySide as the cause of the slowness, but the problem went away when I reverted back to PyQt.Lastly, as of now, the Android kit with PySide does not seem ready for prime time."},
{"body": "I have a list in Python\ne.g.I want to print the array in a single line without the normal \" []Will give the output as;That is not the format I want instead I want it to be like this;Note: It must be in a single row.This, like it sounds, just takes all the elements of the list and joins them with .General solution, works on arrays of non-strings:Here is a simple one. the star unpacks the list and return every element in the list. This is what you needIf the input array is  then you need to first convert array into  array and then use  method for joining with  or  whatever you want. e.g:Direct using of join which will join the integer and string will throw error as show above.You need to loop through the list and use to keep it on one lineI don't know if this is efficient as others but simple logic always works:Output:There are two answers , First is use 'sep' settingThe other is belowdisplaysSam, Peter, James, Julian, Ann"},
{"body": "I have installed firefox and Xvfb on my centos6.4 server to use selenium webdriver.But, when I run the code, I got an error.I read some related pages on stackoverflow and someone suggested to remove all files in tmp folder, so I did it. But, it still doesn't work.Could anyone please give me a help?Thank you in advance!for Googlers, this answer didn't work for me, and I had to use  instead. I am using AWS Ubuntu. Basically, I needed to install Xvfb and then pyvirtualdisplay:Once I had done that, this python code worked:Thanks to @That1Guy for the first answerI was running into this on an (headless) Ubuntu 14.04 server with Jenkins and xvfb installed. I had installed the latest stable Firefox (47) which started a build failing that ran a python script which used the Firefox driver for selenium (version 2.53).Apparently Firefox 47+ is not compatible with the driver used in Selenium 2.53, and Selenium 3+ will be using a new driver called \"Marionette\" or \"Gecko Driver\" (which isn't officially released yet).This page explains how to use the new driver pretty well, in several languages: Basically:For Python, step 4 looked something like the following for me:I too had faced same problem.  I was on Firefox 47 and Selenium 2.53; I downgraded Firefox to 45. This worked.Check your  environment variable. Run  in the command line.If nothing is printed, then you are running FireFox without any DISPLAY assigned. You should assign one! Run  in the command line before running your python script.Check this thread for more information: I think the simplest solution here is just run Python with :Rollback your Firefox to the previous working version. I suggest 2 versions back. Disable Firefox Maintenance Service. I was working on a solution and the Firefox Maintenance Service updated Firefox to the latest build in the background. This broke my code and it was giving me this error. Now it is fixed!Thank you everyone!This error is due to your Xvfb is not running. So restart your xvfb:then check.\nThis works for me.Instead of downgrading firefox from 47 version to 45 or something I'll suggest to upgrade to  or above since they seem to fix an issue.But if your OS doesn't have new packages in repo (for example Ubuntu 14.04 in time of this answer), you can use debs from ubuntuzilla project:For x86 use  postfix.\nThat sold problem for meI fixed this by running a recursive chown against not only the python script using selenium, but against the entire virtualenv that script was running in. I changed the ownership to the user running the file. After that, this error went away.I also faced the same issue, what I did was:update your selenuim version ---> pip install -U seleniumIt can be solved by changing the file permission of the output file ( or related files to the program).\nI used Firefox's webdriver.  Try:This solved me the same trouble you have."},
{"body": "I have a simple script which parses a file and loads it's contents to a database. I don't need a UI, but right now I'm prompting the user for the file to parse using  which is most unfriendly, especially because the user can't copy/paste the path.  I would like a quick and easy way to present a file selection dialog to the user, they can select the file, and then it's loaded to the database.  (In my use case, if they happened to chose the wrong file, it would fail parsing, and wouldn't be a problem even if it was loaded to the database.)This code is close to what I want, but it leaves an annoying empty frame open (which isn't able to be closed, probably because I haven't registered a close event handler).I don't have to use tkInter, but since it's in the Python standard library it's a good candidate for quickest and easiest solution.Whats a quick and easy way to prompt for a file or filename in a script without any other UI?Tkinter is the easiest way if you don't want to have any other dependencies.\nTo show only the dialog without any other GUI elements, you have to hide the root window using the  method:You can use :To install , you can use :It is a single pure Python module () that uses .Try with :If you don't need the UI or expect the program to run in a CLI, you could parse the filepath as an argument. This would allow you to use the autocomplete feature of your CLI to quickly find the file you need.This would probably only be handy if the script is non-interactive besides the filepath input.Check out EasyGUI, a very easy to use module that should do the job - You would use the fileopenbox function - "},
{"body": "I am receiving the following error:Error says that  is local variable but i thought that this variable is globalSo is it global or local and how to solve this error without passing global  as argument to ?In order for you to modify test1 you need to do the following:If you only needed to read that global varaible you could have just printed it without using the keyword  as so:But whenever you need to modify a global variable you have let python know using the reserved keyword  Best solution: Don't use sYou have to specify that test1 is global:\n"},
{"body": "Which is a better way to check for the existence of an attribute? provided this answer:I see that it can also be done this way:Is one approach typically used more than others? because you are never just checking to see if an attribute exists; it is always a part of some larger program.  There are several correct ways and one notable incorrect way.Here is a demonstration which shows this technique failing:Output:Most of the time, you don't want to mess with .  It's a special attribute for doing special things, and checking to see if an attribute exists is fairly mundane.A common idiom in Python is \"easier to ask for forgiveness than permission\", or EAFP for short.  You will see lots of Python code that uses this idiom, and not just for checking attribute existence.Note that this is exactly the same idiom for opening a file that may not exist.Also, for converting strings to integers.Even importing optional modules...The  method, of course, works too.  This technique is called \"look before you leap\", or LBYL for short.(The  builtin actually behaves strangely in Python versions prior to 3.2 with regard to exceptions -- it will catch exceptions that it shouldn't -- but this is probably irrelevant, since such exceptions are unlikely.  The  technique is also slower than , but you don't call it often enough to care and the difference isn't very big.  Finally,  isn't atomic so it could throw  if another thread deletes the attribute, but this is a far-fetched scenario and you'll need to be very careful around threads anyway.  I don't consider any of these three differences to be worth worrying about.)Using  is much simpler than , as long as all you need to know is whether the attribute exists.  The big issue for me is that the LBYL technique looks \"strange\", since as a Python programmer I'm more used to reading the EAFP technique.  If you rewrite the above examples so that they use the  style, you get code that is either clumsy, outright incorrect, or too difficult to write.And LBYL is sometimes outright incorrect:If you want to write a LBYL function for importing optional modules, be my guest... it sounds like the function would be a total monster.If you just need a default value,  is a shorter version of .If the default value is expensive to construct, then you'll end up with something like this:Or if  is a possible value,Internally, the  and  builtins just use  technique (except written in C).  So they all behave the same way where it counts, and picking the right one is due to a matter of circumstances and style.The  EAFP code will always rub some programmers the wrong way, and the  LBYL code will irk other programmers.  They're both correct, and there's often no truly compelling reason to pick one or the other.  (Yet other programmers are disgusted that you would consider it normal for an attribute to be undefined, and some programmers are horrified that it's even possible to have an undefined attribute in Python.) is the way.  is ugly and it doesn't work in many cases.  actually tries to get attribute and catches  internally so it works even if you define custom  method.To avoid requesting the attribute twice the third argument for  could be used:You could just use a default value instead of  sentinel if it is more appropriate in your case.I don't like  it might hide  inside  function. (not only ) if it is not desirable  should be used. is the Pythonic way to do it. Learn it, love it. is to check whether the variable name is in  or :I personally hate to catch exceptions in order to check something. It looks and feels ugly. It's identical to checking if a string contains only digits that way:Instead of gently using . Eww.Very old question but it really needs a good answer. For even a short program, I'd say use a custom function!Here's an example. It's not perfect for all application but it is for mine, for parsing responses from countless APIs and using Django. It's easy to fix for everyone's own requirements."},
{"body": "I'm starting an open source Python project shortly and I'm trying to decide in advance how to write my docstrings. The obvious answer would be using reStructuredText and Sphinx with autodoc, because I  like the idea of simply properly documenting my code in my docstrings then have Sphinx automatically construct an API doc for me.The problem is the reStructuredText syntax it uses - I think it's completely unreadable before it's rendered. For instance:You have to  slow down and take a minute to make any sense out of that syntactic jumble. I like much more the Google way (), which counterpart to the above looks like this: nicer! But of course, Sphinx will have none of that and renders all the text after \"Args:\" in one long line.So to summarize - before I go and defile my code base with this reStructuredText syntax I would like to know if there are any real alternatives to using it and Sphinx, short of just writing my own API doc. For instance, is there an extension for Sphinx that handles Google Style Guide's docstring style?I don't think that there is something better than  for documenting python projects at the moment.To have a clearer docstring my favorite choice is using  together with . Based on your example this would look like:(a full example is ), \nHTML output will look like I think the structure of the rst-file is clearer and more readable. The  gives some more information and conventions. The  extension works with  as well.I have created a  that parses both Google style and NumPy style docstrings, and converts them to standard reStructuredText.To use it, simply install it:And enable it in conf.py:More documentation on napoleon .I use  and not sphinx, so this answer may not apply.The reStructuredText syntax you describe for documenting methods and functions is not the only possible one. By far, I prefer describing parameters using a , which is very similar to the Google way:I would try out if sphix supports it. If it doesn't you may also consider using epydoc just for that (although it is not that actively maintaned right now).Actually,  as well as the style guide from  apply mostly for coding the Python's standard library itself, albeit a lot of third party programmers conform to that as well.I agree with you that the Google's style for arguments is much better from an in-code perspective. But you should be able to  as well, . It doesn't output as nice as  though.Anyway, you don't  use sphinx, and by the way, the  module of sphinx is definitely just a small part of it. You can virtually use any documentation generator which is capable of retrieving the content of docstrings, like  (which support  as well as ) or , or even a more universal one like .But definitely, sphinx is quite , very convenient to use with Python, and make your code consistent with the Python's ecosystem. I think you are  who think this is a \"lack\". Maybe they will take these complaints into account in the future, or maybe you might even consider modyfying the  module by yourself, should not be very difficult, it's in Python, it would be a good exercise.You  write docstrings in any format you like. However, for the sake of every other Python programmer, it's best to use markup and tools that they already know. Their lives is easier if you stick to reST and Sphinx.Python makes the contents of the docstrings available as a  attribute attached to the function/class/variable object.So, you could trivially write a Python program which converts the documentation from whatever format you like into whatever format you like. You could even use Javadoc styling, or XML, or whatever.Incidentally, since Sphinx is written in Python, making it take non-RST input is just a matter of writing a small amount of Python code.you just need to start a new line and add a tap after each variable name. Then it is rendered in several lines with consucutive bold variable names:"},
{"body": "I'm trying to build a one-file EXE with PyInstaller which is to include an image and an icon. I cannot for the life of me get it to work with .If I do  it works all works very well.\nWhen I use , it can't find the referenced additional files (when running the compiled EXE). It finds the DLLs and everything else fine, just not the two images.I've looked in the temp-dir generated when running the EXE ( for example) and the files are indeed in there. When I drop the EXE in that temp-directory it finds them. Very perplexing.This is what I've added to the  fileI should add that I have tried not putting them in subfolders as well, didn't make a difference. Newer versions of PyInstaller do not set the  variable anymore, so Shish's excellent  will not work. Now the path gets set as :pyinstaller unpacks your data into a temporary folder, and stores this directory path in the  environment variable. To get the  dir in packed-mode and use the local directory in unpacked (development) mode, I use this:Instead for rewriting all my path code as suggested, I changed the working directory:Just add those two lines at the beginning of your code, you can leave the rest as is.Slight modification to the accepted answer.I found the existing answers confusing, and took a long time to work out where the problem is. Here's a compilation of everything I found.When I run my app, I get an error  (if  is the main file). To troubleshoot this, don't run PyInstaller with  (or edit  to change  => ). With this, run the executable from a command-line, and you'll see the failure.The first thing to check is that it's packaging up your extra files correctly. You should add tuples like  if you want the folder  to be included.After it crashes,  If you're on Windows, you can use . Look for one of your files (eg. ). You should find the temporary path where it unpacked the files (eg. ). You can browse this directory and make sure it included everything. If you can't find it this way, look for something like  (Windows) or  (if you're using Python 3.5).If the installer includes everything, the next likely problem is file I/O: your Python code is looking in the executable's directory, instead of the temp directory, for files.To fix that, any of the answers on this question work. Personally, I found a mixture of them all to work: change directory conditionally first thing in your main entry-point file, and everything else works as-is:All of the other answers use the  in the case where the application is not PyInstalled (i.e.  is not set). That is wrong, as it prevents you from running your application from a directory other than the one where your script is.A better solution:Perhaps i missed a step or did something wrong but the methods which are above, didn't bundle data files with PyInstaller into one exe file. Let me share the steps what i have done.Conclusion: There's still more than one file in the dist folder.Note: I'm using Python 3.5.EDIT: Finally it works with After the 6. step your one file is ready to use."},
{"body": "I have the following shell script for very simple http serverand I was wondering how I can enable /add [ Access-Control-Allow-Origin: * ] on this server?Unfortunately,  is really that simple that it does not allow any customization, especially not of the headers it sends. You can however create a simple HTTP server yourself, using most of , and just add that desired header.Simply create a file  (or whatever) and put the following inside:Then you can do  and it will launch your modified server which will set the CORS header for every response.With the shebang at the top, make the file executable and put it into your PATH, and you can just run it using  too.For a solution that works with Python 3, you could simple change the imports above to import from the  (all the types are there).Alternatively, you could use this which works on both Python 3 and Python 2. The script first tries to import from the Python 3 locations and falls back to Python 2:As SimpleHTTPServer is not really the kind of server you deploy to production, I'm assuming here that you don't care that much about which tool you use as long as it does the job of exposing your files at  with CORS headers in a simple command lineYou'll need to provide your own instances of do_GET() (and do_HEAD() if choose to support HEAD operations). something like this:"},
{"body": "Is there a generic notion of asynchronous programming in python? Could I assign a callback to a function, execute it and return to the main program flow immediately, no matter how long the execution of that function would take?Take a look here:Worth checking out:What you describe (the main program flow resuming immediately while another function executes) is not what's normally called \"asynchronous\" (AKA \"event-driven\") programming, but rather \"multitasking\" (AKA \"multithreading\" or \"multiprocessing\").  You can get what you described with the standard library modules  and  (the latter allows actual concurrent execution on multi-core machines).Asynchronous (event-driven) programming is supported in the standard Python library in the  and  modules, which are very oriented to networking tasks (indeed they internally use the  module, which, on Windows, only supports sockets -- though on Unixy OSs it can also support any file descriptor).For a more general (though also mostly networking oriented, but not  to that) support for asynchronous (event-driven) programming, check out the  third-party package.Good news everyone! It is currently called  and already has an .As described in  and : Here is the  of it's abilities.The other respondents are pointing you to Twisted, which is a great and very comprehensive framework but in my opinion it has a very un-pythonic design. Also, AFAICT, you have to use the Twisted main loop, which may be a problem for you if you're already using something else that provides its own loop.Here is a contrived example that would demonstrate using the  module:However, in pretty much every useful case, you will want to communicate between threads. You should look into , and become familiar with the concept of  and the related issues.The  module provides many such primitives for you to use, if you know how to use them.You may well want to checkout the Twisted library for Python. They provide many useful tools.You may see my Python Asynchronous Programming tool: "},
{"body": "How do I find multiple occurrences of a string within a string in Python? Consider this:So the first occurrence of  is at 1 as expected. How do I find the next occurrence of it?Same question is valid for a list. Consider:How do I find all the  with their indexes? Using regular expressions, you can use  to find all (non-overlapping) occurences:Alternatively, if you don't want the overhead of regular expressions, you can also repeatedly use  to get the  index:This also works for lists and other sequences.I think what you are looking for is Hope this helps \nNOTE: this only captures non-overlapping occurencesFor the list example, use a comprehension:Similarly for strings:this will list adjacent runs of \"ll', which may or may not be what you want:FWIW, here are a couple of non-RE alternatives that I think are neater than .The first uses  and checks for :The second tests uses  and checks for the sentinel of  by using :To apply any of these functions to a list, tuple or other  of strings, you can use a  \u2014one that takes a function as one of its arguments\u2014 like this one:For your list example:If you wanted all the items in a list that contained 'll', you could also do that.Brand new to programming in general and working through an online tutorial. I was asked to do this as well, but only using the methods I had learned so far (basically strings and loops). Not sure if this adds any value here, and I know this isn't how you would do it, but I got it to work with this:This version should be linear in length of the string, and should be fine as long as the sequences aren't too repetitive (in which case you can replace the recursion with a while loop).bstpierre's list comprehension is a good solution for short sequences, but looks to have quadratic complexity and never finished on a long text I was using.For a random string of non-trivial length, the two functions give the same result:But the quadratic version is about 300 times slowerThis program counts the number of all substrings even if they are overlapped without the use of regex. But this is a naive implementation and for better results in worst case it is advised to go through either Suffix Tree, KMP and other string matching data structures and algorithms.Here is my function for finding multiple occurrences. Unlike the other solutions here, it supports the optional start and end parameters for slicing, just like :A simple iterative code which returns a list of indices where the substring occurs.You can split to get relative positions then sum consecutive numbers in a list and add (string length * occurence order) at the same time to get the wanted string indexes.  This can be done in one line using list comprehensions:Similar technique works for lists:Maybe not so Pythonic, but somewhat more self-explanatory. It returns the position of the word looked in the original string. This  explains how to do the whole thing in O(n) and includes a solution in python as well.If you go further down the sets to '' you'd be able to do the same thing if you had one big string but wanted to search for 1000s of patterns in it."},
{"body": "Python noob, as in this is my first project, so excuse my unfamiliarity.The site was working very well until I clicked \"log out\" on my app. After that, the website would give me this error:\nDoesNotExist at /login/\nSite matching query does not exist.I searched everywhere and the only solution I get relates to setting up the site framework, SITE_ID, etc. I think those items on my computer are fine, but I can't find a walkthrough/guide to help me check on them.Can anyone tell me what the problem is and how to fix it? Thanks in advance :3}If you don't have a site defined in your database and django wants to reference it, you will need to create one. From a  :Now set that site ID in your settings.py to SITE_IDTable  must contain a row with the same value of  (by default equals to ), as  is set to (inside your )."},
{"body": "I have a string in base64 format, which represents PNG image. Is there a way to save this image to the filesystem, as a PNG file?I encoded the image using flex. Actually this is what I get on server\n(can't see any image after any of proposed methods :( )Starting withDecoded the data using the base64 codec, and then write it to the filesystem. Modernizing this example to Python 3, which removed arbitrary codec support from string/bytes  and  functions:If the imagestr was bitmap data (which we now know it isn't) you could use this is the base64 encoded string\n is the width of the image\n is the height of the image  Since the imagestr is just the encoded png dataYou can also save it to a string buffer and then do as you wish with it, In django, you can save it as an uploaded file to save to a model:Or send it as an email:You could probably use the  to do this - decode the base64 string into a regular string (via the  standard library), and pass it to the constructor."},
{"body": "I've been working on a new dev platform using nginx/gunicorn and Flask for my application.Ops-wise, everything works fine - the issue I'm having is with debugging the Flask layer. When there's an error in my code, I just get a straight 500 error returned to the browser and nothing shows up on the console or in my logs.I've tried many different configs/options.. I guess I  be missing something obvious.My gunicorn.conf:An example of some Flask code that borks- testserver.py:And finally, the command to run the flask app in gunicorn:Thanks y'allThe acception solution doesn't work for me.Gunicorn is a pre-forking environment and apparently . Even if you set , you will still only get an empty page with the message  if you run with . The best you can do with gunicorn is to run it with . That gives you the trace in addition to the  message. However, this is just the same text trace that you see in the terminal and not the Flask debugger.Adding the  section to the testserver.py and running  to start the server in development gets you the Flask debugger. Personally I still like to use , instead of  since . To get this to work:Also, dont forget to change the  line in  to something that won't run Flask in debug mode in production.The Flask config is entirely separate from gunicorn's. Following , a good solution would be change my source to this:And in config.py:, there is a simpler solution than creating a bin/web script like suggested by Nick.Instead of , just use   if you want to debug your application in development.Try setting the debug flag on the run command like so  and keep the  in your Flask application. There must be a reason why your debug option is not being applied from the config file but for now the above note should get you going."},
{"body": "I'm looking for an equivalent to  in Python. I want to parse  files, in C I could do something like this:I thought at first to use , however it doesn't split on the given characters, but the  string as a whole:Which should be returning 17, as explained above.Is there a Python equivalent to  (not RE), or a string splitting function in the standard library that splits on any of a range of characters that I'm not aware of?Python doesn't have an  equivalent built-in, and most of the time it actually makes a whole lot more sense to parse the input by working with the string directly, using regexps, or using a parsing tool. Probably mostly useful for translating C, people have implemented , such as in this module: In this particular case if you just want to split the data based on multiple split characters,  is really the right tool.When I'm in a C mood, I usually use zip and list comprehensions for scanf-like behavior.  Like this:Note that for more complex format strings, you do need to use regular expressions:There is also the  module. is designed to be the opposite of  (the newer string formatting function in Python 2.6 and higher).You can split on a range of characters using the  module.You can parse with module  using . It won't parse the substrings to their actual datatypes (e.g. ) but it's very convenient when parsing strings.Given this sample line from :An example mimicking your sscanf example with the variable could be:There is an ActiveState recipe which implements a basic scanf\nyou can turn the \":\" to space, and do the split.egno regex needed (for this case)Upvoted orip's answer. I think it is sound advice to use re module. The Kodos application is helpful when approaching a complex regexp task with Python. Update: The Python documentation for its regex module, , includes a section on simulating scanf, which I found more useful than any of the answers above.If the separators are ':', you can split on ':', and then use x.strip() on the strings to get rid of any leading or trailing whitespace. int() will ignore the spaces.There is a ."},
{"body": "Is there a way to compress an if/else statement to one line in Python? I oftentimes see all sorts of shortcuts and suspect it can apply here too.An example of Python's way of doing \"ternary\" expressions:translates intoThis actually comes in handy when using list comprehensions, or sometimes in return statements, otherwise I'm not sure it helps that much in creating readable code. The readability issue was discussed at length in this recent SO question .It also contains various other  (and somewhat ) ways to accomplish the same task. It's worth a read just based on those posts.Python's  can be used as a :Only for using as a value:orThere is the conditional expression:but this is an expression, not a statement.In if statements, the  (or  or ) can be written on the same line as the body of the block if the block is just one like:but this is discouraged as a matter of formatting-style."},
{"body": "Will google Go use less resources than Python and Java on Appengine? Are the instance startup times for go faster than Java's and Python's startup times? Is the go program uploaded as binaries or source code and if it is uploaded as source code is it then compiled once or at each instance startup?In other words: Will I benefit from using Go in app engine from a cost perspective? (only taking to account the cost of the appengine resources not development time)Yes, Go instances have a lower memory than Python and Java (< 10 MB).Yes, Go instances start faster than Java and Python equivalent because the runtime only needs to read a single executable file for starting an application.Also even if being atm single threaded, Go instances handle incoming request concurrently using goroutines, meaning that if  1 goroutine is waiting for I/O another one can process an incoming request.Go program is uploaded as source code and compiled (once) to a binary when deploying a new version of your application using the SDK.The Go runtime has definitely an edge when it comes to performance / price ratio, however it doesn't affect the pricing of other API quotas as described by Peter answer.The cost of instances is only part of the cost of your app.  I only use the Java runtime right now, so I don't know how much more or less efficient things would be with Python or Go, but I don't imagine it will be orders of magnitude different.  I do know that instances are not the only cost you need to consider.  Depending on what your app does, you may find API or storage costs are more significant than any minor differences between runtimes.  All of the API costs will be the same with whatever runtime you use.The question is mostly irrelevant.The minimum memory footprint for a Go app is less than a Python app which is less than a Java app.  They all cost the same per-instance, so unless your application performs better with extra heap space, this issue is irrelevant.Go startup time is less than Python startup time which is less than Java startup time.  Unless your application has a particular reason to churn through lots of instance startup/shutdown cycles, this is irrelevant from a cost perspective.  On the other hand, if you have an app that is  bursty in , the startup time may be an advantage.As mentioned by other answers, many costs are identical among all platforms - in particular, datastore operations.  To the extent that Go vs Python vs Java will have an effect on the instance-hours bill, it is related to:All three languages have their virtues and curses.  For the most part, you're better off letting other issues dominate - which language do you enjoy working with most?It's probably more about how you allocate the resources than your language choice. I read that GAE was built the be language-agnostic so there is probably no builtin advantage for any language, but you can get an advantage from choosing the language you are comfortable and motivated with. I use python and what made my deployment much more cost-effective was the upgrade to python 2.7 and you can only make that upgrade if you use the correct subset of 2.6, which is good. So if you choose a language you're comfortable with, it's likely that you will gain an advantage from your ability using the language rather than the combo language + environment itself. In short, I'd recommend python but that's the only app engine language I tried and that's my choice even though I know Java rather well the code for a project will be much more compact using my favorite language python. My apps are  to  sized and they cost like nothing:I haven't used Go, but I would strongly suspect it would load and execute instances much faster, and use less memory purely because it is compiled. Anecdotally from the group, I believe that Python is more responsive than Java, at least in instance startup time. Instance load/startup times are important because when your instance is hit by more requests than it can handle, it spins up another instance. This makes that request take much longer, possibly giving the impression that the site is generally slow. Both Java and Python have to startup their virtual machine/interpreter, so I would expect Go to be an order of magnitude faster here.There is one other issue - now Python2.7 is available, Go is the only option that is single-threaded (ironically, given that Go is designed as a modern multi-process language). So although Go requests should be handled faster, an instance can only handle requests serially. I'd be very surprised if this limitation last long, though."},
{"body": "I'm new to Mustache.Many templating languages (e.g.,  / ) will let you extend a \"parent\" template like so...I'm aware of Mustache's  (e.g., ), but those seem to be just .Does template extension exist for Mustache? Or, failing that, is there at least some design pattern that effectively turns  into template extension equivalents.I recently found myself in the same boat, except I came from a mako background.  Mustache does not allow for template extension/inheritance but there are a few options available to you that I know of.I wouldn't, however, use the triple mustache because I don't want unescaped HTML to be appearing anywhere, it's just too risky in my opinion.If someone else has a better solution to this problem I'd love to hear it as well, since I haven't yet taken the plunge in any one of these directions.I've proposed this to the specification for Mustache here:Currently mustache.java, hogan.js and phly_mustache support template inheritance.You could use variables containing HTML. A \"triple mustache\" like  will return unescaped HTML. It's not exactly the same as template extensions, but you could render  and then put its output in a  variable that gets passed to .(I added  to the  filename with the expectation that such a naming pattern will help keep the filenames manageable.)Mustache doesn't do template extension. If you really want template extension then you may want to use a library purpose built with this functionality for you language/framework of choice.FYI, I'm using Node.js/Express, so I will probably end up using I'm playing around with this right now in Python (note I'm the creator of Mako), adding in a dynamic context that captures sections seems to be doing the right thing, though I'd need to test this a lot more.Basically we are using lambdas, where a \"<\" prefix indicates \"inherit from this template\" (similar to the syntax discussed at ) and a \"$\" prefix indicates \"this is an inherited section\".So here's some templates.   base.mustache:hello.mustache:and then to play with three levels deep, subhello.mustache:Rendering hello.mustache like this:output:Rendering subhello.mustache:output:I just wrote this in twenty minutes, and I've only used handlebars.js a little bit in the past and pystache for the first time just now so the whole \"mustache\" idea is not deep for me yet.  But this seems to work ?   In mustache php, template inheritance is supported since version 2.7.0.You can figure out your current version from the file Mustache/Engine.php and search for the line containing:If you're happy with a server-side only code,  - modelled on django. While it works, however, it is no longer maintained by its author.In node.js you can use  or  to have layouts inna mustache templates, but the way they do things is different, in none of them you set the layout at the template itself, layouts are registered in your app code."},
{"body": "I want to get the  in a variable after running the  call.Lets take this line as an example:  will contain the error code (  under Windows or  under some linux for the above example).How can I get the  for the above command without using redirection in the executed command?      If all you need is the  output, then take a look at  (added in Python 2.7):Because you were using , you'd have to set  to get the same behaviour. You do want to heed the  about passing untrusted arguments to your shell.If you need to capture  as well, simply add  to the call:to redirect the error output to the default output stream.These answers didn't work for me. I had to use the following:Or as a function (using shell=True was required for me on Python 2.6.7 and check_output was not added until 2.7, making it unusable here):I would like to expand on the Windows solution. Using IDLE with Python 2.7.5, When I run this code from file Expts.py:...in the Python Shell, I ONLY get the output corresponding to \"cmd.exe\"; the \"dir\" part is ignored. HOWEVER, when I add a switch such as /K or /C ......then in the Python Shell, I get all that I expect including the directory listing. Woohoo !Now, if I try any of those same things in DOS Python command window, without the switch, or with the /K switch, it appears to make the window hang because it is running a subprocess cmd.exe and it awaiting further input - type 'exit' then hit [enter] to release. But with the /K switch it works perfectly and returns you to the python prompt. Allrightee then.Went a step further...I thought this was cool...When I instead do this in Expts.py:...a new DOS window pops open and remains there displaying only the results of \"cmd.exe\" not of \"dir\". When I add the /C switch, the DOS window opens and closes very fast before I can see anything (as expected, because /C terminates when done). When I instead add the /K switch, the DOS window pops open and remain, AND I get all the output I expect including the directory listing.If I try the same thing (subprocess.call instead of subprocess.check_output) from a DOS Python command window; all output is within the same window, there are no popup windows. Without the switch, again the \"dir\" part is ignored, AND the prompt changes from the python prompt to the DOS prompt (since a cmd.exe subprocess is running in python; again type 'exit' and you will revert to the python prompt). Adding the /K switch prints out the directory listing and changes the prompt from python to DOS since /K does not terminate the subprocess. Changing the switch to /C gives us all the output expected AND returns to the python prompt since the subprocess terminates in accordance with /C.Sorry for the long-winded response, but I am frustrated on this board with the many terse 'answers' which at best don't work (seems because they are not tested - like Eduard F's  response above mine which is missing the switch) or worse, are so terse that they don't help much at all (e.g., 'try subprocess instead of os.system' ... yeah, OK, now what ??). In contrast, I have provided solutions which I tested, and showed how there are subtle differences between them. Took a lot of time but...\nHope this helps."},
{"body": "If it is environment-independent, what is the theoretical maximum number of characters in a python string?Also if it differs with version number I'd like to know it for python 2.5.2With a 64-bit Python installation, and (say) 64 GB of memory, a Python 2 string of around 63 GB should be quite feasible (if not maximally fast).  If you can upgrade your memory much beyond that (which will cost you an arm and a leg, of course), your maximum feasible strings should get proportionally longer.  (I don't recommend relying on virtual memory to extend that by much, or your runtimes will get simply ridiculous;-).With a typical 32-bit Python installation, of course, the total memory you can use in your application is limited to something like 2 or 3 GB (depending on OS and configuration), so the longest strings you can use will be much smaller than in 64-bit installations with ridiculously high amounts of RAM.I ran this code on an EC2 instance.and this is the outputmemory error after 116GB.Tested on EC2 r3.4xlarge instance running 64-bit Amazon Linux AMI 2016.09Short answer would be: if you have over 100GB of RAM, one Python string can use up that much memory."},
{"body": "I haveBut I wantWhat magic param forces no tagging?You can use  instead of . Just keep in mind that it won't be able to represent arbitrary Python objects then. Also, when you  the YAML, you will get a  object instead of .How about this:This seems to make dumping unicode objects work the same as dumping str objects for me (Python 2.6).You need a new dumper class that does everything the standard Dumper class does but overrides the representers for str and unicode.Which leads toGranted, I'm still stumped on how to keep this pretty.And it breaks a later yaml.load()little addition to interjay's excellent answer, you can keep your unicode on a reload if you take care of your file encodings. contents in my editor:print outputs:Plus, after reading  I am pretty sure that safe_load/safe_dump is where I want to be anyway.I've just started with Python and YAML, but probably this may also help. Just compare outputs:"},
{"body": "I'm trying to make a list with numbers 1-1000 in it. Obviously this would be annoying to write/read, so I'm attempting to make a list with a range in it. In python 2 it seems that\n    some_list = range(1,1000)\nwould have worked, but in python 3 the range is similar to the xrange of python 2?\nCan anyone provide some insight into this?You can just construct a list from the range object:This is how you do it with generators in python2.x as well.  Typically speaking, you probably don't need a list though since you can come by the value of  more efficiently (), and if you just need to iterate over it, you can just fall back on .Also note that on python2.x,  is still indexable.  This means that  on python3.x also has the same propertyin Python 3.x, the  function got its own type. so in this case you must use iteratorActually, if you want 1-1000 (inclusive), use the  function with parameters 1 and 1001: , because the  function goes from start to (end-1), inclusive.You really shouldn't need to use the numbers 1-1000 in a list. But if for some reason you really do need these numbers, then you could do:List Comprehension in a nutshell:The above list comprehension translates to:This is just the list comprehension syntax, though from 2.x. I know that this will work in python 3, but am not sure if there is an upgraded syntax as wellRange starts inclusive of the first parameter; but ends Up To,  the second Parameter (when supplied 2 parameters; if the first parameter is left off, it'll start at '0')  "},
{"body": "When running....I get the following output:It's prompting to save the login details, despite  already containing this. It then fails to upload files for a package I own, and have full write-access to.Just found , which solves the issue:The above content goes into the  in the user's home directory.Ugh.. I think this may be a good time to give  a try..None of the changes to ~/.pypirc listed here worked for me.This worked for me, with no changes to ~/.pypirc.  I added \"-r \" to the command line:My ~/.pypirc looks like thisI have the same problem\uff0c This is my solution.The python version is 2.7.7, my windows version is Windows-7-6.1.7601-SP1.here is my .pypirc fileIn windows, A file is not allowed named as \u201c.pypirc\u201d\uff0cplz\uff0crefer to\uff1a Then put you \".pypirc\" file in the same folder with \"setup.py\"At last:run : or:I hope this will be help,thank you!I have this problem and solved it by putting the file .pypirc under my home directory (~/), as the last past of the first comment suggests. I didn't have the need to modify the name of the section \"pypi\" of the file pypirc for \"server-login\", as suggested also in the first comment.I changed  to  as per the  and this worked for me.Here is my  file:I had this problem, due to my own fault but hopefully this may help someone else who makes this same mistake.I'm using python 3 on Linux Ubuntu, during registration I issued the setup command using sudo! The result was the .pypirc file in my home directory was owned by root and wasn't readable when trying to perform a module upload immediately after as a none privileged user.By changing the ownership of the file to myself, the issue was resolved.I ran into the same problem. I'm on a new OS X Sierra. Adding [server-login] entry to ~/.pypirc seemed to fix it "},
{"body": "Can someone please explain why the code below does not work when setting the facecolor of the figure?When I specify the height and width of the figure using   these are picked up by the command . However, the facecolor setting is not picked up. Why?Thanks for your help.It's because  overrides the facecolor for the background of the figure.  (This is deliberate, actually... The assumption is that you'd probably want to control the background color of the saved figure with the  kwarg to . It's a confusing and inconsistent default, though!)The easiest workaround is just to do  (I'm specifying the edgecolor here because the default edgecolor for the actual figure is white, which will give you a white border around the saved figure)Hope that helps!I had to use the transparent keyword to get the color I chose with my initial like this: has its own parameter for .\nI think an even easier way than the accepted answer is to set them globally , instead of putting  every time:"},
{"body": "I need to replace part of a string. I was looking through the Python documentation and found re.sub.I was expecting this to print  and not 'bar'. Could anybody tell me what I did wrong?Instead of capturing the part you want to  you can capture the parts you want to  and then refer to them using a reference  to include them in the substituted string.Try this instead:Also, assuming this is HTML you should consider using an HTML parser for this task, for example .Or you could just use the search function instead:"},
{"body": "I'm running an update on my MongoDB from Python. I have this line:But it throws this error:But  looks like an instance of bool to me!How should I correctly write this update?The third argument to PyMongo's  is  and must be passed a boolean, not a dictionary. Change your code to:Or pass  as a keyword argument:Your mistake was likely caused by reading about  in the . The JavaScript version of  takes an object as its third argument containing optional parameters like  and . But since Python allows passing keyword arguments to a function (unlike JavaScript which only has positional arguments), this is unnecessary and PyMongo takes these options as optional function parameters instead.According to  you should indeed pass upsert as a keyword rather than just True, that isOris a better approach than just passing True as if you ever wish to pass other kwargs such as  or  code might break if order of args is not kept.upsert should be passed as either positional parameter, like soor as a keyword argument, like so"},
{"body": "I find particularly difficult reading binary file with Python. Can you give me a hand?\nI need to read this file, which in Fortran 90 is easily read byIn detail, the file format is:How can I read this with Python? I tried everything but it never worked. Is there any chance I might use a f90 program in python, reading this binary file and then save the data that I need to use?Read the binary file content like this:then \"unpack\" binary data using :The start bytes: The body: ignore the heading bytes and the trailing byte (= 24); The remaining part forms the body, to know the number of bytes in the body do an integer division by 4; The obtained quotient is multiplied by the string  to create the correct format for the unpack method:The end byte: In general, I would recommend that you look into using Python's  module for this. It's standard with Python, and it should be easy to translate your question's specification into a formatting string suitable for .Do note that if there's \"invisible\" padding between/around the fields, you will need to figure that out and include it in the  call, or you will read the wrong bits.Reading the contents of the file in order to have something to unpack is pretty trivial:This unpacks the first two fields, assuming they start at the very beginning of the file (no padding or extraneous data), and also assuming native byte-order (the  symbol). The s in the formatting string mean \"unsigned integer, 32 bits\".You could use , which can read data from both text and binary files. You would first construct a data type, which represents your file format, using , and then read this type from file using ."},
{"body": "Is there is any easy way or function to determine the greatest number in a python list?  I could just code it, as I only have three numbers, however it would make the code a lot less redundant if I could tell the greatest with a built in function or something.What about max()You can use the inbuilt function  with multiple arguments:or a list:or in fact anything iterable.Use "},
{"body": "If yes are there any frameworks/Tutorials/tips/etc recommended? N00b at Python but I have tons of PHP experience and wanted to expand my skill set.I know Python is great at server side execution, just wanted to know about client side as well.Have you seen skulpt?You can use  to convert Python code to JS code that runs in the browser (note that pyjamas also offers much more). So YES, you can use Python for client side web development.Pyjamas is basically a port of the , which allows you to write client side code in Java.Silverlight can run IronPython, so you can make Silverlight applications. Which is client-side.Python isn't really used for client side, because no major web browsers have Python built in.However, Javascript has become more and more Python-like as it has evolved.  You might want to study Python and Javascript together.Take a look at  and notice the Python-like nature of the new features.  \"array comprehensions\" are just like Python list comprehensions; iterators and generators are straight out of Python; \"destructuring assignments\" are a standard Python idiom.Javascript is kind of like Python with curly braces.  It uses a very different inheritance model, but libraries are available that implement a more Python-like class system on top of the built-in system in Javascript.Noone has mentioned it yet, but  is/was pretty coolNo. Browsers don't run Python.Have a look at this:It's an interactive python console + tutorial written with Silverlight+IronPython.On Windows, any language that registers for the Windows Scripting Host can run in IE. At least the ActiveState version of Python could do that; I seem to recall that has been superseded by a more official version these days.But that solution requires the user to install a python interpreter and run some script or .reg file to put the correct \"magic\" into the registry for the hooks to work.Python is available for the client: If its client side your interested may be worth checking out"},
{"body": "I have a situation like so...How can I access the  class's method from the  class? -- Thanks for the responses.  I'm concluding that I need to re-assess how I had designed this to be implemented and come up with a more robust method.The methods of a nested class cannot directly access the instance attributes of the outer class. Note that it is not necessarily the case that an instance of the outer class exists even when you have created an instance of the inner class.In fact, it is often recommended against using nested classes, since the nesting does not imply any particular relationship between the inner and outer classes.You're trying to access Outer's class instance, from inner class instance. So just use factory-method to build Inner instance and pass Outer instance to it.maybe I'm mad but this seems very easy indeed - the thing is to make your inner class inside a method of the outer class... Plus... \"self\" is only used by convention, so you could do this:It might be objected that you can't then create this inner class from outside the outer class... but this ain't true:then, somewhere miles away:even push the boat out a bit and extend this inner class (NB to get super() to work you have to change the class signature of mooble to \"class mooble( object )\"Do you mean to use inheritance, rather than nesting classes like this? What you're doing doesn't make a heap of sense in Python.You can access the 's some_method by just referencing  within the inner class's methods, but it's not going to work as you expect it will. For example, if you try this:...you'll get a TypeError when initialising an  object, because  expects to receive an  instance as its first argument. (In the example above, you're basically trying to call  as a class method of .)Building on the great idea an answer for the same question my answer is for (at ) explained, as I wanted to create my own good quality solution to the question, I came up with a Python answer that should be quite simple to understand.The main code, \"production ready\" (without comments, etc.). Remember to replace all of each value in angle brackets (e.g. ) with the desired value.\n  Insert before line 12 in the main code:\nThen insert into this function lines 5-6 (of the main code) and replace lines 4-7 with the following code:\nReplace line 6 with the following code:\nA class, named \"a\" () is created. It has subclasses that need to access it (the parent). One subclass is called \"x1\". In this subclass, the code  is run.Then another class, named \"b\" is created,  from class \"a\" (). After that, some code runs  (calling the sub function \"x1\" of b, a derived sub-class). This function runs , calling the function \"run_func\" of class \"a\",  the function \"run_func\" of its parent, \"b\" (as it should), because the function which was defined in class \"a\" is set to refer to the function of class \"a\", as that was its parent.This would cause problems (e.g. if function  has been deleted) and the only solution without rewriting the code in class  would be to redefine the sub-class  with updated code for all classes derived from class \"a\" which would obviously be difficult and not worth it.i found .Tweaked to suite your question, it is the answer:I\u2019m sure you can somehow write a decorator for this or something :)\n/edit: Another possibility:Expanding on @tsnorri's cogent thinking, that the outer method may be a :Now the line in question should work by the time it is actually called.The last line in the above code gives the Inner class a static method that's a clone of the Outer static method.  This takes advantage of two Python features, that , and ....or current  in our case. So objects \"local\" to the definition of the Outer class ( and ) may be referred to directly within that definition.You can easily access to outer class using metaclass: after creation of outer class check it's attribute dict for any classes (or apply any logic you need - mine is just trivial example) and set corresponding values:Using this approach, you can easily bind and refer two classes between each other."},
{"body": "In the example code below, I'd like to recover the return value of the function .  How can I go about doing this?  Where is this value stored?I can't seem to find the relevant attribute in the objects stored in .Thanks in advance,\nblzUse  to communicate. For example like this:I think the approach suggested by @sega_sai is the better one. But it really needs a code example, so here goes:Which will print the return values:If you are familiar with  (the Python built-in) this should not be too challenging. Otherwise have a look at .Note how little code is needed. (Also note how processes are re-used).It seems that you should use the multiprocessing.Pool class instead and use the methods .apply() .apply_async(), map()You can use the  built-in to set the exit code of a process. It can be obtained from the  attribute of the process:This example shows how to use a list of  instances to return strings from an arbitrary number of processes:This solution uses fewer resources than a  which usesor a  which usesIt is very instructive to look at the source for each of these types.For anyone else who is seeking how to get a value from a  using :"},
{"body": "I am new to Python, and I have a question about how to use Python to read and write CSV files. My file contains like Germany, French, etc. According to my code, the files can be read correctly in Python, but when I write it into a new CSV file, the unicode becomes some strange characters.The data is like:\nAnd my code is:And the result is like:\nWould you please tell me what I should do to solve the problem? Thank you very much!Make sure you encode and decode as appropriate.This example will roundtrip some example text in utf-8 to a csv file and back out to demonstrate:Prints:There is an example at the end of the  that demonstrates how to deal with Unicode.  Below is copied directly from that .  Note that the strings read or written will be Unicode strings.  Don't pass a byte string to , for example.Input (UTF-8 encoded):Output:Another alternative: Use the code from the unicodecsv package ...This module is API compatible with the STDLIB csv module.I had the very same issue. The answer is that you are doing it right already. It is the problem of MS Excel. Try opening the file with another editor and you will notice that your encoding was successful already. To make MS Excel happy, move from UTF-8 to UTF-16. This should work:I couldn't respond to Mark above, but I just made one modification which fixed the error which was caused if data in the cells was not unicode, i.e. float or int data.  I replaced this line into the UnicodeWriter function: \"self.writer.writerow([s.encode(\"utf-8\") if type(s)==types.UnicodeType else s for s in row])\" so that it became:You will also need to \"import types\"."},
{"body": "In python how can you extend a class? For example if I havecolor.pycolor_extended.pyBut this doesn't work...\nI expect that if I work in , then when I make a color object and use the  function then it will return the object with the string \" extended!\" in the end. Also it should have gotton the init from the import.Assume python 3.1ThanksUse:If this were Python 2.x, you would also want to derive  from , to make it a :This is not necessary in Python 3.x."},
{"body": "I need to package my python application, its dependencies and python into a single MSI installer. The end result should desirably be:Kind of a dup of this question about . It boils down to:  on windows,  on Linux, and\n on Mac.I use  (the svn version) to create a stand-alone  version of my program that includes Python and all the dependencies. It takes a little fiddling to get it to work right and include everything (as does py2exe and other similar programs, see ), but then it works very well.You then need to create an installer.  Works great for that and is free, but it creates .exe files not .msi. If .msi is not necessary, I highly recommend it. Otherwise check out the answers to  question for other options.My company uses the free InnoSetup tool.  It is a moderately complex program that has tons of flexibility for building installers for windows.  I believe that it creates .exe and not .msi files, however.  InnoSetup is not python specific but we have created an installer for one of our products that installs python along with dependencies to locations specified by the user at install time.I've had much better results with dependencies and custom folder structures using , and it lets you find and specify hidden imports and hooks for larger dependencies like numpy and scipy.  Also a PITA, though.py2exe is the best way to do this.  It's a bit of a PITA to use, but the end result works very well. will make windows executables with python bundled in.Ok, I have used py2exe before and it works perfectly except for one thing... It only works on executable windows machines. I then learned about Jython which turn a python script into a .Jar file. Which as you know is executable from any machine that has Java (\"To your latest running version\") installed. Which is great because both unix, windows, and ios (Most of the time) Run java. That means its executable from all of the following machines. As long as they run Java. No need for \"py2mac + py2exe + freeze\" just to run on all operating systems. Just JythonFor more information on how it works and how you can use it click here. \n"},
{"body": "Is there a simple way / module to  measure the elapsed time in python? I know that I can simply call  twice and take the difference, but that will yield wrong results if the system time is changed. Granted, that doesn't happen very often, but it does indicate that I'm measuring the wrong thing.Using  to measure durations is incredibly roundabout when you think about it. You take the difference of two absolute time measurements which are in turn constructed from duration measurements (performed by timers) and known absolute times (set manually or via ntp), that you aren't interested in at all.So, is there a way to query this \"timer time\" directly? I'd imagine that it can be represented as a millisecond or microsecond value that has no meaningful absolute representation (and thus doesn't need to be adjusted with system time). Looking around a bit it seems that this is exactly what  does in Java, but I did not find a corresponding Python function, even though it should (hardware-technically) be easier to provide than .Edit: To avoid confusion and address the answers below: This is not about DST changes, and I don't want CPU time either - I want elapsed physical time. It doesn't need to be very fine-grained, and not even particularly accurate. It just shouldn't give me negative durations, or durations which are off by several orders of magnitude (above the granularity), just because someone decided to set the system clock to a different value. Here's what the Python docs say about 'time.time()':This is exactly what I want to avoid, since it can lead to strange things like negative values in time calculations. I can work around this at the moment, but I believe it is a good idea to learn using the proper solutions where feasible, since the kludges will come back to bite you one day.Edit2: Some research shows that you can get a system time independent measurement like I want in Windows by using GetTickCount64(), under Linux you can get it in the return value of times(). However, I still can't find a module which provides this functionality in Python.What you seem to be looking for is a . A  does not jump or go backwards. There have been several attempts to implement a cross platform monotomic clock for Python based on the OS reference of it. (Windows, POSIX and BSD are quite different) See the discussions and some of the attempts at monotonic time in . Mostly, you can just use os.times():But that does not fill in the needed elapsed real time (the fifth tuple) on Windows. If you need Windows support, consider  and you can call GetTickCount64() directly, as has been done in .For measuring elapsed CPU time, look at . This is the equivalent of Linux's  user time field. For benchmarking, use . The , , also has microsecond time if supported by the platform.  Example:If you are concerned about system time changes (from DS -> ST) just check the object returned by datetime.Presumably, the system time could have a small adjustment from an  reference adjustment. , and corrections are , but ntp sync beats can have an effect with very small (millisec or microsec) time references. You can also reference  if you want something of that resolution. I would not go too far to reinvent the wheel. Accurate time is basic and most modern OS's do a pretty good job. Based on your clarifications, it sounds like you need a simple side check if the system's clock has changed. Just compare to a friendly, local ntp server:NTP is accurate to milliseconds over the Internet and has representation resolution of resolution of 2\u221232 seconds (233 picoseconds). Should be good enough?  Be aware that the NTP 64 bit data structure  -- if you really want a robust solution, better check for overflow...Python 3.3 added a  into the standard library, which does exactly what I was looking for. Thanks to Paddy3118 for pointing this out in .Using UTC avoids those embarassing periods when the clock shifts due to daylight saving time.As for using an alternate method rather than subtracting two clocks, be aware that the OS does actually contain a clock which is initialized from a hardware clock in the PC. Modern OS implementations will also keep that clock synchronized with some official source so that it doesn't drift. This is much more accurate than any interval timer the PC might be running.The example functions you state in your edit are two completely different things: Although two different functions, both (potentially) could be used to reveal a system clock that had a \"burp\" with these methods:You could take both a system time with  and a CPU time with . Since wall clock time will ALWAYS be greater than or equal to CPU time, discard any measurements where  the interval between the two  readings is less than the paired  check readings.Example:Getting system uptime is also promising if you are concerned about the system's clock, since a user reset does not reset the uptime tick count in most cases. (that I am aware of...)Now the harder question: getting system uptime in a platform independent way -- especially without spawning a new shell -- at the sub second accuracy. Hmmm...Probably the best bet is . , they use  for Windows and  for BSD / OS X, etc. Unfortunately, these are all 1 second resolution. "},
{"body": "I have an OO hierarchy with docstrings that take as much maintenance as the code itself. E.g.,Now, the problem is that  does not inherit the superclass method's docstring. I know I can keep the docstring using the template method pattern, i.e.and implementing  in each subclass. However, I was wondering whether there's another way to have docstrings be inherited, perhaps some decorator that I hadn't discovered yet?Write a function in a class-decorator style to do the copying for you.  In Python2.5, you can apply it directly after the class is created.  In later versions, you can apply with the  notation.Here's a first cut at how to do it:In newer Python versions, the last part is even more simple and beautiful:This is a Pythonic technique that exactly matches the design of existing tools in the standard library.  For example, the  class decorator add missing rich comparison methods to classes.  And for another example,  the  decorator copies metadata from one function to another.This is a variation on .F.Y.I for people just now stumbling on this topic: As of Python 3.5,  automatically retrieves docstrings from the inheritance hierarchy.The responses above are thus useful for Python 2, or if you want to be more creative with merging the docstrings of parents and children.  I've also created some . These support some nice default docstring styles (numpy, google, reST) out of the box. You can easily use your own docstring style as well The following adaptation also handles properties and mixin classes. I also came across a situation where I had to use  (for \"instancemethod\"s), but I'm not completely sure why the other solutions didn't encouter that problem."},
{"body": "From what I've been aware of, using  or  to instantiate objects returns a new instance of  or  respectively; a new instance object with . This was pretty clear to me until I actually tested it and I noticed that  actually returns  instead of the expected :as expected, this behavior is also manifested when creating objects with ,  and  respectively:The only relevant piece of information I could find in  states:Suffice to say, this isn't sufficient for answering my question. So, why do empty tuples have the same identity whilst others like lists or dictionaries do not?Python internally creates a  list of tuple objects whose first element contains the empty tuple. Every time  or  is used, Python will return the existing object contained in the aforementioned  list and not create a new one.Such mechanism does not exist for  or  objects which are, on the contrary, .This is most likely related to the fact that immutable objects (like tuples) cannot be altered and, as such, are guaranteed to not change during execution. This is further solidified when considering that  returns ; like  an empty  . With mutable objects,  and, as such, there's no incentive to cache their zero element instances (i.e their contents could change with the identity remaining the same).  In the most common case, the implementation of  is compiled with two macros  and  set to positive integers. The positive value for these macros results in the creation of an  with size .When  is called with the parameter  it makes sure to  to the list if it doesn't already exist:Then, if a new empty tuple is requested, the one that is located in the  is going to get returned instead of a new instance: One additional reason causing an incentive to do this is the fact that function calls construct a tuple to hold the positional arguments that are going to be used. This can be seen in the  function in :which is called via  in the same file. If the number of arguments  is zero, an empty tuple is going to be returned. In essence, this might be an operation that's performed frequently so it makes sense to not reconstruct an empty tuple every single time.A couple more answers shed light on 's caching behaviour with immutables:"},
{"body": "Building on:\n1 - ease of design / integration - learning curve2 - support / availability for *nix, Windows, Mac, extra points for native l&f, support for mobile or web3 - pythonic API4 - quality of documentation - I want to do something a bit more complicated, now what?5 - light weight packaging so it's not necessary to include a full installer (py2exe, py2app would ideally work as-is and not generate a gazillion MBs file)6 - licensing7 - others? (specify)1 - tkinter, as currently supported (as of 2.6, 3.0)2 - pyttk library3 - pyGTK4 - pyQt5 - wxPython6 - HTML-CGI via Python-based framework (Django, Turbogears, web.py, Pylons...) or Paste7 - others? (specify)Please don't hesitate to expand this answer.Tkinter is the toolkit that comes with python. That means you already have everything you need to write a GUI. What that also means is that if you choose to distribute your program, most likely everyone else already has what they need to run your program.Tkinter is mature and stable, and is (at least arguably) quite easy to use.I found it easier to use than wxPython, but obviously that's somewhat subjective. Tkinter gets a bad rap for looking ugly and out of date. While it's true that it's easy to create ugly GUIs with Tkinter, it's also pretty easy to create nice looking GUIs. Tkinter doesn't hold your hand, but it doesn't much get in the way, either. Tkinter looks best on the Mac and Windows since it uses native widgets there, but it looks OK on linux, too. The other point about the look of Tkinter is that, for the most part, look isn't as important as people make it out to be. Most applications written with toolkits such as Tkinter, wxPython, PyQT, etc are special-purpose applications. For the types of applications these toolkits are used for, usability trumps looks. If the look of the application is important, it's easy enough to polish up a Tkinter application.Tkinter has some features that other toolkits don't come close to matching. Variable traces, named fonts, geometry (layout) managers, and the way Tkinter processes events are still the standard to which other toolkits should be judged.   On the downside, Tkinter is a wrapper around a Tcl interpreter that runs inside python. This is mostly invisible to anyone developing with Tkinter, but it sometimes results in error messages that expose this architecture. You'll get an error complaining about a widget with a name like \".1245485.67345\" which will make almost no sense to anyone unless you're also familiar with how Tcl/tk works.Another downside is that Tkinter doesn't have as many pre-built widgets as wxPython. The hierarchical tree widget in Tkinter is a little weak, for example, and there's no built-in table widget. On the other hand, Tkinter's canvas and text widgets are extremely powerful and easy to use. For most types of applications you will write, however, you'll have everything you need. Just don't expect to replicate Microsoft Word or Photoshop with Tkinter. I don't know what the license is for Tkinter, I assume the same as for python as a whole. Tcl/tk has a BSD-style license. It's build on top of , a C++ framework. It's quite advanced and has some good tools like the Qt Designer to design your applications. You should be aware though, that it doesn't feel like Python 100%, but close to it. The  is excellentThis framework is really good. It's being actively developed by Trolltech, who is owned by Nokia. The bindings for Python are developed by Riverbank.PyQt is available under the GPL license or a commercial one. The price of a riverbank PyQt license is about 400 euro per developer.Qt is not only a GUI-framework but has a lot of other classes too, one can create an application by just using Qt classes. (Like SQL, networking, scripting, \u2026)Qt used to emulate GUI elements on every platform but now uses native styles of the platforms (although not native GUI toolkits): see  and Packaging is as simple as running py2exe or pyInstaller. The content of my PyQt app looks like this on windows (I have used InnoSetup on top of it for proper installation):QT comes with a widget designer and even in recent versions with an  to help design Qt software.PySide is a LGPL binding to Qt. It's developed by nokia as a replacement for the GPL PyQt.wxPython is a binding for Python using the -Framework. This framework is under the LGPL licence and is developed by the open source community.What I'm really missing is a good tool to design the interface, they have about 3 but none of them is usable.One thing I should mention is that I found a bug in the tab-view despite the fact that I didn't use anything advanced. (Only on Mac OS X) I think  isn't as polished as .wxPython is really only about the GUI-classes, there isn't much else.wxWidgets uses native GUI elements.An advantage wxPython has over Tkinter is that wxPython has a much larger library of widgets from which to choose from. I haven't got any experience with other GUI frameworks, maybe someone else has.I would definitely appreciate it if anyone knows of something better than what's commonly discussed; I see to have headaches finding something appropriate...Qt is great, but PyQt doesn't seem to have the same development resources. It seems to have some clever way to generate bindings, but isn't complete (e.g. PyKDE terminal kpart) and there is a dearth of documentation (as the developers admit). Compatibility with Qt's UI designer is nice.wxpython - controls aren't as nice looking, widget library isn't as large as KDE.OpenGL - doesn't even support fonts by default... pygame is okay, but opengl being a state machine is too annoying (object oriented models prevent making the a call in the wrong state).XUL - neat idea, I wish it worked. The  tutorial didn't work for me, though -- first I had to add the xulrunner /usr/lib path to LD_LIBRARY_PATH, then it still had problems with \"from xpcom import components\"...my wishlist for a ui library would beIn my experience, html is so much easier to get something good-looking up than UI libraries. - after working with PyQt 4 for a while, it gets the job done for simple UI's. I'm currently not developing for end users, so looks don't matter. The QTextBrowser is very useful for displaying basic HTML tables and generating HTML links..You can use either Swing, Applet, or other GUI frameworks available to Java platform. See  for  and . There are plenty of books and  such as .Here's a Hello world Swing application from .Here's a Jython  (60 lines).If you are just interested in drawing lines and circles you can probably cut it down to half.I'm just weighing in to say that TKinter sucks. It sadly seems that it is packed with Python because of backwards compatibility. The documentation is horrible. It looks horrible. I have run into some bizarre bugs that will actually crash Python.Pro wxPython"},
{"body": "I'm running Python 2.5, so this question may not apply to Python 3. When you make a diamond class hierarchy using multiple inheritance and create an object of the derived-most class, Python does the Right Thing (TM). It calls the constructor for the derived-most class, then its parent classes as listed from left to right, then the grandparent. I'm familiar with Python's ; that's not my question. I'm curious how the object returned from super actually manages to communicate to calls of super in the parent classes the correct order. Consider this example code:The code does the intuitive thing, it prints:However, if you comment out the call to super in B's init function, neither A nor C's init function is called. This means B's call to super is somehow aware of C's existence in the overall class hierarchy. I know that super returns a proxy object with an overloaded get operator, but how does the object returned by super in D's init definition communicate the existence of C to the object returned by super in B's init definition? Is the information that subsequent calls of super use stored on the object itself? If so, why isn't super instead self.super?Edit: Jekke quite rightly pointed out that it's not self.super because super is an attribute of the class, not an instance of the class. Conceptually this makes sense, but in practice super isn't an attribute of the class either! You can test this in the interpreter by making two classes A and B, where B inherits from A, and calling . It has no  or  attributes.Change your code to this and I think it'll explain things (presumably  is looking at where, say,  is in the ?):If you run it you'll see:Also it's worth checking out .I have provided a bunch of links below, that answer your question in more detail and more precisely than I can ever hope to. I will however give an answer to your question in my own words as well, to save you some time. I'll put it in points -\n\n\n\n\n\n\n\n\n  I hope this helps clear it up.just guessing: in all the four methods refer to the same object, that is, of class .\nso, in , the call to to  knows the whole diamond ancestry of  and it has to fetch the method from 'after' . in this case, it's the  class. knows the  class hierarchy.  This is what happens inside B's init:This resolves the central question,Namely, in B's init definition,  is an instance of , and thus communicates the existence of .  For example  can be found in ."},
{"body": "I'm looking for a python library for finding the longest common substring from a set of python strings.I'have read that it exist to way to solve this problem :\n- one using suffix trees\n- the other using dynamic programming.The method implemented is not important. Otherwise, it is important to have a implementation that can be use for a set of strings and not only two stringsThese paired functions will find the longest common string in any arbitrary array of strings:No doubt the algorithm could be improved and I've not had a lot of exposure to Python, so maybe it could be more efficient syntactically as well, but it should do the job. in-lined the second is_substr function as demonstrated by J.F. Sebastian. Usage remains the same. Note: no change to algorithm.Hope this helps,Jason.I prefer this for , as I find it a bit more readable and intuitive:From You could use the SuffixTree module that is a wrapper based on an ANSI C implementation of generalised suffix trees. The module is easy to handle....Take a look at:  This adds very little to jtjacques' answer. However, hopefully, this should be more readable  faster  it didn't fit in a comment, hence why I'm posting this in an answer. I'm not satisfied about , to be honest.This can be done shorter:set's are (probably) implemented as hash-maps, which makes this a bit inefficient. If you (1) implement a set datatype as a trie and (2) just store the postfixes in the trie and then force each node to be an endpoint (this would be the equivalent of adding all substrings), THEN in theory I would guess this baby is pretty memory efficient, especially since intersections of tries are super-easy.Nevertheless, this is short and premature optimization is the root of a significant amount of wasted time.If someone is looking for a generalized version that can also take a list of sequences of arbitrary objects:"},
{"body": "It is universally agreed that a list of n  symbols has n! permutations. However, when the symbols are not distinct, the most common convention, in mathematics and elsewhere, seems to be to count only distinct permutations. Thus the permutations of the list  are usually considered to be\n. Indeed, the following C++ code prints precisely those three:On the other hand, Python's  seems to print something else:This printsAs user Artsiom Rudzenka pointed out in an answer, the  says so:My question: why was this design decision made? It seems that following the usual convention would give results that are more useful (and indeed it is usually exactly what I want)... or is there some application of Python's behaviour that I'm missing? [Or is it some implementation issue? The algorithm as in  \u2014 for instance explained on StackOverflow  and  \u2014 seems efficient and implementable in Python, but is Python doing something even more efficient since it doesn't guarantee lexicographic order based on value? And if so, was the increase in efficiency considered worth it?]I can't speak for the designer of  (Raymond Hettinger), but it seems to me that there are a couple of points in favour of the design:First, if you used a -style approach, then you'd be restricted to passing in objects that support a linear ordering. Whereas  provides permutations of  kind of object. Imagine how annoying this would be:Second, by not testing for equality on objects,  avoids paying the cost of calling the  method in the usual case where it's not necessary.Basically,  solves the common case reliably and cheaply. There's certainly an argument to be made that  ought to provide a function that avoids duplicate permutations, but such a function should be in addition to , not instead of it. Why not write such a function and submit a patch?I'm accepting the answer of Gareth Rees as the most appealing explanation (short of an answer from the Python library designers), namely, that Python's  doesn't compare the values of the elements. Come to think of it, this is what the question asks about, but I see now how it could be seen as an advantage, depending on what one typically uses  for.Just for completeness, I compared three methods of generating all  permutations. Method 1, which is very inefficient memory-wise and time-wise but requires the least new code, is to wrap Python's , as in zeekay's answer. Method 2 is a generator-based version of C++'s , from . Method 3 is something I wrote that is even closer to ; it modifies the list in-place (I haven't made it too general).Here are some results. I have even more respect for Python's built-in function now: it's about three to four times as fast as the other methods when the elements are all (or almost all) distinct. Of course, when there are many repeated elements, using it is a terrible idea.The code is  if anyone wants to explore.It's fairly easy to get the behavior you prefer by wrapping , which might have influenced the decision. As described in the documentation,  is designed as a collection of building blocks/tools to use in building your own iterators.However, as pointed out in the comments, this might not be quite as efficient as you'd like:Perhaps if there is enough interest, a new function or an optional argument to  could be added to , to generate permutations without duplicates more efficiently.I find also surprising that  doesn't have a function for the more intuitive concept of unique permutations. Generating repetitive permutations only to select the unique among them is out of the question for any serious application.I have written my own iterative generator function which behaves similarly to  but does not return duplicates. Only permutations of the original list are considered, sublists may be created with the standard  library.Maybe i am wrong but seems that reason for this is in  \nYou have specified (1,1,2) and from your point of view 1 at the 0 index and 1 at the 1 index are the same - but this in not so since permutations python implementation used indexes instead of values.So if we take a look at the default python permutations implementation we will see that it uses indexes:For example if you change your input to [1,2,3] you will get correct permutations([(1, 2, 3), (1, 3, 2), (2, 1, 3), (2, 3, 1), (3, 1, 2), (3, 2, 1)]) since the values are unique."},
{"body": "CPython has a strange behaviour where it sets modules to None during shutdown. This screws up error logging during shutdown of some multithreading code I've written.I can't find any documentation of this behaviour. It's mentioned in passing in :There are  and the .I've also found a  and a :Where is this behaviour documented? Is it Python 2 specific?The behaviour is  well documented, and is present in all versions of Python from about 1.5-ish :The only documentation for the behaviour is the :Note that setting the values to  is an optimisation; the alternative would be to delete names from the mapping, which would lead to different errors ( exceptions rather than s when trying to use globals from a  handler).As you found out on the mailinglist, the behaviour predates the cyclic garbage collector; it was , while the cyclic garbage collector was . Since function objects always reference the module  all function objects in a module involve circular references, which is why the  needed clearing before GC came into play.It was kept in place even when cyclic GC was added, because there might be objects with  methods involved in cycles. These , and cleaning out the module dictionary would at least remove the module  from such cycles. Not doing that would keep  referenced globals of that module alive.The changes made for  now make it possible for the garbage collector to clear cyclic references with objects that provide a  finalizer, removing the need to clear the module  . The code is  but this is only triggered if the  attribute is still alive even after moving the contents of  to weak references and starting a GC collection run when the interpreter is shutting down; the module finalizer simply decrements their reference count.There is a small amount of related documentation at the :"},
{"body": "I don't understand the difference between  and  in pymongo. On the , it saysHowever in pymongo there are two different commands  and , and the documentation for create index has:Am I right in understanding that  will create a permanent index, or do I need to use  for this?Keep in mind that in Mongo 3.x  is deprecated and should be discouraged.The same is in :Which means that you should always use .@andreas-jung is right in that  is a wrapper over , I think the confusion arises with the phrase:It's not that the index is temporary or \"transient\", what happens is that during the specified amount of seconds, a call to  trying to create the same index again will  have any effect and will  call  underneath, but after that \"cache\" expires, a call to   again call  underneath.I perfectly understand your confusion because quite frankly PyMongo's docs don't make a very good job at explaining how this works, but if you head over to the , the explanation is a little clearer:I'm not claiming drivers work exactly the same, it's just that for illustration purposes their explanation is a little better IMHO.The  method in the Interactive Shell and  in the python driver are different things, although the same word is used. Both the  and  method from the python driver create an index permanently.Maybe one would use  with a reasonable TTL in such a situation, because I am not sure if  would recreate the index each time you call it. Recreation normally is not desired and it could be a heavy operation. But even  (of the python or also ruby driver) could possibly recreate the index whenever the TTL is expired or when you call it from a different client instance or after a restart. I am not sure about this.Maybe an even better possibility is to first check, using the method , if the index already exists. If it already exists you would not create it again.I am now demonstrating how the term  (or ) is used with 2 different meanings:This is what the  method  does:Also the  behaves this way:(Search for  in the file .)The same identifier is used with a different meaning here, which I find confusing.The python and the ruby driver store information in memory about indexes that were created recently, and they call this behaviour 'caching'. They do not tell the database about this caching. The result of this mechanism is, if you call  or  for the first time with a TTL value (time to live), then the driver will insert the index in the database and will remember this insertion and also store the TTL information in memory. What is cached here is the time and which index it was.The next time you call  with the same index of the same collection on the same driver instance, the  command will only insert the index again, if TTL seconds have not yet passed since the first call.If you call , the index will always be inserted, no matter how much time passed since the first call, and of course also if this is the first call.This is the python driver, search for  in the file :And the ruby driver, search for  in the file :(Note that different client instances do not know about the caching of the others, this information is kept in memory only and it is per instance. If you restart the client application the new instance does not know about the old 'cached' index inserts. Also other clients do not know, they do not tell each other.)I was not yet able to fully understand, what happens in the db, when the python driver or the ruby driver insert an index that is already there. I would suspect they do nothing in this case, which makes more sense and would also match the behaviour of the  and the JS driver. indexes are permanent.\nensure_index() is just a tiny wrapper around create_index().\"\"\"\nThe ensureIndex() function only creates the index if it does not exist.\n\"\"\"There is nothing like a transient index or a temporary index.I would recommend creating metaclass and ORM.\nFrom metaclass  call init_schema method for initializing the counters, schema, keys etc.\nThis way you prevent calling ensure_index every query or collection update :)"},
{"body": "With Python 3:I wanted to check for equality:But:Do you know why values are not equal?I've tested this with Python 3.4 and 3.5.In Python 3,  and  return special iterable classes - respectively a  and a . The first one inherit it's  method from , the second uses the default  which tests on object identity. In python3,  and  are  objects:Don't compare them as an  object, c\nonvert them to lists and then compare them:Investigating why it works for comparing keys, in  of CPython,  is inheriting from  while  does not:Unfortunately, both current answers don't address why this is but focus on how this is done. That mailing list discussion was amazing, so I'll sum things up:For / and /: Now, about /:As stated in a comment by  and by  in the mailing list, / is a multiset, a generalization of sets that allows multiple instances of it's elements. \nTrying to compare these is not as trivial as comparing  or  due to the inherent duplication, the ordering and the fact that you probably need to take into consideration the keys that correspond to those values. Should  that look like this: actually be equal even though the values that correspond to the keys isn't the same? Maybe? Maybe not? It isn't straight-forward either way and will lead to inevitable confusion.The point to be made though is that it isn't trivial to compare these as is with  and , to sum up, with another comment from @abarnett on :"},
{"body": "I want to remove a Python installed in location that brew complains about, when I run The message from the brew git : \nSpecifically, I want to remove the entire Python.framework, those files located here. \n/Library/Frameworks/Python.framework/I have Python 2.7.5 installed natively with Mavericks that I'll use instead - located in the path below. (The difference being -- I believe -- that its put in the  instead of the .) The good, native location is here:\nand I already installed python 3.x with , which put it here:Can I just delete these files or are their consequences (beyond having to relink)?\nHere are steps to remove python from a  and a Is that approach still sound? Is there anything I should be aware of?I'll self-answer. I went through steps and it's straight forward. Pycharms (the IDE I'm use) automatically found the new libraries too. Here are the steps I followed to remove the extra Python libraries on Mavericks that were not native to it and not installed by brew.\nThe native Python 2.7.x version lives here  (or 2.6, etc), so you can remove any Python that got installed elsewhere.Or, according to this , you should brew install both python 2.7 and python 3.x,\nand avoid using system python in Mavericks.\nRemove python in Applications directory (the one where all your apps are). cd into folder  and  to see what have. Then remove:\nsample output: Run steps recommended by Sample output"},
{"body": "I'm getting an error when trying to execute python program that uses multiprocessing package:It looks like the user doesn't have permission to access shared memory. When executing with root privileges it works fine.Is there any solution to run it as normal user(not root)?Python version 2.6.2 , OS is Linux 2.6.18 (CentOS release 5.4) and it's VPS machine.For POSIX semaphores to work, the users need r/w access to shared memory ().Check the permissions to . On my laptop (Ubuntu) it looks like this:To permanently set the correct permissions (even after a reboot), add the following to your :Haven't tried this, just copied from a .In my OVH VPS Classic, this error was caused by a loop in /dev/shm and /run/shm.\nBoth were symlinks linking to each other.\nSo as root here is what I did:Then I modified the shm line from:To:Restarted the server... And that fixed the problem!\nAlternatively you can mount shm manually:Hope this helps :-)"},
{"body": "In my view function I want to call another view and pass data to it :, where backend is of registration.backends object, and form.cleaned_data is a dict of form data (but both must be either sent as *args or **kwargs to prevent raising  error). From what I've found in the docs :It looks like I need to provide 'some-view-name' argument, but is it just the name of the view function, or the name of the url ? So I would like to make it similar to the way it's done in django-registration, where :Ok so now, can I call directly my view function or do I need to provide a url for it ? And what more important, how my funciotn call (and a url if needed) should look like ? Both backend, and cleaned_data are just passed through this view for a later usage. I've tried this, but it's improper :  As well as this :still NoReverseMatch . But in django-registration, I've seen something like this :Firstly, your URL definition does not accept any parameters at all. If you want parameters to be passed from the URL into the view, you need to define them in the urlconf.Secondly, it's not at all clear what you are expecting to happen to the cleaned_data dictionary. Don't forget you can't redirect to a POST - this is a limitation of HTTP, not Django - so your cleaned_data either needs to be a URL parameter (horrible) or, slightly better, a series of GET parameters - so the URL would be in the form:and so on. In this case, field1, field2 and field3 are  included in the URLconf definition - they are available in the view via .So your urlconf would be:and the view would look like:and the reverse would be (after importing ):The whole point of using redirect and reverse, as you have been doing, is that you go to the URL - it returns an Http code that causes the browser to redirect to the new URL, and call that.If you simply want to call the view from within your code, just do it directly - no need to use reverse at all.That said, if all you want to do is store the data, then just put it in the session:urls.py:views.py:"},
{"body": "I have a vpn connection and when I'm running python -m SimpleHTTPServer, it serves on 0.0.0.0:8000, which means it can be accessed via localhost  via my real ip.\nI don't want robots to scan me and interested that the server will be accessed only via localhost.Is it possible? Any other simple http server which can be executed instantly using the command line is also welcome.If you read the source you will see that only the port can be overridden on the command line.  If you want to change the host it is served on, you will need to implement the  method of the  and  yourself.  But that should be really easy.Here is how you can do it, pretty easily:And to use it:  As  explained, simply doing it by using the nice  method won't be possible, because the IP address is hardcoded in the implementation of the  function.A way of doing it from the command line without writing code to a file first would beIf that still counts as a one liner depends on your terminal width ;-) It's certainly not very easy to remember.In Python versions 3.4 and higher, the  module accepts a  parameter.According to the docs:"},
{"body": "So I want to create a list which is a sublist of some existing list.For example,, I want to create a sublist  such that  contains all the elements in  at odd positions.While I can do it byBut I want to know if there is another way to do the same efficiently and in fewer number of steps. Yes, you can:And this is all. The result will contain the elements placed on the following positions (-based, so first element is at position , second at  etc.):so the result (actual numbers) will be:The  at the end is just a notation for list slicing. Usually it is in the following form:If we omitted , the default () would be used. So the first element (at position , because the indexes are -based) would be selected. In this case the second element will be selected.Because the second element is omitted, the default is being used (the end of the list). So the list is being iterated .We also provided third argument () which is . Which means that one element will be selected, the next will be skipped, and so on...So, to sum up, in this case  means:: @PreetKukreti gave a link for another explanation on Python's list slicing notation. See here: In your code, you explicitly create and increase the counter. In Python this is not necessary, as you can enumerate through some iterable using :The above serves exactly the same purpose as the code you were using:More on emulating  loops with counter in Python: For the  positions, you probably want:I like List comprehensions because of their Math (Set) syntax. So how about this:Basically, if you enumerate over a list, you'll get the index  and the value . What I'm doing here is putting the value  into the output list (even or odd) and using the index  to find out if that point is odd ()."},
{"body": "I am re-factoring my Flask application by scattering the models, blueprints but I am having a runtime error. I have the following problem(the sample project are hosted here: ):I did a research on this topic. The re-factoring is suggested here:The same problem was raised here:And the above thread(2010) suggested a hack like this:Did anyone know how to do this properly? How did you solve it?ThanksThis has to do with Flask's . When initialized with , Flask-SQLAlchemy doesn't know which app is the \"current\" app (remember, Flask allows for  in the same interpreter). You could have multiple apps using the same  instance in the same process, and Flask-SQLAlchemy would need to know which is the \"current\" one (due to Flask's  nature of everything).If you need to do this during runtime, you must explicitly say which app is the \"current\" app for all calls. You can do this by changing your code to do the following:If you are writing a standalone script that needs the app context, you can push the context at the beginning rather than putting everything in a  block.If you write a command for Flask's  the command will automatically have access to the context.Mark's answer was great and it helped me a lot.  However, another way to approach this is to run the code that relies on the app context in a function decorated with @app.before_first_request.  See  for more information.  That's in fact how I ended up doing it, largely because I wanted to be able to call the initialization code outside of flask as well, which I handle this way.In my case I want to be able to test SQLAlchemy models as plain SQLAlchemy models without Flask-SQLAlchemy, though the db in the code below is simply a (Flask) SQLAlchemy db."},
{"body": "Both my professor and  claim that  creates a list of values. I believe this is to be inaccurate because:Furthermore, the only apparent way to access the integers created by  is to iterate through them, which leads me to believe that labeling  as a lists is incorrect.In Python 2.x,  returns a list, but in Python 3.x  returns an immutable sequence, of type .In Python 2.x, if you want to get an iterable object, like in Python 3.x, you can use  function, which returns an immutable sequence of type .Advantage of  over  in Python 2.x:Nope. Since  objects in Python 3 are immutable sequences, they support indexing as well. Quoting from the  function documentation,For example,All these are possible with that immutable  sequence.Recently, I faced a problem and I think it would be appropriate to include here. Consider this Python 3.x codeOne would expect this code to print every ten numbers as a list, till 99. But, it would run infinitely. Can you reason why?In python-2.x,  actually creates a list (which is also a sequence) whereas  creates an  object that can be used to iterate through the values.On the other hand, in python-3.x,  creates an iterable (or more specifically, a sequence)range creates a list if the python version used is 2.x .\nIn this scenario range is to be used only if its referenced more than once otherwise use xrange which creates a generator there by redusing the memory usage  and sometimes time as it has lazy approach. xrange is not there in python 3.x rather range stands for what xrange is for python 2.xrefer to question\n "},
{"body": "For an exercise I'm doing, I'm trying to read the contents of a given file twice using the  method. Strangely, when I call it the second time, it doesn't seem to return the file content as a string?Here's the codeOf course I know that this is not the most efficient or best way, this is not the point here. The point is, why can't I call  twice? Do I have to reset the file handle? Or close / reopen the file in order to do that?Calling  reads through the entire file and leaves the read cursor at the end of the file (with nothing more to read). If you are looking to read a certain number of lines at a time you could use  ,  or iterate through lines with .To answer your question directly, once a file has been read, with  you can use  to return the read cursor to the start of the file (docs are ). If you know the file isn't going to be too large, you can also save the  output to a variable, using it in your findall expressions.Ps. Dont forget to close the file after you are done with it ;)The read pointer moves to after the last read byte/character. Use the  method to rewind the read pointer to the beginning.Everyone who has answered this question so far is absolutely right -  moves through the file, so after you've called it, you can't call it again.What I'll add is that in your particular case, you don't need to seek back to the start or reopen the file, you can just store the text that you've read in a local variable, and use it twice, or as many times as you like, in your program:yeah, as above...i'll write just an example:Every open file has an associated position.\nWhen you read() you read from that position.\nFor example  reads the first 10 bytes from a newly opened file, then another  reads the next 10 bytes.\n without arguments reads all of the contents of the file, leaving the file position at the end of the file. Next time you call  there is nothing to read.You can use  to move the file position. Or probably better in your case would be to do one  and keep the result for both searches. . So, you could  the file, or  to the start before re-reading. Or, if it suites your task, you can use  to consume only  bytes.I always find the read method something of a walk down a dark alley. You go down a bit and stop but if you are not counting your steps you are not sure how far along you are. Seek gives the solution by repositioning, the other option is Tell which returns the position along the file. May be the Python file api can combine read and seek into a read_from(position,bytes) to make it simpler - till that happens you should read ."},
{"body": "Say I have an array :I would like to convert it to a 1D array (i.e. a column vector):but this returnswhich is not the same as:I can take the first element of this array to manually convert it to a 1D array:but this requires me to know how many dimensions the original array has (and concatenate [0]'s when working with higher dimensions)Is there a dimensions-independent way of getting a column/row vector from an arbitrary ndarray?Use  (for a 1D view) or  (for a 1D copy) or  (for an 1D iterator):Note that  returns a  of  when possible. So modifying  also modifies .  returns a  when the 1D elements are contiguous in memory, but would return a  if, for example,  were made from slicing another array using a non-unit step size (e.g. ).If you want a copy rather than a view, useIf you just want an iterator, use :or, simply:"},
{"body": "I'm trying to write a custom filter method that takes an arbitrary number of  and returns a list containing the elements of a database-like list that contain those .For example, suppose  and  = the same thing.  results in True. But suppose  = the same thing plus a bunch of other things. My method needs to be able to tell if , but Python can't do that with dictionaries.Context:I have a Word class, and each object has properties like , , , and so on. I want to be able to call a filter method on the main list of these words, like . I can't figure out how to manage these keys and values at the same time. But this could have larger functionality outside this context for other people.Convert to item pairs and check for containment.Optimization is left as an exercise for the reader.Note for people that need this for unit testing: there's also an  method in Python's  class.It's however deprecated in 3.2, not sure why, maybe there's a replacement for it.for keys and values check use: \nif you need to check only keys:\n        context:For completeness, you can also do this:However, I make no claims whatsoever concerning speed (or lack thereof) or readability (or lack thereof).In Python 3, you can use  to get a set-like view of the dict items.  You can then use the  operator to test if one view is a \"subset\" of the other:In Python 2.7, use the  to do the same:In Python 2.6 and below you will need a different solution, such as using :This seemingly straightforward issue costs me a couple hours in research to find a 100% reliable solution, so I documented what I've found in this answer.My function for the same purpose, doing this recursively:In your example,  should return True even if d2 has other stuff in it, plus it applies also to lower levels:Notes:  There could be even better solution which avoids the  clause and applies to even wider range of cases (like lists of hashes etc).  Also recursion is not limited here so use at your own risk. ;)This function works for non-hashable values. I also think that it is clear and easy to read."},
{"body": "I have a named tuple class in python What I'd like to do is turn this into a dictionary. I'll admit python is not one of my stronger languages. The key is that I dont want it to be rigidly tied to the name or numbers of the fields I have. Is there a way to write it such that I could add more fields, or pass an entirely different named tuple in and get a dictionary.Edit: I cant not alter the original class definition as its in someone elses code. So I need to take an instance of a Town and convert it to a dictionary.Here is a demonstration of the usage:This is a  of namedtuples, i.e. unlike the usual convention in python .  Along with the other methods added to namedtuples, , , , , it has the underscore only to try and prevent conflicts with possible field names.  For some 2.7.5 < python version < 3.5.0 code out in the wild, you might see this version:For a while the documentation had mentioned that  was obsolete (see ), and suggested to use the built-in method .  That advice is now outdated; in order to fix  related to subclassing, the  property which was present on namedtuples has again been removed by .  There's a built in method on  instances for this, .As discussed in the comments, on some versions  will also do it, but it's apparently highly dependent on build details, whereas  should be reliable. In some versions  was marked as deprecated, but comments indicate that this is no longer the case as of 3.4.On Ubuntu 14 LTS versions of python2.7 and python3.4 the  property worked as expected. The   also worked, but I'm inclined to use the standards-defined, uniform, property api instead of the localized non-uniform api.$ python2.7Seeing as  is the semantic way to get a dictionary representing soemthing, (at least to the best of my knowledge).It would be nice to accumulate a table of major python versions and platforms and their support for , currently I only have one platform version and two python versions as posted above.Python 3. Allocate any field to the dictionary as the required index for the dictionary, I used 'name'. "},
{"body": "this is just for academic interest. I encounter the following situation a lot.is there any pythonic way of doing it, or in general better programming way of doing it.\nBasically do something3 executes only if or elif is true.Your code is  optimal, as far as code repetition and evaluation are concerned. The only thing I can think of to avoid repetition would be:This removes one assignment, although the total number of lines doesn't change.The advantage is that this works with  conditions, without adding any other assignment, while your current solution requires an  for every condition.In my opinion they have about the same degree of readability, but the above code will be better with more conditions.Also there's no \"pythonic\" way other then a readable solution that avoids code repetition and is optimal in terms of efficiency, and I don't know of any kind of \"better programming\" to achieve the same result.You could also omit the  flag completely if  is a single line of code (e.g. a function call):It maintains the nice property of evaluating  and  at most once (and  won't be evaluated if  is true).I would handle this by using nested if statements i.e.As some comments have pointed out, the best solution will depend on what x & y are. If easy readability / concise code is your goal then this or other answers given should be fine. If however x & y were expensive function calls then it would be better to do something more like what you have done to avoid calling the function twice.You could wrap some of it in a function:Or all of it in a function:In the spirit of offering a completely different solution to those already proposed, you could set up a list structured dictionaries which allows you to set up multiple cases bound to your predefined \"somethings\"I actually really like this method (and I've only just discovered it when trying to come up with a unique answer to this question, thanks!) - it's really obvious which case is bound to which action, and it's easy to add several more cases without adding any more if/else statements. Of course, this does evaluate  twice.  Usually that's not a big deal, but if it is expensive, you can always evaluate that upfront and save it to a temporary variable.For all of these suggestions and any others you come up with, note that if  and  are expensive expressions:you can assign the values these expressions return to quick-to-evaluate variables first:I would wrap the ..do somethings in functions and write one if-elif:This is nice if the .. do somethings involve a lot of stuff.if do something is quite short, like do(1), do(2) or something like that, you can just do it like this:"},
{"body": "I need to make some command line calls to linux and get the return from this, however doing it as below is just returning  when it should return a time value, like , I am testing the exact same call in regular command line and it returns the time value  so I am confused as to what I am doing wrong as I thought this was how to do it in python. Any advice is appreciated.What gets returned is the return value of executing this command. What you see in while executing it directly is the output of the command in stdout. That 0 is returned means, there was no error in execution.Use popen etc for capturing the output .Some thing along this line:or ON SO : If you're only interested in the output from the process, it's easiest to use subprocess'  function:Then output holds the program output to stdout. Check the link above for more info.Your code returns  if the execution of the commands passed is successful and non zero if it fails. The following program works on python2.7, haven checked 3 and versions above. Try this code.Yes it's counter-intuitive and does not seem very pythonic, but it actually just mimics the unix API design, where these calld are C POSIX functions. Check  && Somewhat more convenient snippet to replace  that I use: The simplest way is like this:This will be returned as a listThis is an old thread, but purely using , the following's a valid way of accessing the data returned by the  call. Note: it does use a pipe to write the data to a file on disk.  OP didn't specifically ask for a solution using . accordingly,No idea why they are all returning zeroes though.For your requirement, Popen function of subprocess python module is the answer. For example,I can not add a comment to  because I do not have \"50 reputation\" so\nI will add a new entry.  My apologies.   is the best for multiple/complicated commands (my opinion) and also for getting the return value in addition to getting stdout like the following more complicated multiple commands:This will list all python files that have the word  anywhere in their name.  The  is a list comprehension to remove (strip) the newline character from each entry.  The  is a shell command to show the return status of the last command executed which will be the grep command and the last item of the list in this example.  the  says to print the  of the  command to  so it does not show up in the output.  The  before the  command is to use the raw string so the shell will not interpret metacharacters like  incorrectly.  This works in python 2.7.  Here is the sample output:okey I believe the fastest way it would be"},
{"body": "Suppose I have a model like this:Can I create a dictionary, and then insert or update the model using it?Here's an example of create using your dictionary d:To update an existing model, you will need to use the QuerySet  method.  Assuming you know the  of the Book you want to update:If you know you want to create it:Assuming you need to check for an existing instance, you can find it with get or create:As mentioned in another answer, you can also use the  function on the queryset manager, but i believe that will not send any signals out (which may not matter to you if you aren't using them). However, you probably shouldn't use it to alter a single object:Use  for creating a new model. Loop through the dictionary and use  in order to update an existing model.From Tom Christie's Django Rest Framework"},
{"body": "I have a long sequence of hex digits in a string, such as only much longer, several kilobytes.  Is there a builtin way to convert this to a bytes object in python 2.6/3?Works in Python 2.7 and higher including python3: There seems to be a bug with the  function in Python 2.6. The python.org documentation states that the function accepts a string as an argument, but when applied, the following error is thrown: You can do this with the hex codec.  ie:Try the "},
{"body": "Problem: When POSTing data with Python's urllib2, all data is URL encoded and sent as Content-Type: application/x-www-form-urlencoded.  When uploading files, the Content-Type should instead be set to multipart/form-data and the contents be MIME encoded. A discussion of this problem is here:\nTo get around this limitation some sharp coders created a library called MultipartPostHandler which creates an OpenerDirector you can use with urllib2 to mostly automatically POST with multipart/form-data. A copy of this library is here:\nI am new to Python and am unable to get this library to work. I wrote out essentially the following code. When I capture it in a local HTTP proxy, I can see that the data is still URL encoded and not multi-part MIME encoded. Please help me figure out what I am doing wrong or a better way to get this done. Thanks :-)EDIT1: Thanks for your response. I'm aware of the ActiveState httplib solution to this (I linked to it above). I'd rather abstract away the problem and use a minimal amount of code to continue using urllib2 how I have been. Any idea why the opener isn't being installed and used?It seems that the easiest and most compatible way to get around this problem is to use the 'poster' module.This worked perfect and I didn't have to muck with httplib. The module is available here:\nFound this recipe to post multipart using  directly (no external libraries involved)Just use , it will set proper headers and do upload for you:I what a coincide, 2 years, 6 months ago I create the project  , that fix MultipartPostHandler for utf-8 systems. I also have done some minor improvements , you are welcome to test it :) I ran into the same problem and I needed to do a multipart form post without using external libraries. I wrote a whole .I ended up using a modified version of . The code in that url actually just appends the content of the file as a string, which can cause problems with binary files. Here's my working code.To answer the OP's question of why the original code didn't work, the handler passed in wasn't an instance of a class.  The lineshould read"},
{"body": "I have:I wanted to make the  statements into a few statements. No. The method for appending an entire sequence is .No.First off,  is a function, so you can't write  because you're trying to get a slice of a thing that isn't a sequence. (You can't get an element of it, either:  is wrong for the same reason.) When you call a function, the argument goes in , i.e. the round ones: . Second, what you're trying to do is \"take a sequence, and put every element in it at the end of this other sequence, in the original order\". That's spelled .  is \"take this thing, and put it at the end of the list, , \". (Recall that a list is a kind of sequence.)But then, you need to be aware that  is a special construct that appears only inside square brackets (to get a slice from a sequence) and braces (to create a  object). You cannot pass it to a function. So you can't  with that. You need to make a sequence of those values, and the natural way to do this is with the  function.You could also:"},
{"body": "So let's say I wanna make a dictionary. We'll call it . But there are multiple ways to initialize a dictionary in Python! For example, I could do this:Or I could do this:Or this, curiously:Or this:And a whole other multitude of ways with the  function. So obviously one of the things  provides is flexibility in syntax and initialization. But that's not what I'm asking about.Say I were to make  just an empty dictionary. What goes on behind the scenes of the Python interpreter when I do  versus ? Is it simply two ways to do the same thing? Does using  have the  call of ? Does one have (even negligible) more overhead than the other? While the question is really completely unimportant, it's a curiosity I would love to have answered.dict() is apparently some C built-in. A really smart or dedicated person (not me) could look at the interpreter source and tell you more. I just wanted to show off dis.dis. :)As far as performance goes:@Jacob: There is a difference in how the objects are allocated, but they are not copy-on-write.  Python allocates a fixed-size \"free list\" where it can quickly allocate dictionary objects (until it fills).  Dictionaries allocated via the  syntax (or a C call to ) can come from this free list.  When the dictionary is no longer referenced it gets returned to the free list and that memory block can be reused (though the fields are reset first).This first dictionary gets immediately returned to the free list, and the next will reuse its memory space:If you keep a reference, the next dictionary will come from the next free slot:But we can delete the reference to that dictionary and free its slot again:Since the  syntax is handled in byte-code it can use this optimization mentioned above.  On the other hand  is handled like a regular class constructor and Python uses the generic memory allocator, which does not follow an easily predictable pattern like the free list above.Also, looking at compile.c from Python 2.6, with the  syntax it seems to pre-size the hashtable based on the number of items it's storing which is known at parse time.Basically, {} is syntax and is handled on a language and bytecode level. dict() is just another builtin with a more flexible initialization syntax. Note that dict() was only added in the middle of 2.x series.: thanks for the responses.  Removed speculation about copy-on-write.One other difference between  and  is that  always allocates a new dictionary (even if the contents are static) whereas  doesn't  do so (see  for when and why):produces:I'm not suggesting you try to take advantage of this or not, it depends on the particular situation, just pointing it out.  (It's also probably evident from the  if you understand the opcodes).dict() is used when you want to create a dictionary from an iterable, like :In order to create an empty set we should use the keyword set before it \ni.e  this creates an empty set where as in dicts only the flower brackets can create an empty dictLets go with an example "},
{"body": "I'm trying to write an  function for one of my models so that I can create an object by doingWhen I write the model, I have This works, and I can save the object to the database, but when I do 'User.objects.all()', it doesn't pull anything up unless I take out my  function.  Any ideas?Relying on Django's built-in functionality and passing named parameters would be the simplest way to go.But if you're set on saving some keystrokes, I'd suggest adding a static convenience method to the class instead of messing with the initializer.Django expects the signature of a model's constructor to be , or some reasonable facsimile. Your changing the signature to something completely incompatible has broken it.The correct answer is to avoid overriding  and write a classmethod as described in the .But this could be done like you're trying, you just need to add in  to be accepted by your , and pass them on to the super method call.Don't create models with args parameters.  If you make a model like so:It becomes very unreadable very quickly as most models require more than that for initialization. You could very easily end up with:Another problem here is that you don't know whether 'Bert' is the first or the last name.  The last two numbers could easily be a phone number and a system id.  But without it being explicit you will more easily mix them up, or mix up the order if you are using identifiers.  What's more is that doing it order-based will put yet another constraint on other developers who use this method and won't remember the arbitrary order of parameters.You should prefer something like this instead:If this is for tests or something like that, you can use a factory to quickly create models.  The factory boy link may be useful: "},
{"body": "I've tried different methods from other questions but still can't seem to find the right answer for my problem.  The critical piece of this is that if the person is counted as Hispanic they can't be counted as anything else.  Even if they have a \"1\" in another ethnicity column they still are counted as Hispanic not a two or more races.  Similarly, if the sum of all the ERI columns is greater than 1 they are counted as two or more races and can't be counted as a unique ethnicity(accept for Hispanic).  Hopefully this makes sense.  Any help will be greatly appreciated. Its almost like doing a for loop through each row and if each record meets a criteria they are added to one list and eliminated from the original.  From the dataframe below I need to calculate a new column based off of the following:=========================  CRITERIA  ===============================Comment: If the ERI Flag for Hispanic is True (1), then employee is classified as \u201cHispanic\u201dComment: If more than 1 non-Hispanic ERI Flag are true, return \u201cTwo or More\u201d======================  DATAFRAME ===========================In [13]: df1Out [13]: OK, two steps to this - first is to write a function that does the translation you want - I've put an example together based on your pseudo-code:You may want to go over this, but it seems to do the trick - notice that the parameter going into the function is considered to be a Series object labelled \"row\".Next, use the apply function in pandas to apply the function - e.g.Note the axis=1 specifier, that means that the application is done at a row, rather than a column level. The results are here:If you're happy with those results, then run it again, posting the results into a new column in your original dataframe.The resultant dataframe looks like this (scroll to the right to see the new column):"},
{"body": "I am interested to trigger a certain action upon receiving an email from specific \naddress with specific subject. In order to be able to do so I need to implement \nmonitoring of my mailbox, checking every incoming mail (in particular, i use gmail).\nwhat is the easiest way to do that?Gmail provides the ability to connect over POP, which you can turn on in the gmail settings panel. Python can make connections over POP pretty easily:You would just need to run this script as a cron job. Not sure what platform you're on so YMMV as to how that's done.Gmail provides an atom  for new email messages. You should be able to monitor this by authenticating with py cURL (or some other net library) and pulling down the feed. Making a GET request for each new message should mark it as read, so you won't have to keep track of which emails you've read.While not Python-specific, I've always loved  wherever I could install it...!Just use as some of your action lines for conditions of your choice  (vertical bar AKA pipe followed by the script you want to execute in those cases) and your mail gets piped, under the conditions of your choice, to the script of your choice, for it to do whatever it wants -- hard to think of a more general approach to \"trigger actions of your choice upon receipt of mails that meet your specific conditions!!  Of course there are no limits to how many conditions you can check, how many action lines a single condition can trigger (just enclose all the action lines you want in  braces), etc, etc.People seem to be pumped up about Lamson:It's an SMTP server written entirely in Python.  I'm sure you could leverage that to do everything you need - just forward the gmail messages to that SMTP server and then do what you will.However, I think it's probably easiest to do the ATOM feed recommendation above.EDIT: Lamson has been abandonedI found a  when I wanted to do this same thing (and the example uses gmail). Also check out the  on this.I recently solved this problem by using procmail and pythonRead the documentation for procmail. You can tell it to send all incoming email to a python script like this in a special procmail config filePython has an \"email\" package available that can do anything you could possibly want to do with email. Read up on the following ones...."},
{"body": "I'm using the sqlite3 module in Python 2.6.4 to store a datetime in a SQLite database. Inserting it is very easy, because sqlite automatically converts the date to a string. The problem is, when reading it it comes back as a string, but I need to reconstruct the original datetime object. How do I do this?If you declare your column with a type of timestamp, you're in clover:See? both int (for a column declared integer) and datetime (for a column declared timestamp) survive the round-trip with the type intact.It turns out that sqlite3 can do this and it's even , kind of - but it's pretty easy to miss or misunderstand.What I had to do is:If I pass in \"datetime\" instead it's silently ignored and I still get a string back. Same if I omit the quotes."},
{"body": "If I had a dictionary  and I wanted to check for  I could either do so in a  block (bleh!) or use the  method, with  as a default value.I'd like to do the same thing for . That is, I already have object to return  if it hasn't been set, but then that gives me errors likeA more direct analogue to  than  is .(Where  is optional, raising an exception on no attribute if not found.)For your example, you would pass .Do you mean  perhaps?You can also do this, which is a bit more cluttered and doesn't work for methods.For checking if a key is in a dictionary you can use : .For checking for attributes in object use the  function: "},
{"body": "Python has a nice  function. Is there a PHP equivalent?As long as all the arrays are the same length, you can use  with  as the first argument.If some of the arrays are shorter, they will be padded with nulls to the length of the longest, unlike python where the returned result is the length of the shortest array.Try this function to create an array of arrays similar to Python\u2019s :You can pass this function as many array as you want with as many items as you want. comes close.Otherwise nothing like coding it yourself:This , and is compatible also with PHP < 5.3:It merges the arrays in the manner Python's  does and does not return elements found after reaching the end of the shortest array.The following:gives the following result:and the following:gives the following result:Check  for a proof of the above.: Added support for receiving single argument ( behaves differently in that case; thanks ).The solution matching  very closely, and using builtin PHP functions at the same time, is:As I , I tend to favor nabnabit's solution (), but in a slightly modified way (shown above).In general this:is counterpart for Python's:(wrap it in  if you want list instead of iterator). Because of this, it does not exactly fit the requirement to mimic 's behaviour (unless all the arrays have the same length).I wrote a  functions for my .\n The code has been modified to allow for a Python-style  as well as Ruby-style. The difference is explained in the comments:Example:you can see  method:output:You can find  as well as other Python functions in . Including  and .To overcome the issues with passing a single array to , you can pass this function...unfortunately you can't pass  as it's not a real function but a builtin thingy.Dedicated to those that feel like it should be related to array_combine:"},
{"body": "I am looking for an efficient way to remove unwanted parts from strings in a DataFrame column.Data looks like:I need to trim these data to:I tried  and ., but got an error:  Any pointers would be greatly appreciated!i'd use the pandas replace function, very simple and powerful as you can use regex. Below i'm using the regex \\D to remove any non-digit characters but obviously you could get quite creative with regex.There's a bug here: currently cannot pass arguments to  and :EDIT: 2012-12-07 this works now on the dev branch:In the particular case where you know the number of positions that you want to remove from the dataframe column, you can use string indexing inside a lambda function to get rid of that parts:Last character:First two characters:I've found big differences in performance between the various methods for doing things like this (i.e. modifying every element of a series within a DataFrame). Often a list comprehension can be fastest - see code race below:Put this right of result column and get the result."},
{"body": "I want to print a character or string like '-' n number of times.Can I do it without using a loop?.. Is there a function like..which would mean printing the  3 times, like this:Python 2.x:Python 3.x:To print a string 3 times in Python 3.x, in this case the string is \"hello\""},
{"body": "Is it possible to call a function without first fully defining it? When attempting this I get the error: \" is not defined\". I am coming from a C++ background so this issue stumps me. Declaring the function before works:However, attempting to call the function without first defining it gives trouble:In C++, you can declare a function after the call once you place its header before it.Am I missing something here?One way that is sort of idiomatic in Python is writing:This allows you to write you code in the order you like as long as you keep calling the function  at the end.When a Python module (.py file) is run, the top level statements in it are executed in the order they appear, from top to bottom (beginning to end). This means you can't reference something until you've defined it. For example the following will generate the error shown:Unlike with many other languages,  and  statements are executable in Python\u2014not just declarative\u2014so you can't reference either until that happens and they're defined. This is why your first example has trouble\u2014you're referencing the  function before its  statement has executed and body have been processed and the resulting function object bound to the function's name, so it's not defined at that point in the script.Programs in languages like C++ are usually preprocessed before being run and during this compilation stage the entire program and any  files it refers to are read and processed all at once. Unlike Python, this language features declarative statements which allow the name and calling sequence of functions (or static type of variables) to be declared (but not defined), before use so that when the compiler encounters their name it has enough information to check their usage, which primarily entails type checking and type conversions, none of which require their actual contents or code bodies to have been defined yet.This isn't possible in Python, but quite frankly you will soon find you don't need it at all. The Pythonic way to write code is to divide your program into modules that define classes and functions, and a single \"main module\" that imports all the others and runs. For simple throw-away scripts get used to placing the \"executable portion\" at the end, or better yet, learn to use an interactive Python shell.Python is a dynamic languuage and the interpreter always takes the state of the variables (functions,...) as they are at the moment of calling them. You could even redefine the functions in some if-blocks and call them each time differently. That's why you have to define them before calling them."},
{"body": "Given  - 2D array - this is the bit of code that does the trick for me at the moment:But it's so ugly that I suspect that somewhere within NumPy there must be a function that does the same with something looking like:but if something like the above exists, I've been unable to find it.It's only worth trying to do this in-place if you are under significant space constraints. If that's the case, it is possible to speed up your code a little bit by iterating over a flattened view of the array. Since  returns a new view , the data itself isn't copied (unless the original has unusual structure). I don't know of a better way to achieve bona fide in-place application of an arbitrary Python function.Some timings:It's about twice as fast as the nested loop version:Of course vectorize is still faster, so if you can make a copy, use that:And if you can rewrite  using built-in ufuncs, then please, please, don't : does operations like  in place, just as you might expect -- so you can get the speed of a ufunc with in-place application at no cost. Sometimes it's even faster! See  for an example.By the way, my original answer to this question, which can be viewed in its edit history, is ridiculous, and involved vectorizing over indices into . Not only did it have to do some funky stuff to bypass 's , it turned out to be just as slow as the nested loop version. So much for cleverness!Q: Is it possible to map a numpy array in place?\nA: . You have to write your own code.Below a script that compares the various implementations discussed in the thread:The output of the above script - at least in my system - is:As you can observe,  compared with the second best and worst alternatives respectively.If using  is not an option, here's a comparison of the other alternatives only:HTH!Why not using numpy implementation, and the out_ trick ?got:if ufuncs are not possible, you should maybe consider using cython. \nit is easy to integrate and give big speedups on specific use of numpy arrays."},
{"body": "I'm trying to teach Komodo to fire up  when I hit the right keystrokes.  I can use the exact path of the shortcut in start menu in the Windows Explorer location bar to launch IDLE so I was hoping Komodo would be able to use it as well.  But, giving this path to Komodo causes it to say that 1 is returned.  This appears to be a failure as IDLE doesn't start up.I thought I'd avoid the shortcut and just use the exact path.  I go to the start menu, find the shortcut for IDLE, right click to look at the properties.  The target is grayed out, but says \"Python 2.5.2\".  The \"Start in\" is set to, \"C:\\Python25\\\".  The \"Open File Location\" button is also grayed out.How do I find out where this shortcut is really pointing?  I have tried starting python.exe and pythonw.exe both in C:\\Python25, but neither starts up IDLE.There's a file called idle.py in your Python installation directory in Lib\\idlelib\\idle.pyIf you run that file with Python, then IDLE should start.In Python 3.2.2, I found  which was useful because it would let me open python files supplied as args in IDLE.Here's another path you can use.  I'm not sure if this is part of the standard distribution or if the file is automatically created on first use of the IDLE.If you just have a Python shell running, type:Python installation folder > Lib > idlelib > idle.pywDouble click on it and you're good to go.You can also assign hotkeys to Windows shortcuts directly (at least in Windows 95 you could, I haven't checked again since then, but I think the option should be still there ^_^).The idle shortcut is an \"Advertised Shortcut\" which breaks certain features like the \"find target\" button. Google for more info.You can view the link with a hex editor or download  to see where it points to.In my case it runs:\nthere is a .bat script to start it (python 2.7).I setup a short cut (using windows) and set the target toworks greatAlso found this works"},
{"body": "I have the following code:but I am getting 500 errors. I am trying to send what XML is being generated and received through SUDs, to the wsdl developer, but I can't figure how to output it? I have been looking in the documentation of SUDs, but can't seem to find it :/ Does anyone know how to output the raw xml that is sent and received? SUDS provides some convenience methods to do just that:These should provide you with what you need. I use them for error logging.\n for Client class should have any extra info you need.You can use the MessagePlugin to do this (this will work on the newer Jurko fork where last_sent and last_received have been removed)Suds supports internal logging, as you have been doing.I am setting info levels like you:And I also sometimes need to override the root logger logging level, depending on the framework being used under Suds calls (Django, Plone). If the root logger has a higher logging threshold, log messaegs may never appear (not sure how logger hierarchies should go). Below is an example how to override:To get only the generated message this also works:try changingto If you want to reduce logging by jurko-suds"},
{"body": "I am trying  to do the following insert operation:The structure of my MYSQL table is:However when I run my program, I get the error The format string is not really a normal Python format string. You must always use  for all fields. Since his data is in integer or decimal format how can you use %s which is usually used for string format %d or %i should be used for integer or decimal values.try this :"},
{"body": "In the  there is a list of notes on using .  I am thoroughly confused by the 1st and 6th items, because they seem to be contradicting each other. It seems to me these items could be better worded or shown through code, but I have been trying to wrap my head around this and am still coming up confused. I do understand how  are , and I am trying to get a better grasp on how they work.Can someone please explain to me in plain language what the conditions are for inheritance of slots when subclassing? (Simple code examples would be helpful but not necessary.)As others have mentioned, the sole reason for defining  is to save some memory, when you have simple objects with a predefined set of attributes and don't want each to carry around a dictionary.  This is meaningful only for classes of which you plan to have many instances, of course.The savings may not be immediately obvious -- consider...:From this, it would seem the with-slots size is  than the no-slots size!  But that's a mistake, because  doesn't consider \"object contents\" such as the dictionary:Since the dict alone takes 140 bytes, clearly the \"32 bytes\" object  is alleged to take are not considering all that's involved in each instance.  You can do a better job with third-party extensions such as :This shows much more clearly the memory footprint that's saved by : for a simple object such as this case, it's a bit less than 200 bytes, almost 2/3 of the object's overall footprint.  Now, since these days a megabyte more or less doesn't really matter all that much to most applications, this also tells you that  is not worth the bother if you're going to have just a few thousand instances around at a time -- however, for millions of instances, it sure does make a very important difference. You can also get a microscopic speedup (partly due to better cache use for small objects with ):but this is somewhat dependent on Python version (these are the numbers I measure repeatably with 2.5; with 2.6, I see a larger relative advantage to  for  an attribute, but none at all, indeed a tiny advantage, for  it).Now, regarding inheritance: for an instance to be dict-less,  classes up its inheritance chain must also have dict-less instances.  Classes with dict-less instances are those which define , plus most built-in types (built-in types whose instances have dicts are those on whose instances you can set arbitrary attributes, such as functions). Overlaps in slot names are not forbidden, but they're useless and waste some memory, since slots are inherited:as you see, you can set attribute  on an  instance --  itself only defines slot , but it inherits slot  from .  Repeating the inherited slot isn't forbidden:but does waste a little memory:so there's really no reason to do it.You probably won't need to use  in the near future. It's only intended to save memory at the cost of some flexibility. Unless you have tens of thousands of objects it won't matter.From the answer you linked:\"When inheriting from a class without , the  attribute of that class will always be accessible\", so adding your own  cannot prevent objects from having a , and cannot save space.The bit about  not being inherited is a little obtuse.  Remember that it's a magic attribute and doesn't behave like other attributes, then re-read that as saying this magic slots behavior isn't inherited.  (That's really all there is to it.)My understanding is as follows:Those items don't actually contradict each other. The first regards subclasses of classes that don't implement , the second regards subclasses of classes that  implement .I am increasingly aware that as great as the Python docs are (rightly) reputed to be, they are not perfect, especially regarding the less used features of the language. I would alter the  as follows: is still meaningful for such a class. It documents the expected names of attributes of the class. It also  slots for those attributes - they will get the faster lookups and use less space. It just allows for other attributes, which will be assigned to the .Well that's not quite right either. The action of a  declaration is  entirely limited to the class where it is defined. They can have implications for multiple inheritance, for example.I would change that to:(For more on , .)"},
{"body": "I'm trying to encode data to JSON in Python and I been having a quite a bit of trouble. I believe the problem is simply a misunderstanding. I'm relatively new to Python and never really got familiar with the various Python data types, so that's most likely what's messing me up. Currently I am declaring a list, looping through and another list, and appending one list within another:So I either: Python  translate to JSON . What it is giving you is a perfectly valid JSON string that could be used in a Javascript application. To get what you expected, you would need to use a :I think you are simply exchanging  and . The first returns as a (JSON encoded) string its data argument:The second does the opposite, returning the data corresponding to its (JSON encoded) string argument:In  (or the library  in Python 2.6 and later),  takes a JSON string and returns a Python data structure,  takes a Python data structure and returns a JSON string. JSON string can encode Javascript arrays, not just objects, and a Python list corresponds to a JSON string encoding an array. To get a JSON string such asthe Python object you pass to  could be:though the JSON string is also valid Python syntax for the same . I believe the specific string you say you expect is simply invalid JSON syntax, however.The data you are encoding is a keyless array, so JSON encodes it with [] brackets. See www.json.org for more information about that. The curly braces are used for lists with key/value pairs.From www.json.org:JSON uses square brackets for lists (  ) and curly brackets for key/value dictionaries (also called objects in JavaScript, ).The dump is quite correct, you get a list of three elements, each one is a list of two strings.if you wanted a dictionary, maybe something like this:your expected string ('') isn't valid JSON.  ASo, simplejson.loads takes a json string and returns a data structure, which is why you are getting that type error there.simplejson.dumps(data) comes back with Which is a json array, which is what you want, since you gave this a python array.If you want to get an \"object\" type syntax you would instead dowhich is javascript will come out as an object.Try:NB: Based on Paolo's answer."},
{"body": "I have a numpy array that contains some image data. I would like to plot the 'profile' of a transect drawn across the image. The simplest case is a profile running parallel to the edge of the image, so if the image array is , then the profile at a selected point  is simply  (horizontal) or  (vertical).Now, I want to take as input two points  and , both lying inside . I would like to plot the profile of the values along the line connecting these two points.What is the best way to get values from a numpy array, along such a line? More generally, along a path/polygon?I have used slicing and indexing before, but I can't seem to arrive at an elegant solution for such a where consecutive slice elements are not in the same row or column. Thanks for your help.@Sven's answer is the easy way, but it's rather inefficient for large arrays.  If you're dealing with a relatively small array, you won't notice the difference, if you're wanting a profile from a large (e.g. >50 MB) you may want to try a couple of other approaches. You'll need to work in \"pixel\" coordinates for these, though, so there's an extra layer of complexity.There are two more memory-efficient ways.  1) use  if you need bilinear or cubic interpolation.  2) if you just want nearest neighbor sampling, then just use indexing directly.As an example of the first:The equivalent using nearest-neighbor interpolation would look something like this:However, if you're using nearest-neighbor, you probably would only want samples at each pixel, so you'd probably do something more like this, instead...Probably the easiest way to do this is to use :I've been testing the above routines with galaxy images and think I found a small error. I think a transpose needs to be added to the otherwise great solution provided by Joe. Here is a slightly modified version of his code that reveals the error. If you run it without the transpose, you can see the profile doesn't match up; with the transpose it looks okay. This isn't apparent in Joe's solution since he uses a symmetric image.Here's the version WITHOUT the transpose. Notice that only a small fraction on the left should be bright according to the image but the plot shows almost half of the plot as bright.Here's the version WITH the transpose. In this image, the plot seems to match well with what you'd expect from the red line in the image.For a canned solution look into 's  function.It's built on top of  as in 's  and has some extra useful functionality baked in.Combining this answer with the , here's the code to allow for GUI-based dragging to draw/update your slice, by dragging on the plot data (this is coded for pcolormesh plots):This results in the following (after adding axis labels etc.), after dragging on the pcolor plot:\n"},
{"body": "When I add a # in insert mode on an empty line in Vim while editing python files, vim moves the # to the beginning of the line, but I would like the # to be inserted at the tab level where I entered it.For example, when writing this in vimthe # does not stay there where I entered it.It is moved like so, by vim.Does anyone know of a configuration item in vim that would change this?If it helps, I am using Ubuntu 8.10.I found an answer here It seems that the vim smartindent option is the cause of the problem.\nThe referenced page above describes work-a-rounds but after reading the help in smartindent in vim itself (:help smartindent), I decided to try cindent instead of smartindent.I replaced with in my .vimrc fileand so far it is working perfectly.This changed also fixed the behavior of '<<' and '>>' for indenting visual blocks that include python comments.There are more configuration options for and information on indentation in the vim help for smartindent and cindent (:help smartindent and :help cindent).@PolyThinker Though I see that response a lot to this question, in my opinion it's not a good solution. The editor still thinks it should be indented all the way to left - check this by pushing == on a line that starts with a hash, or pushing = while a block of code with comments in it is highlighted to reindent.I would strongly recommend , and remove the  and  (or ) lines from your vimrc. Someone else (appparently David Bustos) was kind enough to write a full indentation parser for us; it's located at $VIMDIRECTORY/indent/python.vim.(Paul's  solution probably works for python, but  is much more generally useful.)I have the following lines in my .vimrc, seems to be installed by default with my Ubuntu 8.10And I don't observe the problem. Maybe you can try this. (Note that ^H should be entered by Ctrl-V Ctrl-H)It's caused by the 'smartindent' feature. If you have  in your .vimrc you need to remove it.My solution to the unindenting of #:If you use cindent, recognize that it is designed for C and C++ coding.  Here, a # means you are creating a #DEFINE or #MACRO(), so the behavior is correct.  But for other languages where # is a comment, it is irritating.The following worked for me:My Vim configuration doesn't do that.  You might try the python.vim script available from this link:  I removed  from  but it still didn't disable smartindent. When I opened a .py file and ran  it displayed .Turns out that further down in the  was this line:Once I deleted \"smartindent\" from that line, then smartindent was finally disabled and my comments were indented properly again."},
{"body": "My organization currently delivers a web application primarily based on a SQL Server 2005/2008 back end, a framework of Java models/controllers, and ColdFusion-based views.  We have decided to transition to a newer framework and after internal explorations and mini projects have narrowed the choice down to between Python and C#/.NET.Let me start off by saying that   These languages have a lot in common, and a lot not--I'm looking for your take on their key differences.While it seems you can accomplish more with less code and be more creative with Python, since .NET is more structured it may be easier to take over understanding and modifying code somebody else wrote.Our engineering team is about 20 large and we work in small teams of 5-7 of which we rotate people in and out frequently.  We work on code that somebody else wrote as much as we write new code.With python we would go the Django route, and with .NET we would go with MVC2.  Our servers are windows servers running IIS.Some things we like about ColdFusion include that its very easy to work with queries and that we can \"hot-deploy\" fixes to our webservers without having to reboot them or interrupt anybody on them.I've read some of the other X vs Y threads involving these two languages and found them very helpful, but would like to put Python head-to-head against .Net directly.  Thanks in advance for letting me tap your experiences for this difficult question!\".NET\" is not a language. Perhaps it's Python vs. C# or Python/Django vs C#/ASP.NET (or pick whatever \"webwork\" you want; there are many, many different solutions for both Python and \".NET\" and picking Django or MVC2 of the bat might severely limiting better viable options). As a counter to the Python vs. \".NET\": There is  (Python \"in .NET\")I would consider:  with a language and, if they are equal in Python and \".NET\", then I would consider turnaround times for development and choose the language/\"webwork\" that minimized this (again, it need not be previous constraints).While unit/integration-testing is a must for any [sizable] project, I find that a  (C#/F#) can  the number of \"stupid bugs\" relating to types.Open up the playing field :-)Then you're just comparing languages.In which case, C# is a very boring imperative statically typed language with Single-Inheritance/Interface Class-Based OO (but a few more neat tricks than Java, which is just downright stone-age). This is  and excluding the static/dynamic bit,  (the mechanics are different, but the end result is quite similar in the language spectrum). Actually, python has MI, but that seems less accepted in python as the use of the 'lambda' keyword and since python is dynamically typed there is no compile-time support for determining interface/type contracts (there are, however, some modules that try to provide this). It's not a paradigm shift. Some keywords here, braces there, need to say what type you mean there, a different base library... different environment (you have to fight some to get to a REPL, but it's doable in VS.) How developers like/learn/use it is another story. While I did call C# imperative before, it's nice to see the addition of some \"functional-like\" features such as LINQ/IEnumerable extensions and closures-without-delegates, even if the basic C# syntax is very procedural -- once again, pretty much like python (for-expressions, nested functions, statement/expression divide).While the new 'dynamic' does blur the line (there is very rarely a good use for it -- in about all the same places one might have had to fall back to reflection in prior C# versions -- this isn't true, but the point is it's generally \"the wrong way\", except in the few cases when it just happens to be \"the best/only way\"), 'var' does not. That is, the type of a 'var' variable is  and ; it is all type inference. Some language like F#/SML and Haskell have much, much more powerful type inference removing the need for \"all those ugly type declarations\" (although explicitly annotating allowed types or set of types can make intent more clear) while preserving static typing.Personally, , I would use a statically typed language. I'm not saying C# (and I'm definitely not saying Java!), but  (this is a big, big win for me). While you do miss out on some neat dynamic tricks, there is almost always a better way to perform the same action in the target language -- you just have to think in terms of that language and use a screwdriver for a screw and a hammer for a nail. E.g. don't expect to bring Python code relying on the (ab)use of local() or global() into C# as-is.On the \"down-side\", most statically typed languages (C# here) require an explicit compile-first (but this isn't so bad as it makes pretty assemblies) and tools like the \"REPL\" aren't taken as first-class citizens (it is a first-class citizen in F#/VS2010). Also, if you have an essential library for Python/C# (and it isn't available in the other language), that may be a deciding factor as to why to choose one language over the other.I wrote a very comprehensive answer on Quora about this: I would also suggest we must compare runtimes and not limit to language features before making such moves. Python runs via interpreter CPython where C# runs on CLR in their default implementations. Multitasking is very important in any large scale project; .NET can easily handle this via threads... and also it can take benefits of worker processes in IIS (ASP.NET). CPython doesn't offer true threading capabilities because of GIL...a lock that every thread has to acquire before executing any code, for true multitasking you have to use multiple processes.When we host ASP.NET application on IIS on single worker process, ASP.NET can still take advantage of threading to serve multiple web requests simultaneously on different cores where CPython depends on multiple work processes to achieve parallel computing on different cores. All of this leads to a big question, how we are going to host Python/Django app on windows. We all know forking process on windows is much more costly than Linux. So ideally to host Python/Django app; best environment would be Linux rather than windows.If you choose Python, the right environment to developed and host Python would be Linux...and if you are like me coming from windows, choosing Python would introduce new learning curve of Linux as well...although is not very hard these days...The main problem in industrie is the dynamic nature of python.\nBecause you have some kind of safety with a static typed language.But now we have modern IDE's like PyCharm. They integrate pylint and pep8 \"code-checking\" and \"styleguide-check\" when you type in your code. That eliminates the most stupid errors. So you have almost have the same safety in python now.The other thing is, if you need \"static type checking\" do it by yourself\nwhen you need it. That's the pragmatic nature of python.The GIL is a problem, but you can use gevent or ZMQ to do a kind of threading. But work in progress on PyPy STM.Python runs almost anywhere and you have the choice of different, mostly compatible, runtimes (12 on Wikipedia)\n"},
{"body": "How to debug python programs in emacs? I use python-mode.elI get reference like \n   import pdb; pdb.set_trace();but not sure how to use it.Type  to change directory to the location of the program you wish to debug.\nType . You'll be prompted with . Enter the name of the program (e.g. ). At the  prompt, type  to learn about how to use pdb.Alternatively, you can putright inside your program (e.g. ). Now type  to get a shell prompt. When you run your program, you'll be dumped into  at the point where  is executed.For me, I needed to replace the default \"pdb\" with The  package (available from MELPA) supports PDB (among a gazillion other debuggers), and has a host of neat features that Emac's PDB doesn't have.The one I like best is the shortkeys mode. Once you start debugging a program, you can press , ,  etc. right in the source window, instead of having to type these commands in the PDB buffer. It also supports Visual-Studio style keybindings with function keys (, , , etc).After installing RealGUD, you need to run  to load it, and you can start  with ."},
{"body": "Generally speaking, what should the unary  do in Python?I'm asking because, so far, I have never seen a situation like this:Where  is a generic object implementing .So I'm wondering: why do  and  exist? Can you provide a real-world example where the expression above evaluates to ?Here's a \"real-world\" example from the  package:I believe that Python operators where inspired by C, where the  operator was introduced  for symmetry (and also some useful hacks, see comments).In weakly typed languages such as PHP or Javascript, + tells the runtime to coerce the value of the variable into a number. For example, in Javascript:Python is strongly typed, so strings don't work as numbers, and, as such, don't implement an unary plus operator.It is certainly possible to implement an object for which +obj != obj :As for an example for which it actually makes sense, check out the\n. They are a superset of the reals which includes\ninfinitesimal values (+ epsilon, - epsilon), where epsilon is \na positive value which is smaller than any other positive number, but\ngreater than 0; and infinite ones (+ infinity, - infinity). You could define , and .While  is still undefined,  is , and  = . It is \nnothing more than taking limits of  as  aproaches  from the right (+) or from the left (-).As  and  behave differently, it makes sense that . In Python 3.3 and above,  uses the  operator to remove non-positive counts.So if you use negative counts in a , you have a situation where .For symmetry, because unary minus is an operator, unary plus must be too. In most arithmetic situations, it doesn't do anything, but keep in mind that users can define arbitrary classes and use these operators for anything they want, even if it isn't strictly algebraic.I know it's an old thread, but I wanted to extend the existing answers to provide a broader set of examples:That, plus all the typecasting reasons mentioned by others.And after all... it's nice to have one more operator in case you need it. exists in  to give programmers similar possibilities as in  - to , in this case the  .( operators means give them a , e . g.  behaves differently for  and for  - numbers are  while strings are .)Objects may implement (beside others) these  functions (methods):So  means the same as  - they are interchangeable. However  is more easy on the eye.Creator of a particular object has free hands to implement these functions as he wants - as other people showed in their real world's examples. And my contribution -   ."},
{"body": "Is there any way to get the effect of running python -u from within my code? Failing that, can my program check if it is running in -u mode and exit with an error message if not? This is on linux (ubuntu 8.10 server)The best I could come up with:Tested on GNU/Linux. It seems it should work on Windows too. If I knew how to reopen sys.stdout, it would be much easier:References:\n\n\n[Edit]Note that it would be probably better to close sys.stdout before overwriting it.You could always pass the -u parameter in the shebang line:You might use the fact that stderr is never buffered and try to redirect stdout to stderr:Assuming you're on Windows:"},
{"body": "I am putting together the admin for a satchmo application. Satchmo uses OneToOne relations to extend the base  model, and I'd like to edit it all on one page.It is possible to have a OneToOne relation as an Inline? If not, what is the best way to add a few fields to a given page of my admin that will eventually be saved into the OneToOne relation?for example:I tried this for my admin but it does not work, and seems to expect a Foreign Key:Which throws this error: Is the only way to do this a ? Just tried the following code to add the fields directly... also does not work:It's perfectly possible to use an inline for a OneToOne relationship. However, the actual field defining the relationship has to be on the inline model, not the parent one - in just the same way as for a ForeignKey. Switch it over and it will work.: you say the parent model is already registered with the admin: then unregister it and re-register.Maybe use inheritance instead OneToOne relationshipOr use proxy classesin admin.pyIn this variant your product will be in inline.Referring to the last question, what would be the best solution for multiple sub-types. E.g class Product with sub-type class Book and sub-type class CD. The way shown here you would have to edit a product the general items plus the sub-type items for book AND the sub-type items for CD. So even if you only want to add a book you also get the fields for CD. If you add a sub-type e.g. DVD, you get three sub-type field groups, while you actually only want one sub-type group, in the mentioned example: books.You can also try setting 'parent_link=True' on your OneToOneField?"},
{"body": "Is there a way to write an aggregation function as is used in  method, that would have access to more than one column of the data that is being aggregated? Typical use cases would be weighted average, weighted standard deviation funcs.I would like to be able to write something likeYes; use the  function, which will be called on each sub-. For example:The following (based on Wes McKinney' answer) accomplishes exactly what I was looking for.  I'd be happy to learn if there's a simpler way of doing this within .The function  returns a dataframe that's grouped by the \"groupby\" column, and that returns the sum of the weights for the weights column.  Other columns are either the weighted averages or, if non-numeric, the  function is used for aggregation.I do this a lot and found the following quite handy:This will compute the weighted average of all the numerical columns in the  and drop non-numeric ones.Accomplishing this via  is non-performant. Here's a solution that I use all the time (essentially using kalu's logic)."},
{"body": "I'm writing some code that takes a filename, opens the file, and parses out some data. I'd like to do this in a class. The following code works:But it involves me putting all of the parsing machinery in the scope of the  function for my class. That looks fine now for this simplified code, but the function  has quite a few levels of indention as well. I'd prefer to define the function  as a class function like below:Of course this code doesn't work because the function  is not within the scope of the  function. Is there a way to call a class function from within  of that class? Or am I thinking about this the wrong way?Call the function in this way:You also need to define your parse_file() function like this:The parse_file method has to be bound to an object upon calling it (because it's not a static method). This is done by calling the function on an instance of the object, in your case the instance is .If I'm not wrong, both functions are part of your class, you should use it like this:replace your line:with:How about:By the way, if you have variables named , , etc., the situation is begging for a tuple:\n.So let  return a tuple, and store the tuple in\n.Then, for example, you can access what used to be called  with .In , take the  argument (just like in ). If there's any other context you need then just pass it as additional arguments as usual.You must declare parse_file like this; . The \"self\" parameter is a hidden parameter in most languages, but not in python. You must add it to the definition of all that methods that belong to a class.\nThen you can call the function from any method inside the class using your final program is going to look like this:I think that your problem is actually with not correctly indenting  function.It should be like this"},
{"body": "Is it possible to start an interactive python shell inside a python program?I want to use such an interactive python shell (which is running  my program's execution) to inspect some program-internal variables.The  module provides an interactive console:In ipython 0.13+ you need to do this:I've had this code for a long time, I hope you can put it to use.To inspect/use variables, just put them into the current namespace. As an example, I can access  and  from the command line. if you wanted to strictly debug your application, I'd  suggest using an IDE or .Using IPython you just have to call:Another trick (besides the ones already suggested) is opening an interactive shell and importing your (perhaps modified) python script. Upon importing, most of the variables, functions, classes and so on (depending on how the whole thing is prepared) are available, and you could even create objects interactively from command line. So, if you have a  file, you could open Idle or other shell, and type  (if it is in current working directory)."},
{"body": "I'm running pylint on some code, and receiving the error \"Too few public methods (0/2)\". What does this message mean? The  are not helpful: The error basically says that classes aren't meant to  store data, as you're basically treating the class as a dictionary. Classes should have at least a few methods to operate on the data that they hold.If your class looks like this:Consider using a dictionary or a  instead. Although if a class seems like the best choice, use it. pylint doesn't always know what's best.If you are extending a class, then my suggestion is to systematically disable this warning and move on, e.g., in the case of Celery tasks:Even if you are only extending a single function, you definitely need a class to make this technique function, and extending is definitely better than hacking on the third-party classes!"},
{"body": "Since Java doesn't allow passing methods as parameters, what trick do you use to implement Python like list comprehension in Java ?I have a list (ArrayList) of Strings. I need to transform each element by using a function so that I get another list. I have several functions which take a String as input and return another String as output. How do I make a generic method which can be given the list and the function as parameters so that I can get a list back with each element processed. It is not possible in the literal sense, but what trick should I use ?The other option is to write a new function for each smaller String-processing function which simply loops over the entire list, which is kinda not so cool.Basically, you create a Function interface:and then pass in an anonymous subclass to your method.Your method could either apply the function to each element in-place:or create a new  (basically creating a mapping from the input list to the output list):Which one is preferable depends on your use case. If your list is extremely large, the in-place solution may be the only viable one; if you wish to apply many different functions to the same original list to make many derivative lists, you will want the  version.The  has lots of classes for working with collections and iterators at a much higher level than plain Java supports, and in a functional manner (filter, map, fold, etc.). It defines Function and Predicate interfaces and methods that use them to process collections so that you don't have to. It also has convenience functions that make dealing with Java generics less arduous. I also use ** for filtering collections.The two libraries are easy to combine with adapter classes.** Declaration of interest: I co-wrote HamcrestIn Java 8 you can use method references:Or, if you want to create a new list instance:Apache Commons   is another option.I'm building this project to write list comprehension in Java, now is a proof of concept in ExamplesAnd if we want to transform the output expression in some way likeYou can use lambdas for the function, like so:the usage:will return:"},
{"body": "I am trying to extract the content of a single \"value\" attribute in a specific \"input\" tag on a webpage. I use the following code:I get a TypeError: list indices must be integers, not streven though from the Beautifulsoup documentation i understand that strings should not be a problem here... but i a no specialist and i may have misunderstood. Any suggestion is greatly appreciated!\nThanks in advance. returns list of all found elements, so: is a list (probably containing only one element). Depending on what you want exactly you either should do:or use  method which returns only one (first) found element:If you want to retrieve multiple values of attributes from the source above, you can use  and a list comprehension to get everything you need:I would actually suggest you a time saving way to go with this assuming that you know what kind of tags have those attributes.suppose say a tag xyz has that attritube named \"staininfo\"..And i wan't you to understand that full_tag is a listThus you can get all the attrb values of staininfo for all the tags xyzIn , simply use  on your tag object that you get using :against XML file  that looks like:prints:"},
{"body": "Consider a dict likeHow do I access for instance a particular element of this dictionary ?\nfor instance I would like to print first element after some formatting the first element of Apple which in our case is 'American' only ?Additional information\nThe above data structure was created by parsing an input file in a python function. Once created however it remains the same for that run.I am using this data structure in my function.So if the file changes, the next time this application is run the contents of file are different and hence the contents of this data structure will be different but the format would be same.\nSo you see I in my function I don't know that the first element in Apple is 'American' or anything else\nso I can't directly use 'American' as a key Given that it is a dictionary you access it by using the keys. Getting the dictionary stored under \"Apples\", do the following:And getting how many of them are American (16), do like this:If the questions is, if I know that I have a dict of dicts that contains 'Apple' as a fruit and 'American' as a type of apple, I would use:as others suggested. If instead the questions is, you don't know whether 'Apple' as a fruit and 'American' as a type of 'Apple' exist when you read an arbitrary file into your dict of dict data structure, you could do something like:or better yet so you don't unnecessarily iterate over the entire dict of dicts if you know that only Apple has the type American:In all of these cases it doesn't matter what order the dictionaries actually store the entries. If you are really concerned about the order, then you might consider using an :As I noticed your description, you just know that your parser will give you a dictionary that its values are dictionary too like this:So you have to iterate over your parent dictionary. If you want to print out or access all first dictionary keys in  list, you may use something like this:If you want to just access first key of the first item in , this may be useful:If you use the example you gave in the question, I mean:The output for the first code is:And the output for the second code is:You can use  to get the first key in the  dictionary, but there's no guarantee that it will be . The order of keys in a dictionary can change depending on the contents of the dictionary and the order the keys were added.You can't rely on order on dictionaries. But you may try this:If you want the order to be preserved you may want to use this:\nAs a bonus, I'd like to offer kind of a different solution to your issue. You seem to be dealing with nested dictionaries, which is usually tedious, especially when you have to check for existence of an inner key.There are some interesting libraries regarding this on pypi, here is a  for you.In your specific case,  seems suited."},
{"body": "In Python we can \"dir\" a module, like this:And it lists all functions in the module. Is there a similar way to do this in Ruby?As far as I know not exactly but you get somewhere withI like to have this in my .irbrc:So when I'm in irb:Or even cuter - with grep:You can take a module, such as , and send the  method which lists all the methods the module defines. Classes that include this module will respond to these methods.Tip for \"searching\" for a method in irb: Tip for trying out methods on a value for comparison:Also, note that you won't get all the same information as Python's dir with object.methods. You have to use a combination of object.methods and class.constants, also class.singleton_methods to get the class methods.I'd go for something like this:Which will give you a yaml representation of the sorted array of methods. Note that this can be used to list the methods of both classes and objects.Not really.  Like the others said, you can get part of what you want by listing class instance methods (e.g. ) but that doesn't help you if a file you open reopens a class (unless you check before and after).If you don't need programmatic access to the list of methods, consider checking out the documentation for a class, module or method using the  command line tool.I would have made this a comment to jonelf's answer, but apparently I don't have enough rep.some_object.methods.sort - Object.new.methodsThis isn't exactly what you were asking as others have said, but it gives you the info you are after.If I stricly read your question, I must answer it that way: a file as specified by  in Ruby is just a container and does not have necessarely have any relation with a class.  The content can be:or any combination of the above, several times. So you can not directly ask for all methods in a given file.If you meant to list all methods of a given module or class, then the other answers are what you seek (mainly using the  method on a module name or class).Maybe not answering the original question (depends on the use case), but for those who are looking for this to be used in the  only, you can use \"double-TAB\" for autocompletion. Which, effectively, can also list (almost all) the methods available for a given object.Put the following line into your  file:Now, (re)start the , start typing a method and hit TAB twice - irb autocompletes the input!I actually learned it here: "},
{"body": "I have a directory, 'Dst Directory', which has files and folders in it and I have 'src Directory' which also has files and folders in it. What I want to do is move the contents of 'src Directory' to 'Dst Directory' and overwrite anyfiles that exist with the same name. So for example 'Src Directory\\file.txt' needs to be moved to 'Dst Directory\\' and overwrite the existing file.txt. The same applies for some folders, moving a folder and merging the contents with the same folder in 'dst directory'I'm currently using shutil.move to move the contents of src to dst but it won't do it if the files already exist and it won't merge folders; it'll just put the folder inside the existing folder.Update: To make things a bit clearer; What I'm doing is unzipping an archive to the Dst Directory and then moving the contents of Src Directory there and rezipping, effectively updating files in the zip archive. This will be repeated for adding new files or new versions of files etc which is why it needs to overwrite and mergeSolved: I solved my problem by using distutils.dir_util.copy_tree(src, dst), this copies the folders and files from src directory to dst directory and overwrites/merges where neccesary. Hope that helps some people!Hope that makes sense,\nthanks!Use  instead, which is willing to overwrite destination files. If you then want the first tree to go away, just  it separately once you are done iterating over it.Do an  over the source tree. For each directory, check if it exists on the destination side, and  it if it is missing. For each file, simply  and the file will be created or overwritten, whichever is appropriate.This will go through the source directory, create any directories that do not already exist in destination directory, and move files from source to the destination directory:Any pre-existing files will be removed first (via ) before being replace by the corresponding source file. Any files or directories that already exist in the destination but not in the source will remain untouched.Since none of the above worked for me, so I wrote my own recursive function. Call Function copyTree(dir1, dir2) to merge directories. Run on multi-platforms Linux and Windows. If you also need to overwrite files with read only flag use this:Have a look at:  to remove existing files.I had a similar problem. I wanted to move files and folder structures and overwrite existing files, but not delete anything which is in the destination folder structure.I solved it by using , recursively calling my function and using  on files which I wanted to overwrite and folders which did not exist.It works like , but with the benefit that existing files are only overwritten, but not deleted."},
{"body": "MySQL ResultSets are by default retrieved completely from the server before any work can be done. In cases of huge result sets this becomes unusable. I would like instead to actually retrieve the rows one by one from the server.In Java, following the instructions  (under \"ResultSet\"), I create a statement like this:This works nicely in Java. My question is: is there a way to do the same in python?One thing I tried is to limit the query to a 1000 rows at a time, like this:However, this seems to get slower the higher start_row is.And no, using  instead of  doesn't change anything.The naive code I use to reproduce this problem looks like this:On a ~700,000 rows table, this code runs quickly. But on a ~9,000,000 rows table it prints \"Executing Query\" and then hangs for a long long time. That is why it makes no difference if I use  or .I think you have to connect passing :The default cursor fetches all the data at once, even if you don't use .Edit:  or any other cursor class that supports server side resultsets - check the module docs on .The limit/offset solution runs in quadratic time because mysql has to rescan the rows to find the offset.  As you suspected, the default cursor stores the entire result set on the client, which may consume a lot of memory.Instead you can use a server side cursor, which keeps the query running and fetches results as necessary.  The cursor class can be customized by supplying a default to the connection call itself, or by supplying a class to the cursor method each time.But that's not the whole story.  In addition to storing the mysql result, the default client-side cursor actually fetches every row regardless.  This behavior is undocumented, and very unfortunate.  It means full python objects are created for all rows, which consumes far more memory than the original mysql result.In most cases, a result stored on the client wrapped as an iterator would yield the best speed with reasonable memory usage.  But you'll have to roll your own if you want that.Did you try this version of fetchone?  Or something different?Also, did you try this?Not all drivers support these, so you may have gotten errors or found them too slow.Edit.When it hangs on execute, you're waiting for the database.  That's not a row-by-row Python thing; that's a MySQL thing.  MySQL prefers to fetch all rows as part of it's own cache management.  This is turned off by providing a the fetch_size of Integer.MIN_VALUE (-2147483648L).The question is, what part of the Python DBAPI becomes the equivalent of the JDBC fetch_size?I think it might be the arraysize attribute of the cursor.  TryAnd see if that forces MySQL to stream the result set instead of caching it.Try to use I found the best results mixing a bit from some of the other answers. This included setting  (for MySQLdb) or  (for PyMySQL) as part of the connection settings. This will let the server hold the query/results (the \"SS\" stands for server side as opposed to the default cursor which brings the results client side) and build a dictionary out of each row (e.g. {'id': 1, 'name': 'Cookie Monster'}). Then to loop through the rows, there was an infinite loop in both Python 2.7 and 3.4 caused by  because even when  was called and there were no results left, the method returned an empty list () instead of None. Actual example:"},
{"body": "I thought I had a pretty good handle on Python's scoping rules, but this problem has me thoroughly stymied, and my google-fu has failed me (not that I'm surprised - look at the question title ;)I'm going to start with a few examples that work as expected, but feel free to skip to example 4 for the juicy part.Straightforward enough: during class definition we're able to access the variables defined in the outer (in this case global) scope.Again (ignoring for the moment  one might want to do this), there's nothing unexpected here: we can access functions in the outer scope.: as Fr\u00e9d\u00e9ric pointed out below, this function doesn't seem to work. See Example 5 (and beyond) instead. As , I botched this example in my original posting. I believe this fails because only functions (not other code blocks, like this class definition) can access variables in the enclosing scope. For non-function code blocks, only local, global and built-in variables are accessible. A more thorough explanation is available in One more:Um...excuse me?What makes this any different from example 2?I'm completely befuddled. Please sort me out.\nThanks!P.S. on the off-chance that this isn't just a problem with my understanding, I've tried this on Python 2.5.2 and Python 2.6.2. Unfortunately those are all I have access to at the moment, but they both exhibit the same behaviour.\nAccording to : at any time during execution, there are at least three nested scopes whose namespaces are directly accessible:#4. seems to be a counter-example to the second of these.As @Fr\u00e9d\u00e9ric pointed out the assignment of to a variable of the same name as it has in the outer scope seems to \"mask\" the outer variable, preventing the assignment from functioning.So this modified version of Example 4 works:However this doesn't:I still don't fully understand why this masking occurs: shouldn't the name binding occur when the assignment happens?This example at least provides some hint (and a more useful error message):So it appears that the local variable is defined at function creation (which succeeds), resulting in the local name being \"reserved\" and thus masking the outer-scope name when the function is called.Interesting.Thanks Fr\u00e9d\u00e9ric for the answer(s)!For reference, from :This seemingly confusing behaviour is caused by Python's . It actually has nothing to do with .From PEP 227:Lets run two simpler versions of Tim's example:when  doesn't find  in its inner scope, it dynamically searches outwards, finding the  in 's scope, which has been bound to  through the  assignment.But changing the order the final two statements in  causes an error:Remembering that PEP 227 said \"The name resolution rules are typical for statically scoped languages\", lets look at the (semi-)equivalent C version offer:compile and run:So while C will happily use an unbound variable (using whatever happens to have been stored there before: 134520820, in this case), Python (thankfully) refuses.As an interesting side-note, statically nested scopes enable what  \"the single most important optimization the Python compiler does: a function's local variables are not kept in a dict, they're in a tight vector of values, and each local variable access uses the index in that vector, not a name lookup.\"That's an artifact of Python's name resolution rules:  The above was poorly worded, you  have access to the variables defined in outer scopes, but by doing  or  from a non-global namespace, you're actually masking the outer variable with the one you're defining locally.In example 2, your immediate outer scope is the global scope, so  can see , but in example 4 your immediate outer scope is , so it can't, because the outer definition of  is already masked by its local definition.See  for more details about nonlocal name resolution.Also note that, for the reasons explained above, I can't get example 3 to run under either Python 2.6.5 or 3.1.2:But the following would work:This post is a few years old, but it is among the rare ones to discuss the important problem of scope and static binding in Python. However, there is an important misunderstanding of the author for example 3 that might confuse readers. (do not take as granted that the other ones are all correct, it is just that I only looked at the issues raised by example 3 in details).\nLet me clarify what happened.In example 3 must return an error, unlike what the author of the post said. I believe that he missed the error because in example 1  was assigned to  in the global scope. Thus a wrong understanding of what happened. The explanation is extensively described in this post\n"},
{"body": "I seem to be getting an IOError: request data read error quite a lot when i'm doing an Ajax upload. For example out of every 5 file uploads it errors out on atleast 3. Other people seem to have had the same issue. Eg. Some other observations:I get this exception, too. In the Apache error logfile I see this:Versions:First I was confused, because the last line \"failed to  data\" does not fit to the django code \"load post data\". But I guess that django wants to write an error page to the client. But the client has canceled the tcp connection. And now http 500 page can't be written to the client.The client disconnected after sending the request, and before getting the response:I have seen this only with POST-Requests (not GET). If POST is used, the webserver does read at least twice: First to get the headers, the second to get the data. The second read fails.It is easy to reproduce:Insert some code which waits before the first access to request.POST happens (be sure, that no middleware accesses request.POST before time.sleep()):Now do a big POST (e.g. file upload). I don't know the apache buffer size. But 5 MB should be enough. When the browser shows the hourglass, browse to an other page. The browser will cancel the request and the exception should be in the logfile.This is my Middleware, since I don't want to get the above traceback in our logfiles:as you might think, this is no django error.see have the error myself (but IE ajax requests only, no file upload, just post data).will add an complete answer if i ever find out how to fix this.We were seeing this error on uploads to Django Rest Framework when the content-type header was incorrectly set to application/json. The post was actually multipart form data. The errors stopped when we removed the incorrect content-type header.This happened to me recently. I was using  and small files were uploading successfully but large files e.g. 100MB were breaking in between with .I checked my Apache configuration and found these setting  which means .File is received in chunks at backend, which means if file size is big  seconds are not enough for some uploads. So how to determine the best value (seconds) for the requests?So I have used this setting:Defining the  solves the issue for me. The above setting states that:As the client is sending data continuously (ajax upload) it makes sense to automatically increase the timeout if data is received. More information/variations about the  can be found .Taking this from the thread: Extending the possible solution by @dlowe for Django 1.3, to suppress the IOError in concern, we can write the full working example as:I got this error when validating my site on a Win8 machine with IE 10. When I tested file upload from IE the upload was stucked on 1% and after +/- 1 minute I got the error on server logs. I just discovered that it was caused by the TrendMicro complement. Once I disabled the complement the upload occurred without any problem.This issue has been open for a long time and has something to do with lower level libraries. I was using boto to upload files to S3. A temporary stopgap I found was to add an explicit HTTP socket timeout of 10 seconds. I haven't seen the error after that. You can do that by creating a boto config on the server:Also make sure the file is readable by the app. \nSee my original post on google group: "},
{"body": " is a new documentation tool for Python. It looks very nice. What I'm wondering is:As mentioned  and , [Edit inserted below]: I tested the doxygen+breathe+sphinx toolchain on a multi-10k \nC++ library consisting of 10 different modules/domains. My bottom\nline is: Let me elaborate these points:As a final pointer, also note the  contrib project for sphinx,\nwhich may provide an intermediate solution: build up a surrounding tutorial-like\nstructure which references the (css-style matched) old doxygen documentation\n(i think you could even inject the same header into sphinx and on top of the\n doxygen documentation for look'n'feels). That way, your project keeps an\naffinity to sphinx, and when breathe is fully there, you are prepared to\njump on. But again: consider showing breathe some love if it fits your agenda.First, keep two directory trees,  and .  Put  under version control.  Don't put  under version control, rebuild it as part of installation.Second, read .Use the  to build a practice documentation tree.  Play with this for a few days to learn how it works.  Then use it again to build the real thing in SVN directories.Organize your documentation in a well-planned tree.  Some sections need an \"index.rst\" for that section, some don't.  It depends on how \"stand-alone\" the section is.Our top-level  looks like this.Note, we don't \"include\" the API, we just reference it with an ordinary HTML link.Sphinx has a very cool add-on, called automodule, which picks the docstrings out of Python modules.  As of Sphinx 1.0, C and C++ are supported.  Have a look at  for an XML approach."},
{"body": "I'm using  and the autodoc plugin to generate API documentation for my Python modules.  Whilst I can see how to nicely document specific parameters, I cannot find an example of how to document a  parameter.Does anyone have a good example of a clear way to document these?I think  is a good example. Give an exhaustive list of all parameters for a . Then just refer to that list for all other occurrences of .After finding this question I settled on the following, which is valid Sphinx and works fairly well:The  is required to make this a \"raw\" docstring and thus keep the  intact (for Sphinx to pick up as a literal  and not the start of \"emphasis\").The chosen formatting (bulleted list with parenthesized type and m-dash-separated description) is simply to match the automated formatting provided by Sphinx.Once you've gone to this effort of making the \"Keyword Arguments\" section look like the default \"Parameters\" section, it seems like it might be easier to roll your own parameters section from the outset (as per some of the other answers), but as a proof of concept this is one way to achieve a nice look for supplementary  if you're already using Sphinx.Disclaimer: not tested.From this cutout of the , the  and  are left :I would  the following solution for compactness:Notice how,  is not required for  arguments. , you can try to explicitly list the *args under  and  under the  (see parsed ):If anyone else is looking for some valid syntax.. Here's an example docstring. This is just how I did it, I hope it's useful to you, but I can't claim that it's compliant with anything in particular.There is a  for Sphinx in their documentation. \nSpecifically they show the following: Though you asked about  explicitly, I would also point to the . Their docstring example seems to imply that they don't call out kwargs specifically. (other_silly_variable=None) A-B-B has a question about the accepted answer of referencing the subprocess management documentation. If you import a module, you can quickly see the module docstrings via inspect.getsource. An example from the python interpreter using Silent Ghost's recommendation: Of course you can also view the module documentation via help function. For example help(subprocess) I'm not personally a fan of the subprocess docstring for kwargs as an example, but like the Google example it doesn't list kwargs seperately as shown in the Sphinx documentation example. I'm including this answer to A-B-B's question because it's worth noting that you can review any module's source or documentation this way for insights and inspiration for commenting your code."},
{"body": "I sometimes write Python programs which are very difficult to determine how much memory it will use before execution. As such, I sometimes invoke a Python program that tries to allocate massive amounts of RAM causing the kernel to heavily swap and degrade the performance of other running processes.Because of this, I wish to restrict how much memory a Python heap can grow. When the limit is reached, the program can simply crash. What's the best way to do this? If it matters, much code is written in Cython, so it should take into account memory allocated there. I am not married to a pure Python solution (it does not need to be portable), so anything that works on Linux is fine.Check out .  It only works on Unix systems but it seems like it might be what you're looking for, as you can choose a maximum heap size for your process and your process's children with the resource.RLIMIT_DATA parameter.EDIT: Adding an example:I'm not sure what your use case is exactly but it's possible you need to place a limit on the size of the stack instead with resouce.RLIMIT_STACK.  Going past this limit will send a SIGSEGV signal to your process, and to handle it you will need to employ an alternate signal stack as described in the setrlimit Linux .  I'm not sure if sigaltstack is implemented in python, though, so that could prove difficult if you want to recover from going over this boundary.Have a look at . It allows resource quotas to be set. May need appropriate kernel settings as well."},
{"body": "I was unsuccessful browsing web for a solution for the following simple question:How to draw 3D polygon (say a filled rectangle or triangle) using vertices values?\nI have tried many ideas but all failed, see:I appreciate in advance any idea/comment.  Here is the result:\nI think you've almost got it.   Is this what you want?\nYou might also be interested in art3d.pathpatch_2d_to_3d."},
{"body": "How can I convert  time to  time and the other way round in ,  or  or ? I know I can convert to  and . But  is not an option.To be more specific, I'm talking about the Hindu lunar calendar. The following website is working and does exactly what I want: . For example, it 'translates'  (Gregorian) to  (Hind. Lun.). How can I accomplish the same task? And if there are absolutely no scrips out there, how can I write it myself?Paper: \nProvides Common Lisp code in the appendix.While a Python (or other language) solution could be written according to the paper, the authors enumerate the Indian calendar rules pretty well, so it's a pretty solid paper if you're willing to consider taking the provided Common Lisp code.Did you check  family of modules? At least  \nseems to have a method to convert traditional date into UTC values (utc_rd_values).I suppose  may be useful as well for many users (as I have known, it's the Indian national calendar), in particular,  and  methods.For Python, use  (note: this is not the built-in calendar module).Sample use:Seems to be a difficult task. According to  there is no clear way to accomplish a 100% correct conversion. But it seems that they are wrong when they assume that the Hindu calendar has only 364 days instead of 365 (or 366 in leap years).Here you can find a good conversion table including the handling of leap years: If it is as easy as written there you can try something like this (php code):Output of this code:But according to the calculator of , it should be 05-11-2068 instead of 07-11-2068. So, there are still some conversion rules missing. Maybe you can give me some more information so that i can correct the code above.I dont know whether it is correct approach or not but\nPlease go to  site and download two javascript files astro.js and calendar.js and follow onchange events of Gregorian Date and fetch Indian Civil Date parameters."},
{"body": "I'm trying to catch an exception in a thread and re-raise it in the main thread:This basically works and yields the following output:However, the source of the exception points to line 16, where the re-raise occurred. The original exception comes from line 7. How do I have to modify the  thread so that the output reads:You need to use all three arguments to raise:passing the traceback object in as the third argument preserves the stack.From :In this particular case you cannot use the no expression version.Could you write it somewhat like this:and then use the stacktrace from the exception?This code snippet works in both python 2 & 3:"},
{"body": "This code almost does what I need it to..Except it removes all the '>' delimiters.So,Turns intoIs there a way to use the split() method but  the delimiter, instead of removing it?With these results..If you are parsing HTML with splits, you are most likely doing it wrong, except if you are writing a one-shot script aimed at a fixed and secure content file. If it is supposed to work on any HTML input, how will you handle something like ?Anyway, the following works for me:How about this:Just split it, then for each element in the array/list (apart from the last one) add a trailing \">\" to it."},
{"body": "In , I could pass custom function to sorted and .sort functionsBecause, in  language, consonents are comes with this orderBut In , looks like I could not pass  keywordIs there any alternatives or should I write my own sorted function too?Use the  argument (and follow the  on how to convert your old  function to a  function).Use the  keyword and  to transform your comparison function:Instead of a customsort(), you need a function that translates each word into something that Python already knows how to sort.  For example, you could translate each word into a list of numbers where each number represents where each letter occurs in your alphabet.  Something like this:Since your language includes multi-character letters, your custom_key function will obviously need to be more complicated.  That should give you the general idea though.I don't know if this will help, but you may check out the  module.  It looks like you can set the locale to your language and use  to compare strings using your language's sorting rules.Use the  argument instead. It takes a function that takes the value being processed and returns a single value giving the key to use to sort by."},
{"body": "Is there a way to integrate background unit tests with the Pydev Eclipse environment?My unit tests run well, but I would like to integrate them to run in the background based on source file changes (e.g. with nose) and to integrate the result back to Eclipse (I'm thinking big red X when tests fail with a console and trace log view).No, a command prompt running nose on the side does not count.I have had this Eclipse integration when developing RoR stuff.Thanks,Tal. Check out the new Pydev (1.6.4) This feature has been added to PyDev 2.0.1 with an option to relaunch the tests in the last test run whenever a python file change, with an additional option to rerun only the errors -- although it'll run the full test suite if no errors were found, as the idea is that you work through your errors and when all pass a final launch for the whole suite is done (then you can move on to another task).The current nightly build has this feature incorporated.Pydev does have some unit-test integration, but that's only as a run configuration...so...This is not a very elegant way, but if you:Then at least you will get something that outputs the test results to the console on resource saves.I just realized that PyDev has rather powerful scripting support. Unfortunately I don't have the time to do it all for you (but if you complete this, please post it here :)If you create a file named  that looks like this in an otherwise empty folder :and set up Preferences->PyDev->Scripting Pydev to point to this directory you will get all projects in your workspace marked with an error every time a file is saved.By executing a script that returns the test results in some easy to parse format rather than  and parsing the output you should be able to put your markers in the right places.See this for some starting points:I enhanced the \"nosy\" script to automatically build documentation and runs tests \ncontinuously. Nothing stellar, but gets the job done. Posting it here because the original\nlink went down. Unlike the original nosy script, this one scans the directory recursively\nand allows looking for multiple patterns.Hope this helps.=)I run the test by hand the first time (). After that, I use\n to have the files saved and the tests executed, instead of saving with  and expecting some magic to happen. The  key combination relaunches the last run configuration.: I'm new to Eclipse and to PyDev, so I may be suggesting something silly/obvious/wrongI use  (available on pypi): "},
{"body": "How can I get the class that defined a method in Python?I'd want the following example to print \"\":Thanks Sr2222 for pointing out I was missing the point...Here's the corrected approach which is just like Alex's but does not require to import anything. I don't think it's an improvement though, unless there's a huge hierarchy of inherited classes as this approach stops as soon as the defining class is found, instead of returning the whole inheritance as  does. As said, this is a  unlikely scenario.And the Example:Alex solution returns the same results. As long as Alex approach can be used, I would use it instead of this one.I started doing something somewhat similar, basically the idea was checking whenever a method in a base class had been implemented or not in a sub class. Turned out the way I originally did it I could not detect when an intermediate class was actually implementing the method.My workaround for it was quite simple actually; setting a method  and testing its presence later. Here's an simplification of the whole thing:UPDATE: Actually call  from  (isn't that the spirit?) and have it pass all arguments unmodified to the method.P.S.: This answer does not directly answer the question. IMHO there are two reasons one would want to know which class defined a method; first is to point fingers at a class in debug code (such as in exception handling), and the second is to determine if the method has been re-implemented (where method is a stub meant to be implemented by the programmer). This answer solves that second case in a different way."},
{"body": "I just read about , i.e. zip files that contain very large amount of highly compressible data (00000000000000000...).When opened they fill the server's disk.How can I detect a zip file is a zip bomb  unzipping it? Can you tell me how is this done in Python or Java?Try this in Python:Zip is, erm, an \"interesting\" format. A robust solution is to stream the data out, and stop when you have had enough. In Java, use  rather than . The latter also requires you to store the data in a temporary file, which is also not the greatest of ideas.Reading over the description on Wikipedia - Deny any compressed files that contain compressed files.\n\u00a0\u00a0\u00a0\u00a0 Use  to retrieve a list of files, then  to find the file extension.\nDeny any compressed files that contain files over a set size, or the size can not be determined at startup.\n\u00a0\u00a0\u00a0\u00a0 While iterating over the files use  to retrieve the file size.Check a zip header first :)If the ZIP decompressor you use can provide the data on original and compressed size you can use that data. Otherwise start unzipping and monitor the output size - if it grows too much cut it loose.Don't allow the upload process to write enough data to fill up the disk, ie solve the problem, not just one possible cause of the problem.Make sure you are not using your system drive for temp storage. I am not sure if a virusscanner will check it if it encounters it. Also you can look at the information inside the zip file and retrieve a list of the content. How to do this depends on the utility used to extract the file, so you need to provide more information hereWhy would you want to? The moment you zip the first layer and find so many multiple files with weird numbering, you'd automatically know. There is no autoextracting sfx archive zip bomb that I know of."},
{"body": "In their , the original authors of Julia mention the following: What do they mean by saying that Julia does not support ? What is a native thread?Do other interpreted languages such as  or  support this type of parallelism? Is Julia alone in this?\"Native threads\" are separate contexts of execution, managed by the operating system kernel, accessing a shared memory space and potentially executing concurrently on separate cores. Compare this with separate processes, which may execute concurrently on multiple cores but have separate memory spaces. Making sure that processes interact nicely is easy since they can only communicate with each other via the kernel. Ensuring that threads don't interact in unpredictable, buggy ways is very hard since they can read and write to the same memory in an unrestricted manner.The R situation is fairly straightforward: . Python is a little more complicated: Python does support threading, but due to the , no actual concurrent execution of Python code is possible. Other popular open source dynamic languages are in various mixed states with respect to native threading (Ruby: ; Node.js: ), but in general, the answer is no, they do not support fully concurrent native threading, so Julia is not alone in this.When we do add shared-memory parallelism to Julia,  \u2013 whether using native threads or multiple processes with shared memory \u2013\u00a0it will be true concurrency and there will be no GIL preventing simultaneous execution of Julia code. However, this is an incredibly tricky feature to add to a language, as attested by the non-existent or limited support in other very popular, mature dynamic languages. Adding a shared-memory concurrency model is technically difficult, but the real problem is designing a programming model that will allow programmers to make effective use of hardware concurrency in a productive and safe way. This problem is generally unsolved and is a very active area of research and experimentation \u2013 there is no \"gold standard\" to copy. We could just add POSIX threads support, but that programming model is general considered to be dangerous and incredibly difficult to use correctly and effectively. Go has an excellent concurrency story, but it is designed for writing highly concurrent servers, not for concurrently operating on large data, so it's not at all clear that simply copying Go's model is a good idea for Julia."},
{"body": "I keep seeing functions and documentation like  and   which operate on or refer to .I'm quite aware of what exactly an actual list is (), and can deduce what () methods from a list are necessary in most references to a \"list-like object\", however the number of times I see it referenced has left me with the following question:    Is it as simple as actualizing, or is it agreed that additional things like  and are required as well? This may seem like semantics, but I can't help but think that if there does not exist a standard minimal interface requirement, various ideas of  could cause some issues/improper handling. Perhaps this is just a slight downside to Python's duck typing? See the  module. Of the abstract base classes listed there,  in Python implements , , ,  and . Now, of these, ,  and  could be casually called .However, I would understand the term  to mean that it is a  - has at least the methods , ,  and , expecting also it to have the  mixin methods mentioned in the documentation, such as .If there is no need for  and  it should be called a sequence instead - the assumption is that if something accepts a sequence, it does not need to be mutable, thus , ,  etc also work there.Your two links highlight the vagueness of the term:The plotly API requires that the  objects will be serialized to a JSON array by the internal  that delegates most of the encoding to the Python . However, the latter encodes only  and  (and subclasses) to a JSON array; thus the  here means a , a  or subclasses thereof. A custom sequence object that is not a subclass of either will result in .The  you linked to requires an object that behaves like a , (mutability is not required), thus a  or , or any custom object implementing  will do there.TL;DR  is a vague term. It is preferable to use the terms iterable, sequence and mutable sequence instead, now that these are defined in .The technical term for a \"list-like object\" is . At the very least it supports ordering (i.e. two objects with the same elements but different order are not equal), indexing ( such that  is an integer less than the length of the sequence), and containment checking (), and has a given length. It  support iteration, but if not then Python will simulate it using indexing.Pretty much any time you see \"-like object\" in Python documentation the author is being deliberately vague. The author has decided that enumerating all the required interfaces would be too much trouble, and is only saying that some of its interfaces are required. An object that implemented all the interfaces is guaranteed to work, but in most cases it will work with an object that implements much less.With a \"list-like object\" probably the best you can do, short of inspecting the source code, is to infer whether it needs any of the mutable interfaces. If it only needs read-only access to the list, you can be pretty sure you don't need to implement any of the mutable sequence operations. If it says \"list-like object or iterator\" you can provide something that implements the much simpler iterator interface."},
{"body": "Can I extend syntax in python for dict comprehensions for other dicts, like the OrderedDict in  module or my own types which inherit from ?  Just rebinding the  name obviously doesn't work, the  comprehension syntax still gives you a plain old dict for comprehensions and literals.  So, if it's possible how would I go about doing that?  It's OK if it only works in CPython.  For syntax I guess I would try it with a  prefix like we have on the .  Of course we can use a generator expression instead, but I'm more interested seeing how hackable python is in terms of the grammar.There is no direct way to change Python's syntax from within the language. A dictionary comprehension (or plain display) is always going to create a , and there's nothing you can do about that. If you're using CPython, it's using special bytecodes that generate a dict directly, which ultimately call the  API functions and/or the same underlying functions used by that API. If you're using PyPy, those bytecodes are instead implemented on top of an RPython  object which in turn is implemented on top of a compiled-and-optimized Python . And so on.There is an  way to do it, but you're not going to like it. If you read the docs on , you'll see that it's the importer that searches for cached compiled code or calls the compiler, and the compiler that calls the parser, and so on. In Python 3.3+, almost everything in this chain either is written in pure Python, or has an alternate pure Python implementation, meaning you can fork the code and do your own thing. Which includes parsing source with your own PyParsing code that builds ASTs, or compiling a dict comprehension AST node into your own custom bytecode instead of the default, or post-processing the bytecode, or\u2026In many cases, an  is sufficient; if not, you can always write a custom finder and loader.If you're not already using Python 3.3 or later, I'd strongly suggest migrating before playing with this stuff. In older versions, it's harder, and less well documented, and you'll ultimately be putting in 10x the effort to learn something that will be obsolete whenever you do migrate.Anyway, if this approach sounds interesting to you, you might want to take a look at . You could borrow some code from it\u2014and, maybe more importantly, learn how some of these features (that have no good examples in the docs) are used.Or, if you're willing to settle for something less cool, you can just use  to build an \"odict comprehension macro\" and use that. (Note that MacroPy currently only works in Python 2.7, not 3.x.) You can't quite get , but you can get, say, , which isn't too bad. Download , , and , and run  to see it working. The key is this code, which takes a  AST, converts it to an equivalent  on key-value s, and wraps it in a  to :A different alternative is, of course, to modify the Python interpreter.I would suggest dropping the  syntax idea for your first go, and just making normal dict comprehensions compile to odicts. The good news is, you don't really need to change the grammar (which is beyond hairy\u2026), just any one of:The bad news, while all of those are a lot easier than changing the grammar, none of them can be done from an extension module. (Well, you can do the first one by doing basically the same thing you'd do from pure Python\u2026 and you can do any of them by hooking the .so/.dll/.dylib to patch in your own functions, but that's the exact same work as hacking on Python plus the extra work of hooking at runtime.)If you want to hack on , the code you want is in , , and , and the  tells you how to find everything you need. But you might want to consider hacking on  instead, since it's mostly written in (a subset of) Python rather than C.As a side note, your attempt wouldn't have worked even if everything were done at the Python language level.  creates a binding named  in your module's globals, which  the name in builtins, but doesn't replace it. You  replace things in builtins (well, Python doesn't guarantee this, but there are implementation/version-specific things-that-happen-to-work for every implementation/version I've tried\u2026), but what you did isn't the way to do it.Sorry, not possible. Dict literals and dict comprehensions map to the built-in dict type, in a way that's hardcoded at the C level. That can't be overridden.You can use this as an alternative, though:Slightly modifying the response of @Max Noel, you can use list comprehension instead of a generator to create an OrderedDict in an ordered way (which of course is not possible using dict comprehension)."},
{"body": "For example purposes...So I end up with string1, string2, string3... all equaling \"Hello\"Sure you can; its called a :I said this somewhat tongue in check, but really the best way to associate one value with another value is a dictionary. That is what it was designed for!    It is really bad idea, but...and then for example:will give you:However this is bad practice. You should use dictionaries or lists instead, as others propose. Unless, of course, you really wanted to know how to do it, but did not want to use it.It's simply pointless to create variable variable names. Why? Using a list is much easier:I would use a list:This way, you would have 9 \"Hello\" and you could get them individually like this:Where  would identify which \"Hello\" you want.So,  would print .Don't do this use a dictionarydon't do this use a dictglobals() has risk as it gives you what the namespace is currently pointing to but this can change and so modifying the return from  globals() is not a good idea"},
{"body": "I want to log how long something takes in real walltime. Currently I'm doing this:But that will fail (produce incorrect results) if the time is adjusted while the SQL query (or whatever it is) is running.I don't want to just benchmark it. I want to log it in a live application in order to see trends on a live system.I want something like clock_gettime(CLOCK_MONOTONIC,...), but in Python. And preferably without having to write a C module that calls clock_gettime().That function is simple enough that you can use ctypes to access it:Now, in Python 3.3 you would use .As pointed out in , avoiding NTP readjustments on Linux requires CLOCK_MONOTONIC_RAW. That's defined as 4 on Linux (since 2.6.28).Portably getting the correct constant #defined in a C header from Python is tricky; there is h2py, but that doesn't really help you get the value at runtime. might be useful:"},
{"body": "I need to completely remove elements, based on the contents of an attribute, using python's lxml.  Example:I would like this to print:Is there a way to do this without storing a temporary variable and printing to it manually, as:Use the  method of an xmlElement : If I had to compare with the @Acorn version, mine will work even if the elements to remove are not directly under the root node of your xml.You're looking for the  function. Call the tree's remove method and pass it a subelement to remove.I met one situation: will remove the  part which I didn't mean to.following the answer , I found that  is a better solution for me, which you can control whether you will remove the text behind with  param.But I still don't know if this can use xpath filter for tag. Just put this for informing.Here is the doc:"},
{"body": "I am a bit confused on what  does in Python. For example, why does the below trials do what they do (consistently)? I couldn't find good documentation on this. Thanks in advance!Pseudo-random number generators work by performing some operation on a value. Generally this value is the previous number generated by the generator. However, the first time you use the generator, there is no previous value.Seeding a pseudo-random number generator gives it its first \"previous\" value. Each seed value will correspond to a sequence of generated values for a given random number generator. That is, if you provide the same seed twice, you get the same sequence of numbers twice.Generally, you want to seed your random number generator with some value that will change each execution of the program. For instance, the current time is a frequently-used seed. The reason why this doesn't happen automatically is so that if you want, you can provide a specific seed to get a known sequence of numbers.All the other answers don't seem to explain the use of random.seed(). \nHere is a simple example ():You try this. Let's say 'random.seed' gives a value to random value generator ('random.randint()') which generates these values on the basis of this seed. One of the must properties of random numbers is that they should be reproducible. Once you put same seed you get the same pattern of random numbers. So you are generating them right from the start again. You give a different seed it starts with a different initial (above 3).You have given a seed now it will generate random numbers between 1 and 10 one after another. So you can assume one set of numbers for one seed value.                    In this case, random is actually pseudo-random. Given a seed, it will generate numbers with an equal distribution. But with the same seed, it will generate the same number sequence every time. If you want it to change, you'll have to change your seed. A lot of people like to generate a seed based on the current time or something.Imho, it is used to generate same random course result when you use  again.Here is a small test that demonstrates that feeding the  method with the same argument will cause the same pseudo-random result:"},
{"body": "Python uses the reference count method to handle object life time. So an object that has no more use will be immediately destroyed.But, in Java, the GC(garbage collector) destroys objects which are no longer used at a specific time.Why does Java choose this strategy and what is the benefit from this?Is this better than the Python approach?There are drawbacks of using reference counting. One of the most mentioned is circular references: Suppose A references B, B references C and C references B. If A were to drop its reference to B, both B and C will still have a reference count of 1 and won't be deleted with traditional reference counting. CPython (reference counting is not part of python itself, but part of the C implementation thereof) catches circular references with a separate garbage collection routine that it runs periodically...Another drawback: Reference counting can make execution slower. Each time an object is referenced and dereferenced, the interpreter/VM must check to see if the count has gone down to 0 (and then deallocate if it did). Garbage Collection does not need to do this.Also, Garbage Collection can be done in a separate thread (though it can be a bit tricky). On machines with lots of RAM and for processes that use memory only slowly, you might not want to be doing GC at all! Reference counting would be a bit of a drawback there in terms of performance...Actually reference counting and the strategies used by the Sun JVM are all different types of garbage collection algorithms.There are two broad approaches for tracking down dead objects: tracing and reference counting. In tracing the GC starts from the \"roots\" - things like stack references, and traces all reachable (live) objects. Anything that can't be reached is considered dead. In reference counting each time a reference is modified the object's involved have their count updated. Any object whose reference count gets set to zero is considered dead.With basically all GC implementations there are trade offs but tracing is usually good for high through put (i.e. fast) operation but has longer pause times (larger gaps where the UI or program may freeze up). Reference counting can operate in smaller chunks but will be slower overall. It may mean less freezes but poorer performance overall.Additionally a reference counting GC requires a cycle detector to clean up any objects in a cycle that won't be caught by their reference count alone. Perl 5 didn't have a cycle detector in its GC implementation and could leak memory that was cyclic.Research has also been done to get the best of both worlds (low pause times, high throughput):\nDarren Thomas gives a good answer.  However, one big difference between the Java and Python approaches is that with reference counting in the common case (no circular references) objects are cleaned up immediately rather than at some indeterminate later date.For example, I can write sloppy, non-portable code in CPython such asand the file descriptor for that file I opened will be cleaned up immediately because as soon as the reference to the open file goes away, the file is garbage collected and the file descriptor is freed.  Of course, if I run Jython or IronPython or possibly PyPy, then the garbage collector won't necessarily run until much later; possibly I'll run out of file descriptors first and my program will crash.So you SHOULD be writing code that looks likebut sometimes people like to rely on reference counting to always free up their resources because it can sometimes make your code a little shorter.I'd say that the best garbage collector is the one with the best performance, which currently seems to be the Java-style generational garbage collectors that can run in a separate thread and has all these crazy optimizations, etc.  The differences to how you write your code should be negligible and ideally non-existent.I think the article \"\" from IBM should help explain some of the questions you have.Garbage collection is faster (more time efficient) than reference counting, if you have enough memory. For example, a copying gc traverses the \"live\" objects and copies them to a new space, and can reclaim all the \"dead\" objects in one step by marking a whole memory region. This is very efficient,  you have enough memory. Generational collections use the knowledge that \"most objects die young\"; often only a few percent of objects have to be copied.[This is also the reason why gc can be faster than malloc/free]Reference counting is much more space efficient than garbage collection, since it reclaims memory the very moment it gets unreachable. This is nice when you want to attach finalizers to objects (e.g. to close a file once the File object gets unreachable). A reference counting system can work even when only a few percent of the memory is free. But the management cost of having to increment and decrement counters upon each pointer assignment cost a lot of time, and some kind of garbage collection is still needed to reclaim cycles.So the trade-off is clear: if you have to work in a memory-constrained environment, or if you need precise finalizers, use reference counting. If you have enough memory and need the speed, use garbage collection.One big disadvantage of Java's tracing GC is that from time to time it will  \"stop the world\" and freeze the application for a relatively long time to do a full GC. If the heap is big and the the object tree complex, it will freeze for a few seconds. Also each full GC visits the whole object tree over and over again, something that is probably quite inefficient. Another drawback of the way Java does GC is that you have to tell the jvm what heap size you want (if the default is not good enough); the JVM derives from that value several thresholds that will trigger the GC process when there is too much garbage stacking up in the heap.I presume that this is actually the main cause of the jerky feeling of Android (based on Java), even on the most expensive cellphones, in comparison with the smoothness of iOS (based on ObjectiveC, and using RC). I'd love to see a jvm option to enable RC memory management, and maybe keeping GC only to run as a last resort when there is no more memory left. The latest Sun Java VM actually have multiple GC algorithms which you can tweak.  The Java VM specifications intentionally omitted specifying actual GC behaviour to allow different (and multiple) GC algorithms for different VMs.For example, for all the people who dislike the \"stop-the-world\" approach of the default Sun Java VM GC behaviour, there are VM such as  which allows real-time application to run on Java.Since the Java VM spec is publicly available, there is (theoretically) nothing stopping anyone from implementing a Java VM that uses CPython's GC algorithm.Reference counting is particularly difficult to do efficiently in a multi-threaded environment. I don't know how you'd even start to do it without getting into hardware assisted transactions or similar (currently) unusual atomic instructions.Reference counting is easy to implement. JVMs have had a lot of money sunk into competing implementations, so it shouldn't be surprising that they implement very good solutions to very difficult problems. However, it's becoming increasingly easy to target your favourite language at the JVM.Late in the game, but I think one significant rationale for RC in python is its simplicity. See this , for example.(I could not find a link outside google cache, the email date from 13th october 2005 on python list)."},
{"body": "I really don't understand where are  and  used in Python. I mean, I get that  returns the string representation of an object. But why would I need that? In what use case scenario? Also, I read about the usage of But what I don't understand is, where would I use them? Use  if you have a class, and you'll want an informative/informal output, whenever you use this object as part of string. E.g. you can define  methods for Django models, which then gets rendered in the Django administration interface. Instead of something like  you'll get like first and last name of a person, the name and date of an event, etc. and  are similar, in fact sometimes equal (Example from  class in  from the standard library):The one place where you use them both a lot is in an interactive session. If you print an object then its  method will get called, whereas if you just use an object by itself then its  is shown:The  is intended to be as human-readable as possible, whereas the  should aim to be something that could be used to recreate the object, although it often won't be exactly how it was created, as in this case.It's also not unusual for both  and  to return the same value (certainly for built-in types).Grasshopper, when in doubt  and . In them you will find that __repr__() should: "},
{"body": "I am using python to work out how many children would be born in 5 years if a child was born every 7 seconds. The problem is on my last line. How do I get a variable to work when I'm printing text either side of it? Here is my code:Use  to separate strings and variables while printing: in print statement separtes the items by a single space:or better use :String formatting is much more powerful and allows you to do some other things as well, like : padding, fill, alignment,width, set precision etcDemo:two moreWhen adding strings, they concatenate. Also the  (Python 2.6 and newer) method of strings is probably the standard way:This  method can be used with lists as wellor dictionariesYou can either use a formatstring:or in this simple case:On a current python version you have to use parenthesis, like so :print (\"If there was a birth every 7 seconds\", X)I copied and pasted your script into a .py file. I ran it as-is with Python 2.7.10 and received the same syntax error. I also tried the script in Python 3.5 and received the following output:Then, I modified the last line where it prints the number of births as follows:The output was (Python 2.7.10):I hope this helps.If you want to work with python 3, it's very simple: You can use  to do this:or you can give  multiple arguments, and it will automatically separate them by a space:You would first make a variable: for example: D = 1. Then Do This but replace the string with whatever you want:"},
{"body": "I am trying to do this:But that's not right...the documentation is vague...how do you do this in Jinja2?As of version 2.6, Jinja2's built-in sort filter allows you to specify an attribute to sort by:See Usually we sort the list before giving it to Jinja2.  There's no way to specify a key in Jinja's  filter.However, you can always try .  That's the syntax.  You don't get to provide any sort of key information for the sorting.You can also try and write a custom filter for this.  Seems silly when you can sort before giving the data to Jinja2.If  is a list of objects, then you can define the various comparison methods (, , etc.) for the class of those objects.If  is a list of tuples or lists, the rating must be first.  Or you'll have to do the sorting outside Jinja2.If  is a list of dictionaries, then you can use , which does accept a key specification for the sorting.  Read this:  for an example.If you want to sort in ascending orderIf you want to sort in descending order"},
{"body": "I've got a Python list of dictionaries, as follows:I'd like to check whether a dictionary with a particular key/value already exists in the list, as follows:Here's one way to do it:The part in parentheses is a generator expression that returns  for each dictionary that has the key-value pair you are looking for, otherwise .If the key could also be missing the above code can give you a . You can fix this by using  and providing a default value.Perhaps a function along these lines is what you're after:Maybe this helps:"},
{"body": "I'm trying to install Pillow on Ubuntu 14.04 using this command:but the installation fails with this error:The problem was that the package  was not installed. To solve the problem you should do this:Make sure Python-development packages are installed, if not then install it using the following commands : After installing the development packages install the following :You have to install the missing dependencies and libraries that Pillow requires for it to work. Find the dependencies  This are the current dependancies/libraries for Ubuntu 14.04 for Pilllow 3.0.0+. Install them by running the command belowThere may be a problem where pip is relying on a cached version of the dependencies, and clearing the cache can sometimes solve the problem.  Just typeSource: You need to follow this tutorial .If you had installed, just uninstall and reinstall again:"},
{"body": "I would like to wipe out all data for a specific kind in Google App Engine. What is the\nbest way to do this?\nI wrote a delete script (hack), but since there is so much data is\ntimeout's out after a few hundred records. The  from Google is that you have to delete in chunks spread over multiple requests. You can use AJAX, , or request your URL from a script until there are no entities left.I am currently deleting the entities by their key, and it seems to be faster.from the terminal, I run curl -N http://...You can now use the Datastore Admin for that: If I were a paranoid person, I would say Google App Engine (GAE) has not made it easy for us to remove data if we want to. I am going to skip discussion on index sizes and how they translate a 6 GB of data to 35 GB of storage (being billed for). That's another story, but they do have ways to work around that - limit number of properties to create index on (automatically generated indexes) et cetera.The reason I decided to write this post is that I need to \"nuke\" all my Kinds in a sandbox. I read about it and finally came up with this code:I have over 6 million records. That's a lot. I have no idea what the cost will be to delete the records (maybe more economical not to delete them). Another alternative would be to request a deletion for the entire application (sandbox). But that's not realistic in most cases.I decided to go with smaller groups of records (in easy query). I know I could go for 500 entities, but then I started receiving very high rates of failure (re delete function).Try using  then you dont even have to deploy any special codePresumably your hack was something like this:As you say, if there's sufficient data, you're going to hit the request timeout before it gets through all the records. You'd have to re-invoke this request multiple times from outside to ensure all the data was erased; easy enough to do, but hardly ideal.The admin console doesn't seem to offer any help, as (from my own experience with it), it seems to only allow entities of a given type to be listed and then deleted on a page-by-page basis.When testing, I've had to purge my database on startup to get rid of existing data.I would infer from this that Google operates on the principle that disk is cheap, and so data is typically orphaned (indexes to redundant data replaced), rather than deleted. Given there's a fixed amount of data available to each app at the moment (0.5 GB), that's not much help for non-Google App Engine users.I've tried db.delete(results) and App Engine Console, and none of them seems to be working for me. Manually removing entries from Data Viewer (increased limit up to 200) didn't work either since I have uploaded more than 10000 entries. I ended writing this script The trick was to include redirect in html instead of using self.redirect. I'm ready to wait overnight to get rid of all the data in my table. Hopefully, GAE team will make it easier to drop tables in the future.  The fastest and efficient way to handle bulk delete on Datastore is by using the new  announced on the latest .If your language of choice is , you just have to register your mapper in a  file  and define a function like this:On  you should have a look to  that suggests a function like this:One tip. I suggest you get to know the  for these types of uses (bulk deleting, modifying, etc.).  But, even with the remote api, batch size can be limited to a few hundred at a time.Unfortunately, there's no way to easily do a bulk delete. Your best bet is to write a script that deletes a reasonable number of entries per invocation, and then call it repeatedly - for example, by having your delete script return a 302 redirect whenever there's more data to delete, then fetching it with \"wget --max-redirect=10000\" (or some other large number).With django, setup url: Setup viewThen run in powershell:If you are using Java/JPA you can do something like this:Java/JDO info can be found here: Yes you can:\nGo to Datastore Admin, and then select the Entitiy type you want to delete and click Delete.\nMapreduce will take care of deleting!You can use the task queues to delete chunks of say 100 objects.\nDeleting objects in GAE shows how limited the Admin capabilities are in GAE. You have to work with batches on 1000 entities or less. You can use the bulkloader tool that works with csv's but the documentation does not cover java.\nI am using GAE Java and my strategy for deletions involves having 2 servlets, one for doing the actually delete and another to load the task queues. When i want to do a delete, I run the queue loading servlet, it loads the queues and then GAE goes to work executing all the tasks in the queue.How to do it:\nCreate a servlet that deletes a small number of objects.\nAdd the servlet to your task queues.\nGo home or work on something else ;)\nCheck the datastore every so often ...I have a datastore with about 5000 objects that i purge every week and it takes about 6 hours to clean out, so i run the task on Friday night.\nI use the same technique to bulk load my data which happens to be about 5000 objects, with about a dozen properties. This worked for me:Thank you all guys, I got what I need. :D\nThis may be useful if you have lots db models to delete, you can dispatch it in your terminal. And also, you can manage the delete list in DB_MODEL_LIST yourself.\nDelete DB_1:Delete All DB:Here is the bulkdel.py file:And here is the modified version of alexandre fiori's code.And of course, you should map the link to model in a file(like main.py in GAE), ;)\nIn case some guys like me need it in detail, here is part of main.py:On a , one can cd to his app's directory then run it like this:Doing so will start the app and clear the datastore.  If you already have another instance running, the app won't be able to bind to the needed IP and therefore fail to start...and to clear your datastore. In javascript, the following will delete all the entries for on page:given that you are on the admin-page (.../_ah/admin) with the entities you want to delete."},
{"body": "At the moment, I'm doing stuff like the following, which is getting tedious:I'm guessing there is some more accepted way of handling this stuff?What I'm looking for is having a function execute once, on demand. For example, at the press of a certain button. It is an interactive app which has a lot of user controlled switches. Having a junk variable for every switch, just for keeping track of whether it has been run or not, seemed kind of inefficient.I would use a decorator on the function to handle keeping track of how many times it runs.Now  will only run once. Other calls to it will return . Just add an  clause to the  if you want it to return something else. From your example, it doesn't need to return anything ever.If you don't control the creation of the function, or the function needs to be used normally in other contexts, you can just apply the decorator manually as well.This will leave  available for other uses.Finally, if you need to only run it once twice, then you can just doAnother option is to set the   for your function to be a code object for a function that does nothing. This should be done at the end of your function body.For example:Here  replaces your function's executable code with the code for lambda:None, so all subsequent calls to  will do nothing.This technique is less flexible than the decorator approach suggested in the , but may be more concise if you only have one function you want to run once. Run the function before the loop. Example:This is the obvious solution. If there's more than meets the eye, the solution may be a bit more complicated.I'm assuming this is an action that you want to be performed at most one time, if some conditions are met. Since you won't always perform the action, you can't do it unconditionally outside the loop. Something like lazily retrieving some data (and caching it) if you get a request, but not retrieving it otherwise.There are many ways to do what you want; however, do note that it is quite possible that \u2014as described in the question\u2014 you don't have to call the function inside the loop.If you insist in having the function call inside the loop, you can also do:I've thought of another, very effective way to do this that doesn't require decorator function or classes. Instead it just uses a mutable keyword argument, which ought to work in most versions of Python. Most of the time these are something to be avoided since normally you wouldn't want a default argument value to be able to change from call-to-call -- but that ability can be leveraged and used as cheap storage mechanism. Here's how it would work:This could be simplified a little further by doing  and using an iterator (which were introduced in Python 2.2.):Here's an answer that doesn't involve reassignment of functions, yet still prevents the need for that ugly \"is first\" check. is supported by Python 2.5 and above.Output:Assuming there is some reason why  can't be called before the loopHere's an explicit way to code this up, where the state of which functions have been called is kept locally (so global state is avoided). I don't much like the non-explicit forms suggested in other answers: it's too surprising to see f() and for this not to mean that f() gets called.This works by using dict.pop which looks up a key in a dict, removes the key from the dict, and takes a default value to use in case the key isn't found.Why is this any different from your code?If I understand the updated question correctly, something like this should workOne object-oriented approach and make your function a class, aka as a \"functor\", whose instances automatically keep track of whether they've been run or not when each instance is created. Since your updated question indicates you may need many of them, I've updated my answer to deal with that by using a  pattern. This is a bit unusual, and iy may have been down-voted for that reason (although we'll never know for sure because they never left a comment). It could also be done with a metaclass, but it's not much simpler.Output:Note: You could make a function/class able to do stuff again by adding a  method to its subclass that reset the shared  attribute. It's also possible to pass regular and keyword arguments to the  method when the functor is created and the method is called, if desired.And, yes, it would be applicable given the information you added to your question.If the condition check needs to happen only once you are in the loop, having a flag signaling that you have already run the function helps. In this case you used a counter, a boolean variable would work just as fine.I'm not sure that I understood your problem, but I think you can divide loop. On the part of the function and the part without it and save the two loops."},
{"body": "I want to return status code  from a Django view. It is in response to an automatic POST which updates a database and I just need to indicate the update was successful (without redirecting the client).There are subclasses of  to handle most other codes but not 204.What is the simplest way to do this?Either what Steve Mayne answered, or build your own by subclassing HttpResponse:When using , there is a  keyword argument.(Note that in the case of status 204 there shouldn't be a response body, but this method is useful for other status codes.)"},
{"body": "i am trying to get the latest django model object but cannot seem to succeed. neithernoris working. please help. See the docs from django:\nYou need to specify a field in latest(). eg. Or if your model\u2019s Meta specifies get_latest_by, you can leave off the  argument to . Django will use the field specified in  by default. is really designed to work with date fields (it probably does work with other total-ordered types too, but not sure). And the only way you can use it without specifying the field name is by setting the  meta attribute, as mentioned ."},
{"body": "I have a prefix that I want to add to every route.  Right now I add a constant to the route at every definition.  Is there a way to do this automatically?The answer depends on how you are serving this application.Assuming that you are going to run this application inside of a WSGI container (mod_wsgi, uwsgi, gunicorn, etc); you need to actually  the application as a sub-part of that WSGI container (anything that speaks WSGI will do) and to set your  config value to your prefix:Setting the  config value simply limit Flask's session cookie to that URL prefix.  Everything else will be automatically handled for you by Flask and Werkzeug's excellent WSGI handling capabilities.If you are not sure what the first paragraph means, take a look at this example application with Flask mounted inside of it:If, on the other hand, you will be running your Flask application at the root of its WSGI container and proxying requests to it (for example, if it's being FastCGI'd to, or if nginx is -ing requests for a sub-endpoint to your stand-alone  /  server then you can either:You can put your routes in a blueprint:Then you register the blueprint with the application using a prefix:All you have to do is to write a middleware to make the following changes:Like this:Wrap your app with the middleware, like this:Visit ,You will get the right result: And don't forget to set the cookie domain if you need to.This solution is given by . The  is not for this job, although it looks like to be. It's really confusing.This is more of a python answer than a Flask/werkzeug answer; but it's simple and works.If, like me, you want your application settings (loaded from an  file) to also contain the prefix of your Flask application (thus, not to have the value set during deployment, but during runtime), you can opt for the following:Arguably, this is somewhat hackish and relies on the fact that the Flask route function  a  as a first positional argument.You can use it like this:So, I believe that a valid answer to this is: the prefix should be configured in the actual server application that you use when development is completed. Apache, nginx, etc.However, if you would like this to work during development while running the Flask app in debug, take a look at .I'll copy the code here for posterity:Now, when running the above code as a standalone Flask app,  will display .In a comment on another answer, I expressed that I wished to do something like this:Applying  to my contrived example:"},
{"body": "I'm writing a script to check permissions of files in user's directories and if they're not acceptable I'll be warning them, but I want to check permissions of not just the logged in user, but also group and others. How can i do this? It seems to me that .access() in Python can only check the permissions for the user running the script.You're right that , like the underlying  syscall, checks for a specific user (real rather than effective IDs, to help out with suid situations). is the right way to get more general info about a file, including permissions per user, group, and others.  The  attribute of the object that  returns has the permission bits for the file.To help interpret those bits, you may want to use the  module.  Specifically, you'll want the bitmasks defined , and you'll use the  operator (bit-and) to use them to mask out the relevant bits in that  attribute -- for example, if you just need a True/False check on whether a certain file is group-readable, one approach is:Take care: the  call can be somewhat costly, so make sure to extract all info you care about with a single call, rather than keep repeating calls for each bit of interest;-).Use  with flags , , and .: Check out  if you are testing directory permissions on Windows.You can check file permissions via  in conjunction with the  module for interpreting the results. and the associated  for the mode."},
{"body": "I have a tar file which has number of files within it.\nI need to write a python script which will read the contents of the files and gives the count o total characters, including  total number of letters, spaces, newline characters, everything,  without untarring the tar file.you can use getmembers()After that, you can use extractfile() to extract the members as file object. Just an exampleWith the file object \"f\" in the above example, you can use read(), readlines() etc. you need to use the tarfile module. Specifically, you use an instance of the class TarFile to access the file, and then access the names with TarFile.getnames()     If instead you want to read the , then you use this methodAn implementation of the methods mentioned by @stefano-borini\nAccess a tar archives member via file name like so Credits:"},
{"body": "If I had two strings,  and , I could get all combinations of them using two for loops:However, I would like to be able to do this using list comprehension. I've tried many ways, but have never managed to get it. Does anyone know how to do this?orif you want tuples.Like in the question,  is the outer loop,  is the inner loop.Essentially, you can have as many independent 'for x in y' clauses as you want in a list comprehension just by sticking one after the other.Since this is essentially a Cartesian product, you can also use . I think it's clearer, especially when you have more input iterables.Try recursion too:Gives you the 8 combinations:"},
{"body": "When running python26 under windows OS (64bits).....\nI have got errors like:orI have done the msi installation for python26\nall dlls can be found under C:\\Python26\\Lib\\site-packages\\pywin32_system32After I move/copy pywintypes26.dll and pythoncom26.dll to c:\\Python26\\Lib\\site-packages\\win32 -> Solve the problem!I also hit a problem importing win32api. The post-install script for pywin32 failed, which should copy , , and , among other things.  I ran it by hand and my installation was fixed.I had a similar problem when installing under 64 bit Python 3.4.2. I ran the install executable pywin32\u2011219.win\u2011amd64\u2011py3.4.exe from  . On the site it states clearly: \"Python 3.4 users must manually run  from an elevated command prompt.\" which I did not do first time round; I installed from a normal prompt getting the following feedback:I only read the last sentence and I started to run some code resulting in getting these dll load fails.So, did some research, and started an elevated prompt (how: see \"\") and again ran:And now my code runs happily (as far as this matter is concerned... sigh, so much other stuff to do).Run the installer as Administrator and it works:I always install the Active State Python distro which installs the win32 packages for you and gets it right.There appears to be a . The recommended workaround in the ticket is the same one as proposed by Dave Bremer.I could fix this situation by removing all installed Python3.4 versions (had forgotten to uninstall 3.4.1 before installing 3.4.2), deleting C:\\Python34 and after installing Python 3.4.2 pywin32-219.win32-py34.exe could be installed and called without problems.\nSo, copying around DLLs should NOT be necessary!As suggested above the post install script is not run, this issue can be seen when installing from a wheel (As I encountered)If find you have this issue when installing via wheels, then installing it from here can solve the above issue.If the above didn't fix the problem, you're still missing the  file.  It's either in your  folder, or more likely in your  folder.That fixed it for me after hours of search to no avail, even though it looks like the import still isn't resolved as it should be (PyCharm still gives me the squiggly underline), but it works."},
{"body": "I have code that relies heavily on yaml for cross-language serialization and while working on speeding some stuff up I noticed that yaml was insanely slow compared to other serialization methods (e.g., pickle, json).So what really blows my mind is that json is so much faster that yaml when the output is nearly identical.PyYaml's CSafeDumper and cjson are both written in C so it's not like this is a C vs Python speed issue. I've even added some random data to it to see if cjson is doing any caching, but it's still way faster than PyYaml. I realize that yaml is a superset of json, but how could the yaml serializer be 2 orders of magnitude slower with such simple input?In general, it's not the complexity of the output that determines the speed of parsing, but the complexity of the accepted input. The JSON grammar is . The YAML parsers are , leading to increased overheads.I'm not a YAML parser implementor, so I can't speak specifically to the orders of magnitude without some profiling data and a big corpus of examples. In any case, be sure to test over a large body of inputs before feeling confident in benchmark numbers.  Whoops, misread the question. :-( Serialization can still be blazingly fast despite the large input grammar; however, browsing the source, it looks like PyYAML's Python-level serialization  whereas simplejson encodes builtin Python datatypes directly into text chunks.In applications I've worked on, the type inference between strings to numbers (float/int) is where the largest overhead is for parsing yaml is because strings can be written without quotes.  Because all strings in json are in quotes there is no backtracking when parsing strings.  A great example where this would slow down is the value 0000000000000000000s.  You cannot tell this value is a string until you've read to the end of it.  The other answers are correct but this is a specific detail that I've discovered in practice.Speaking about efficiency, I used YAML for a time and felt attracted by the simplicity that some name/value assignments take on in this language. However, in the process I tripped so and so often about one of YAML\u2019s finesses, subtle variations in the grammar that allow you to write special cases in a more concise style and such. In the end, although YAML\u2019s grammar is almost for certain formally consistent, it has left me with a certain feeling of \u2018vagueness\u2019. I then restricted myself to not touch existing, working YAML code and write everything new in a more roundabout, fail-safe syntax\u2014which made me abandon all of YAML. The upshot is that YAML tries to look like a W3C standard, and produces a small library of hard to read literature concerning its concepts and rules.This, I feel, is by far more intellectual overhead than needed. Look at SGML/XML: developed by IBM in the roaring 60s, standardized by the ISO, known (in a dumbed-down and modified form) as HTML to uncounted millions of people, documented and documented and documented again the world over. Comes up little JSON and slays that dragon. How could JSON become so widely used in so short a time, with just one meager website (and a javascript luminary to back it)? It is in its simplicity, the sheer absence of doubt in its grammar, the ease of learning and using it.XML and YAML are hard for humans, and they are hard for computers. JSON is quite friendly and easy to both humans and computers.A cursory look at python-yaml suggests its design is much more complex than cjson's:More complex designs almost invariably mean slower designs, and this is far more complex than most people will ever need."},
{"body": "Google didn't turn up anything that seemed relevant.I have a bunch of existing, working C++ code, and I'd like to use python to crawl through it and figure out relationships between classes, etc.EDIT: Just wanted to point out: I don't think I need or want to parse every bit of C++; I just need something smart enough to pick up on class, function and member variable declarations, and to skip over function definitions. C++ is notoriously hard to parse. Most people who try to do this properly end up taking apart a compiler. In fact this is (in part) why LLVM started: Apple needed a way they could parse C++ for use in XCode that matched the way the compiler parsed it. That's why there are projects like  which you could combine with a python xml library.Some non-compiler projects that seem to do a pretty good job at parsing C++ are:Not an answer as such, but just to demonstrate how hard parsing C++ correctly actually is. My favorite demo:This is perfectly valid, standard-compliant C++, but the exact meaning of commented line depends on your implementation. If  (typical on 32-bit platforms), it is a declaration of local variable  of type . If the condition doesn't hold, then it is a no-op expression . Adding a constructor for  will actually let you expose the difference via presence/absence of side effects.For many years I've been using , which is a very nice Python wrapper around GCC-XML.  It's a very full featured package that forms the basis of some well used code-generation tools out there such as  which is from the same author.You won't find a drop-in Python library to do this. Parsing C++ is fiddly, and few parsers have been written that aren't part of a compiler. You can find a good summary of the issues .The best bet might be , as its C++ support is . Though this is not a Python solution, it sounds as though it would be amenable to re-use within a Python wrapper, given the emphasis on encapsulation and good design in its development. is a complete and functional parser for ANSI C.\nPerhaps you can extend it to c++ :-)If you've formatted your comments in a compatible way,  does a fantastic job. It'll even draw inheritance diagrams if you've got  installed.For example, running DOxygen over the following:Will turn all those comments into entries in .html files. With more complicated designs, the result is even more beneficial - often much easier than trying to browse through the source. shows a C++ grammar written in Antlr, and you  from it.There also seems to be someone who was working on a C++ parser in pyparsing, but I was not able to find out who or its current status.There is no (free) good library to parse C++ in  language.\nYour best choices are probably  g++ plugin, , or .The pyparsing wiki shows this example - all it does is parse struct declarations, so this might give you just a glimpse at the magnitude of the problem.I suggest you (or even better, your employer) shell out $200 and buy .  This software is amazingly powerful for the price, and includes pretty good code reverse engineering features.  You will spend far more than this in your own time to only get about 2% of the job done.  In this case, \"buys\" wins over \"make\". uses  for code generation. It's possible that cpptypes does also.  Even if it doesn't, you could use gcc-xml to generate XML from your C++ file, then parse the xml with one of the built-in or third-party Python XML parsers.Here's a SourceForge project that claims to parse c++ headers. As the other commenters have pointed out, there's no general solution, but you this sounds like it will do enough for your needs. (I just ran across it for a similar need and haven't tried it myself yet.)The Clang project provides libraries for just parsing C++ code.Either with Clang and GCC you can generate an XML representation of the codeIf you prefer a more Pythonian solution you could also search for a C++ yacc grammar and use py-ply (Yacc for Python), but that seems the solution that needs more workI would keep an eye on the  as it seems like plugins are the way to go. Also the  seems like it has a nice implementation."},
{"body": "How do I convert  to ?Are you trying to represent it with only one digit:or actually round off the other decimal places?or even round strictly down?Or use the builtin round:"},
{"body": "Can anyone recommend a Socket.IO client library for Python?\nI've had a look around, but the only ones I can find are either server implementations, or depend on a framework such as Twisted.I need a client library that has no dependencies on other frameworks.Simply using one of the many connection types isn't sufficient, as the python client will need to work with multiple socketio servers, many of which won't support websockets, for example.Archie1986's answer was good but has become outdated with socketio updates (more specifically, its protocol : )... as far as i can tell, you need to perform the handshake manually before you can ask for a transport (e.g., websockets) connection... note that the code below is incomplete and insecure... for one, it ignores the list of supported transports returned in the handshake response and always tries to get a websocket... also it assumes that the handshake always succeeds... nevertheless, it's a good place to startyou might also want to read up on python-websockets: First of all, I'm not sure why some of your Socket.IO servers won't support websockets...the intent of Socket.IO is to make front-end browser development of web apps easier by providing an abstracted interface to real-time data streams being served up by the Socket.IO server. Perhaps Socket.IO is not what you should be using for your application? That aside, let me try to answer your question...At this point in time, there aren't any Socket.IO client libraries for Python (gevent-socketio is not a Socket.IO  library for Python...it is a Socket.IO  library for Python). For now, you are going to have to piece some original code together in order to interface with Socket.IO directly as a client while accepting various connection types.I know you are looking for a cure-all that works across various connection types (WebSocket, long-polling, etc.), but since a library such as this does not exist as of yet, I can at least give you some guidance on using the WebSocket connection type based on my experience.For the WebSocket connection type, create a WebSocket client in Python. From the command line install this Python WebSocket Client package  with pip so that it is on your python path like so:Once you've done that try the following, replacing  and  with the appropriate location of your Socket.IO server: At this point you have a medium of interfacing with a Socket.IO server directly from Python. To send messages to the Socket.IO server simply send a message through this WebSocket connection. In order for the Socket.IO server to properly interpret incoming messages through this WebSocket from your Python Socket.IO client, you need to adhere to the Socket.IO protocol and encode any strings or dictionaries you might send through the WebSocket connection. For example, after you've accomplished everything above do the following:The  library supports event callbacks and channels thanks to the work of contributors and is available on  under the MIT license.The  library with the popular asynchronous  is also one of the options available for python.Wrote one: . It only supports websockets so it may have only marginal utility for you.Did you have a look at ?Hope it helps."},
{"body": "I'm trying to write a fabric script that does a ; however, if there is nothing to commit, git exits with a status of . The deploy script takes that as unsuccessful, and quits. I do want to detect  failures-to-commit, so I can't just give fabric a blanket ignore for  failures. How can I allow empty-commit failures to be ignored so that deploy can continue, but still catch errors caused when a real commit fails?Catch this condition beforehand by checking the exit code of git diff?For example (in shell):From the  man page:This causes fabric to ignore the failure. Has the advantage of not creating empty commits.You can wrap it in a additional layer of  to totally suppress output, otherwise you'll get a note in the fabric output that the commit failed (but the fabfile continues to execute).  try/catch baby!"},
{"body": "Suppose I have the following code in a Python unit test:Is there an easy way to assert that a particular method (in my case ) was called during the second line of the test? e.g. is there something like this:I use  for this:For your case, it could look like this:Mock supports quite a few useful features, including ways to patch an object or module, as well as checking that the right thing was called, etc etc.  (Buyer beware!)If you mistype  (to  or ) your test may still run, as Mock will think this is a mocked function and happily go along, unless you use . For more info read .I'm not aware of anything built-in.  It's pretty simple to implement:This requires that the object itself won't modify self.b, which is almost always true.Yes if you are using Python 3.3+. You can use the built-in  to assert method called. Here is a quick example in your case:Yes, I can give you the outline but my Python is a bit rusty and I'm too busy to explain in detail.Basically, you need to put a proxy in the method that will call the original, eg:This  about callable may help you understand the above.Although the answer was accepted, due to the interesting discussion with Glenn and having a few minutes free, I wanted to enlarge on my answer:You can mock out , either manually or using a testing framework like . Manually, you'd do it using something like this:Using pymox, you'd do it like this:"},
{"body": "How do I serialize a Python dictionary into a string, and then back to a dictionary? The dictionary will have lists and other dictionaries inside it.It depends on what you're wanting to use it for. If you're just trying to save it, you should use  (or , which is faster, if using CPython as you probably are).However, if you want it to be readable, you could use or . and  are very limited in what they will support.  can be used for objects (if it doesn't work automatically, the class can define  to specify precisely how it should be pickled).Use Python's  module, or  if you don't have python 2.6 or higher. you fully trust the string and don't care about  then this is very simple solution:If you're more safety conscious then  is a better bet.Pickle is great but I think it's worth mentioning  from the  module for an even lighter weight solution if you're only serializing basic python types. It's basically a \"safe\" version of the notorious  function that only allows evaluation of basic python types as opposed to any valid python code.Example:One benefit is that the serialized data is just python code, so it's very human friendly. Compare it to what you would get with :The downside is that as soon as the the data includes a type that is not supported by  you'll have to transition to something else like pickling.While not strictly serialization, json may be reasonable approach here. That will handled nested dicts and lists, and data as long as your data is \"simple\": strings, and basic numeric types.  should also be mentioned here. It is both human readable and can serialize any python object.\npyyaml is hosted here:\n One thing  cannot do is  indexed with numerals. The following snippetwill throw Because keys are converted to strings.  preserves the numeric type and the unpacked  can be used right away.If you are trying to only serialize then pprint may also be a good option. It requires the object to be serialized and a file stream.Here's some code:I am not sure if we can deserialize easily. I was using json to serialize and deserialze earlier which works correctly in most cases.However, in one particular case, there were some errors writing non-unicode data to json."},
{"body": "How can I add, subtract, and compare binary numbers in Python without converting to decimal?You can convert between a string representation of the binary using bin() and int()I think you're confused about what binary is.  Binary and decimal are just different representations of a number - e.g. 101 base 2 and 5 base 10 are the same number.  The operations add, subtract, and compare operate on numbers - 101 base 2 == 5 base 10 and addition is the same logical operation no matter what base you're working in.  The fact that your python interpreter may store things as binary internally doesn't affect how you work with it - if you have an integer type, just use +, -, etc.If you have strings of binary digits, you'll have to either write your own implementation or convert them using the int(binaryString, 2) function.If you're talking about bitwise operators, then you're after:Otherwise, binary numbers work exactly the same as decimal numbers, because numbers are numbers, no matter how you look at them. The only difference between decimal and binary is how we represent that data when we are looking at it.Binary, decimal, hexadecimal... the base only matters when reading or outputting numbers, adding binary numbers is just the same as adding decimal number : it is just a matter of representation.Not sure if helpful, but I leave my solution here:I think you're confused about what binary is. Binary and decimal are just different representations of a number - e.g. 101 base 2 and 5 base 10 are the same number. The operations add, subtract, and compare operate on numbers - 101 base 2 == 5 base 10 and addition is the same logical operation no matter what base you're working in."},
{"body": "So I recently stumbled upon this great library for handling HTTP requests in Python; found here .I love working with it, but I can't figure out how to add headers to my get requests. Help?According to the , the headers can all be passed in using requests.get:Seems pretty straightforward, according to the  on the page you linked (emphasis mine)."},
{"body": "Suppose I have a list of tuples and I want to convert to multiple lists.For example, the list of tuples is Is there any built-in function in Python that convert it to:This can be a simple program. But I am just curious about the existence of such built-in function in Python.The built-in function  will almost do what you want:The only difference is that you get tuples instead of lists.  You can convert them to lists usingFrom the :Specific example:Or, if you really want lists:Use:Adding to Claudiu's and Claudiu's answer and since map needs to be imported from itertools in python 3, you also use a list comprehension like:"},
{"body": "What is the equivalent of this SQL statement in django?How do I implement this in django? I triedBut that did not work. How do i implement this? Use  or :contains and icontains mentioned by falsetrue make queries like  Along with them, you might need these ones with similar behavior:\n, , , making or  Case insensitive search for string in a field."},
{"body": "Requirements:Here is my code:Other things I tried include the following code ... but this is waaaaay slower.Here is a schematic of how this is called:I tried a few different things, with timing.And this is how I timed it:So it looks like regular old Python lists are pretty good ;)np.append() copy all the data in the array every time, but list grow the capacity by a factor (1.125). list is fast, but memory usage is larger than array. You can use array module of the python standard library if you care about the memory.Here is a discussion about this topic:  Using the class declarations in Owen's post, here is a revised timing with some effect of the finalize.In short, I find class C to provide an implementation that is over 60x faster than the method in the original post. (apologies for the wall of text)The file I used:Now, the resulting timings:Class A is destroyed by the updates, class B is destroyed by the finalizes. Class C is robust in the face of both of them.there is a big performance difference in the function that you use for finalization. Consider the following code:Using concatenate seems to be twice as fast as the first version and more than 10 times faster than the second version.If you want improve performance with list operations, have a look to blist library. It is a optimized implementation of python list and other structures.I didn't benchmark it yet but the results in their page seem promising."},
{"body": "I have a module foo, containing util.py and bar.py.I want to import it in IDLE or python session. How do I go about this?I could find no documentation on how to import modules not in the current directory or the default python PATH.\nAfter trying ,\nand  The closest I could get wasWhich gave me Permission denied on windows 7.One way is to simply amend your :Note that this requires foo to be a python package, i.e. contain a  file. If you don't want to modify , you can  or . Beware that this means that other directories or  files in that directory may be loaded inadvertently.Therefore, you may want to use  instead. It needs the filename, not a directory (to a file which the current user is allowed to read):You could customize the module search path using the  environment variable, or manually modify the  directory list.See  documentation on python.org.Give this a tryFollowing phihag's tip, I have this solution. Just give the path of a source file to  and it will load it. You must also provide a name, so you can import this module using this name. I prefer to do it this way because it's more explicit:Another (less explicit) way is this: the method is rewritten to make it clearer."},
{"body": "I'm trying to get started with Sphinx and seem to have relentless problems.Command: I answer all the questions and everything works fine.Command: Everything looks normal. Result:  Command: It seems to work.  I was able to open the index.html file and see a \"shell\" of what I'm wanting.When I try and put my actual source code as the  folder I run into problems.Command: Result:I am a complete newbie to Sphinx and relatively new to this kind of documentation.  Can anyone offer some suggestions?Edit:I'd like to be able to use a Makefile to handle this.  As of now I have two folders in my project.I need  to generate the HTML for  and all other modules I'm going to have.Autodoc can't find your modules, because they are not in . You have to include the path to your modules in in the  in your .\nLook at the top of your  (just after the import of ), there is a  statement, which you can adapt.By the way: you can use the  created by Sphinx to create your documentation.\nJust callto see the options.If something went wrong before try:before running .in just add the path to your project folder.I think I did this the first time I tried to add a file to the toctree.  I think it was because I left out the blank line between the :maxdepth line and the file name.Above is my index.rst file.  stuff.rst resides in the same directory as it.Sphinx is not very  compatible, running    both work in my interpreter, but not in sphinx.I tried checking out the master branch of sphinx, changed my interpreter to python3.4 in the  and got errors on modules that were removed in the 3.x series. You can see my issue report here:You can use  and noweb formatting to generate rst documents that include the output of the code embedded in them. Basically, you write your rst file, with python code embedded in marked chunks like this:and Pweave will execute those chunks, and replace them with their output in a resulting rst file, which you can then use with sphinx. See the  for more details of how it looks.I tried to use autodoc to document my sphinx code, but it would skip over one of my files because I didn't make a class within that file. Here's what the file originally looked something like this:This file would never successfully get documented by sphinx. To get it to be documented, I had to do the following:So, it seems like Sphinx requires you to wrap all your files up in a class to get them documented. hope that helps, cause I spent hours trying to figure out why Sphinx wasn't documenting"},
{"body": "I'm trying to use Test-Driven Development with the  module.\npytest will not  to the console when I write .I use  to run it...The  seems to say that it should work by default: But:Nothing gets printed to my standard output console (just the normal progress and how many many tests passed/failed).And the script that I'm testing contains print:In  module, everything gets printed by default, which is exactly what I need. However, I wish to use  for other reasons. It seems like such basic functionality that perhaps I'm missing it!?By default,  captures the result of standard out so that it can control how it prints it out. If it didn't do this, it would spew out a lot of text without the context of what test printed that text.However, if a test fails, it will include a section in the resulting report that shows what was printed to standard out in that particular test.For example,Results in the following output:Note the  section.If you would like to see  statements as they are executed, you can pass the  flag to . However, note that this can sometimes be difficult to parse.Using  option will print output of all functions, which may be too much. If you need particular output, the doc page you mentioned offers few suggestions:That is rather hackish way to do stuff, but may be it is the stuff you need: after all, TDD means you mess with stuff and leave it clean and silent when it's ready :-).I needed to print important warning about skipped tests exactly when  muted literally . I didn't want to fail a test to send a signal, so I did a hack as follow:The  module allows me to print stuff   released the output streams. The output looks as follow:Message is printed even when  is in silent mode, and is  printed if you run stuff with , so everything is tested nicely already."},
{"body": "I need to write, or find, a script to create a Debian package, using python-support, from a Python package. The Python package will be pure Python with no C extensions.The Python package for testing purposes will just be a directory with an empty  file and a single Python module, .The packaging script  use python-support to provide the correct bytecode for possible multiple installations of Python on a target platform, i.e. v2.5 and v2.6 on Ubuntu Jaunty.Most advice I find while googling are just examples of nasty hacks that don't even use python-support or python-central.  I have spent hours researching this, and the best I can come up with is to hack around the script from an existing open source project, but I don't know which bits are required for what I'm doing.Has anyone here made a Debian package out of a Python package in a reasonably non-hacky way?I'm starting to think that it will take me more than a week to go from no knowledge of Debian packaging and python-support to getting a working script.  How long has it taken others?I would take the sources of an existing Debian package, and replace the actual package in it with your package. To find a list of packages that depend on python-support, doPick a package that is , so that it is a pure-Python package. Going through this list, I found that e.g. python-flup might be a good starting point.\nTo get the source of one such package, doTo build it, doWhen editing it, expect that you only need the files in the  folder; replace all references to flup with your own package name.Once you get started, it should take you a day to complete.I think what you want is :Most of the answers posted here are outdated, fortunately a great Debian wiki post has been made recently, which explains the current best practices and describes how to build Debian packages for Python modules and applications.The right way of building a deb package is using  but sometimes it is a little bit complicated. Instead you can use  and it will create your Debian package.These are the basics for creating a Debian package with  with any binary or with any kind of script that runs automatically without needing manual compilation (Python, Bash, Pearl, Ruby):The scripts placed at  are directly called from the terminal, note that I didn't add an extension to the script. Also you can notice that the structure of the deb package will be the structure of the program once it's installed. So if you follow this logic if your program has a single file, you can directly place it under , but if you have multiple files, you should place them under  and place only one file under  that will call your scripts from Here is an example of the  file. You only need to copy-paste it in to an empty file called \"control\" and put it in the DEBIAN folderFirst off, there are plenty of Python packages already in Debian; you can download the source (including all the packaging) for any of them either using  or by visiting .You may find the following resources of use:"},
{"body": "I have a path (including directory and file name).\nI need to test if the file-name is a valid, e.g. if the file-system will allow me to create a file with such a name.\nThe file-name  in it.It's safe to assume the directory segment of the path is valid and accessible ().I very much do not want to have to escape anything unless I  to. I'd post some of the example characters I am dealing with, but apparently they get automatically removed by the stack-exchange system. Anyways, I want to keep standard unicode entities like , and only escape things which are invalid in a filename.Here is the catch.  I need to keep that file if it does exist, and not create a file if it does not.Basically I want to check if I  write to a path  (and the automatic file creation/file clobbering that typically entails).As such:Is not acceptable, because it will overwrite the existent file, which I do not want to touch (if it's there), or create said file if it's not.I know I can do:But that will  the file at the , which I would then have to .In the end, it seems like it's spending 6 or 7 lines to do something that should be as simple as  or similar.As an aside, I need this to run on (at least) Windows and MacOS, so I'd like to avoid platform-specific stuff.``Call the  function defined below.Strictly Python 3. That's just how we roll.The question of \"How do I test pathname validity and, for valid pathnames, the existence or writability of those paths?\" is clearly two separate questions. Both are interesting, and neither have received a genuinely satisfactory answer here... or, well,  that I could grep.'s  probably hews the closest, but has the remarkable disadvantages of:We're gonna fix all that.Before hurling our fragile meat suits into the python-riddled moshpits of pain, we should probably define what we mean by \"pathname validity.\" What defines validity, exactly?By \"pathname validity,\" we mean the  of a pathname with respect to the  of the current system \u2013 regardless of whether that path or parent directories thereof physically exist. A pathname is syntactically correct under this definition if it complies with all syntactic requirements of the root filesystem.By \"root filesystem,\" we mean:The meaning of \"syntactic correctness,\" in turn, depends on the type of root filesystem. For  (and most but  all POSIX-compatible) filesystems, a pathname is syntactically correct if and only if that pathname:Syntactic correctness. Root filesystem. That's it.Validating pathnames in Python is surprisingly non-intuitive. I'm in firm agreement with  here: the official  package should provide an out-of-the-box solution for this. For unknown (and probably uncompelling) reasons, it doesn't. Fortunately, unrolling your own ad-hoc solution isn't  gut-wrenching... It's hairy; it's nasty; it probably chortles as it burbles and giggles as it glows. But what you gonna do? We'll soon descend into the radioactive abyss of low-level code. But first, let's talk high-level shop. The standard  and  functions raise the following exceptions when passed invalid pathnames:Crucially, this implies that  The  and  functions raise generic  exceptions when passed pathnames residing in non-existing directories, regardless of whether those pathnames are invalid or not. Directory existence takes precedence over pathname invalidity.Does this mean that pathnames residing in non-existing directories are  validatable? Yes \u2013 unless we modify those pathnames to reside in existing directories. Is that even safely feasible, however? Shouldn't modifying a pathname prevent us from validating the original pathname?To answer this question, recall from above that syntactically correct pathnames on the  filesystem contain no path components  containing null bytes or  over 255 bytes in length. Hence, an  pathname is valid if and only if all path components in that pathname are valid. This is true of   of interest.Does that pedantic insight actually help us? Yes. It reduces the larger problem of validating the full pathname in one fell swoop to the smaller problem of only validating all path components in that pathname. Any arbitrary pathname is validatable (regardless of whether that pathname resides in an existing directory or not) in a cross-platform manner by following the following algorithm:Is there a directory guaranteed to exist? Yes, but typically only one: the topmost directory of the root filesystem (as defined above).Passing pathnames residing in any other directory (and hence not guaranteed to exist) to  or  invites race conditions, even if that directory was previously tested to exist. Why? Because external processes cannot be prevented from concurrently removing that directory  that test has been performed but  that pathname is passed to  or . Unleash the dogs of mind-fellating insanity!There exists a substantial side benefit to the above approach as well:  (Isn't  nice?) Specifically:The above approach obviates this by only validating the path components of a pathname against the root directory of the root filesystem. (If even  stale, slow, or inaccessible, you've got larger problems than pathname validation.)Lost?  Let's begin. (Python 3 assumed. See \"What Is Fragile Hope for 300, ?\") Don't squint at that code. ()Testing the existence or creatability of possibly invalid pathnames is, given the above solution, mostly trivial. The little key here is to call the previously defined function  testing the passed path: and  Except not quite.There exists a caveat. Of course there does.As the official  admits:To no one's surprise, Windows is the usual suspect here. Thanks to extensive use of Access Control Lists (ACL) on NTFS filesystems, the simplistic POSIX permission-bit model maps poorly to the underlying Windows reality. While this (arguably) isn't Python's fault, it might nonetheless be of concern for Windows-compatible applications.If this is you, a more robust alternative is wanted. If the passed path does  exist, we instead attempt to create a temporary file guaranteed to be immediately deleted in the parent directory of that path \u2013 a more portable (if expensive) test of creatability:Note, however, that even  may not be enough.Thanks to User Access Control (UAC), the ever-inimicable Windows Vista and all subsequent iterations thereof  about permissions pertaining to system directories. When non-Administrator users attempt to create files in either the canonical  or  directories, UAC superficially permits the user to do so while  isolating all created files into a \"Virtual Store\" in that user's profile. (Who could have possibly imagined that deceiving users would have harmful long-term consequences?)This is crazy. This is Windows.Dare we? It's time to test-drive the above tests.Since NULL is the only character prohibited in pathnames on UNIX-oriented filesystems, let's leverage that to demonstrate the cold, hard truth \u2013 ignoring non-ignorable Windows shenanigans, which frankly bore and anger me in equal measure:Beyond sanity. Beyond pain. You will find Python portability concerns.Note that  can fail for more reasons than just  so you might have to do finer tests like testing if the containing directory exists and so on.After my discussion with the OP it turned out, that the main problem seems to be, that the file name might contain characters that are not allowed by the filesystem. Of course they need to be removed but the OP wants to maintain as much human readablitiy as the filesystem allows. Sadly I do not know of any good solution for this.\nHowever  takes a closer look at detecting the problem.will open the file or give an error if it doesn't exist. If there's an error, then you can try to write to the path, if you can't then you get a second errorAlso have a look  about permissions on windowstry  this will check for the path and return  if exists and  if not."},
{"body": "I can't seem to get code coverage with Nose to work, despite having the plugin installed.Any ideas on how to fix this?Have you tried ? The coverage plugin depends on separate coverage module, which is not a nose's dependency, so needs to be installed manually."},
{"body": "How do I do I set the syntax highlighting in Vim 7 for python?I would like to set my own colorschemes, and syntax highlighting for a type of code file.The command to enable syntax highlighting in vim is , if you want it to be active everytime you launch vim, just add a line containing  in your .vimrc file.Put the line  in your . on how to setup syntax highlighting in Python for Ubuntu 12.10.  What you see is what you get:There are hundreds of fashionable color schemes here: Pick one that you like.  Then download the  and  files and put them in the right spot as defined here:   Set the appropriate commands in the ~/.vimrc and logout login.Learn to edit the code that causes vim syntax highlighting to work.  Make it better than it ever was."},
{"body": "I want to do something like the following:but this actually tries to run , which is not what I want to do.Is there a general way to turn variables into strings? Something like the following:IPython expands variables with , bash-style.  This is true for , not just .So you would do:myscript.py contains:Via Python's fancy string formatting, you can even put expressions inside :Use  to get a reference to the current , then call the  method: See  \u2014 that's a much better way to do it.It seems this is impossible with the built-in  magic function. Your question led me down a rabbit hole, though, and I wanted to see how easy it would be to do something similar. At the end, it seems somewhat pointless to go to all this effort to create another magic function that just uses . Maybe this will be of some use to someone, somewhere.Using this pair of classes, (and given a python script ) it's possible to create and use a \"macro\" variable with the newly created \"my_run\" magic function like so:Yes, this is a huge and probably wasteful hack. In that vein, I wonder if there's a way to have the name bound to the Macro object be used as the macro's actual name. Will look into that."},
{"body": "I'm just trying to time a piece of code. The pseudocode looks like:How does this look in Python?More specifically, how do I get the number of ticks since midnight (or however Python organizes that timing)?In the  module, there are two timing functions:  and .  gives you \"wall\" time, if this is what you care about.However, the python  say that  should be used for benchmarking. Note that  behaves different in separate systems:Apart from all that, the  module has the  class that is supposed to use what's best for benchmarking from the available functionality.\u00b9 unless threading gets in the way\u2026\u00b2 Python \u22653.3: there are .  is being used by the  module.What you need is  function from  module:You can use  module for more options though.Here's a solution that I started using recently:You use it like this (You need at least Python 2.5):When your code finishes, Timer automatically prints out the run time.  Sweet!  If I'm trying to quickly bench something in the Python Interpreter, this is the easiest way to go.  And here's a sample implementation of 'now' and 'format_delta', though feel free to use your preferred timing and formatting method.Please let me know if you have a preferred formatting method, or if there's an easier way to do all of this!The  in python gives you access to the clock() function, which returns time in seconds as a floating point.Different systems will have different accuracy based on their internal clock setup (ticks per second) but it's generally at least under 20milliseconds, and in some cases better than a few microseconds.-AdamFrom midnight:If you have many statements you want to time, you could use something like this:Then your code could look like:That way you don't need to type  before each block and  after it, while still keeping control over formatting (though you could easily put that in  too).It's a minimal gain, but I think it's kind of convenient."},
{"body": "I am looking for something similar to 'clear' in Matlab: A command/function which removes all variables from the workspace, releasing them from system memory. Is there such a thing in Python?EDIT: I want to write a script which at some point clears all the variables.The following sequence of commands does remove  name from the current module:I doubt you actually DO want to do this, because \"every name\" includes all built-ins, so there's not much you can do after such a total wipe-out.  Remember, in Python there is really no such thing as a \"variable\" -- there are , of many kinds (including modules, functions, class, numbers, strings, ...), and there are , bound to objects; what the sequence does is remove every name from a module (the corresponding objects go away if and only if every reference to them has just been removed).Maybe you want to be more selective, but it's hard to guess exactly what you mean unless you want to be more specific.  But, just to give an example:This sequence leaves alone names that are private or magical, including the  special name which houses all built-in names.  So, built-ins still work -- for example:As you see, name  (the control variable in that ) also happens to stick around (as it's re-bound in the  clause every time through), so it might be better to name that control variable , for example, to clearly show \"it's special\" (plus, in the interactive interpreter, name  is re-bound anyway after every complete expression entered at the prompt, to the value of that expression, so it won't stick around for long;-).Anyway, once you have determined exactly what it  you want to do, it's not hard to define a function for the purpose and put it in your start-up file (if you want it only in interactive sessions) or site-customize file (if you want it in every script).No, you are best off restarting the interpreter is an excellent replacement for the bundled interpreter and has the  command which usually worksWrite a function. Once you leave it all names inside disappear. It is very pointless to do this yourself in any kind of way.The concept is called  and it's so good, it made it into the :This is a modified version of Alex's answer.\nWe can save the state of a module's namespace and restore it by using the following 2 methods...You can also add a line \"clear = restoreContext\" before calling saveContext() and clear() will work like matlab's clear."},
{"body": "I am trying to use Python to login to a website and gather information from several webpages and I get the following error:I used  and it works, but it seems unintelligent and unreliable, is there any other way to dodge this error?Here's my code:Receiving a status 429 is , it is the other server \"kindly\" asking you to please stop spamming requests. Obviously, your rate of requests has been too high and the server is not willing to accept this.You should not seek to \"dodge\" this, or even try to circumvent server security settings by trying to spoof your IP, you should simply respect the server's answer by not sending too many requests.If everything is set up properly, you will also have received a \"Retry-after\" header along with the 429 response. This header specifies the number of seconds you should wait before making another call. The proper way to deal with this \"problem\" is to read this header and to sleep your process for that many seconds.You can find more information on status 429 here: Another workaround would be to spoof your IP using some sort of Public  VPN or Tor network. This would be assuming the rate-limiting on the server at IP level.There is a brief blog post demonstrating a way to use tor along with urllib2:Writing this piece of code fixed my problem:"},
{"body": "I am trying to install MySQLdb package. I found the source code .I did the following:As the result I got the following:Does anybody knows how to solve this problem?\nBy the way, if I am able to do the described step, I will need to do the following:And I have no system-administrator-rights. Do I still have a chance to install MySQLdb?Thank you.If MySQLdb's now distributed in a way that requires , your choices are either to download the latter (e.g. from ) or refactor MySQLdb's setup.py to bypass  (maybe just importing  and  from plain  instead might work, but you may also need to edit some of the  files in the same directory).Depending on how your site's Python installation is configured, installing extensions for your own individual use without requiring sysadm rights may be hard, but it's never truly impossible if you have shell access. You'll need to tweak your Python's sys.path to start with a directory of your own that's your personal equivalent of the system-wide site pacages directory, e.g. by setting  persistently in your own environment, and then manually place in said personal directory what normal installs would normally place in site-packages (and/or subdirectories thereof).After trying many suggestions, simply using  worked for me.More info: \nI resolved this issue on centos5.4 by running the following command to install setuptoolsyum install python-setuptoolsI hope that helps.This was sort of tricky for me too, I did the following which worked pretty well.Not sure if you'll need to use sudo if you're just installing it for you current user. You'd definitely need it to install it for all users.@main:and it will be installed (MySQLdb).For Python 2.7, one can easily install using this Also, you can see the build dependencies in the file setup.cfgI am experiencing the same problem right now. According to  you need to have a C Compiler or GCC. I'll try to fix the problem by installing C compiler. I'll inform you if it works (we'll I guess you don't need it anymore, but I'll post the result anyway) :)well installing C compiler or GCC didn't work but I found a way to successfully install mysqldb package kindly follow Mike schrieb's (Thanks to him) instructions  . In my case, I used setuptools-0.6c11-py2.7.egg and setuptools-0.6c11 . Then download the executable file  then install that file. hope it helps :)When you need to install modules in Linux/Unix and you lack sudo / admin rights, one simple way around it is to use the user scheme installation, basically run \"python setup.py install --user\" from the command line in the folder of the module / library to be installed(see  for further details)"},
{"body": "This code bit produces error: Can anyone tell me how to get this series of date time strings into a DataFrame as  objects?Use pandas.to_datetime(pd.Series(..)). It's concise and much faster than above code."},
{"body": "I need to store some data in a Django model. These data are not equal to all instances of the model.At first I thought about subclassing the model, but I\u2019m trying to keep the application flexible. If I use subclasses, I\u2019ll need to create a whole class each time I need a new kind of object, and that\u2019s no good. I\u2019ll also end up with a lot of subclasses only to store a pair of extra fields.I really feel that a dictionary would be the best approach, but there\u2019s nothing in the Django documentation about storing a dictionary in a Django model (or I can\u2019t find it).Any clues?If it's really dictionary like arbitrary data you're looking for you can probably use a two-level setup with one model that's a container and another model that's key-value pairs. You'd create an instance of the container, create each of the key-value instances, and associate the set of key-value instances with the container instance. Something like:It's not pretty, but it'll let you access/search the innards of the dictionary using the DB whereas a pickle/serialize solution will not.If you don't need to query by any of this extra data, then you can store it as a serialized dictionary.   Use  to turn the dictionary into a string, and  to turn the string back into a dictionary.  Take care with eval that there's no user data in the dictionary, or use a safe_eval implementation.As Ned answered, you won't be able to query \"some data\" if you use the dictionary approach.If you still need to store dictionaries then the best approach, by far, is the PickleField class documented in Marty Alchin's new book .  This method uses Python class properties to pickle/unpickle a python object, only on demand, that is stored in a model field.The basics of this approach is to use django's  method to dynamically add a new field to your model and uses getattr/setattr to do the serializing on demand.One of the few online examples I could find that is similar is this definition of a .I came to this post by google's 4rth result to \"django store object\"A little bit late, but  looks like good solution to me.Example from doc:To use, just define a field in your model:and assign whatever you like (as long as it's picklable) to the field:Another clean and fast solution can be found here: For convenience I copied the simple instructions.I'm not sure exactly sure of the nature of the problem you're trying to solve, but it sounds curiously similar to .Expandos allow you to specify and store additional fields on an database-backed object instance at runtime. To quote from the docs:Google App Engine currently supports both Python and the Django framework. Might be worth looking into if this is the best way to express your models.Traditional relational database models don't have this kind of column-addition flexibility. If your datatypes are simple enough you could break from traditional RDBMS philosophy and hack values into a single column via serialization as  proposes; however, if you  to use an RDBMS, Django model inheritance is probably the way to go. Notably, it will create  relation for each level of derivation.Being \"not equal to all instances of the model\" sounds to me like a good match for a \"Schema-free database\".  is the poster child for that approach and you might consider that. In a project I moved several tables which never played very nice with the Django ORM over to CouchDB and I'm quite happy with that. I use  without any of the Django-specific CouchDB modules. A description of the data model can be found . The movement from five \"models\" in Django to 3 \"models\" in Django and one CouchDB \"database\" actually slightly reduced the total lines of code in my application.Think it over, and find the commonalities of each data set... then define your model.  It may require the use of subclasses or not.  Foreign keys representing commonalities aren't to be avoided, but encouraged when they make sense.Stuffing random data into a SQL table is not smart, unless it's truly non-relational data.  If that's the case, define your problem and we may be able to help.Django-Geo includes a \"DictionaryField\" you might find helpful:In general, if you don't need to query across the data use a denormalized approach to avoid extra queries. User settings are a pretty good example!I agree that you need to refrain stuffing otherwise structured data into a single column. But if you must do that, Django has an  build-in.There's also  at Django snipplets.If you are using Postgres, you can use an hstore field: ."},
{"body": "what's the meaning of  in the following statement?I think I've heard of , , and  but never heard of this.Background:In Python, there are two builtin functions for turning an object into a string:  vs. .  is supposed to be a friendly, human readable string.  is supposed to include detailed information about an object's contents (sometimes, they'll return the same thing, such as for integers). By convention, if there's a Python expression that will eval to another object that's ==,  will return such an expression e.g.If returning an expression doesn't make sense for an object,  should return a string that's surrounded by < and > symbols e.g. .To answer your original question: <-> \n <-> In addition:You can control the way an instance of your own classes convert to strings by implementing  and  methods.It calls  on the object and inserts the resulting string.It prints the replacement as a string with .Adding to the replies given above, '%r' can be useful in a scenario where you have a list with heterogeneous data type.\nLet's say, we have a list = [1, 'apple' , 2 , 'r','banana']\nObviously in this case using '%d' or '%s' would cause an error. Instead, we can use '%r' to print all these values.See  in the docs. Notice that %s and %d etc, might work differently to how you expect if you are used to the way they work in another language such as C.In particular, %s also works well for ints and floats unless you have special formatting requirements where %d or %f will give you more control."},
{"body": "How to test if a module has been imported in python?for example I need the basics:alsoThanks!Rgs.Thanks for all of your comments:\nthe code been pasted here.\nIf you want to optimize by not importing things twice, save yourself the hassle because Python already takes care of this.If you  this to avoid  or something: Fix your sloppy coding - make sure you don't need this, i.e. define (import) everything before you ever use it (in the case if imports: once, at startup, at module level).In case you do have a good reason:  is a dictionary containing all modules already imported somewhere. But it only contains modules, and because of the way  works (import the whole module as usual, extract the things you import from it),  would only add  to  (if it wasn't already imported on startup).  adds  as you probably expect.I feel the answer that has been accepted is not fully correct.Python  when importing the same module multiple times. Python , sure, but that doesn't mean it won't slow down your script. As you will see from the URL below, there is  overhead when importing a module multiple times. For example, in a situation where you may not need a certain module except under a particular condition, if that module is large or has a high overhead then there is reason to import only on condition. That does not explicitly mean you are a sloppy coder either.this is my solution of changing code at runtime!"},
{"body": "How can I export a list of DataFrames into one Excel spreadsheet?\nThe docs for  state:Following this, I thought I could write a function which saves a list of DataFrames to one spreadsheet as follows:However (with a list of two small DataFrames, each of which can save  individually), an exception is raised :Presumably I am not calling  correctly, how should I be in order to do this?You should be using pandas own  class:Then the  function works as expected:"},
{"body": "I have two Django models which inherit from a base class:Request has two foreign keys to the built-in User model.For some reason I'm getting the errorEverything I've read says that setting the  should prevent the clash, but I'm still getting the same error. Can anyone think of why this would be happening? Thanks!The related_name would ensure that the fields were not conflicting with each other, but you have two models, each of which has both of those fields. You need to put the name of the concrete model in each one, which you can do with some special : for related name in Django 1.9"},
{"body": "There is form with two . But when i'm sending it, second submit causes error.::First submit(add) works well, but second(remove)...:How can i fix this error?:It was pretty simple:\nrequest.form returns ImmutableMultiDict:As @Blubber points out, the issue is that Flask raises an HTTP error when it fails to find a key in the  and  dictionaries.  What Flask assumes by default is that if you are asking for a particular key  then something got left out of the request and the entire request is invalid.There are two other good ways to deal with your situation:You can start by posting the actual exception that is being thrown instead of that more or less useless output from your webserver. Anyway, my guess is you should be checking whether or not the 'add' and 'remove' keys are in the request.form dict, i.e.:When you click  it doesn't fail because the first condition is met, and hence the second is never checked. But if the  button is clicked, that first condition will thrown a  exception because the request.form dict doesn't contain a key named ."},
{"body": "Is there a way to make Python's optparse print the default value of an option or flag when showing the help with --help?Try using the  string placeholder:And if you need programmatic access to the default values, you can get to them via the  attribute of the parser (it's a dict)And if you want to add default values automatically to all options that you have specified, you can do the following:The comments to your question already indicate there's another way to parse arguments called . It's been introduced in Python 3.2. It actually deprecates  but is used similarly. comes with different formatting classes and for instance  will also print the default values without you having to manipulate the help string manually.An example from the python docs:see Add  to your parserfrom documentation:"},
{"body": "There are many questions related to Stackless Python. But none answering this my question, I think (correct me if wrong - please!). There's some buzz about it all the time so I curious to know. What would I use Stackless for? How is it better than CPython?Yes it has green threads (stackless) that allow quickly create many lightweight threads as long as no operations are blocking (something like Ruby's threads?). What is this great for? What other features it has I want to use over CPython?It allows you to work with massive amounts of concurrency. Nobody sane would create one hundred thousand system threads, but you can do this using stackless.This article tests doing just that, creating one hundred thousand tasklets in both Python and Google Go (a new programming language): Surprisingly, even if Google Go is compiled to native code, and they tout their co-routines implementation, Python still wins.Stackless would be good for implementing a map/reduce algorithm, where you can have a very large number of reducers depending on your input data.Stackless Python's main benefit is the support for very lightweight coroutines. CPython doesn't support coroutines natively (although I expect someone to post a generator-based hack in the comments) so Stackless is a clear improvement on CPython when you have a problem that benefits from coroutines. I think the main area where they excel are when you have many concurrent tasks running within your program. Examples might be game entities that run a looping script for their AI, or a web server that is servicing many clients with pages that are slow to create.You still have many of the typical problems with concurrency correctness however regarding shared data, but the deterministic task switching makes it easier to write safe code since you know exactly where control will be transferred and therefore know the exact points at which the shared state must be up to date.Thirler already mentioned that stackless was used in Eve Online. Keep in mind, that:(got this citation from )At PyCon 2009 there was given , describing why and how Stackless is used at CCP Games.Also, there is a very good , which describes why stackless is a good solution for Your applications. (it may be somewhat old, but I think that it is worth reading).EVEOnline is largely programmed in Stackless Python. They have several dev blogs on the use of it. It seems it is very useful for high performance computing.While I've not used Stackless itself, I have used Greenlet for implementing highly-concurrent network applications.  Some of the use cases Linden Lab has put it towards are: high-performance smart proxies, a fast system for distributing commands over huge numbers of machines, and an application that does a ton of database writes and reads (at a ratio of about 1:2, which is very write-heavy, so it's spending most of its time waiting for the database to return), and a web-crawler-type-thing for internal web data.  Basically any app that's expecting to have to do a lot of network I/O will benefit from being able to create a bajillion lightweight threads.  10,000 connected clients doesn't seem like a huge deal to me.Stackless or Greenlet aren't really a complete solution, though.  They are very low-level and you're going to have to do a lot of monkeywork to build an application with them that uses them to their fullest.  I know this because I maintain a library that provides a networking and scheduling layer on top of Greenlet, specifically because writing apps is so much easier with it.  There are a bunch of these now; I maintain Eventlet, but also there is Concurrence, Chiral, and probably a few more that I don't know about.  If the sort of app you want to write sounds like what I wrote about, consider one of these libraries.  The choice of Stackless vs Greenlet is somewhat less important than deciding what library best suits the needs of what you want to do.The basic usefulness for green threads, the way I see it, is to implement a system in which you have a large amount of objects that do high latency operations. A concrete example would be communicating with other machines:Threads let you write the above code naturally, but if the number of objects is large enough, threads just cannot perform adequately. But you can use green threads even for in really large amounts. The  above could switch out to some scheduler where other work is waiting and return later. You get all the benefits of being able to call \"blocking\" functions  without using threads.This is obviously very useful for any kind of distributed computing if you want to write code in a straightforward way.It is also interesting for multiple cores to mitigate waiting for locks:The  function would basically attempt to acquire the lock and just switch out to a main scheduler if it fails due to other cores using the object. Again, green threads are being used to mitigate blocking, allowing code to be written naturally and still perform well."},
{"body": "I switched from Perl to Python about a year ago and haven't looked back.  There is only  idiom that I've ever found I can do more easily in Perl than in Python:The corresponding Python code is not so elegant since the if statements keep getting nested:Does anyone have an elegant way to reproduce this pattern in Python?  I've seen anonymous function dispatch tables used, but those seem kind of unwieldy to me for a small number of regular expressions...With thanks to :Using named groups and a dispatch table:With a little bit of introspection you can auto-generate the regexp and the dispatch table.Yeah, it's kind of annoying.  Perhaps this will work for your case.   Brian correctly pointed out that my first attempt did not work.  Unfortunately, this attempt is longer.For example:I'd suggest this, as it uses the least regex to accomplish your goal.  It is still functional code, but no worse then your old Perl.Alternatively, something not using regular expressions at all:Whether that is suitable depends on your actual problem. Don't forget, regular expressions aren't the swiss army knife that they are in Perl; Python has different constructs for doing string manipulation.To speed it up, one could turn all regexes into one internally and create the dispatcher on the fly. Ideally, this would be turned into a class then.Here's the way I solved this issue:Not nearly as clean as the original pattern.  However, it is simple, straightforward and doesn't require extra modules or that you change the original regexs.how about using a dictionary?however, you must ensure there are no duplicate match_objects dictionary keys ( mo_foo, mo_bar, ... ), best by giving each regular expression its own name and naming the match_objects keys accordingly, otherwise match_objects.setdefault() method would return existing match object instead of creating new match object by running re_xxx.search( text ).Expanding on the solution by Pat Notz a bit, I found it even the more elegant to:\n\u00a0 - name the methods the same as  provides (e.g.  vs. ) and\n\u00a0\u00a0- implement the necessary methods like  on the holder object itself:So instead of e.g. this:One does just this:Looks very natural in the end, does not need too many code changes and avoids the problems with global state like some other solutions.My solution would be:"},
{"body": "Ok this is one of those trickier than it sounds questions so I'm turning to stack overflow because I can't think of a good answer. Here is what I want: I need Python to generate a simple a list of numbers from 0 to 1,000,000,000 in random order to be used for serial numbers (using a random number so that you can't tell how many have been assigned or do timing attacks as easily, i.e. guessing the next one that will come up). These numbers are stored in a database table (indexed) along with the information linked to them. The program generating them doesn't run forever so it can't rely on internal state.No big deal right? Just generate a list of numbers, shove them into an array and use Python \"random.shuffle(big_number_array)\" and we're done. Problem is I'd like to avoid having to store a list of numbers (and thus read the file, pop one off the top, save the file and close it). I'd rather generate them on the fly. Problem is that the solutions I can think of have problems:1) Generate a random number and then check if it has already been used. If it has been used generate a new number, check, repeat as needed until I find an unused one. Problem here is that I may get unlucky and generate a lot of used numbers before getting one that is unused. Possible fix: use a very large pool of numbers to reduce the chances of this (but then I end up with silly long numbers).2) Generate a random number and then check if it has already been used. If it has been used add or subtract one from the number and check again, keep repeating until I hit an unused number. Problem is this is no longer a random number as I have introduced bias (eventually I will get clumps of numbers and you'd be able to predict the next number with a better chance of success).3) Generate a random number and then check if it has already been used. If it has been used add or subtract another randomly generated random number and check again, problem is we're back to simply generating random numbers and checking as in solution 1.4) Suck it up and generate the random list and save it, have a daemon put them into a Queue so there are numbers available (and avoid constantly opening and closing a file, batching it instead). 5) Generate much larger random numbers and hash them (i.e. using MD5) to get a smaller numeric value, we should rarely get collisions, but I end up with larger than needed numbers again.6) Prepend or append time based information to the random number (i.e. unix timestamp) to reduce chances of a collision, again I get larger numbers than I need.Anyone have any clever ideas that will reduce the chances of a \"collision\" (i.e. generating a random number that is already taken) but will also allow me to keep the number \"small\" (i.e. less than a billion (or a thousand million for your europeans =)).Answer and why I accepted it:So I will simply go with 1, and hope it's not an issue, however if it is I will go with the deterministic solution of generating all the numbers and storing them so that there is a guarentee of getting a new random number, and I can use \"small\" numbers (i.e. 9 digits instead of an MD5/etc.). This is a neat problem, and I've been thinking about it for a while (with solutions similar to ), but in the end, here's what I think:Use your point 1) and stop worrying.Assuming real randomness, the probability that a random number has already been chosen before is the count of previously chosen numbers divided by the size of your pool, i.e. the maximal number.If you say you only need a billion numbers, i.e. nine digits: Treat yourself to 3 more digits, so you have 12-digit serial numbers (that's three groups of four digits \u2013 nice and readable).Even when you're close to having chosen a billion numbers previously, the probability that your new number is already taken is still only 0,1%.Do step 1 and draw again. You can still check for an \"infinite\" loop, say don't try more than 1000 times or so, and then fallback to adding 1 (or something else).You'll win the lottery before that fallback ever gets used.You could use  to encrypt a counter. Your counter just goes from 0 upwards, and the encryption uses a key of your choice to turn it into a seemingly random value of whatever radix and width you want.Block ciphers normally have a fixed block size of e.g. 64 or 128 bits. But Format-Preserving Encryption allows you to take a standard cipher like AES and make a smaller-width cipher, of whatever radix and width you want (e.g. radix 10, width 9 for the parameters of the question), with an algorithm which is still cryptographically robust.It is guaranteed to never have collisions (because cryptographic algorithms create a 1:1 mapping). It is also reversible (a 2-way mapping), so you can take the resulting number and get back to the counter value you started with. is one proposed standard method to achieve this.I've experimented with some basic Python code for AES-FFX-- (but note that it doesn't fully comply with the AES-FFX specification). It can e.g. encrypt a counter to a random-looking 7-digit decimal number. E.g.:For another example in Python, using another non-AES-FFX (I think) method, see  which does FPE using a Feistel cipher. It generates numbers from 0 to 2^32-1.With some modular arithmic and prime numbers, you can create all numbers between 0 and a big prime, out of order. If they don't have to be random, but just not obviously linear (1, 2, 3, 4, ...), then here's a simple algorithm:Pick two prime numbers.  One of them will be the largest number you can generate, so it should be around one billion.  The other should be fairly large.Just store the previous serial each time so you know where you left off.  I can't prove mathmatically that this works (been too long since those particular classes), but it's demonstrably correct with smaller primes:You could also prove it empirically with 9-digit primes, it'd just take a bit more work (or a lot more memory).This does mean that given a few serial numbers, it'd be possible to figure out what your values are--but with only nine digits, it's not likely that you're going for unguessable numbers anyway.If you don't need something cryptographically secure, but just \"sufficiently obfuscated\"...You could try operations in , e.g. GF(2), to map a simple incrementing counter  to a seemingly random serial number :Many of these operations have an inverse, which means, given your serial number, you can calculate the original counter value from which it was derived.As for finding a library for Galois Field for Python... good question. If you don't need speed (which you wouldn't for this) then you could make your own. I haven't tried these:Pick a suitable 32\u00d732 invertible matrix in GF(2), and multiply a 32-bit input counter by it. This is conceptually related to LFSR, as described in .A related possibility is to use a  calculation. Based on the remainder of long-division with an irreducible polynomial in GF(2). Python code is readily available for CRCs (, ), although you might want to pick a different irreducible polynomial than is normally used, for your purposes. I'm a little fuzzy on the theory, but I think a 32-bit CRC should generate a unique value for every possible combination of 4-byte inputs. Check this. It's quite easy to experimentally check this, by feeding the output back into the input, and checking that it produces a complete cycle of length 2-1 (zero just maps to zero). You may need to get rid of any initial/final XORs in the CRC algorithm for this check to work.I think you are overestimating the problems with approach 1). Unless you have hard-realtime requirements just checking by random choice terminates rather fast. The probability of needing more than a number of iterations decays exponentially. With 100M numbers outputted (10% fillfactor) you'll have one in billion chance of requiring more than 9 iterations. Even with 50% of numbers taken you'll on average need 2 iterations and have one in a billion chance of requiring more than 30 checks. Or even the extreme case where 99% of the numbers are already taken might still be reasonable - you'll average a 100 iterations and have 1 in a billion change of requiring 2062 iterationsThe standard Linear Congruential random number generator's seed sequence CANNOT repeat until the full set of numbers from the starting seed value have been generated.  Then it MUST repeat precisely.  The internal seed is often large (48 or 64 bits).  The generated numbers are smaller (32 bits usually) because the entire set of bits are not random.  If you follow the seed values they will form a distinct non-repeating sequence.  The question is essentially one of locating a good seed that generates \"enough\" numbers.  You can pick a seed, and generate numbers until you get back to the starting seed.  That's the length of the sequence.  It may be millions or billions of numbers.There are some guidelines in Knuth for picking suitable seeds that will generate very long sequences of unique numbers.You can run 1) without running into the problem of too many wrong random numbers if you just decrease the random interval by one each time.For this method to work, you will need to save the numbers already given (which you want to do anyway) and also save the quantity of numbers taken.It is pretty obvious that, after having collected 10 numbers, your pool of possible random numbers will have been decreased by 10. Therefore, you must not choose a number between 1 and 1.000.000 but between 1 an 999.990. Of course this number is not the real number but only an index (unless the 10 numbers collected have been 999.991, 999.992, \u2026); you\u2019d have to count now from 1 omitting all the numbers already collected.Of course, your algorithm should be smarter than just counting from 1 to 1.000.000 but I hope you understand the method.I don\u2019t like drawing random numbers until I get one which fits either. It just feels wrong.My solution , i think you should extend matrix for 1,000,000,000 variations and have fun.I'd rethink the problem itself... You don't seem to be doing anything sequential with the numbers... and you've got an index on the column which has them. Do they actually  to be ?Consider a sha hash... you don't actually need the entire thing. Do what git or other url shortening services do, and take first 3/4/5 characters of the hash. Given that each character now has 36 possible values instead of 10, you have 2,176,782,336 combinations instead of 999,999 combinations (for six digits). Combine that with a quick check on whether the combination exists (a pure index query) and a seed like a timestamp + random number and it should do for almost any situation.  Do you need this to be cryptographically secure or just hard to guess?  How bad are collisions? Because if it needs to be cryptographically strong and have zero collisions, it is, sadly, impossible.I started trying to write an explanation of the approach used below, but just implementing it was easier and more accurate.  This approach has the odd behavior that it gets faster the more numbers you've generated.  But it works, and it doesn't require you to generate all the numbers in advance.  As a simple optimization, you could easily make this class use a probabilistic algorithm (generate a random number, and if it's not in the set of used numbers add it to the set and return it) at first, keep track of the collision rate, and switch over to the deterministic approach used here once the collision rate gets bad.If it is enough for you that a casual observer can't guess the next value, you can use things like a  or even a simple  to generate the values and keep the state in the database in case you need more values. If you use these right, the values won't repeat until the end of the universe. You'll find more ideas in the .If you think there might be someone who would have a serious interest to guess the next values, you can use a database sequence to count the values you generate and encrypt them with an encryption algorithm or another cryptographically strong perfect has function. However you need to take care that the encryption algorithm isn't easily breakable if one can get hold of a sequence of successive numbers you generated - a simple , for instance, won't do it because of the .Bit late answer, but I haven't seen this suggested anywhere.Why not use the  module to create To generate a list of totally random numbers within a defined threshold, as follows:I bumped into the same problem and opened a  before getting to this one.  is a random sample generator of indexes (i.e. non-repeating numbers) in the interval , called . Here are some usage examples:or generates non-repeating random integers, storage need is limited to picked numbers, and the time needed to pick  numbers should be (as some tests confirm) , regardelss of .Here is the code of :You are stating that you store the numbers in a database.Wouldn't it then be easier to store all the numbers there, and ask the database for a random unused number?\nMost databases support such a request.ExamplesMySQL:PostgreSQL:"},
{"body": "Without subclassing dict, what would a class need to be considered a mapping so that it can be passed to a method with **At least to the point where it throws errors of missing functionality of mapping, so I can begin implementing.I reviewed emulating container types but simply defining magic methods has no effect, and using ABCMeta to override and register it as a dict validates assertions as subclass, but fails isinstance(o, dict). Ideally, I dont even want to use ABCMeta.The  and  methods will suffice:If you're trying to create a Mapping \u2014 not just satisfy the requirements for passing to a function \u2014 then you really should inherit from .  As described in the , you need to implement just:The Mixin will implement everything else for you:  , , , , , , and ."},
{"body": "I use Python a lot, and I am just quickly learning JavaScript right now (or should I say re-learning). So, I wanted to ask, what is the equivalent of  and  in JavaScript?The closest idiom for  would betaking advantage of the fact that  is the number of arguments given in the function definition.You could package this up in a little helper routine likeand then doIf you're in the mood for syntactic sugar, write a function which transforms one function into another one which is called with required and optional arguments, and passes the required arguments along, with any additional optional arguments as an array in final position:Use this as follows:Then again, you could just skip all this mumbo jumbo if you are willing to ask the caller to pass the optional arguments as an array to start with:There is really no analogous solution for , since JS has no keyword arguments. Instead, just ask the caller to pass the optional arguments in as an object:For completeness, let me add that ES6 solves this problem with the rest parameter feature. See .I found a good solution here:\nBasically, use  instead of . apply takes an array as the 2nd arg and 'splats' it for you.ES6 adds a spread operator to JavaScript.  already have it implemented. The following code was tested on Firefox 35.The nearest equivalent is the .ECMAScript 6 will have  which do the same thing as the splat operator."},
{"body": "Say I have a string that looks like this:You'll notice a lot of locations in the string where there is an ampersand, followed by a character (such as \"&y\" and \"&c\").  I need to replace these characters with an appropriate value that I have in a dictionary, like so:What is the fastest way to do this?  I could manually find all the ampersands, then loop through the dictionary to change them, but that seems slow.  Doing a bunch of regex replaces seems slow as well (I will have a dictionary of about 30-40 pairs in my actual code).Any suggestions are appreciated, thanks.As has been pointed out in comments throught this question, my dictionary is defined before runtime, and will never change during the course of the applications life cycle.  It is a list of ANSI escape sequences, and will have about 40 items in it.  My average string length to compare against will be about 500 characters, but there will be ones that are up to 5000 characters (although, these will be rare).  I am also using Python 2.6 currently.\nI accepted Tor Valamos answer as the correct one, as it not only gave a valid solution (although it wasn't the  solution), but took all others into account and did a tremendous amount of work to compare all of them.  That answer is one of the best, most helpful answers I have ever come across on StackOverflow.  Kudos to you.I took the liberty of comparing a few solutions:Results in Python 2.6Both claudiu's and andrew's solutions kept going into 0, so I had to increase it to 10 000 runs.I ran it in  (because of unicode) with replacements of chars from 39 to 1024 (38 is ampersand, so I didn't wanna include it). String length up to 10.000 including about 980 replacements with variable random inserts of length 0-20. The unicode values from 39 to 1024 causes characters of both 1 and 2 bytes length, which could affect some solutions.Results:(** Note that gnibbler's code uses a different dict, where keys don't have the '&' included. Andrew's code also uses this alternate dict, but it didn't make much of a difference, maybe just 0.01x speedup.)Try this, making use of regular expression substitution, and standard string formatting:The re.sub() call replaces all sequences of ampersand followed by single letter with the pattern %(..)s containing the same pattern.The % formatting takes advantage of a feature of string formatting that can take a dictionary to specify the substitution, rather than the more commonly occurring positional arguments.An alternative can do this directly in the re.sub, using a callback:This time I'm using a closure to reference the dictionary from inside the callback function.  This approach could give you a little more flexibility.  For example, you could use something like  to avoid raising exceptions if you had strings with unrecognized code sequences.(By the way, both \"dict\" and \"str\" are builtin functions, and you'll get into trouble if you use those names in your own code much.  Just in case you didn't know that.  They're fine for a question like this of course.)  I decided to check Tor's test code, and concluded that it's nowhere near representative, and in fact buggy.  The string generated doesn't even have ampersands in it (!).  The revised code below generates a representative dictionary and string, similar to the OP's example inputs.I also wanted to verify that each algorithm's output was the same.  Below is a revised test program, with only Tor's, mine, and Claudiu's code -- because the others were breaking on the sample input.  (I think they're all brittle unless the dictionary maps basically  possible ampersand sequences, which Tor's test code was doing.) This one properly seeds the random number generator so each run is the same.  Finally, I added a minor variation using a generator which avoids some function call overhead, for a minor performance improvement.I forgot to include benchmark results before:Also, snippets of the inputs and correct output:Comparing with what I saw from Tor's test code output:If you really want to dig into the topic take a look at this: The obvious solution by iterating over the dictionary and replacing each element in the string takes  time, where n is the size of the dictionary, m is the length of the string.Whereas the Aho-Corasick-Algorithm finds all entries of the dictionary in  where f is the number of found elements.If the number of keys in the list is large, and the number of the occurences in the string is low (and mostly zero), then you could iterate over the occurences of the ampersands in the string, and use the dictionary keyed by the first character of the substrings. I don't code often in python so the style might be a bit off, but here is my take at it:Of course there is a question what happens when there is an ampersand that is coming from the string itself, you would need to escape it in some way before feeding through this process, and then unescape after this process. Of course, as is pretty much usual with the performance issues, timing the various approaches on your typical (and also worst-case) dataset and comparing them is a good thing to do.EDIT: place it into a separate function to work with arbitrary dictionary:EDIT2: get rid of an unneeded concatenation, seems to still be a bit faster than the previous on many iterations.Here is the C Extensions Approach for pythonPython codes I have testedResultsIts suppose to able to run at , and \nOnly took  for  string in My Mobile Celeron 1.6 GHz PCIt will also skip unknown characters as is, for example  will return as isLet me know If you have any problem with compiling, bugs, etc... seems like it does what you want - multiple string replace at once using RegExps. Here is the relevant code:A general solution for defining replacement rules is to use regex substitution using a function to provide the map (see ).This is particularly nice for non-trivial substitutions (e.g anything requiring mathmatical operations to create the substitute).Here is a version using split/joinIn case there are ampersands with invalid codes you can use this to preserve themPeter Hansen pointed out that this fails when there is double ampersand. In that case use this versionNot sure about the speed of this solution either, but you could just loop through your dictionary and repeatedly call the built-in   This might perform decently well if the original string isn't too long, but it would obviously suffer as the string got longer.The problem with doing this mass replace in Python is immutability of the strings: every time you will replace one item in the string then entire new string will be reallocated again and again from the heap.So if you want the fastest solution you either need to use mutable container (e.g. list), or write this machinery in the plain C (or better in Pyrex or Cython). In any case I'd suggest to write simple parser based on simple finite-state machine, and feed symbols of your string one by one.Suggested solutions based on regexps working in similar way, because regexp working using fsm behind the scene.Since someone mentioned using a simple parser, I thought I'd cook one up using pyparsing.  By using pyparsing's transformString method, pyparsing internally scans through the source string, and builds a list of the matching text and intervening text.  When all is done, transformString then ''.join's this list, so there is no performance problem in building up strings by increments.  (The parse action defined for ANSIreplacer does the conversion from the matched &_ characters to the desired escape sequence, and replaces the matched text with the output of the parse action.  Since only matching sequences will satisfy the parser expression, there is no need for the parse action to handle undefined &_ sequences.) The FollowedBy('&') is not strictly necessary, but it shortcuts the parsing process by verifying that the parser is actually positioned at an ampersand before doing the more expensive checking of all of the markup options.Prints:This will certainly not win any performance contests, but if your markup starts to get more complicated, then having a parser foundation will make it easier to extend.try thistr.replace(\"&y\",dict[\"&y\"])tr.replace(\"&c\",dict[\"&c\"])tr.replace(\"&b\",dict[\"&b\"])tr.replace(\"&Y\",dict[\"&Y\"])tr.replace(\"&u\",dict[\"&u\"])"},
{"body": "I would like to use my samsung chromebook to develop for app engine using python, unfortunately now it is not possible as I only have browser there.There are online IDE's like  but they are not good enough yet.So in this regards I have 3 questions:Just enable , and you will get the access to the .You can also use  to install a chroot jail and then you are free to apt-get install any library that's compiles to armv7.I have a Samsung Chromebook and I have installed the GAE python sdk , emacs and clozure common lisp for development.Its super easy to set this up:I have blogged about my experience Let me try and answer each of your 3 questions:You can read a blog post I wrote in the past about 'developers and chromebook' - Good luck!Here is a guide for the new Samsung Chromebook that was introduced in October 2012 with flash memory. It will detail how to install Ubuntu onto an SSD drive to allow for dual boot without needing to modify the internal ChromeOS. To boot into Ubuntu after install you type Ctrl-U.     link:\nThese days, you can easily set up  on a VM with any of a number of cloud service providers. EC2 has been done a bunch of times and is well documented, but pretty much any service that you can run a full Linux VM on should work.It works like a regular webapp. Your code runs on the server, so you can't program your Chomebook, just the VM, but that's better in a lot of circumstances.It's one popular option, even with people who have IPython installed locally, just because Cloud, but on Chrome OS, it makes even more sense to look at trying to get something like that set up, even if you use one of the other options to code offline.There are some new information for the new Samsung Chromebook that was introduced in March 2014 with flash memory. It will detail how to install Ubuntu onto an SSD drive to allow for dual boot without needing to modify the internal ChromeOS. To boot into Ubuntu after install you type Ctrl-U. I will post some more info to my blog  just launched a  with native keyboard shortcuts and a bunch of popular starter templates including python, and a google cloud template. You can get the chrome application for free here: Here are some of the features of the app -- the native keyboard shortcuts are hugely helpful since normally the keybindings are overwritten by the browser (for things like new file, close file, find in project, etc...): "},
{"body": "I'm confused as to where I should put my virtualenvs.With my first django project, I created the project with the command, \"django-admin.py startproject djangoproject\".I then cd'd into the djangoproject directory and ran the command, \"virtualenv env\" which created the virtual environment directory at the same level as the inner \"djangoproject\" directory.Is this the wrong place in which to create the virtualenv for this particular project?I'm getting the impression that most people keep all their virtualenvs together in an entirely different directory, e.g. ~/virtualenvs, and then use virtualenvwrapper to switch back and forth between them.Is there a correct way to do this?Many people use the  tool, which keeps all virtualenvs in the same place (the  directory) and allows shortcuts for creating and keeping them there. For example, you might do:and then later:It's probably a bad idea to keep the virtualenv directory in the project itself, since you don't want to distribute it (it might be specific to your computer or operating system). Instead, keep a requirements.txt file using :and distribute that. This will allow others using your project to reinstall all the same requirements into their virtualenv with:This is a major advantage of putting the directory outside of the repository tree, e.g. under  with .Otherwise, if you keep it in the project tree, moving the project location will break the virtualenv.See: There is  but it is known to not be perfect.Another minor advantage: you don't have to  it.If it weren't for that, I'd just leave my virtualenvs gitignored in the project tree itself to keep related stuff close together.This is fine since you you will likely never reuse a given virtualenv across projects.The generally accepted place to put them is the same place that the default installation of virtualenvwrapper puts them: Related: virtualenvwrapper is an excellent tool that provides shorthands for the common virtualenv commands. "},
{"body": "I'm using Python 3.4 on Windows. When I run a script, it complainsSo I tried to install it, but  givesalthough it does show up when I run . I tried to , which installed successfully but that didn't solve the problem.What am I doing wrong?Here are Windows wheel packages built by Chris Golke - Since Qt is a more complicated system with a compiled C++ codebase underlying the python interface it provides you, it can be more complex to build than just a pure python code package, which means it can be hard to install it from source.Make sure you grab the correct Windows wheel file (python version, 32/64 bit), and then use  to install it - e.g:Should properly install if you are running an x64 build of Python 3.5.You can't use pip. You have to download from the Riverbank website and run the installer for your version of python. If there is no install for your version, you will have to install Python for one of the available installers, or build from source (which is rather involved). Other answers and comments have the links. If you install PyQt4 on Windows, files wind up here by default:but it also leaves a file here:If you copy the both the sip.pyd and PyQt4 folder into your virtualenv things will work fine.For example:Then with windows explorer copy from  the file (sip.pyd) and folder (PyQt4) mentioned above to Then back at CLI:The problem with trying to launch a script which calls PyQt4 from within virtualenv is that the virtualenv does not have PyQt4 installed and it doesn't know how to reference the default installation described above. But follow these steps to copy PyQt4 into your virtualenv and things should work great.It looks like you may have to do a bit of manual installation for PyQt4.This might help a bit more, it's a bit more in a tutorial/set-by-step format:Earlier PyQt .exe installers were available directly from the website download page. Now with the release of PyQt4.12 , installers have been deprecated. You can make the libraries work somehow by compiling them but that would mean going to great lengths of trouble. you can use the previous distributions to solve your purpose. The .exe windows installers can be downloaded from :"},
{"body": "I am running Mac OSX 10.5.8. I installed matplotlib using macports. I get some examples from the matplotlib gallery like this one, without modification:I run it, get no error, but the picture does not show up. In Linux Ubuntu I get it.Do you know what could be wrong here?ThanksI can verify this on my end as well. To fix, here's what I didAlso, we need to change the default backend to a GUI based one.Edit the file , and add:Also, you can try the following, which may allow you to not need the GTK or Cairo backends.\nEdit  and add:With the port with those variants installed, this works as well, but it doesn't require X11.By the way, the error that I saw was the following:I had the same problem, even I could see how a new application window was created and immediately disappeared. Simple solution - just check if you have after the plotThis is what worked for me. I just changed the import of MatplotlibWhen you try instead of does that save the correct image named  in the current path?just to add a note, The matplotlibrc file was not present on my system and I had to to download a copy from the matplotlib website. Future users may have to do the same.This is what worked for me:References:\n\nI only had python 2.5 and I did not want to install python 2.6 on my mac. So I used different procedure mentioned in the following link to solve this problem:1) Searching where is the directory \"pygtk-2.0.pc\" and locate it. For example mine was located in the following directory: /opt/local/lib/pkgconfig2) Adding the path information to envirement variable. For example:3) Download the configuration information file \"matplotlibrc\" from matplotlib website\n  4) Change backend to MacOSX in the file and save it5) Copy the file to directory .matplotlib\n   You can locate the directory in python by the following command: "},
{"body": "I am using python 2.6.5\nI want to write some japanese characters to a file.\nI am getting this error & I don't know how to change the encoding.you're going to have to 'encode' the unicode string.try this out for a bit of a friendly look at unicode and python: As an alternative, you can use the  module:The  function in 2.6 is very similar to the built-in  function in python3.x (which makes sense since Py3k strings are  Unicode). For future proofing your code in case it is used under Py3k you could do the following.Now your code should work the same in both 2.x and 3.3+.Inserting this at the beginning of my script tends to solve unicode problems."},
{"body": "I have a list of booleans in python. I want to AND (or OR or NOT) them and get the result. The following code works but is not very pythonic.Any suggestions on how to make it more pythonic appreciated.Logical  across all elements in :Logical  across all elements in :If you feel creative, you can also do:keep in mind that those aren't evaluated in short circuit, whilst the built-ins are ;-)another funny way:and yet another:and we could go on all day, but yes, the pythonic way is to use  and  :-)By the way, Python has not tail recursion elimination, so don't try to translate LISP code directly ;-)ANDing and ORing is easy:NOTing is also fairly easy:Of course, how you would use those results might require some interesting applications of DeMorgan's theorem. can do this:As fortran mentioned, all is the most succinct way to do it. But reduce answers the more general question \"How to apply a logical operator to all elements in a python list?\"The idiom for such operations is to use the  function (global in Python 2.X, in module  in Python 3.X) with an appropriate binary operator either taken from the  module or coded explicitly. In your case, it's Here's another solution:ANDing all elements will return True if all elements are True, hence no False in a list.\nORing is similar, but it should return True if at least one True value is present in a list.As the other answers show, there are multiple ways to accomplish this task. Here's another solution that uses functions from the standard library:"},
{"body": "In my django app, I have an authentication system. So, If I do not log in and try to access some profile's personal info, I get redirected to a login page. Now, I need to write a test case for this. The responses from the browsers I get is :      How do I write my test ? This what I have so far:what could possibly come next ?Django 1.4:Django 1.7:You could also follow the redirect with:which would mirror the user experience in the browser and make assertions of what you expect to find there, such as:You can check  and see if it matchs with the expected url. Check also that status code is 302. doesn't exist in 1.9. Use this instead:"},
{"body": "The first examples that I googled didn't work. This should be trivial, right?Here's the documentation link: "},
{"body": "I am using  and parsing some HTMLs.I'm getting a certain data from each HTML  and adding that data to a certain list.The problem is, some of the HTMLs have different format .So, I was trying to use exception handling and add value  to the list For instance, I have a code like:and some of the links don't have any , so what I want to do is add string  to the list instead.The error appears: What I have done tried is to add some lines like this:But it doesn't work out. It still shows error:What should I do about this? Should I use exception handling? or is there any easier way?Any suggestions? Any help would be really great!Handling the exception is the way to go:Of course you could also check the  of ; but handling the exception is more intuitive.You have two options; either handle the exception or test the length:orUse the first if there  is no second item, the second if there  is no second item.A ternary will suffice. change:tothis is a short hand forTaking reference of ThiefMaster\u2666 sometimes we get an error with value given as '\\n' or null and perform for that required to handle ValueError:Handling the exception is the way to go"},
{"body": "Is there a way to do the following preprocessor directives in Python?There's , which is a special value that the compiler does preprocess.  will be replaced with a constant 0 or 1 by the compiler, and the optimizer will remove any  lines before your source is interpreted.I wrote a python preprocessor called pypreprocessor that does exactly what you're describing...Here's an example to accomplish what you're describing.pypreprocessor is capable of a lot more than just on-the-fly preprocessing. To see more use case examples check out the project on Google Code.The way I accomplish the preprocessing is simple. From the example above, the preprocessor imports a pypreprocessor object that's created in the pypreprocessor module. When you call parse() on the preprocessor it self-consumes the file that it is imported into and generates a temp copy of itself that comments out all of the preprocessor code (to avoid the preprocessor from calling itself recursively in an infinite loop) and comments out all of the unused portions.Commenting out the lines is, as opposed to removing them, is necessary to preserve line numbers on error tracebacks if the module throws an exception or crashes. And I've even gone as far as to rewrite the error traceback to report reflect the proper file name of the module that crashed.Then, the generated file containing the postprocessed code is executed on-the-fly.The upside to using this method over just adding a bunch of if statements inline in the code is, there will be no execution time wasted evaluating useless statements because the commented out portions of the code will be excluded from the compiled .pyc files.The downside (and my original reason for creating the module) is that you can't run both python 2x and python 3x in the same file because pythons interpreter runs a full syntax check before executing the code and will reject any version specific code before the preprocessor is allowed to run ::sigh::. My original goal was to be able to develop 2x and 3x code side-by-side in the same file that would create version specific bytecode depending on what it is running on.Either way, the preprocessor module is still very useful for implementing common c-style preprocessing capabilities. As well as, the preprocessor is capable of outputting the postprocessed code to a file for later use if you want.Also, if you want to generate a version that has all of the preprocessor directives as well as any of the #ifdefs that are excluded removed it's as simple as setting a flag in the preprocessor code before calling parse(). This makes removing unwanted code from a version specific source file a one step process (vs crawling through the code and removing if statements manually). I suspect you're gonna hate this answer.  The way you do that in Python isSince python is an interpreter, there's no preprocessing step to be applied, and no particular advantage to having a special syntax.  You can use the preprocessor in Python. Just run your scripts through the cpp (C-Preprocessor) in your bin directory. However I've done this with Lua and the benefits of easy interpretation have outweighed the more complex compilation IMHO.You can just use the normal language constructs:An alternative method is to use a bash script to comment out portions of code which are only relevant to debugging. Below is an example script which comments out lines that have a '#DEBUG' statement in it. It can also remove these comment markers again."},
{"body": "I have a program which interfaces with a radio I am using via a gui I wrote in PyQt.  Obviously one of the main functions of the radio is to transmit data, but to do this continuously, I have to loop the writes, which causes the gui to hang. Since I have never dealt with threading, I tried to get rid of these hangs using  The radio needs to sleep between transmissions, though, so the gui still hangs based on how long these sleeps last.  Is there a simple way to fix this using QThread?  I have looked for tutorials on how to implement multithreading with PyQt, but most of them deal with setting up servers and are much more advanced than I need them to be.  I honestly don't even really need my thread to update anything while it is running, I just need to start it, have it transmit in the background, and stop it.I created a little example that shows 3 different and simple ways of dealing with threads. I hope it will help you find the right approach to your problem.Very nice example from Matt, I fixed the typo and also pyqt4.8 is common now so I removed the dummy class as well and added an example for the dataReady signalAccording to the Qt developers, subclassing QThread is incorrect (see ).  But that article is really hard to understand (plus the title is a bit condescending).  I found a better blog post that gives a more detailed explanation about why you should use one style of threading over another:   In my opinion, you should probably never subclass thread with the intent to overload the run method.  While that does work, you're basically circumventing how Qt wants you to work.  Plus you'll miss out on things like events and proper thread safe signals and slots.  Plus as you'll likely see in the above blog post, the \"correct\" way of threading forces you to write more testable code.Here's a couple of examples of how to take advantage of QThreads in PyQt (I posted a separate answer below that properly uses QRunnable and incorporates signals/slots, that answer is better if you have a lot of async tasks that you need to load balance).Take this answer updated for PyQt5, python 3.4 Use this as a pattern to start a worker that does not take data and return data as they are available to the form.1 - Worker class is made smaller and put in its own file worker.py for easy memorization and independent software reuse.2 - The main.py file is the file that defines the GUI Form class3 - The thread object is not subclassed.4 - Both thread object and the worker object belong to the Form object5 - Steps of the procedure are within the comments.And the main file is:In PyQt there are a lot of options for getting asynchronous behavior.  For things that need event processing (ie. QtNetwork, etc) you should use the QThread example I provided in my other answer on this thread.  But for the vast majority of your threading needs, I think this solution is far superior than the other methods.The advantage of this is that the QThreadPool schedules your QRunnable instances as tasks.  This is similar to the task pattern used in Intel's TBB.  It's not quite as elegant as I like but it does pull off excellent asynchronous behavior.This allows you to utilize most of the threading power of Qt in Python via QRunnable and still take advantage of signals and slots.  I use this same code in several applications, some that make hundreds of asynchronous REST calls, some that open files or list directories, and the best part is using this method, Qt task balances the system resources for me.When exiting the application you'll want to make sure you cancel all of the tasks or the application will hang until every scheduled task has completedBased on the Worker objects methods mentioned in other answers, I decided to see if I could expand on the solution to invoke more threads - in this case the optimal number the machine can run and spin up multiple workers with indeterminate completion times.\nTo do this I still need to subclass QThread - but only to assign a thread number and to 'reimplement' the signals 'finished' and 'started' to include their thread number.I've focused quite a bit on the signals between the main gui, the threads, and the workers.Similarly, others answers have been a pains to point out not parenting the QThread but I don't think this is a real concern.  However, my code also is careful to destroy the QThread objects.However, I wasn't able to parent the worker objects so it seems desirable to send them the deleteLater() signal, either when the thread function is finished or the GUI is destroyed.  I've had my own code hang for not doing this.Another enhancement I felt was necessary was was reimplement the closeEvent of the GUI (QWidget) such that the threads would be instructed to quit and then the GUI would wait until all the threads were finished.   When I played with some of the other answers to this question, I got QThread destroyed errors.Perhaps it will be useful to others. I certainly found it a useful exercise.  Perhaps others will know a better way for a thread to announce it identity.And the worker code below"},
{"body": "What would be an  way to check list monotonicity? i.e. that it has monotonically increasing or decreasing values?Examples:This approach is  in the length of the list. If you have large lists of numbers it might be best to use numpy, and if you are:should do the trick.@6502 has the perfect code for lists, I just want to add a general version that works for all sequences:Here is a functional solution using  of complexity :Replace  with the top limit of your values, and  with the bottom limit.  For example, if you are testing a list of digits, you can use  and .I tested its performance against  and its faster.:I timed all of the answers in this question under different conditions, and found that:Here is the code to try it out:If the list was already monotonically increasing (), the fastest to slowest was:If the list was mostly monotonically increasing (), the fastest to slowest was:(Whether or not the starmap or zip was fastest depended on the execution and I couldn't identify a pattern. Starmap appeared to be usually faster)If the list was completely random (), the fastest to slowest was:"},
{"body": "I am trying to post a request to log in to a website using the Requests module in Python but its not really working. I'm new to this...so I can't figure out if I should make my Username and Password cookies or some type of HTTP authorization thing I found (??). So now, I think I'm supposed to use \"post\" and cookies....I have a feeling that I'm doing the cookies thing wrong...I don't know.If it doesn't log in correctly, the title of the home page should come out to \"Locationary.com\" and if it does, it should be \"Home Page.\"If you could maybe explain a few things about requests and cookies to me and help me out with this, I would greatly appreciate it. :DThanks....It still didn't really work yet. Okay...so this is what the home page HTML says before you log in:So I think I'm doing it right, but the output is still \"Locationary.com\"2nd EDIT:I want to be able to stay logged in for a long time and whenever I request a page under that domain, I want the content to show up as if I were logged in.Lets call your  variable  instead, like in the  docs:See  below.I know you've found another solution, but for those like me who find this question, looking for the same thing, it can be achieved with requests as follows:Firstly, as Marcus did, check the source of the login form to get three pieces of information - the url that the form posts to, and the name attributes of the username and password fields. In his example, they are inUserName and inUserPass.Once you've got that, you can use a  instance to make a post request to the login url with your login details as a payload. Making requests from a session instance is essentially the same as using requests normally, it simply adds persistence, allowing you to store and use cookies etc.Assuming your login attempt was successful, you can simply use the session instance to make further requests to the site. The cookie that identifies you will be used to authorise the requests.Let me try to make it simple, suppose URL of the site is  and let's suppose you need to sign up by filling username and password, so we go to the login page say  now and view it's source code and search for the action URL it will be in form tag something like now take userinfo.php to make absolute URL which will be '', now run a simple python script I Hope that this helps someone somewhere someday.Find out the name of the inputs used on the websites form for usernames  and passwords  and replace them in the script below. Also replace the url to point at the desired site to log into.The use of  will silence any output from the script when trying to log into sites with unverified SSL certificates. Extra:To run this script from the command line on a UNIX based system place it in a directory, i.e.  and add this directory to your path in  or a similar file used by the terminal.Then create a link to this python scipt inside Close your terminal, start a new one, run "},
{"body": "How can I extract whatever follows the last slash in a URL in Python? For example, these URLs should return the following:I've tried urlparse, but that gives me the full path filename, such as .You don't need fancy things, just see  and you can easily split your url between 'filename' part and the rest:So you can get the part you're interested in simply with:One more (idio(ma)tic) way: should be up to the task:urlparse is fine to use if you want to (say, to get rid of any query string parameters).Output:You cand do like this:Where tail will be your file name. and  are also handy for such things:Output: .Split the url and pop the last element\n"},
{"body": "I'm writing a script to import some model objects into the database my django application uses. In the past I've solved this by running  and then . I'm sure there's a better way. I'd like to be able to call a script from anywhere on my HD using , and in the first few lines of that script it would do whatever imports / other operations necessary so that it can access model objects and behave as though it was run using .What do I need to add to my script to achieve this?Based on @Melug's answer, with addition of dynamically setting Python path to address the 'anywhere on my HD' part of the question:You need to setup django environment first:At last import your models, everything goes just like django. you should avoid using  (post by Melug) because it is deprecated. Use the following instead and you will be able to access your modelHere is an example to access and modify your model:Example model:To get models loaded too, I had to combine this with , otherwise I get As an extra, I add this to the  of my django projects, it will automatically discover the app name so it is copy/paste-able:Then I can just do:I think the best way is to create your . Then you can call  from anywhere.Here is the answer for Django versions > 1.4:Upper solutions did not work, but gave me an error:For me solution from  worked out:For Django version 1.9 or later you can use this:so you can use object as same django object:Since at least Django 1.11, your main app includes a wsgi module that does the neccessary setup on import. Assuming  is where your settings.py is, in your script just import:"},
{"body": "I have thisIs there a more pythonic (and/or shorter) way of writing this in python?Shortest one should be:Generally this might look a bit confusing, so you should only use it when it is clear what it means. Don't use it for big boolean clauses, since it begins to look ugly fast.This is:EDIT: As per request, the generalized form is:Explanation:  can be anything that evaluates to a Boolean.  It is then treated as an integer since it is used to index the tuple: , , which then selects the right item from the tuple.Well, not being a python guy please take this with a huge grain of salt, but having written (and, with more difficulty, ) a lot of clever code over the years, I find myself with a strong preference now for readable code.  I got the gist of what your original code was doing even though I'm a nobody as a Python guy.  To be sure, you could hide it and maybe impress a Python wonk or two, but why?Or you could use an inline if statement:There's a bit of a writeup on that feature at , and the relevant PEP is .  The inline if statement was introduced in Python 2.5.This one might be a little less pythonic, but you can use and/or in this fashion:This one is used more often in lambda statements than on a line by itself, but the formis similar toI was going to write out a little bit longer explanation, but they covered it better .  They also noted a couple caveats that you probably need to know.Another possibility is to use a dict if you can compute the values outside of the function that accesses them (i.e. the values are static, which also addresses the evaluation issue in scrible's answer's comments).I prefer this and/or the tuple indexing solutions under the general rubric of preferring computation to testing.You can use,but if you are using a python version prior to 2.5,can do the trick also."},
{"body": "I need to write these four s in Python. Notice what it does, is changing between four possible states in a loop:  and back to first.Can anyone suggest me a better/nicer way to write this?When in doubt, apply maths. ;)Magnus' suggestion is undeniably the right answer to your question as posed, but  you want to use a dictionary for problems like this:Even in this case I  argue using a dictionary is better, since it's clear that there are exactly four states and that they repeat, but it's hard to resist the sheer beauty of By the way, the code in your question has a bug in it, and, assuming that the values you test for are the only possible values, is equivalent to:The bug is that you need  for the second and subsequent conditions, otherwise you're continuing to test  and  after changing them. If they're  and , then  and they end up the same at the end! If they start out as  and  then the second and all subsequent conditions will be true, and you again end up with . And so on...Just extending Magnus answer. If you imagine [dx, dy] as a vector, what you're actually doing is a  of 90 degrees (or PI/2).To calculate this, you can use the following transformation:Which in your case translate to:Since  and  we simplify it to:And there you have it!The values you're working with appear to be a unit vector that continuously rotates - in other words, a . , so:Assuming my interpretation of the meaning of the dx,dy values is correct, this gives you extra flexibility in case it turns out later that you want to rotate by some other amount in each step.While I would go with Magnus' answer, here's yet another approach for rotating over a set of values:Note that there should be a  somewhere in the  loop or else it will never end."},
{"body": " I assume this is a bug in the backend? I am writing some code for processing a fairly large amount of data automatically. The code first of all parses my data files and stores all of the relevant bits. I then have different functions for producing each of the graphs I need (there are about 25 in all). However, I keep running into some kind of memory error and I think it is because Matplotlib / PyPlot are not releasing the memory correctly. Each plotting function ends with a pyplot.close(fig) command and since I just want to save the graphs and not look at them immediately they do  include a pyplot.show(). If I run the plotting functions individually in an interpreter then I don't get any problems. However, if I make a separate function which calls each plotting function in turn then I run into a \"MemoryError: Could not allocate memory for path\". Has anyone came across a problem like this? It would seem to be related to  but pyplot.close() doesn't fix my problem. This is what a typical plot function looks like in my code:If I now run in a terminalThen I don't get any errors. The same is true for all of my plot functions. If I make a new function which does this:Then it runs through about half of the graphs and then I get \"MemoryError: Could not allocate memory for path\".I think the reason it is doing this is because as it goes through all of the different graphs it then runs out of memory probably because it isn't releasing it properly.Why don't you try creating about 3 or so programs each of which do a few graphs instead of one program doing all the graphs:Program 1: Graphs 1-8Program 2: Graphs 9-16Program 3: Graphs 17-25Hope this helps  @FakeDIY : )I run into a very similar problem once. I assume matplotlib keeps references for each plots internally. Given the following code, creating three separate figures:This is counter intuitive, because we redefined  and  several times.\nWith every block, we created a new figure, which can be referenced via . Creating another figure changes the topmost references accessible by . But there must be some internal reference, which allows  to show all figures. Those references seem to be persistent and thus the figures won't be collected by the gc.The workaround I settled with, was changing the data of the plot. In hindsight it was a better approach anyway:Only drawback is you have to keep the Window with the figure open. And without the  the program will just run through"},
{"body": "I had my fair chance of getting through the python management of modules, and every time is a challenge: packaging is not what people do every day, and it becomes a burden to learn, and a burden to remember, even when you actually do it, since this happens normally once.I would like to collect here the definitive overview of how import, package management and distribution works in python, so that this question becomes the definitive explanation for all the magic that happens under the hood. Although I understand the broad level of the question, these things are so intertwined that any focused answer will not solve the main problem: understand how all works, what is outdated, what is current, what are just alternatives for the same task, what are the quirks.The list of keywords to refer to is the following, but this is just a sample out of the bunch. There's a lot more and you are welcome to add additional details. Linking to other answers is probably a good idea. As I said, this question is for the high-level overview.For the most part, this is an attempt to look at the packaging/distribution side, not the mechanics of .  Unfortunately, packaging is the place where Python provides way more than one way to do it.  I'm just trying to get the ball rolling, hopefully others will help fill what I miss or point out mistakes.  First of all there's some messy terminology here.  A directory containing an  file is a package.  However, most of what we're talking about here are specific versions of packages published on PyPI, one of it's mirrors, or in a vendor specific package management system like Debian's Apt, Redhat's Yum, Fink, Macports, Homebrew, or ActiveState's pypm.These published packages are what folks are trying to call \"Distributions\" going forward in an attempt to use \"Package\" only as the Python language construct.  You can see some of that usage in  .  Now, your list of keywords relate to several different aspects of the Python Ecosystem:The above are all services that provide a place to publish Python distributions in various formats.  Some, like PyPI mirrors and apt / yum repositories can be run on your local machine or within your companies network but folks typically use the official ones.  Most, if not all provide a tool (or multiple tools in the case of PyPI) to help find and download distributions.Distutils is the standard infrastructure on which Python packages are compiled and built into distributions.  There's a ton of functionality in  but most folks just know:And to some extent that's a most of what you need.  With the prior 9 lines of code you have enough information to install a pure Python package and also the minimal metadata required to publish that package a distribution on PyPI.Setuptools provides the hooks necessary to support the Egg format and all of it's features and foibles.  Distribute is an alternative to Setuptools that adds some features while trying to be mostly backwards compatible.  I believe Distribute is going to be included in Python 3 as the successor to Distutil's .Both Setuptools and Distribute provide a custom version of the  setup command\nthat does useful things like support the Egg format.Distributions are typically provided either as source archives (tarball or zipfile).  The standard way to install a source distribution is by downloading and uncompressing the archive and then running the  file inside.For example, the following will download, build, and install the Pygments syntax highlighting library:Alternatively you can download the Egg file and install it.  Typically this is accomplished by using easy_install or pip: were inspired by Java's Jarfiles and they have quite a few features you should read about A normal python package is just a directory containing an  file and an arbitrary number of additional modules or sub-packages.  Python also has support for finding and loading source code within *.zip files as long as they are included on the  ().The above tools are used to help automate and manage dependencies for a Python project.  Basically they give you tools to describe what distributions your application requires and automate the installation of those specific versions of your dependencies.By default, installing a python distribution is going to drop it into the site-packages directory.  That directory is usually something like .A simple programmatic way to find your site-packages directory:Python's import statement will only find packages that are located in one of the directories included in your .You can inspect and change your path from within Python by accessing:Besides that, you can set the  environment variable like you would any other environment variable on your OS or you could use:To add the above  to site-packages with a *.pth file you'd create a *.pth file.  The name of the file doesn't matter but you should still probably choose something sensible.Let's create :That's it.  Drop that into  on your system or any other directory in your  and  will be added to the path.For packaging question, this should help For import, the old article from Fredrik Lundh  still a very good starting point.I recommend  There's a chapter dedicated to packaging and distribution.I don't think  needs to be explored (Python's namespacing and importing functionality is intuitive IMHO).I use  exclusively now. I haven't run into any issues with it.However, the topic of packaging and distribution is something worth exploring. Instead of giving a lengthy answer, I will say this:I learned how to package and distribute my own \"packages\" by simply copying how Pylons or many other open-source packages do it. I then combined that sort-of template with reading up of the docs to flesh it out even further and have come up with a solid distribution method.When you grok package management and distribution for python (distutils and pypi) it's actually quite powerful. I like it a lot.[edit]I also wanted to add in a bit about virtualenv. USE IT. I create a virtualenv for every project and I always use ; I install all the packages I need for that particular project (even if it's something common amongst them all, like ) inside the virtualev. It keeps everything isolated and it's much easier for me to maintain the grouping in my head (rather than trying to keep track of what's where and for which version of python!)[/edit]"},
{"body": "I'm relatively new to the world of TensorFlow, and pretty perplexed by how you'd  read CSV data into a usable example/label tensors in TensorFlow. The example from the  is pretty fragmented and only gets you part of the way to being able to train on CSV data.Here's my code that I've pieced together, based off that CSV tutorial:And here is an brief example from the CSV file I'm loading - pretty basic data - 4 feature columns, and 1 label column:All the code above does is , which, while nice, is pretty darn useless for training.What I'm struggling with here is how you'd actually turn those individual examples, loaded one-by-one, into a training dataset. For example,  I was working on in the Udacity Deep Learning course. I basically want to take the CSV data I'm loading, and plop it into something like  and :I've tried using , like this, but it just inexplicably hangs:So to sum up, here are my questions:\nAs soon as Yaroslav pointed out that I was likely mixing up imperative and graph-construction parts here, it started to become clearer. I was able to pull together the following code, which I think is closer to what would typically done when training a model from CSV (excluding any model training code):I think you are mixing up imperative and graph-construction parts here. The operation  creates a new queue node, and a single node can be used to process the entire dataset. So I think you are hanging because you created a bunch of  queues in your for loop and didn't start queue runners for them. Normal input pipeline usage looks like this:--- end of graph construction, beginning of imperative programming --To be more scalable (to avoid Python GIL), you could generate all of your data using TensorFlow pipeline. However, if performance is not critical, you can hook up a numpy array to an input pipeline by using  Here's an example with some  nodes to see what's going on (messages in  go to stdout when node is run)You should see something like thisThe \"8, 9\" numbers didn't fill up the full batch, so they didn't get produced. Also  are printed to sys.stdout, so they show up in separately in Terminal for me.PS: a minimal of connecting  to a manually initialized queue is in Also, for debugging purposes you might want to set  on your session so that your IPython notebook doesn't hang on empty queue dequeues. I use this helper function for my sessionsScalability Notes:Or you could try this, the code loads the Iris dataset into tensorflow using pandas and numpy and a simple one neuron output is printed in the session. Hope it helps for a basic understanding.... [ I havent added the way of one hot decoding labels]."},
{"body": "I know that there is a  but is there, currently or planned, a Microsoft Visual C++ Compiler for Python 3.4 or eve Microsoft Visual C++ Compiler for Python 3.x for that matter?  It would be supremely beneficial if I didn't have to install a different version of visual studio on my entire lab.Unfortunately to be able to use the extension modules provided by others you'll be forced to use the official compiler to compile Python.  These are:Alternatively, you can use MinGw to compile extensions in a way that won't depend on others.See:  or This allows you to have one compiler to build your extensions for both versions of Python, Python 2.x and Python 3.x. suffices to build extensions for Python 3.5. It's free but a 6 GB download (overkill). On my computer it installed vcvarsall at  For Python 3.4 you'd need Visual Studio 2010. I don't think there's any free edition. See For the different python versions:Source: Also refer: "},
{"body": "I am new to celery.I know how to install and run one server but I need to distribute the task to multiple machines.\nMy project uses celery to assign user requests passing to a web framework to different machines and then returns the result.\nI read the documentation but there it doesn't mention how to set up multiple machines.\nWhat am I missing?My understanding is that your app will push requests into a queueing system (e.g. rabbitMQ) and then you can start any number of workers on different machines (with access to the same code as the app which submitted the task). They will pick out tasks from the message queue and then get to work on them. Once they're done, they will update the tombstone database. The upshot of this is that you don't have to do anything special to start multiple workers. Just start them on separate identical (same source tree) machines. The server which has the message queue need not be the same as the one with the workers and needn't be the same as the machines which submit jobs. You just need to put the location of the message queue in your  and all the workers on all the machines can pick up jobs from the queue to perform tasks. The way I deployed it is like this:"},
{"body": "Could any one suggest good Python-related podcasts out there, it could be anything about Python or its eco-system (like django, pylons, etc). (several languages there)\n\n\nI didn't think much of Python411 - the episode I downloaded primarily consisted of the host talking about how he was planning on writing a GAE site. as pointed out by Geo is (or possibly was) a good Python podcast. Obviously, as it's focused on Django development there is a lot of Djangoisms discussed however there's also a lot of general Python knowledge shared as well. TWID is currently going through a revamp, keep an eye on  for updates.  (read the blog, listen to the podcast, it is very good) has some Python material, for instance:There has been a few IronPython-related shows on some of the .NET podcasts:"},
{"body": "Greetings,I'm trying to write a program in Python which would print a string every time it gets a tap in the microphone. When I say 'tap', I mean a loud sudden noise or something similar.I searched in SO and found this post: I think PyAudio library would fit my needs, but I'm not quite sure how to make my program wait for an audio signal (realtime microphone monitoring), and when I got one how to process it (do I need to use Fourier Transform like it was instructed in the above post)?Thank you in advance for any help you could give me.If you are using LINUX, you can use . \nFor windows, we have  and there is also a library called .I found an example for Linux :If you want a \"tap\" then I think you are interested in amplitude more than frequency. So Fourier transforms probably aren't useful for your particular goal. You probably want to make a running measurement of the short-term (say 10 ms) amplitude of the input, and detect when it suddenly increases by a certain delta. You would need to tune the parameters of:Although I said you're not interested in frequency, you might want to do some filtering first, to filter out especially low and high frequency components. That might help you avoid some \"false positives\". You could do that with an FIR or IIR digital filter; Fourier isn't necessary."},
{"body": "I get an error message when I use this expression:I checked the regex at  and it returns  as expected.  But when I try it in Python I get this error message:Can someone please explain?It seems to be a python bug (that works perfectly in vim).\nThe source of the problem is the (\\s*...)+ bit. Basically , you can't do  which make sense , because you are trying to repeat something which can be null.However  should not be null,  but we know it only because we know what's in \\1. Apparently python doesn't ... that's weird.That is a Python bug between \"*\" and special characters.Instead ofTry:It works, however does not make the same regular expression.This bug seems to have been fixed between 2.7.5 and 2.7.6."},
{"body": "I realize this is a bit of a lazyweb question, but I wanted to see which python library for Twitter people have had good experiences with.I've used  and like its brevity and beauty of interface, but it doesn't seem to be one of the popular ones - it's not even listed on the . There are, however, plenty of others listed:My requirements are fairly simple:Twisted one aside (I'm not using twisted in this case), have you used any of the others, and if so, do you recommend them?[Update] FWIW, I ended up going with  again. The new version supported OAuth nicely, and it's a very clever API, so I stuck to it. should cover the first four requirements. I've used it before, and it's fairly easy to start developing with it. For leveraging Twitter's streaming API, I would recommend . It's a fantastic Python module that grabs tweets in real-time as they are posted. Based on whether you have gardenhose/firehose access to the twitter stream, you'll only get a small fraction of tweets posted. With tweetstream, you can also provide a list of search predicates to filter specific tweets that you are looking for. I used it for a project that involved mining tweets over an 8 hour period and it worked flawlessly. Both of these modules should be available through Python's .EDIT: I don't know what you intend on doing with Python/Twitter but if you do plan on capturing a lot of tweets, keep in mind that Twitter receives myriad tweets in languages besides English. Remember to encode everything properly. Full disclosure: I'm the author of Twython.As such, I'd recommend using mine. It supports OAuth now, and ships with a skeleton Django application to get you up and running in ~5 minutes.It can handle everything you're looking for, sans the Streaming API - I'm of the opinion that something like that should be implemented on a case-by-case basis, as it's generally a fairly custom setup. There's been very little demand for library support for it, either, so I've a hard time dedicating cycles to supporting it.I've used tweepy for playing around and thought it was pretty easy and fun to use. Didn't really look that much into the alternatives however so take my opinion with suitable amount of salt :). "},
{"body": "I am currently working on a project in python, and I would like to make use of the GPU for some calculations. At first glance it seems like there are many tools available; at second glance, I feel like im missing something.Copperhead looks awesome but has not yet been released. It would appear that im limited to writing low-level CUDA or openCL kernels; no thrust, no cudpp. If id like to have something sorted, im going to have to do it myself.That doesnt seem quite right to me. Am I indeed missing something? Or is this GPU-scripting not quite living up to the hype yet?Edit: GPULIB seems like it might be what I need. Documentation is rudimentary, and the python bindings are mentioned only in passing, but im applying for a download link right now. Anyone has experience with that, or links to similar free-for-academic-use GPU libraries? ReEdit: ok, python bindings are infact nonexistant.Edit2: So I guess my best bet is to write something in C/CUDA and call that from python? provides very good integration with CUDA and has several helper interfaces to make writing CUDA code easier than in the straight C api.   is an example from the Wiki which does a 2D FFT without needing any C code at all.I know that this thread is old, but I think I can bring some relevant information that answers to the question asked.Continuum Analytics has a package that contains libraries that resolves the CUDA computing for you. Basically you instrument your code that needs to be parallelized (within a function) with a decorator and you need to import a library. Thus, you don't need any knowledge about CUDA instructions. Information can be found on NVIDIA pageor  you can go directly to the Continuum Analytics' pageThere is a 30 day trial period and a free licence for academics.I use this extensively and accelerates my code between 10 to 50 times.I will publish here some information that I read on redit. It will be useful for people who are coming without a clear idea of what different packages do and how they connect cuda with Python:From:\nThere's a lot of confusion in this thread about what various projects aim to do and how ready they are. There is no \"GPU backend for NumPy\" (much less for any of SciPy's functionality). There are a few ways to write CUDA code inside of Python and some GPU array-like objects which support subsets of NumPy's ndarray methods (but not the rest of NumPy, like linalg, fft, etc..) looks like it might be what you're looking for.  From what I understand, it is very capable of doing some heavy mathematical lifting with the GPU and appears to be actively maintained.Good luck!Have you taken a look at PyGPU? I can recommend  . but for that  you need to download  full version(free for students.) . Another is  .\nIf you are looking for something better and ready to pay for that,you can also take a look at .Write now I am using scikits and quite satisfy so far."},
{"body": "I didn't find a good comparison of jinja2 and Mako. What would you use for what tasks ? I personnaly was satisfied by mako (in a pylons web app context) but am curious to know if jinja2 has some nice features/improvements that mako doesn't ? -or maybe downsides ?-I personally prefer Jinja2's syntax over Mako's. Take this example from the One of the removed construct here is the : . It was quite unfair to place it there since you should actually pass variable inside the context. But Mako still allows you to use plain python code within templates. It shouldn't be used often but when you really need it. It's there just to make your life easier. If you need something in one template but not anywhere else and Passing it to the context is overkill. That's what you really need.There are so many constructs here that I would have to consult the documentation before I could even begin. Which tags begin like  and close with ? Which of those are allowed to close with ? Why is there yet another way to enter the template language when I want to output a variable ()? What's with this  XML where some directives close like tags and have attributes?This is the equivalent example in Jinja2:Jinja2 has filters, which I'm told Mako also has but I've not seen them. Filter functions don't act like regular functions, they take an implicit first parameter of the value being filtered. Thus in Mako you might write:That's horrible. In Jinja2 you would write:In my opinion, the Jinja2 examples are exceedingly more readable. Jinja2's more regular, in that tags begin and end in a predictable way, either with  for processing and control directives, or  for outputting variables.But these are all personal preferences. I don't know of one more substantial reason to pick Jinja2 over Mako or vice-versa. And Pylons is great enough that you can use either! included Jinja2 macros. Although contrived in any case, in my opinion the Jinja2 example is easier to read and understand. Mako's guiding philosophy is \"Python is a great scripting language. Don't reinvent the wheel...your templates can handle it!\" But Jinja2's macros (the entire language, actually) look more like Python that Mako does!Take a look at  example:It is optimized for  (more  and ), well  and ."},
{"body": "If I have  how can I build a  out of the ? The only argument  seems to take is the factory function, will I have to initialize and then go through the original  and update the ?:Or given a dictionary : "},
{"body": "I currently use Cython to link C and Python, and get speedup in slow bits of python code. However, I'd like to use goroutines to implement a really slow (and very parallelizable) bit of code, but it must be callable from python. (I've already seen )I'm (sort of) happy to go via C (or Cython) to set up data structures etc if necessary, but avoiding this extra layer would be good from a bug fix/avoidance point of view.What is the simplest way to do this without having to reinvent any wheels?: possible as of Go 1.5 Unfortunately, this is not currently possible. Go can run C code (and that C code can then call back into Go), but the , so the Go runtime can set things up. There is a  package precisely to help you write Python extensions in Go:I've written an extension to setuptools which allows you to write cpython extensions that interface with go: There's a couple example extensions here:The neat thing is these can be installed just like any other  package and support both cpython and pypy. manylinux1 wheels can also be built to provide pre-built wheels via the  tool.The approach is nearly identical to the one in @ColonelPanic's answer but uses some additional tricks to enable python2 + python3 compatibility."},
{"body": "Defining a parameterless exception :When raised, is there any difference between :andWhen trying, I could find none - Is it simply an overloaded syntax?The short answer is that both  and  do the same thing.  This first form auto instantiates your exception.The  says, \" evaluates the first expression as the exception object. It must be either a subclass or an instance of BaseException. If it is a class, the exception instance will be obtained when needed by instantiating the class with no arguments.\"That said, even though the semantics are the same,  the first form is microscopically faster, and the second form is more flexible (because you can pass it arguments if needed).The usual style that most people use in Python (i.e. in the standard library, in popular applications, and in many books) is to use  when there are no arguments.  People only instantiate the exception directly when there some arguments need to be passed.  For example:  .Go look at the raise keyword in the docs \nIt is creating an instance of myException"},
{"body": "I'm teaching a python class on Object Oriented Programming and as I'm brushing up on how to explain Classes, I saw an empty class definition:the example then goes on to define a name and other attributes for an object of this class:interesting!I'm wondering if there's a way to dynamically define a function for a class like this? something like:this doesn't work in my python interpreter but is there another way of doing it?A class is more or less a fancy wrapper for a  of attributes to objects. When you instantiate a class you can assign to its attributes, and those will be stored in ; likewise, you can look in  for any attributes you have already written.This means you can do some neat dynamic things like:as well as assigning to a particular instance. (EDIT: added  parameter)Try with :The you'll be able to do:: Thanks  for the note - this works on  and not for Python2, where  appeared to be . But this will work for s, without  (right? Sorry, I know only  (: )You could use AttrDictInstall attrdict from PyPI: It's useful in other situations too - like when you need attribute access to dict keys.You could also use \"named tuples\" from the  standard module. Named tuples work like \"ordinary\" tuples but the elements have names and you can access the elements using the \"dot syntax\". From the :"},
{"body": "I'm trying to write a python class which uses a decorator function that needs information of the instance state. This is working as intended, but if I explicitly make the decorator a staticmetod, I get the following error:Why?Here is the code:And if I just remove the line , everything works, but I do not understand why. Shouldn't it need  as a first argument?This is not how  is supposed to be used.   objects are  that return the wrapped object, so they only work when accessed as .  ExampleprintsInside the scope of , you would always get the latter object, which is not callable.I'd strongly recommend to move the dcorator to the module scope -- it does not seem to belong inside the class.  If you want to keep it inside the class, don't make it a , but rather simply  it at the end of the class body -- it's not meant to be used from outside the class in this case.Python classes are created at runtime, after evaluating the contents of the class declaration. The class is evaluated by assigned all declared variables and functions to a special dictionary and using that dictionary to call  (see ).So,is equivalent to:When you annotate a method with @staticmethod, there is some special magic that happens AFTER the class is created with . Inside class declaration scope, the @staticmethod function is just an instance of a staticmethod object, which you can't call. The decorator probably should just be declared above the class definition in the same module OR in a separate \"decorate\" module (depends on how many decorators you have). In general decorators should be declared outside of a class. One notable exception is the property class (see ).  In your case having the decorator inside a class declaration might make sense if you had something like a color class: is returning a  method that isn't decorated by You can return a non-static method to a static_method"},
{"body": "I'm writing a small Python script which will periodically pull information from a 3rd party service using a username and password combo. I don't need to create something that is 100% bulletproof (does 100% even exist?), but I would like to involve a good measure of security so at the very least it would take a long time for someone to break it. This script won't have a GUI and will be run periodically by , so entering a password each time it's run to decrypt things won't really work, and I'll have to store the username and password in either an encrypted file or encrypted in a SQLite database, which would be preferable as I'll be using SQLite anyway, and I  need to edit the password at some point. In addition, I'll probably be wrapping the whole program in an EXE, as it's exclusively for Windows at this point. How can I securely store the username and password combo to be used periodically via a  job?I recommend a strategy similar to .  If you can't use ssh-agent directly you could implement something like it, so that your password is only kept in RAM.  The cron job could have configured credentials to get the actual password from the agent each time it runs, use it once, and de-reference it immediately using the  statement.The administrator still has to enter the password to start ssh-agent, at boot-time or whatever, but this is a reasonable compromise that avoids having a plain-text password stored anywhere on disk.After looking though the answers to this and related questions, I've put together some code using a few of the suggested methods for encrypting and obscuring secret data. This code is specifically for when the script has to run without user intervention (if the user starts it manually, it's best to have them put in the password and only keep it in memory as the answer to this question suggests). This method isn't super-secure; fundamentally, the script can access the secret info so anyone who has full system access has the script and its associated files and can access them. What this does do id obscures the data from casual inspection and leaves the data files themselves secure if they are examined individually, or together without the script.My motivation for this is a project that polls some of my bank accounts to monitor transactions - I need it to run in the background without me re-entering passwords every minute or two.Just paste this code at the top of your script, change the saltSeed and then use store() retrieve() and require() in your code as needed:The security of this method would be significantly improved if os permissions were set on the secret files to only allow the script itself to read them, and if the script itself was compiled and marked as executable only (not readable). Some of that could be automated, but I haven't bothered. It would probably require setting up a user for the script and running the script as that user (and setting ownership of the script's files to that user).I'd love any suggestions, criticisms or other points of vulnerability that anyone can think of. I'm pretty new to writing crypto code so what I've done could almost certainly be improved.I'm currently researching this topic and the following seems to be the best solution for my needs...maybe it will work for others coming across this question. integrates with the  API on Windows which encrypts data using the users logon credentials.Usage is very simple:Items are encrypted with the users Windows credentials, thus other applications running in your user account would be able to access the password.  To obscure that vulnerability a bit you could encrypt/obfuscate the password in some manner before storing it on the keyring.  Of course, anyone who was targeting your script would just be able to look at the source and figure out how to unencrypt/unobfuscate the password, but you'd at least prevent some application vacuuming up all passwords in the vault and getting yours as well.I think the best you can do is protect the script file and system it's running on.Basically do the following:There's not much point trying to encrypt the password: the person you're trying to hide it from has the Python script, which will have the code to decrypt it.  The fastest way to get the password will be to add a print statement to the Python script just before it uses the password with the third-party service.So store the password as a string in the script, and base64 encode it so that just reading the file isn't enough, then call it a day.operating systems often have support for securing data for the user.  in the case of windows it looks like it's you can call win32 apis from python using as far as i understand, this will store the data so that it can be accessed only from the account used to store it.  if you want to edit the data you can do so by writing code to extract, change and save the value.I used  because I had troubles installing (compiling) other commonly mentioned libraries on my system. (Win7 x64, Python 3.5)My script is running in a physically secure system/room. I encrypt credentials with an \"encrypter script\" to a config file. And then decrypt when I need to use them.\n\"Encrypter script\" is not on the real system, only encrypted config file is. Someone who analyses the code can easily break the encryption by analysing the code, but you can still compile it into an EXE if necessary."},
{"body": "What's the difference between  and  in Python?which yieldsandwhich yieldsThe difference is that when you use , the  is set and the message states that the exception was . If you omit the  then no  is set, but the  may be set as well, and the traceback then shows the context as .Setting the  happens if you used  in an exception handler; if you used  anywhere else no  is set either.If a  is set, a  flag is also set on the exception; when  is set to , the  is ignored when printing a traceback.When raising from a exception handler where you  want to show the context (don't want a  message), then use  to set  to .In other words, Python sets a  on exceptions so you can introspect where an exception was raised, letting you see if another exception was replaced by it. You can also add a  to an exception, making the traceback explicit about the other exception (use different wording), and the context is ignored (but can still be introspected when debugging). Using  lets you suppress the context being printed.See the :Also see the  for details on the context and cause information attached to exceptions."},
{"body": "Since  and  are instances of , the following is valid in Python:I understand why this happens. However, I find this behaviour a bit unexpected and can lead to hard-to-debug bugs. It has certainly bitten me a couple of times.Can anyone think of a legit use of indexing lists with  or ?In the past, some people have used this behaviour to produce a poor-man's :However, with a  having been added to the language in Python 2.5, this is very much frowned upon, for the reasons you state: relying on booleans being a subclass of integers is too 'magical' and unreadable for a maintainer.So, unless you are code-golfing ( producing very compact and obscure code), useinstead, which has the added advantage that the two expressions this selects between are  evaluated; if  is false, the expression before the  is never executed.If you are puzzled why  is a valid index argument: this is simply for  with the fact that  is a subclass of  and in Python it  a numerical type. If you are asking why  is a numerical type in the first place then you have to understand that  wasn't present in old releases of Python and people used s instead.I will add a bit of historic arguments. First of all the addition of  in python is shortly described in Guido van Rossum (aka BDFL) blogpost: . The type was added via .The PEP contains the  rationales used for this decisions. I'll quote some of the portions of the PEP below.:There are often better ways, but Boolean indices do have their uses.  I've used them when I want to convert a boolean result to something more human readable:"},
{"body": "I have a class that I'm testing which has as a dependency another class (an instance of which gets passed to the CUT's init method).  I want to mock out this class using the Python Mock library.What I have is something like:Which is fine, but \"methodfromdepclass\" is a parameterized method, and as such I want to create a single mock object where depending on what arguments are passed to methodfromdepclass it returns different values.  The reason I want this parameterized behaviour is I want to create multiple instances of ClassUnderTest that contain different values (the values of which are produced by what gets returned from the mockobj).Kinda what I'm thinking (this of course does not work):How do I achieve this \"ifcalledwith\" kind of semantics?Try A little sweeter:or for multiple arguments:or with a default value:or a combination of both:and merrily on high we go.I've ran into this when I was doing my own testing.  If you don't care about capturing calls to your methodfromdepclass() but just need it to return something, then the following may suffice:Here's a parameterized version:"},
{"body": "I have one field in a pandas DataFrame that was imported as string format. \nIt should be a datetime variable.\nHow do I convert it to a datetime column and then filter based on date.Example:Use the  function, specifying a  to match your data.You can use the DataFrame method  to operate on the values in Mycol:works, however it results in a Python warning of \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using  insteadI would guess this is due to some chaining indexing."},
{"body": "I have the following codeI need to set the value of  to  if  is ; and do nothing otherwise. So, here is my code for thatIs there someway I could avoid the  part to make it look cleaner? An equivalent to I tried replacing it with  like this: . All I got was syntax error. Nor I could just omit the  part.ThanksI don't think this is possible in Python, since what you're actually trying to do probably gets expanded to something like this:If you exclude , you'll receive a syntax error since I'm quite sure that the assignment must actually return something.As others have already mentioned, you could do this, but it's bad because you'll probably just end up confusing yourself when reading that piece of code the next time:I'm not a big fan of the  for the exact same reason. I have to actually think twice on what that line is doing.The best way to actually achieve what you want to do is the original version:The reason that's the best verison is because it's very obvious what you want to do, and you won't confuse yourself, or whoever else is going to come in contact with that code later.Also, as a side note,  is valid Ruby code, because Ruby works a bit differently.Use this:In one line:But don\u2019t do that. This style is normally not expected. People prefer the longer form for clarity and consistency.(Equally, camel caps should be avoided. So rather use .)Note that an in-line   without an  part does not exist because there would not be a return value if the predicate were false. However, expressions must have a clearly defined return value in all cases. This is different from usage as in, say, Ruby or Perl.No. I guess you were hoping that something like  would work, but it doesn't. I think the best way is with the  statement as you have written it:you can use one of the following:"},
{"body": "I have a few Pandas DataFrames sharing the same value scale, but having different columns and indices. When invoking , I get separate plot images. what I really want is to have them all in the same plot as subplots, but I'm unfortunately failing to come up with a solution to how and would highly appreciate some help. You can manually create the subplots with matplotlib, and then plot the dataframes on a specific subplot using the  keyword. For example for 4 subplots (2x2):Here  is an array which holds the different subplot axes, and you can access one just by indexing .\nIf you want a shared x-axis, then you can provide  to .You can see e.gs. in the  demonstrating joris answer. Also from the documentation, you could also set  and  within the pandas  function:  You could also use  which takes subplot grid parameters such as 221, 222, 223, 224, etc. as described in the post . Nice examples of plot on pandas data frame, including subplots, can be seen in .You can use the familiar Matplotlib style calling a  and , but you simply need to specify the current axis using . An example:etc...Building on @joris response above, if you have already established a reference to the subplot, you can use the reference as well. For example, "},
{"body": "The above python code gives the output quite different from expected. I want to loop over items so that I can skip an item while looping.Please explain.I've been bitten before by (someone else's) \"clever\" code that tries to modify a list while iterating over it.  I resolved that I would never do it under any circumstance.You can use the slice operator  to skip across to every third item in your list.Other points about my example relate to new syntax in . Never alter the container you're looping on, because iterators on that container are not going to be informed of your alterations and, as you've noticed, that's quite likely to produce a very different loop and/or an incorrect one.  In normal cases, looping on a copy of the container helps, but in your case it's clear that you  want that, as the container will be empty after 50 legs of the loop and if you then try popping again you'll get an exception.What's anything BUT clear is, what behavior are you trying to achieve, if any?!  Maybe you can express your desires with a ...?The general rule of thumb is that you don't modify a collection/array/list while iterating over it. Use a secondary list to store the items you want to act upon and execute that logic in a loop after your initial loop.Try this. It avoids mutating a thing you're iterating across, which is generally a code smell.See .Use a while loop that checks for the truthfulness of the array:And it should do it without any errors or funny behaviour.I guess this is what you want:It is quite handy to code when the number of item to be popped is a run time decision.\nBut it runs with very a bad efficiency and the code is hard to maintain.This slice syntax makes a copy of the list and does what you want:"},
{"body": "How can I get a  (degree) character into a string?Put this line at the top of your sourceIf your editor uses a different encoding, substitute for utf-8  Then you can include utf-8 characters directly in the sourceThis is the most coder-friendly version of specifying a unicode character:Note: must be a capital N in the  construct to avoid confusion with the '\\n' newline character. The character name inside the curly braces can be any case.It's easier to remember the  of a character than its unicode index. It's also more readable, ergo debugging-friendly. The character substitution happens at compile time: the  file will contain a constant for :BTW, all I did was search \"unicode degree\" on Google. This brings up two results:\n\"Degree sign U+00B0\" and \"Degree Celsius U+2103\", which are actually different:Above answers assume that UTF8 encoding can safely be used - this one is specifically targetted for Windows.The Windows console normaly uses CP850 encoding and  utf-8, so if you try to use a source file utf8-encoded, you get those 2 (incorrect) characters  instead of a degree .Demonstration (using python 2.7 in a windows console):effectively outputs .Fix: just force the correct encoding (or better use unicode):or if you use a source file that explicitely defines an encoding:just use  python will convert it automatically"},
{"body": "I'm using something like this in my templateWhen I view the POST data in Firebug or the Django debug, I see it only sends one value.  Am I doing something wrong or misunderstanding a concept?Just FYI, I had to use:because omitting the [] caused a blank list to be returned instead of the correct values. I'm using jQuery to fetch the values of a multiple select element, and jQuery appears to be adding the [] Watch out! getlist method from QueryDict returns an empty list if the key doesn't exist. It does not throw an exception. "},
{"body": "I've just read in  that \"tuples are faster than lists\".Tuple is immutable, and list is mutable, but I don't quite understand why tuple is faster.Anyone did a performance test on this?The reported \"speed of construction\" ratio only holds for  tuples (ones whose items are expressed by literals).  Observe carefully (and repeat on your machine -- you just need to type the commands at a shell/command window!)...:I didn't do the measurements on 3.0 because of course I don't have it around -- it's totally obsolete and there is absolutely no reason to keep it around, since 3.1 is superior to it in every way (Python 2.7, if you can upgrade to it, measures as being almost 20% faster than 2.6 in each task -- and 2.6, as you see, is faster than 3.1 -- so, if you care seriously about performance, Python 2.7 is really the only release you should be going for!).Anyway, the key point here is that, in each Python release, building a list out of constant literals is about the same speed, or slightly slower, than building it out of values referenced by variables; but tuples behave very differently -- building a tuple out of constant literals is typically three times as fast as building it out of values referenced by variables!  You may wonder how this can be, right?-)Answer: a tuple made out of constant literals can easily be identified by the Python compiler as being one, immutable constant literal itself: so it's essentially built just once, when the compiler turns the source into bytecodes, and stashed away in the \"constants table\" of the relevant function or module.  When those bytecodes execute, they just need to recover the pre-built constant tuple -- hey presto!-)This easy optimization cannot be applied to lists, because a list is a mutable object, so it's crucial that, if the same expression such as  executes twice (in a loop -- the  module makes the loop on your behalf;-), a fresh new list object is constructed anew each time -- and that construction (like the construction of a tuple when the compiler cannot trivially identify it as a compile-time constant and immutable object) does take a little while.That being said, tuple construction (when both constructions actually have to\noccur) still is about twice as fast as list construction -- and  discrepancy can be explained by the tuple's sheer simplicity, which other answers have mentioned repeatedly.  But, that simplicity does not account for a speedup of six times or more, as you observe if you only compare the construction of lists and tuples with simple constant literals as their items!_)With the power of the  module, you can often resolve performance related questions yourself:This shows that tuple is negligibly faster than list for iteration. I get similar results for indexing, but for construction, tuple destroys list:So if speed of iteration or indexing are the only factors, there's effectively no difference, but for construction, tuples win.Alex gave a great answer, but I'm going to try to expand on a few things I think worth mentioning. Any performance differences are generally small and implementation specific: so don't bet the farm on them.In CPython, tuples are stored in a single block of memory, so creating a new tuple involves at worst a single call to allocate memory. Lists are allocated in two blocks: the fixed one with all the Python object information and a variable sized block for the data. That's part of the reason why creating a tuple is faster, but it probably also explains the slight difference in indexing speed as there is one fewer pointer to follow.There are also optimisations in CPython to reduce memory allocations: de-allocated list objects are saved on a free list so they can be reused, but allocating a non-empty list still requires a memory allocation for the data. Tuples are saved on 20 free lists for different sized tuples so allocating a small tuple will often not require any memory allocation calls at all.Optimisations like this are helpful in practice, but they may also make it risky to depend too much on the results of 'timeit' and of course are completely different if you move to something like IronPython where memory allocation works quite differently.Essentially because tuple's immutability means that the interpreter can use a leaner, faster data structure for it, compared to list.One area where a list is notably faster is construction from a generator, and in particular, list comprehensions are much faster than the closest tuple equivalent,  with a generator argument:Note in particular that  seems to be a tiny bit faster than , but  is much faster than both of them."},
{"body": "I feel like I should know this, but I haven't been able to figure it out...I want to get the name of a method--which happens to be an integration test--from inside it so it can print out some diagnostic text.  I can, of course, just hard-code the method's name in the string, but I'd like to make the test a little more DRY if possible.The answers involving introspection via  and the like are reasonable.  But there may be another option, depending on your situation:If your integration test is written with the  module, then you could use  within your TestCase.This seems to be the simplest way using module :You could generalise this with:Credit to  which was found via google.This decorator makes the name of the method available inside the function by passing it as a keyword argument.You would use it this way:But maybe you'd want to write what you want directly inside the decorator itself. Then the code is an example of a way to get the function name in a decorator. If you give more details about what you want to do in the function, that requires the name, maybe I can suggest something else.Output:Use \"back = 1\" to find info regarding two levels back down the stack, etc.I think the  module might have what you're looking for. In particular, the  function looks like it will do the job."},
{"body": "So that I am able to work with it within my python scripts?Installing Boto depends on the Operating system. \nFor e.g in Ubuntu you can use the aptitude command:Or you can download the boto code from their site and move into the unzipped directory to run Edit: pip is now by far the preferred way to install packagesswitch to the  directory and type .Best way to install boto in my opinion is to use:This ensures you'll have the boto glacier code. If you already have boto installed in one python version and then install a higher python version, boto is not found by the new version of python. For example, I had python2.7 and then installed python3.5 (keeping both). My script under python3.5 could not find boto. Doing \"\" told me that boto was already installed in .So I didThis allowed my script under python3.5 to find boto.While trying out the command, I encounter the error To resolve this, issue another command to handle the setuptools using curlAfter doing that, the following command will work perfectly."},
{"body": "I basically want a python equivalent of this in Cbut in python I declare an array likebut the problem is I want to assign random slots with values likebut I can't do that with python, since the array is emptyIf by \"array\" you actually mean a Python list, you can useorYou can't do exactly what you want in Python (if I read you correctly).  You need to put values in for each element of the list (or as you called it, array).But, try this:For lists of other types, use something besides 0.   is often a good choice as well.You can use numpy:import numpy as npExample from :also you can extend that with extend method of list.Just declare the list and append each element. For ex:"},
{"body": "I just need a python script that copies text to the clipboard.After the script gets executed i need the output of the text to be pasted to another source.\nIs it possible to write a python script that does this job?See .  Example (taken from Pyperclip site):Also, see .  But it appears to have more dependencies.On mac i use this function.It will copy \"hello world\" to the clipboard. Use Tkinter: (Original author: ) seems to be up to the task.GTK3:One more answer to improve on:\n\nand  (Tkinter).Tkinter is nice, because it's either included with Python (Windows) or easy to install (Linux), and thus requires little dependencies for the end user.Here I have a \"full-blown\" example, which copies the arguments or the standard input, to clipboard, and - when not on Windows - waits for the user to close the application:This showcases:I try this clipboard 0.0.4 and it works well.This is an altered version of @Martin Thoma's answer for . I found that the original solution resulted in the process never ending and my terminal hung when I called the script. Changing the script to the following resolved the issue for me.You will probably want to change what clipboardText gets assigned to, in this script it is assigned to the parameter that the script is called with.To use native python directories, use:Then use:to call the function.(also please accept an answer)PyQt5:This is the only way that worked for me using  plus it's the easiest to implement w/ using the standard  suiteShout out to  for the answer (I copied it completely) from I wrote a little wrapper for it that I put in my  profile <3 "},
{"body": "How can I use the logging class in python to write to a file? Every time I try to use it, it just prints out the message.An example of using  rather than  Taken from the \"\":And you're good to go.P.S. Make sure to read the  as well.I prefer to use a configuration file. It allows me to switch logging levels, locations, etc without changing code when I go from development to release. I simply package a different config file with the same name, and with the same defined loggers.Here is my code for the log config file"},
{"body": "What's the best way to rename photos with a unique filename on the server as they are uploaded, using django? I want to make sure each name is used only once. Are there any pinax apps that can do this, perhaps with GUID?Use uuid. To tie that into your model see  for  FileField upload_to.For example in your models.py define the following function:Then, when defining your FileField/ImageField, specify  as the  value.Prior to Django 1.6.6, 1.5.9, and 1.4.14, the  function would automatically give files a unique name by adding an underscore. So, for example, if you save one file \"test.jpg\" and then another file, \"test.jpg\" to your server, the first will be called test.jpg, and the second will be called test_1.jpg.Alas, that turns out to be a vector for DDOSing a machine, by sending it thousands of zero-byte files to store, each one checking thousands of previous files to see what its name should be.As you'll , the new system appends seven random digits after the underscore to fix this problem.A better way could be using a common class in your helpers.py. This way you could reuse the random file generator across your apps.In your helpers.py:And then in your model just import the helper class:And then use it:Ref: How about concatenating the filename with the date / time the photo was uploaded and then using  to create a message digest? That should give you unique filenames.Alternatively you could re-use  which creates unique filenames and then use the full path to that file as the input to your hash call. That gives you unique constant length strings which you can map to your files.As of the writing of this answer it seems like you no longer need to do anything special to make this happen. If you set up a FileField with a static upload_to property, the Django storage system will automatically manage naming so that if a duplicate filename is uploaded, Django will randomly generate a new unique filename for the duplicate.Works on Django 1.10."},
{"body": "I'm having a hard time trying to use .strip with the following line of code. Thanks for the help.You can use the strip() to remove trailing and leading spaces.Note: the internal spaces are preservedExpand your one liner into multiple lines. Then it becomes easy:"},
{"body": "The  function in python, puts the leading characters  in front of the number. Is there anyway to tell it NOT to put them? So  will be .The code isUse this code:it allows you to specify a number of digits too:For Python 2.6 useorYou can simply writeto get the first two characters removed.Old style string formatting:New styleUsing capital letters as format characters yields uppercase hexadecimal are here."},
{"body": "I've searched up and down, but can't find a de-compiler that will work for Python 2.7 .pyc. Does anybody know of one that will work for Python 2.7? ThanksThis sounds like it works: Issue 8 says it supports 2.7: UPDATE (2013-09-03) - As noted in the comments and in other answers, you should look at  or  instead of unpyc.In case anyone is still struggling with this, as I was all morning today, I have found a solution that works for me:Installation instructions:Once the program is installed (note: it will be installed to your system-wide-accessible Python packages, so it should be in your ), you can recover your Python files like so:The decompiler adds some noise mostly in the form of comments, however I've found it to be surprisingly clean and faithful to my original code. You will have to remove a little line of text beginning with +++ near the end of the recovered file to be able to run your code.Decompyle++ (pycdc) appears to work for a range of python versions:  For example:Here is a great tool to decompile pyc files.It was coded by me and supports python 1.0 - 3.3Its based on uncompyle2 and decompyle++Ned Batchelder has posted a  that will unmarshal a .pyc file and disassemble any code objects within, so you'll be able to see the Python bytecode. \nIt looks like with newer versions of Python, you'll need to comment out the lines that set  and print it (but don't comment the line that sets ).Turning that back into Python source would be somewhat more difficult, although theoretically possible. I assume all these programs that work for older versions of Python do that."},
{"body": "All I need is the contents inside the parenthesis.If your problem is really just this simple, you don't need regex:Use :If you want to find all occurences: Building on tkerwin's answer, if you happen to have nested parentheses like in his answer will not work if you need to take everything between the  and the  to get , because find searches from the left of the string, and would stop at the first closing parenthesis.To fix that, you need to use  for the second part of the operation, so it would become "},
{"body": "At work we used to program our Python in a pretty standard OO way.  Lately, a couple guys got on the functional bandwagon. And their code now contains lots more lambdas, maps and reduces.  I understand that functional languages are good for concurrency but does programming Python functionally really help with concurrency?   I am just trying to understand what I get if I start using more of Python's functional features.: I've been taken to task in the comments (in part, it seems, by fanatics of FP in Python, but not exclusively) for not providing more explanations/examples, so, expanding the answer to supply some., even more so  (and ),  most especially , are hardly ever the right tool for the job in Python, which is a strongly multi-paradigm language. main advantage (?) compared to the normal  statement is that it makes an  function, while  gives the function a name -- and for that very dubious advantage you pay an enormous price (the function's body is limited to one expression, the resulting function object is not pickleable, the very lack of a name sometimes makes it much harder to understand a stack trace or otherwise debug a problem -- need I go on?!-).Consider what's probably the single most idiotic idiom you sometimes see used in \"Python\" (Python with \"scare quotes\", because it's obviously  idiomatic Python -- it's a bad transliteration from idiomatic Scheme or the like, just like the more frequent overuse of OOP in Python is a bad transliteration from Java or the like):by assigning the lambda to a name, this approach immediately throws away the above-mentioned \"advantage\" -- and doesn't lose any of the DISadvantages!  For example,  doesn't  its name --  is the useless string  -- good luck understanding a stack trace with a few of these;-).  The proper Python way to achieve the desired semantics in this simple case is, of course:  is the string , as it clearly should be, and the object is pickleable -- the semantics are otherwise identical (in this simple case where the desired functionality fits comfortably in a simple expression --  also makes it trivially easy to refactor if you need to temporarily or permanently insert statements such as  or , of course). is (part of) an expression while  is (part of) a statement -- that's the one bit of syntax sugar that makes people use  sometimes. Many FP enthusiasts (just as many OOP and procedural fans) dislike Python's reasonably strong distinction between expressions and statements (part of a general stance towards ). Me, I think that when you use a language you're best off using it \"with the grain\" -- the way it was  to be used -- rather than fighting against it; so I program Python in a Pythonic way, Scheme in a Schematic (;-) way, Fortran in a Fortesque (?) way, and so on:-).Moving on to  -- one comment claims that  is the best way to compute the product of a list. Oh, really?  Let's see...:so the simple, elementary, trivial loop is about twice as fast (as well as more concise) than the \"best way\" to perform the task?-)  I guess the advantages of speed and conciseness must therefore make the trivial loop the \"bestest\" way, right?-)By further sacrificing compactness and readability...:...we can get  back to the easily obtained performance of the simplest and most obvious, compact, and readable approach (the simple, elementary, trivial loop). This points out another problem with , actually: performance! For sufficiently simple operations, such as multiplication, the overhead of a function call is quite significant compared to the actual operation being performed --  (and  and ) often forces you to insert such a function call where simple loops, list comprehensions, and generator expressions, allow the readability, compactness, and speed of in-line operations.Perhaps even worse than the above-berated \"assign a lambda to a name\" anti-idiom is actually the following anti-idiom, e.g. to sort a list of strings by their lengths:instead of the obvious, readable, compact, speedierHere, the use of  is doing nothing but inserting a level of indirection -- with no good effect whatsoever, and plenty of bad ones.The motivation for using  is often to allow the use of  and  instead of a vastly preferable loop or list comprehension that would let you do plain, normal computations in line; you still pay that \"level of indirection\", of course. It's not Pythonic to have to wonder \"should I use a listcomp or a map here\": just always use listcomps, when both appear applicable and you don't know which one to choose, on the basis of \"there should be one, and preferably only one, obvious way to do something\". You'll often write listcomps that could not be sensibly translated to a map (nested loops,  clauses, etc), while there's no call to  that can't be sensibly rewritten as a listcomp.Perfectly proper functional approaches in Python often include list comprehensions, generator expressions, , higher-order functions, first-order functions in various guises, closures, generators (and occasionally other kinds of iterators)., as a commenter pointed out, does include  and : the difference is that, like all of itertools, these are stream-based (like  and  builtins in Python 3, but differently from those builtins in Python 2).   offers a set of building blocks that compose well with each other, and splendid performance: especially if you find yourself potentially dealing with very long (or even unbounded!-) sequences, you owe it to yourself to become familiar with itertools -- their whole  in the docs makes for good reading, and the  in particular are quite instructive.Writing your own higher-order functions is often useful, especially when they're suitable for use as  (both function decorators, as explained in that part of the docs, and class decorators, introduced in Python 2.6).  Do remember to use  on your function decorators (to keep the metadata of the function getting wrapped)!So, summarizing...: anything you can code with , , and , you can code (more often than not advantageously) with  (named functions) and listcomps -- and usually moving up one notch to generators, generator expressions, or , is even better.   meets the legal definition of \"attractive nuisance\"...: it's  the right tool for the job (that's why it's not a built-in any more in Python 3, at long last!-).FP is important not only for concurrency; in fact, there's virtually no concurrency in the canonical Python implementation (maybe 3.x changes that?).  in any case, FP lends itself well to concurrency because it leads to programs with no or fewer (explicit) states.  states are troublesome for a few reasons.  one is that they make distributing the computation hard(er) (that's the concurrency argument), another, far more important in most cases, is the tendency to inflict bugs.  the biggest source of bugs in contemporary software is  (there's a close relationship between variables and states).  FP may reduce the number of variables in a program: bugs squashed!see how many bugs can you introduce by mixing the variables up in these versions:versus (warning, 's parameter list differs from that of python's ; rationale given later)as you can see, it's a matter of fact that FP gives you fewer opportunities to shoot yourself in the foot with a variables-related bug.also, readability: it may take a bit of training, but  is way easier to read than : you see  (\"ok, it's reducing a sequence to a single value\"),  (\"by multiplication\").  wherease  has the generic form of a  cycle, peppered with variables and assignments.  these  cycles all look the same, so to get an idea of what's going on in , you need to read almost all of it.then there's succintness and flexibility.  you give me  and I tell you I like it, but want something to sum sequences as well.  no problem, you say, and off you go, copy-pasting:what can you do to reduce the duplication?  well,  operators were values, you could do something likeoh wait!  provides operators that  values!  but.. Alex Martelli condemned  already...  looks like if you want to stay within the boundaries he suggests, you're doomed to copy-pasting plumbing code. is the FP version any better?  surely you'd need to copy-paste as well?well, that's just an artifact of the half-assed approach!  abandoning the imperative , you can contract both versions toor even( is the reason for )what about runtime speed?  yes, using FP in a language like Python will incur some overhead.  here i'll just parrot what a few professors have to say about this:I'm not very good at explaining things.  Don't let me muddy the water too much, read the first half of the  John Backus gave on the occasion of receiving the Turing Award in 1977.  Quote:I program in Python everyday, and I have to say that too much 'bandwagoning' toward OO or functional could lead toward missing elegant solutions. I believe that both paradigms have their advantages to certain problems - and I think that's when you know what approach to use. Use a functional approach when it leaves you with a clean, readable, and efficient solution. Same goes for OO.And that's one of the reasons I love Python - the fact that it is multi-paradigm and lets the developer choose how to solve his/her problem.This answer is completely re-worked.  It incorporates a lot of observations from the other answers.As you can see, there is a lot of strong feelings surrounding the use of functional programming constructs in Python.  There are three major groups of ideas here.First, almost everybody but the people who are most wedded to the purest expression of the functional paradigm agree that list and generator comprehensions are better and clearer than using  or .    Your colleagues should be avoiding the use of  and  if you are targeting a version of Python new enough to support list comprehensions.  And you should be avoiding  and  if your version of Python is new enough for generator comprehensions.Secondly, there is a lot of ambivalence in the community as a whole about .  A lot of people are really annoyed by a syntax in addition to  for declaring functions, especially one that involves a keyword like  that has a rather strange name.  And people are also annoyed that these small anonymous functions are missing any of the nice meta-data that describes any other kind of function.  It makes debugging harder.  Lastly the small functions declared by  are often not terribly efficient as they require the overhead of a Python function call each time they are invoked, which is frequently in an inner loop.Lastly, most (meaning > 50%, but most likely not 90%) people think that  is a little strange and obscure.  I myself admit to having  whenever I want to use it, which isn't all that often.  Though when I see it used, the nature of the arguments (i.e. function, list or iterator, scalar) speak for themselves.As for myself, I fall in the camp of people who think the functional style is often very useful.  But balancing that thought is the fact that Python is not at heart a functional language.  And overuse of functional constructs can make programs seem strangely contorted and difficult for people to understand.To understand when and where the functional style is very helpful and improves readability, consider this function in C++:This loop seems very simple and easy to understand.  And in this case it is.  But its seeming simplicity is a trap for the unwary.  Consider this alternate means of writing the loop:Suddenly, the loop control variable no longer varies in an obvious way.  You are reduced to looking through the code and reasoning carefully about what happens with the loop control variable.  Now this example is a bit pathological, but there are real-world examples that are not.  And the problem is with the fact that the idea is repeated assignment to an existing variable.  You can't trust the variable's value is the same throughout the entire body of the loop.This is a long recognized problem, and in Python writing a loop like this is fairly unnatural.  You have to use a while loop, and it just looks wrong.  Instead, in Python you would write something like this:As you can see, the way you talk about the loop control variable in Python is not amenable to fooling with it inside the loop.  This eliminates a lot of the problems with 'clever' loops in other imperative languages.  Unfortunately, it's an idea that's semi-borrowed from functional languages.Even this lends itself to strange fiddling.  For example, this loop:Oops, we again have a loop that is difficult to understand.  It superficially resembles a really simple and obvious loop, and you have to read it carefully to realize that one of the variables used in the loop's computation is being messed with in a way that will effect future runs of the loop.Again, a more functional approach to the rescue:Now by looking at the code we have some strong indication (partly by the fact that the person is using this functional style) that the lists a and b are not modified during the execution of the loop.  One less thing to think about.The last thing to be worried about is c being modified in strange ways.  Perhaps it is a global variable and is being modified by some roundabout function call.  To rescue us from this mental worry, here is a purely function approach:Very concise, and the structure tells us that x is purely an accumulator.  It is a local variable everywhere it appear.  The final result is unambiguously assigned to c.  Now there is much less to worry about.  The structure of the code removes several classes of possible error.That is why people might choose a functional style.  It is concise and clear, at least if you understand what  and  do.  There are large classes of problems that could afflict a program written in a more imperative style that you know won't afflict your functional style program.In the case of factorial, there is a very simple and clear way to write this function in Python in a functional style:The question, which seems to be mostly ignored here: No.  The value FP brings to concurrency is in eliminating state in computation, which is ultimately responsible for the hard-to-grasp nastiness of unintended errors in concurrent computation.  But it depends on the concurrent programming idioms not themselves being stateful, something that doesn't apply to Twisted.  If there are concurrency idioms for Python that leverage stateless programming, I don't know of them.Here's a short summary of positive answers when/why to program functionally.than to use an imperative construct (loop).You should NOT overuse those features - there are many traps, see Alex Martelli's post. I'd subjectively say the most serious danger is that excessive use of those features will destroy readability of your code, which is a core attribute of Python.The standard functions filter(), map() and reduce() are used for various operations on a list and all of the three functions expect two arguments: A function and a listWe could define a separate function and use it as an argument to filter() etc., and its probably a good idea if that function is used several times, or if the function is too complex to be written in a single line. However, if it's needed only once and it's quite simple, it's more convenient to use a lambda construct to generate a (temporary) anonymous function and pass it to filter().This helps in Using these function, would also turn out to be , because the looping on the elements of the list is done in C, which is a little bit faster than looping in python.And object oriented way is forcibly needed when states are to be maintained, apart from abstraction, grouping, etc., If the requirement is pretty simple, I would stick with functional than to Object Oriented programming.Map and Filter have their place in OO programming.  Right next to list comprehensions and generator functions.Reduce less so.  The algorithm for reduce can rapidly suck down more time than it deserves; with a tiny bit of thinking, a manually-written reduce-loop will be more efficient than a reduce which applies a poorly-thought-out looping function to a sequence. Lambda never.  Lambda is useless.  One can make the argument that it actually  something, so it's not completely .   First: Lambda is  not syntactic \"sugar\"; it makes things bigger and uglier.  Second: the one time in 10,000 lines of code that think you need an \"anonymous\" function turns into two times in 20,000 lines of code, which removes the value of anonymity, making it into a maintenance liability.However.The functional style of no-object-state-change programming is still OO in nature.  You just do more object creation and fewer object updates.  Once you start using generator functions, much OO programming drifts in a functional direction.Each state change appears to translate into a generator function that builds a new object in the new state from old object(s).  It's an interesting world view because reasoning about the algorithm is much, much simpler.But that's no call to use reduce or lambda."},
{"body": "How do I find the previous Monday's date, based off of the current date using Python? I thought maybe I could use:  to do it, but I am getting stuck.I basically want to find today's date and Mondays date to construct a date range query in django using: .  Take todays date. Subtract the number of days which already passed this week (this gets you 'last' monday). Add one week.: The above is for 'next monday', but since you were looking for 'last monday' you could useChristopheD's post is close to what you want. I don't have enough rep to make a comment :(Instead of (which actually gives you the next upcoming monday):I would say:If you want the previous week, add the 'weeks=1' parameter.This makes the code more readable since you are subtracting a timedelta. This clears up any confusion caused by adding a timedelta that has negative and positive offsets.I think the easiest way is using  like this:Note: The OP says in the comments, \"\". I take this to mean we are looking for the last Monday that occurred strictly before today. The calculation is a little difficult to get right using only the  module (especially given the above interpretation of \"past Monday\" and if you wish to avoid clunky ). For example, if  is a Monday such as ,returns , which is the same day as  (not the past Monday).The advantage of using the  is that you don't have to do tricky mental calculations nor force the reader to do the same to get the right date.  does it all for you:Note that  is needed to guarantee that  is a different day than .You can use . I tried  and . Comparing these three, I think  is the best one.To get your result, use like this:Github Link : Try it, It can do more!Using timedeltas and datetime module:gives you today's day of the week, counting 0 (monday) to 6 (sunday)(7-d)%7 gives you days until Monday, or leaves you where you are if today is Monday"},
{"body": "This works almost fine but the number starts with 0 sometimes:I've found a lot of examples but none of them guarantee that the sequence won't start with .We generate the first digit in the 1 - 9 range, then take the next 3 from the remaining digits:The generated numbers are equiprobable, and we get a valid number in one step.Just loop until you have something you like:This is very similar to the other answers but instead of  or  you could draw a random integer in the range 1000-9999 until you get one that contains only unique digits:As @Claudio pointed out in the comments the range actually only needs to be 1023 - 9876 because the values outside that range contain duplicate digits.Generally  will be much faster than  or  so even if it's more likely one needs to draw multiple times (as pointed out by @karakfa) it's up to 3 times faster than any ,  approach that also needs to  the single digits.I do not know Python well, but something likeA more useful iteration, actually creating a number:After stealing pieces from other solutions, plus applying the tip from @DavidHammen:[Fixed] Shift all four digits on one position is not right. Swap leading zero with fixed position is not right too. But random swap of the leading zero with any of nine positions is correct and gives equal probability:rejection sampling method.  Create a 4 digit random combination from 10 digits and resample if it doesn't match the criteria.noticed that this is essentially the same as @Austin Haskings's You could use full range for 3 numbers, then choose the leading number among the remaining numbers:Another way if the choice needs to be repeated (and if you remain reasonable on the number of digits), is to pre-compute the list of the possible outputs using , filtering out the ones with a leading zero, and building a list of integers from it:That's some computation time, but after than you can call:as many times you want. It's very fast and provides an evenly distributed random.I don't know Python so I will post a pseudo-code-ish solution for this specific problem:It is possible to generalize this idea a little. For example you can create a function that accepts a list (of digits) and a number (desired length of result); the function will return the number and mutate the list by removing used-up digits. Below is a JavaScript implementation of this solution:Here's how I'd do itMore generally, given a generator, you can use the built-ins  and  to take the first element that satisfies some test function.A Pythonic way to write would be to use 2 nested generators and :It's basically a one-liner variant of If you need many random numbers, you could invest some time and memory for preprocessing all the acceptable numbers: and  are used as boundaries because no int lower than 1023 or greater than 9876 can have 4 unique, distince numbers.Then, you'd just need  for a very fast generation:Disclaimer: this is a terrible anti-Python approach, strictly for the benchmarking part (see @DavidHammen's comments around, and )\nThe idea is to generate the sequence numbers of the digits in one step, and then fix any collisions:Now we have d1=0..8, d2=0..8, d3=0..7, d4=0..6, it can be tested via running the snippet with rnd=4535 (4535=9*9*8*7-1, by the way)First, d1 has to be patched upThen d2 has to \"skip\" d1 if necessaryThen the same has to be done with the remaining digits, getting ugly fast:And the final part is the catastrophic one:For longer numbers, it might work faster with bitfields, but I do not see a trivial way.\n(Checking the >= relations once is not enough, because the collision can easily occur after doing an incrementation.\ne.g. d1=1, d2=2, d3=1: d3 collides with d1, but it does not collide with d2 initially. However after \"puching the hole\" at 1, d3 becomes 2 and now it collides with d2. There is no trivial way to spot this collision in advance)As the code stinks as hell, I put a verification step at the endIt is already faster than the other really fast code (the commented verification displayed the original digits preserved after the divmod-s, for debugging purposes. This is not the kind of code which works immediately...). Commenting both verifications makes it even faster.EDIT: about checking this and thatThis is an approach maintaining an 1:1 relation between the minimal set of valid inputs (0...4535) and valid outputs (the 9*9*8*7 possible 4-digit numbers with distinct digits, not-starting-with-0). So a simple loop can and should generate all the numbers, they can be checked one-by-one and they can be collected into a set for example in order to see if they are all distinct resultsPractically:1) It will not print anything in the loop (all results are 4-digit numbers with distinct digits)2) It will print 4536 at the end (all results are distinct)One can add a verification for the first digit (d1), here and now I just assume that\"(something mod 9)+1\" will not be 0.This will allow zeros after the first digit - "},
{"body": "In the function , if a referred variable is not found then it gives an error. How can I check to see if a variable or method exists as part of an object?How about  function before ?It's easier to ask forgiveness than to ask permission.Don't check to see if a method exists.  Don't waste a single line of code on \"checking\"Check if class has such method?orYou can use  instead of You can try using 'inspect' module:How about looking it up in ?Maybe like this, assuming all method is callableIf your method is outside of a class and you don't want to run it and raise an exception if it doesn't exist:I think you should look at the  package. It allows you to 'wrap' some of the things. When you use the  method it also list built in methods, inherited methods and all other attributes making collisions possible, e.g.:The array you get from  contains both  and  and a lot of built in stuff. With  you can do this:This examples still list both methods in the two classes but if you want to limit the inspection to only function in a specific class it requires a bit more work, but it is absolutely possible."},
{"body": "I have a Python script that works perfectly fine on my development PC. Both are Windows\u00a07 with the same Python version (2.7.9). However on the target machine I get a seems to come from pywin32 module.The code uses a third-party library invoked by pywin32:and then fails later on:Now I'm lost why this is working on my PC and not on the target machine. Both have a \"corporate install\" of Windows\u00a07, for example, the same Regional and date-time settings.What is the issue? How might I resolve it?EDIT:See comments. The cause is probably which  runtime is used. I'm still investigating. I now suspect that it matters which runtimes are present at install time of pywin32. Why? Because DependenyWalker on my development PC says that pywin depends on  in my Lotus Notes installation. This tells me it sure isn't \"hard\" linked.I was all wrong...The issue now also happens on my PC.Some further info. The script reads data files and inserts the read\nmeta-data into a database. Only older files seemed to be affected by the\nbug, not new ones (I now think this is assumption is wrong). So the idea was to to the initial load on my Dev PC and then hope the issue will never occur again with new files.In case of the PC were the script will run, the files it reads are on a\nWindows Shared drive (mapped network drive). I don't have access to that\ndrive so I just copied the files into my PC. Now for doing the initial\nload I requested access to said network drive and BOOM. It also does not\nwork from my Dev. machine when reading from the shared drive.The issue does not always happen with the same file. I now think it has nothing to do with a specific file. I also tried it on a 64-bit PC with 64-bit python. There it took longer till the error occurred. In fact a file was successfully read which failed on my PC. I now think it is some kind of memory issue? I believe that it then always fails on the date line because all other lines just return null or an empty string which causes no problem and is entirely possible such a value can be null. But for the date it is a problem and it should not be null and then the error is thrown.EDIT of Update:On my PC it always fails on the same file. Loading that file alone works perfectly fine. I now think it's some kind of counter/number overflow that after reading n files, the issue occurs. It has to do with the amount of files I load per run of the script and not the file itself. Files that fail work when loading them individually.Turns out the issue was in fact trivial and somewhat due to my lack of experience with python and misleading error message. The COM object  is used to open proprietary files in a loop. To solve the issue one must \"clean-up\" the object before next iteration. This is done either by  \nor \n. That completely solves the issue. It has absolutely nothing to do with dates or date formating.  So Peter Brittain was probably right that this file limit was reached."},
{"body": "I had installed Pymacs, rope, ropemode, ropemacs, and when I executed by accident, I couldn't save modified buffers. It first asked me - . If I answered \"yes\", it threw - . If I answered \"no\", it threw:I managed to work around by executing , loading  module, and answering yes to Pymacs helper restart question. The buffer was saved, but then I started to get another error everytime I saved the file:This is my init-file: describes death of Pymacs helper. It tells that I shouldn't close  buffer, because this kills the helper, and should also restart Emacs if helper is killed. This is unacceptable as I have a habit of closing all buffers from time to time and also rarely restart Emacs. I have several related questions now:The easiest solution I can think of is to use  hook to prevent  to be killed.  Like this:It will ask you if you really want to kill  buffer or not.  You can even make it impossible to kill from keybinds by this:I use  to forcefully reload all modules. I have a function similar to  in .Probably you can add  to  (locally in  buffer) for more graceful termination.  But I am not sure.  For the rest of your question, I guess it's better to ask/request in the Pymacs .If you accidentally kill the *Pymacs* buffer or execute  you can recover the process by executing the following command and answering \"yes\" at the prompt.You can modify your init-file function to allow for the restart to be called interactively with . Restarting Pymacs in this manner will avoid the  error."},
{"body": "I'm trying to work out how best to locate the centroid of an arbitrary shape draped over a unit sphere, with the input being ordered (clockwise or anti-cw) vertices for the shape boundary. The density of vertices is irregular along the boundary, so the arc-lengths between them are not generally equal. Because the shapes may be very large (half a hemisphere) it is generally not possible to simply project the vertices to a plane and use planar methods, as detailed on Wikipedia (sorry I'm not allowed more than 2 hyperlinks as a newcomer). A slightly better approach involves the use of planar geometry manipulated in spherical coordinates, but again, with large polygons this method fails, as nicely illustrated . On that same page, 'Cffk' highlighted  which describes a method for calculating the centroid of spherical triangles. I've tried to implement this method, but without success, and I'm hoping someone can spot the problem?I have kept the variable definitions similar to those in the paper to make it easier to compare. The input (data) is a list of longitude/latitude coordinates, converted to [x,y,z] coordinates by the code. For each of the triangles I have arbitrarily fixed one point to be the +z-pole, the other two vertices being composed of a pair of neighboring points along the polygon boundary. The code steps along the boundary (starting at an arbitrary point), using each boundary segment of the polygon as a triangle side in turn. A sub-centroid is determined for each of these individual spherical triangles and they are weighted according to triangle area and added to calculate the total polygon centroid. I don't get any errors when running the code, but the total centroids returned are clearly wrong (I have run some very basic shapes where the centroid location is unambiguous). I haven't found any sensible pattern in the location of the centroids returned...so at the moment I'm not sure what is going wrong, either in the math or code (although, the suspicion is the math).The code below should work copy-paste as is if you would like to try it. If you have matplotlib and numpy installed, it will plot the results (it will ignore plotting if you don't). You just have to put the longitude/latitude data below the code into a text file called example.txt.Thanks in advance for any suggestions or insight.EDIT: Here is a figure that shows a projection of the unit sphere with a polygon and the resulting centroid I calculate from the code. Clearly, the centroid is wrong as the polygon is rather small and convex but yet the centroid falls outside its perimeter.\nEDIT: Here is a highly-similar set of coordinates to those above, but in the original [lon,lat] format I normally use (which is now converted to [x,y,z] by the updated code).EDIT: A couple more examples...with 4 vertices defining a perfect square centered at [1,0,0] I get the expected result:\n\nHowever, from a non-symmetric triangle I get a centroid that is nowhere close...the centroid actually falls on the far side of the sphere (here projected onto the front side as the antipode):\n\nInterestingly, the centroid estimation appears 'stable' in the sense that if I invert the list (go from clockwise to counterclockwise order or vice-versa) the centroid correspondingly inverts exactly.I think this will do it. You should be able to reproduce this result by just copy-pasting the code below.Legend:Here is the longitude and latitude data you can paste into I think a good approximation would be to compute the center of mass using weighted cartesian coordinates and projecting the result onto the sphere (supposing the origin of coordinates is ). Let be  the n points of the polygon. The approximative (cartesian) centroid can be computed by:whereas  is the sum of all weights and whereas  is a polygon point and  is a weight for that point, e.g. whereas  is the length of a vector . \nI.e. a point is weighted with half the length to the previous and half the length to the next polygon point.This centroid  can now projected onto the sphere by:whereas  is the radius of the sphere. To consider orientation of polygon  the result may beTo clarify: the quantity of interest is the projection of the true 3d centroid\n(i.e. 3d center-of-mass, i.e. 3d center-of-area) onto the unit sphere.Since all you care about is the direction from the origin to the 3d centroid,\nyou don't need to bother with areas at all;\nit's easier to just compute the moment (i.e. 3d centroid times area).\nThe moment of the region to the left of a closed path on the unit sphere\nis half the integral of the leftward unit vector as you walk around the path.\nThis follows from a non-obvious application of Stokes' theorem; see  Problem 13-12.In particular, for a spherical polygon, the moment is the half the sum of\n(a x b) / ||a x b|| * (angle between a and b) for each pair of consecutive vertices a,b.\n(That's for the region to the  of the path;\nnegate it for the region to the  of the path.)(And if you really did want the 3d centroid, just compute the area and divide the moment by it.  Comparing areas might also be useful in choosing which of the two regions to call \"the polygon\".)Here's some code; it's really simple:For the example polygon, this gives the answer (unit vector):This is roughly the same as, but more accurate than, the answer computed by @KobeJohn's code, which uses rough tolerances and planar approximations to the sub-centroids:The directions of the two answers are roughly opposite (so I guess KobeJohn's code\ndecided to take the region to the  of the path in this case)."},
{"body": "I'm about to embark on some large Python-based App Engine projects, and I think I should check with Stack Overflow's \"wisdom of crowds\" before committing to a unit-testing strategy.  I have an existing unit-testing framework (based on  with custom runners and extensions) that I want to use, so anything \"heavy-weight\"/\"intrusive\" such as , , or   doesn't seem appropriate.  The crucial unit tests in my worldview are extremely lightweight and fast ones, ones that run in an extremely short time, so I can keep running them over and over all the time without breaking my development rhythm (e.g., for a different project, I get 97% or so coverage for a 20K-lines project with several dozens of super-fast tests that take 5-7 seconds, elapsed time, for a typical run, overall -- that's what I consider a decent suite of small, fast unit-tests).  I'll have richer/heavier tests as well of course, all the way to integration tests with selenium or windmill, that's  what I'm asking about;-) -- my focus in this question (and in most of my development endeavors;-) is on the small, lightweight unit-tests that lightly and super-rapidly cover my code, not on the deeper ones.So I think what I need is essentially a set of small, very lightweight simulations of the various key App Engine subsystems -- data store, memcache, request/response objects and calls to webapp handlers, user handling, mail, &c, roughly in this order of priority.  I haven't found exactly what I'm looking for, so it seems to me that I should either rely on , as I've done often in the past, which basically means mocking each subsystem used in a given test and setting up all expectations &c (strong, but lots of work each time, and very sensitive to the tested-code's internals, i.e. very \"white-box\"y), or rolling my own simulation of each subsystem (and doing asserts on the simulated subsystems' states as part of the unit tests). The latters seems feasible, given GAE's Python-side strong \"stubs\" architecture... but I can't believe I need to roll my own, i.e., that nobody's already written such simple-minded simulators!-)  E.g., for the datastore, it looks like what I need is more or less the \"datastore on file\" stub that's already part of the SDK, plus a way to mark it readonly and easy-to-use accessors for assertions about the datastore's state; and so forth, subsystem by subsystem -- each seems to need \"just a bit more\" than what's already in the SDK, \"perched on top\" of the existing \"stubs\" architecture.So, before diving in and spending a day or two of precious development time \"rolling my own\" simulations of GAE subsystems for unit testing purposes, I thought I'd double check with the SO crowd and see what y'all think of this... or, if there's already some existing open source set of such simulators that I can simply reuse (or minimally tweak!-), and which I've just failed to spot in my searching!-): to clarify, if I do roll my own, I do plan to leverage the SDK-supplied stubs where feasible; but for example there's no stub for a datastore that gets initially read in from a file but then not saved at the end, so I need to subclass and tweak the existing one (which also doesn't offer particularly convenient ways to do asserts on its state -- same for the mail service stub, etc). That's what I mean by \"rolling my own\" -- not \"rewriting from scratch\"!-): \"why not GAEUnit\" -- GAEUnit is nice for its own use cases, but running dev_appserver and seeing results in my browser (or even via urllib.urlopen) is definitely not what I'm after -- I want to use a fully automated setup, suitable for running within an existing test-running framework which is based on extending unittest, and no HTTP in the way (said framework defines a \"fast\" test as one that among other thing does no sockets and minimal disk I/O -- we simulate or mock these -- so via gaeunit I could do no better than \"medium\" tests) + no convenient way to prepopulate datastore for each test (and no OO structure to help customize things).You don't need to write your own stubs - the SDK includes them, since they're what it uses to emulate the production APIs. Not all of them are suitable for use in unit-tests, but most are. Check out  for an example of the setup/teardown code you need to make use of the built in stubs. is a nose plugin that support unittests by automatically setting up the development environment and a test datastore for you. Very useful when developing on dev_appserver.I use GAEUnit for my Google App Engine App and I am quite happy with the speed of the tests. The thing that I like about GAEUnit,and I am sure Webtest does it, is that it creates its own version for stubs of everything for testing leaving your \"live\" versions alone for testing.So your datastore that you may be using for development will be left as is when you run your GAETests.I might also add that  has been very useful in my unit tests. It lets you create models in a declarative syntax, which it converts into stored entities that you can load in your tests. This way you have the same data set at the beginning of every test case!, which saves you from having to create data by hand at the start of every test. Here is an example, from the Fixture documentation:\nGiven this model:Your fixture would look like this:Note however, that I ran into the following issues:\n1. , but the included patch does remedy it. \n2. The datastore is not -by default- reset between test cases. So I use this to force a reset for each test case:The   provides easy configuration of stub libraries for local integration tests.Since  there is the build-in .It is Java only right now but I feel like:So does the author of this framework -  and he explicitly tells us about it in his I/O presentation Does anyone have any updates on this topic?"},
{"body": "This problem sounds simple at first glance, but turns out to be a lot more complicated than it seems. It's got me stumped for the moment.There are 52c5 = 2,598,960 ways to choose 5 cards from a 52 card deck. However, since suits are interchangeable in poker, many of these are equivalent - the hand 2H 2C 3H 3S 4D is equivalent to 2D 2S 3D 3C 4H - simply swap the suits around. According to , there are 134,459 distinct 5 card hands once you account for possible suit recolorings.The question is, how do we efficiently generate all these possible hands? I don't want to generate all hands, then eliminate duplicates, as I want to apply the problem to larger numbers of cards, and the number of hands to evaluate fast spirals out of control. My current attempts have centered around either generating depth-first, and keeping track of the currently generated cards to determine what suits and ranks are valid for the next card, or breadth-first, generating all possible next cards, then removing duplicates by converting each hand to a 'canonical' version by recoloring. Here's my attempt at a breadth-first solution, in Python:Unfortunately, this generates too many hands:Can anyone suggest a better way to generate just the distinct hands, or point out where I've gone wrong in my attempt?Your overall approach is sound.  I'm pretty sure the problem lies with your  function.  You can try printing out the hands with num_cards set to 3 or 4 and look for equivalencies that you've missed.I found one, but there may be more:For reference, below is my solution (developed prior to looking at your solution).  I used a depth-first search instead of a breadth-first search.  Also, instead of writing a function to transform a hand to canonical form, I wrote a function to check if a hand is canonical.  If it's not canonical, I skip it.  I defined rank = card % 13 and suit = card / 13.  None of those differences are important.It generates the correct number of permutations:Here's a Python solution that makes use of numpy and generates the canonical deals as well as their multiplicity. I use Python's itertools module to create all 24 possible permutations of 4 suits and then to iterate over all 2,598,960 possible 5-card deals. Each deal is permuted and converted to a canonical id in just 5 lines. It's quite fast as the loop only goes through 10 iterations to cover all deals and is only needed to manage the memory requirements. All the heavy lifting is done efficiently in numpy except for the use of . It's a shame this is not supportedly directly in numpy.Your problem sounded interesting, so i simple tried to implements it by just looping over all possible hands in a sorted way. I've not looked at your code in details, but it seems my implementation is quite different from yours. Guess what count of hands my script found: 160537Are you sure, the number on wikipedia is correct?I'm not a poker player, so the details of hand precedence are beyond me.  But it seems like the problem is that you are traversing the space of \"sets of 5 cards\" by generating sets from the deck, when you should be traversing the space of \"distinct poker hands\".The space of distinct hands will require a new grammar.  The important thing is to capture exactly the information that is relevant to hand precedence.  For example, there are only 4 hands that are royal flushes, so those hands can be described as the symbol \"RF\" plus a suit designator, like \"RFC\" for royal flush in clubs.  A 10-high heart flush could be \"FLH10\" (not sure if there are other precedence characteristics of flushes, but I think that's all you need to know).  A hand that is \"2C 2S AH 10C 5D\" would be a longer expression, something like \"PR2 A 10 5\" if I undestand your initial problem statement. Once you have defined the grammar of distinct hands, you can express it as regular expressions and that will tell you how to generate the entire space of distinct hands.  Sounds like fun!You could simply give all hands a canonical ordering of values (A to K), then assign abstract suit letters according to their order of first appearance in that order.Example:  JH 4C QD 9C 3D would convert to 3a 4b 9b Jc Qa.Generation should work best as dynamic programming:Initial input:Step 1: for each rank greater than or equal the highest rank used, set all suits in that rank to 0.  you can get away with only checking higher cards because lower combinations will be checked by the lower starting points.Step 2: Collapse to distinct rowsStep 3: Climb back up determining first suit that match each distinct row, and choose the suits which match the distinct rows (identified by a *)Now showing the repeat for rank 3Step 4: Once there are 5 cells set to 1, increment the total possible suit abstracted hands count by 1 and recurse up.The total number of suit abstracted hands possible is 134,459.  This is the code I wrote to test it out:Hopefully it is broken up enough to be easily understandable.Here is a simple and straightforward algorithm for reducing hands to a canonical one based on suit permutatoins.This is what the algorithm looks like in C++, with some implied Suit and CardSet classes.  Note that the return statement converts the hand by concatenating the bitstrings.Look at . The problem gets even worse when you're considering completing hands given some cards already drawn.The guy behind PokerStove did a great job in this direction, but the source is disclosed.Generating equivalence classes for 5 card hands is not an easy task.\nWhen I need this I usually use the  webpage. At  you can choose which variety of poker game you need, and in the \"Programming tab\" you have an section on \"Unique Suit Patterns\". So just copying that and loading into program might be easier than trying to generate your own.Take a look here:These regard a 5-card hand (and a 7-card hand) as an integer, the sum the individual cards, which is independent of the suit. Exactly what you need.This is part of a scheme for quickly ranking 7- and 5-card hands, written in Objective-C and Java.If you are just interested in hands that result in different hand rankings, there are actually only 7462 distinct hand classes that have to be considered (see ).By creating a table with an example for each class and their accompanying multiplicity you can check all relevant hands weighted with their probability quite fast. That is, assuming that no cards are known and therefore fixed beforehand already.Chances are you really want to generate the number of distinct hands, in the sense of non-equivalent. In that case, according to the wikipedia article there are 7462 possible hands. Here is a python snippet that will enumerate them all. The logic is simple: there is one hand for each 5-set of ranks; in addition, if all the ranks are distinct, another, different kind of hand can be formed by making all the suits match."},
{"body": "Inspired by Why is there no list.clear() method in python? I've found several questions here that say the correct way to do it is one of the following, but no one has said why there isn't just a method for it.While it may go against the \"zen of python\" to have more than one way of doing something, it certainly seems more obvious to me to have a \"list.clear()\" method. It would also fall in line with dicts and sets, both of which have .clear().I came across a few posts to the python-dev and python-ideas concerning this and didn't come to a definitive answer (see  (2006) and  (2009)). Has Guido weighed in on it? Is it just a point of contention that hasn't been resolved yet over the last 4-5 years? list.clear() was added to python in 3.3 - In the threads you linked  of adding that method. When it comes to language design, it's really important to be conservative. See for example the  principle the C# team has. You don't get something as clean as Python by adding features willy-nilly. Just take a look at some of the more cruftier popular scripting languages to see where it takes you.I guess the  method just never did cross the implicit -100 points rule to become something worth adding to the core language. Although given that the methodname is already used for an equivalent purpose and the alternative can be hard to find, it probably isn't all that far from it.While there was no  when this question was asked, 3.3 now has one (as requested in ). Additionally,  methods have been added to  and  to ease switching between lists and other collections (set, dict etc).Full details of the change can be found .I can't answer to the why; but there absolutely should be one, so different types of objects can be cleared with the same interface.An obvious, simple example:This only requires that the object support iteration, and that it support clear().  If lists had a clear() method, this could accept a list or set equally.  Instead, since sets and lists have a different API for deleting their contents, that doesn't work; you end up with an unnecessarily ugly hack, like:As far as I'm concerned, using del obj[:] or obj[:] = [] are just unpleasant, unintuitive hacks to work around the fact that list is missing clear().This is taking \"reducing redundancy\" to a fault, where it damages the  of the language, which is even more important.As to which you should use, I'd recommend del obj[:].  I think it's easier to implement for non-list-like objects.When testing, it is often useful to setup the data forming the domain of the tests in global variables, so that tests can build on one another if necessary/simpler.\nIn such cases, having a method to clear the list would allow you to do it without the need to declare those variables as global within a function (i.e. the tests).\nI know, the tests should not depend on one another...The question should be why  was deemed necessary in the first place. The following works to clear any Python collection that I can think of."},
{"body": "I am doing something like this... automatically commits the changes. But the docs say nothing about closing the connection. Actually I can use  in later statements (which I have tested). Hence it seems that the context manager is NOT closing the connection.Do I have to manually close the connection. What if I leave it open?My conclusions...In answer to the specific question of what happens if you do not close a SQLite database, the answer is quite simple and applies to using SQLite in any programming language.  When the connection is closed explicitly by code or implicitly by program exit then any outstanding transaction is rolled back.  (The rollback is actually done by the next program to open the database.)  If there is no outstanding transaction open then nothing happens.This means you do not need to worry too much about always closing the database before process exit, and that you should pay attention to transactions making sure to start them and commit at appropriate points.You have a valid underlying concern here, however it's also important to understand how sqlite operates too:in terms of , you only need to worry about transactions and not open handles. sqlite only holds a lock on a database inside a transaction(*) or statement execution.however in terms of , e.g. if you plan to remove sqlite file or use so many connections you might run out of file descriptors, you do care about open out-of-transaction connections too.there are two ways a connection is closed: either you call  explicitly after which you still have a handle but can't use it, or you let the connection go out of scope and get garbage-collected.if you , close it explicitly, according to Python's motto \".\"if you are only checking code for side-effects, letting a last variable holding reference to connection go out of scope may be acceptable, but keep in mind that exceptions capture the stack, and thus references in that stack. if you pass exceptions around, connection lifetime may be extended arbitrarily., sqlite uses \"deferred\" transactions by default, that is the transaction only starts when you execute a statement. In the example above, transaction runs from 3 to 4, rather than from 2 to 4.Your version leaves conn in scope after connection usage.EXAMPLE:    For managing a connection to a database I usually do this, doing so, I'm sure that the connection is explicitly closed."},
{"body": "My current setup.py script works okay, but it installs tvnamer.py (the tool) as \"tvnamer.py\" into site-packages or somewhere similar..Can I make setup.py install tvnamer.py as tvnamer, and/or is there a better way of installing command-line applications?Try the  parameter in the setup() call. As described in the , this should do what I think you want.To reproduce here:"},
{"body": "i have:the output should be:This works for dictionaries of any length:And as generator-oneliner:In Python 2.7 and 3.x this can and should be written as dict comprehension (thanks, @katrielalex):In case of Python 3.3+, there is a :Also see:Note: the order of 'b' and 'c' doesn't match your output because dicts are unorderedif the dicts can have more than one key/valueFor flat dictionaries you can do this:This is asymmetrical because you need to choose what to do with duplicate keys; in this case,  will overwrite . Exchange them for the other way.EDIT: Ah, sorry, didn't see that.It is possible to do this in a single expression:No credit to me for this last!However, I'd argue that it might be more Pythonic (explicit > implicit, flat > nested ) to do this with a simple  loop. YMMV.Note I added a second key/value pair to the last dictionary to show it works with multiple entries.\nAlso keys from dicts later in the list will overwrite the same key from an earlier dict.You can use  function from  library:dic1 = {'Maria':12, 'Paco':22, 'Jose':23}\ndic2 = {'Patricia':25, 'Marcos':22 'Tomas':36}dic2 = dict(dic1.items() + dic2.items())and this will be the outcome:dic2\n{'Jose': 23, 'Marcos': 22, 'Patricia': 25, 'Tomas': 36, 'Paco': 22, 'Maria': 12}"},
{"body": "I'm working with Python and whenever I've had to validate function input, I assumed that the input worked, and then caught errors.In my case, I had a universal  class which I used for a few different things, one of which is addition. It functioned both as a  class and as a , so when I add a scalar to the , it should add that constant to each individual component.  and  addition required component-wise addition.This code is being used for a raytracer so any speed boosts are great.Here's a simplified version of my  class:I'm currently using the  method. Does anybody know of a faster method? Thanks to the answers, I tried and tested the following solution, which checks specifically for a class name before adding the  objects:I ran a speed test with these two blocks of code using , and the results were pretty significant:I haven't tested the  class with  input validation whatsoever (i.e. moving the checking out of the class and into the actual code), but I'd imagine that it's even faster than the  method.: Looking back at this code, this is  an optimal solution.OOP makes this even faster:I upvoted Matt Joiner's answer, but wanted to include some additional observations to make it clear that, along with a couple of other factors, there are  times that matter when choosing between pre-checking conditions (known as LBYL or \"Look Before You Leap\") and just handling exceptions (known as EAFP or \"Easier to Ask Forgiveness than Permission\").Those timings are:The additional factors are:That last point is the one that needs to be addressed first: if there is a potential for a race condition, then you have no choice, you  use exception handling. A classic example is:Since LBYL doesn't rule out the exception is such cases, it offers no real benefit and there's no judgement call to be made: EAFP is the only approach that will handle the race condition correctly.But if there's no race condition, either approach is potentially viable. They offer different trade-offs:That then leads to the following decision criteria:As a rough rule of thumb:*People will vary as to what they consider \"most of the time\" in this context. For me, if I expect the operation to succeed more than half the time, I would just use EAFP as a matter of course, until I had reason to suspect this piece of code was an actual performance bottleneck.In Python, exceptions are often faster due to the reduced number of lookups. However a friend once said (and it should apply to any language), pretend that everytime an exception is caught, there is a small delay. Avoid using exceptions where a delay could be a problem.In the example you've given, I'd go with the exception."},
{"body": "The docs say that calling sys.exit() raises a SystemExit exception which can be caught in outer levels.  I have a situation in which I want to definitively and unquestionably exit from inside a test case, however the unittest module catches SystemExit and prevents the exit.  This is normally great, but the specific situation I am trying to handle is one where our test framework has detected that it is configured to point to a non-test database.  In this case I want to exit and prevent any further tests from being run.  Of course since unittest traps the SystemExit and continues happily on it's way, it is thwarting me.The only option I have thought of so far is using ctypes or something similar to call   exit(3) directly but this seems like a pretty fugly hack for something that should be really simple.You can call  to directly exit, without throwing an exception:This bypasses all of the python shutdown logic, such as the atexit module, and will not run through the exception handling logic that you're trying to avoid in this situation.  The argument is the exit code that will be returned by the process.As Jerub said,  is your answer. But, considering it bypasses  cleanup procedures, including  blocks, closing files, etc, and should really be avoided at all costs, may I present a \"safer-ish\" way of using it?If you problem is  being caught at outer levels (ie, unittest), then  Wrap your main code in a try/except block, catch SystemExit, and call os._exit there,  This way you may call  normally anywhere in the code, let it bubble out to the top level, gracefully closing all files and running all cleanups, and  calling os._exit.You can even choose which exits are the \"emergency\" ones. The code below is an example of such approach:"},
{"body": "I have enum and use the variables like , , etc.  When I return one of these variables from a function, can I print their names (such as ) instead of the value they returned?Short answer: no. Long answer: this is possible with some ugly hacks using traceback, inspect and the like, but it's generally probably not recommended for production code. For example see: Perhaps you can use a workaround to translate the value back to a name/representational string. If you post some more sample code and details about what you're wanting this for maybe we can provide more in-depth assistance.There is no such thing as a unique or original variable name\nTo add to , some concepts...Python \"variables\" are simply references to values.  Each value occupies a given memory location (see ) From the above, you may notice that the value \"1\" is present at the memory location 10052552.  It is referred to 569 times in this instance of the interpreter.  Now, see that because yet another name is bound to this value, the reference count went up by one.Based on these facts, it is not realistic/possible to tell what single variable name is pointing to a value.I think the best way to address your issue is to add a mapping and function to your enum reference back to a string name.Please comment if you would like sample code.Just use the text you want to print as the value of the enum, as incomparing strings for identity is almost as efficient in Python as is comparing integer values (this is due to the fact the strings are immutable as have a hash value)Of course there are easier ways to create the enum in the first place:Then, create you enumeration like this:ans access the same way as above:There are two answers to this question:  and .It depends on an implementation of enums. For example:It doesn't matter whether there are 100 names refers to an integer whose name you'd like to find .For the example above  you can inspect '' using :In this case you even don't have to know all enum names. If enums implemented differently then you might need to use a different hack. There will be some solution in most cases e.g., see . But it doesn't matter because..You could store the canonical name as an attribute of the instance, and then assign it to a variable with the same name. This might work:Regardless, it's not a particularly \"Pythonic\" thing to do.On second thought:Since Python does not provide native Enum types, you should not ask for one, but instead use other, more powerful construct to build your program. Otherwise, the next step will invariably be \"Why does Python not have a  statement, and how do I best emulate it?\"Since Enums are often used to define some kind of state, a much better approach is this:\nCreate a base class that define all the abstract properties, attributes and methods belonging to a state. Then, for each state, derive a sub class that implements the specific behavior of this state. You can then pass around these classes (or maybe instances thereof) to handle the state and its behaviour.If you use classes instead of instances (the Python way of a \"singleton\"), you can simply check for any given state (not that it should be necessary) by  (note the  instead of ) with no performance penalty over comparing integer values.And of course, you can define a  attribute and a  method to access and print the state's name.As far as I know, that will require some introspection. You can try using the inspect module.There are a few simple things you may want to try before that:All that said, there aren't standard enumerations in Python. It would help to know how you are creating them.On second thoughts, you can maintain your variables as a dictionary in the enum, keyed by variable name and provide a method of the enumeration to find the right variable and print its name. This solution (keeping a dict) is bad because variable values aren't necessarily unique.: \nThe problem is not trivial, so you may want to use a tried and tested solution. IMHO, you would be better off avoiding the situation if you can.Erlang has a concept called \"atoms\" -- they are similar to string constants or enumerations.  Consider using a string constant as the value of your enum -- the same as the name of the enum."},
{"body": "How can I calculate matrix mean values along a matrix, but to remove  values from calculation? (For R people, think ).Here is my [non-]working example:With NaNs removed, my expected output would be:I think what you want is a masked array: Combining all of the timing dataReturns:If performance matters, you should use  instead:Assuming you've also got SciPy installed:A masked array with the nans filtered out can also be created on the fly:You can always find a workaround in something like:Numpy 2.0's  has a  option which should take care of that.This is built upon the solution suggested by JoshAdel.Define the following function:Example use:Will print out:How about using Pandas to do this:Gives:From numpy 1.8 (released 2013-10-30) onwards,  does precisely what you need:Or you use laxarray, freshly uploaded, which is among other a wrapper for masked arrays.following JoshAdel's protocoll I get:So laxarray is marginally slower (would need to check why, maybe fixable), but much easier to use and allow labelling dimensions with strings. check out: EDIT: I have checked with another module, \"la\", larry, which beats all tests:Impressive !One more speed check for all proposed approaches:So the best is 'bottleneck.nanmean(dat, axis=1)'\n'scipy.stats.nanmean(dat)' is not faster then ."},
{"body": "Cannot figure out, where to change EOF in PyCharm.\nMy scripts, started with:Outputs something like this, when I try to run it like executable (chmode +x):What to do and how to be?Set line separator to :The issue is not EOF but EOL. The shell sees a ^M as well as the end of line and thus tries to find  .The usual way of getting into this state is to edit the python file with a MSDOS/Windows editor and then run on Unix. The simplest fix is to run dos2unix on the file or edit the file in an editor that explicitly allows saving with Unix end of lines.You may find the answers here: As a Mac OS X user, I didn't find the command . Alternatively, I use vi/vim:  and then save the file you may want to try Install dos2unix:\nand let it do the magic:\nIf you are using Vim, just enter the following command:For MacOS you can install it via  like this:And next do Similar to Jiangwei Yu's post. On UNIX/Linux, I used vi to edit the Python file. Using vi, you can see the ^M at the end of each line. Find the following line Hit  to get to the end of the lineHit  to remove the ^MTo save the file and quit, type in: This worked for me.you may try to do this:[ to type in ^M, press ctrl+v,ctrl+m ]Just a Question of format beween win and unix:try command: dos2unix fileNameAfter it run again, it should work"},
{"body": "I'm aware that with Boto 2 it's possible to open an S3 object as a string with:get_contents_as_string()\nIs there an equivalent function in boto3 ? This isn't in the boto3 documentation. This worked for me:object being an s3 object:  will return bytes. At least for Python 3, if you want to return a string, you have to decode using the right encoding:I had a problem to read/parse the object from S3 because of  using Python 2.7 inside an AWS Lambda.I added json to the example to show it became parsable :)NOTE: My object is all ascii, so I don't need If body contains a io.StringIO, you have to do like below:"},
{"body": "I need to setup environment by running 'which abc' command.\nIs there python  equivalent function for 'which' command?\nThis is my code.There is .()See the Twisted implementation: I know this is an older question, but if you happen to be using Python 3.3+ you can use . You can find the documentation . It has the advantage of being in the standard library.An example would be like so:There's not a command to do that, but you can iterate over  and look if the file exists, which is actually what  does.Good luck!You could try something like the following:If you use , then your command will be run through the system shell, which will automatically find the binary on the path:"},
{"body": "A Pandas  contains column named  that contains non-unique  values. \nI can group the lines in this frame using:However, this splits the data by the  values. I would like to group these data by the year stored in the \"date\" column.  shows how to group by year in cases where the time stamp is used as an index, which is not true in my case.How do I achieve this grouping?ecatmur's solution will work fine. This will be better performance on large datasets, though:I'm using pandas 0.16.2. This has better performance on my large dataset:Using the  option and playing around with ,  etc. becomes far easier.This should work:"},
{"body": "I just finished installing my  package for Python 2.6, and now when I import it using , a user warning appear will appearIs there a way how to get rid of this?You can change  to not be writeable by group/everyone. I think this works:You can suppress warnings using the :If you just want to flat out ignore warnings, you can use :"},
{"body": "Is there a built-in function in Python that would replace (or remove, whatever) the extension of a filename (if it has one) ?Example:In my example:  would become I don't know if it matters, but I need this for a SCons module I'm writing. (So perhaps there is some SCons specific function I can use ?)I'd like something . Doing a simple string replacement of all occurrences of  within the string is obviously not clean. (This would fail if my filename is )Try  it should do what you want.As @jethro said,  is the neat way to do it. But in this case, it's pretty easy to split it yourself, since the extension  the part of the filename coming after the final period:The  tells Python to perform the string splits starting from the right of the string, and the  says to perform at most one split (so that e.g.  -> ). Since  will always return a non-empty array, we may safely index  into it to get the filename minus the extension.Another way to do is to use the  method. For example:I prefer the following one-liner approach using :Example:Expanding on AnaPana's answer, how to  an extension using pathlib (Python >= 3.4):/some/path/somefile.ext\n/some/path/somefileFor Python >= 3.4:"},
{"body": "Basically, I'm converting a float to an int, but I don't always have the expected value.Here's the code I'm executing:x = 2.51And here's the result (first value is the result of the operation, second value is int() of the same operation):2.51 and 4.02 are the only values that lead to that strange behaviour on the 2.50 -> 5.00 range. Every other two digits value in that range converts to int without any problem when given the same operations.So, what am I missing that leads to those results? I'm using Python 2.7.2 by the way.The int() function simply truncates the number at the decimal point, giving 250. Use to get 251 as an integer. In general, floating point numbers cannot be represented exactly. One should therefore be careful of round-off errors. As mentioned, this is not a Python-specific problem. It's a recurring problem in all computer languages.Floating-point numbers cannot represent all the numbers.  In particular, 2.51 cannot be represented by a floating-point number, and is represented by a number very close to it:If you use int, which truncates the numbers, you get:Have a look at the  type.Languages that use binary floating point representations (Python is one) cannot represent all fractional values exactly. If the result of your calculation is 250.99999999999 (and it might be), then taking the integer part will result in 250.A canonical article on this topic is .the floating point numbers are inaccurate. in this case, it is 250.99999999999999, which is really close to 251, but int() truncates the decimal part, in this case 250.you should take a look at the Decimal module or maybe if you have to do a lot of calculation at the mpmath library  :),"},
{"body": "I am using python MySQL API to connect to Mysql database from python program. I am facing a problem from few days. I am unable to insert records into the database and dont know whats the reason. Here is the way i connect and insert records into the database.Why is it so?Before closing the connection, you should add ."},
{"body": "How to convert decimal to hex in the following format (at least two digits, zero-padded, without an 0x prefix)?Input:     Output: Input:        Output: I tried  but it seems that it displays the first example but not the second one. Use the  with a  format.The  part tells  to use at least 2 digits and to use zeros to pad it to length,  means lower-case hexadecimal.The  also gives you  for uppercase hex output, and you can prefix the field width with  to include a  or  prefix (depending on wether you used  or  as the formatter). Just take into account that you need to adjust the field width to allow for those extra 2 characters:I think this is what you want:The first answer is the best, but I have an archaic answer but functional"},
{"body": "Can anyone tell me how can I do this?first open a file: I like the accepted answer: it is straightforward and will get the job done. I would also like to offer an alternative implementation:The code I suggest is essentially the same idea as your accepted answer: read a given number of bytes from the file. The difference is that it first reads a good chunk of data (4006 is a good default for X86, but you may want to try 1024, or 8192; any multiple of your page size), and then it yields the characters in that chunk one by one.The code I present may be faster for larger files. Take, for example, . These are my timing results (Mac Book Pro using OS X 10.7.4; so.py is the name I gave to the code I pasted):Now: do not take the buffer size at  as a universal truth; look at the results I get for different sizes (buffer size (bytes) vs wall time (sec)):As you can see, you can start seeing gains earlier on (and my timings are likely very inaccurate); the buffer size is a trade-off between performance and memory. The default of 4096 is just a reasonable choice but, as always, measure first.Python itself can help you with this, in interactive mode:Just:I learned a new idiom for this today while watching Raymond Hettinger's :Just read a single characterYou should try , which is definitely correct and the right thing to do.This will also work:It goes through every line in the the file and every character in every line.To make a supplement, \nif you are reading file that contains a line that is vvvvery huge, which might break your memory, you might consider read them into a buffer then yield the each char "},
{"body": "Is there a way to access a list(or tuple, or other iterable)'s next, or previous element while looping through with for loop?Expressed as a generator function:Usage:One simple way. When dealing with generators where you need some context, I often use the below utility function to give a sliding window view on an iterator:It'll generate a view of the sequence N items at a time, shifting step places over.  eg.When using in lookahead/behind situations where you also need to deal with numbers without having a next or previous value, you may want pad the sequence with an appropriate value such as None.Check out the looper utility from the . It gives you a wrapper object around the loop item that provides properties such as previous, next, first, last etc.Take a look at the  for the looper class, it is quite simple. There are other such loop helpers out there, but I cannot remember any others right now.Example:I know this is old, but why not just use ?If you run this you will see that it grabs previous and next items and doesn't care about repeating items in the list.I don't think there is a straightforward way, especially that an iterable can be a generator (no going back). There's a decent workaround, relying on explicitly passing the index into the loop body:The  function is a builtin.Iterators only have the next() method so you cannot look forwards or backwards, you can only get the next item.enumerate(iterable) can be useful if you are iterating a list or tuple.Immediately previous?  You mean the following, right?If you want  previous items, you can do this with a kind of circular queue of size .If you want the solution to work on iterables, the  has a recipe that does exactly what you want:If you're using Python 2.x, use  instead of Not very pythonic, but gets it done and is simple:TO DO: protect the edges The most simple way is to search the list for the item:Of course, this only works if the list only contains unique items. The other solution is:"},
{"body": "I am working with flask in a virtual environment. I was able to install matplotlib with pip, and I can  in a Python session. However, when I import it as I get the following error: I am confused about why it asks me to install Python as framework. Doesn't it already exists? What does it mean to \"install Python as framework\", and how do I install it? This  worked for me. If you already installed matplotlib using pip on your virtual environment, you can just type the following:And then, write  in there. \nIf you need more information, just go to the solution link. I got the same error, and tried 's answer:I run the program, no error, but also no plots, and I tried ,\nit prints out that I haven't got PyQt4 installed.Then I tried another backend: , it works!So maybe we can try difference backends and some may work or install the requeired packages like PyQt4.Here is a sample python snippet that you can try and test matplotlib.I had similar problem when I used pip to install matplotlib. By default, it installed the latest version which was 1.5.0. However, I had another virtual environment with Python 3.4 and matplotlib 1.4.3 and this environment worked fine when I imported matplotlib.pyplot. Therefore, I installed the earlier version of matplotlib using the following:I know this is only a work-around, but it worked for me as a short-term fix.You can fix this issue by using the backend Go to  and open/create  and add the following line  and it should work for you. If you do not want to set a  configuration file, you can circumvent this issue by setting the  backend at runtime right after importing  and before importing :Although most answers seem to point towards patching the  script to use the system python, I was having trouble getting that to work and an easy solution for me - though a little cringey - was to install matplotlib to the global environment and use that instead of a virtualenv instance. You can do this either by creating your virtualenv with the --system-site-packages flag like , or to use the universal flag when pip installing like .A clean and easy solution is to create a kernel that sets  to \u00b4VIRTUAL_ENV` and then uses the system Python executable (instead of the one in the virtualenv).If you want to automate the creation of such a kernel, you can use the  script."},
{"body": "I have a models  and , that are like this:Now I have some  and  objects, and I'd like to get a query that selects all  objects that have less then 2  pointing at them.A is something like a pool thing, and users (the B) join pool. if there's only 1 or 0 joined, the pool shouldn't be displayed at all.Is it possible with such model design? Or should I modify that a bit?Sounds like a job for .If the B count is something you often need as a filtering or ordering criterion, or needs to be displayed on list views, you could consider denormalisation by adding a b_count field to your A model and using signals to update it when a B is added or deleted:Another solution would be to manage a status flag on the A object when you're adding or removing a related B.The question and selected answer are from 2008 and since then this functionality has been integrated into the django framework.  Since this is a top google hit for \"django filter foreign key count\" I'd like to add an easier solution with a recent django version using .In my case I had to take this concept a step further.  My \"B\" object had a boolean field called is_available, and I only wanted to return A objects who had more than 0 B objects with is_available set to True.I'd recommend modifying your design to include some status field on A.The issue is one of \"why?\"  Why does A have < 2 B's and why does A have >= 2 B's.  Is it because user's didn't enter something?  Or is because they tried and their input had errors.  Or is it because the < 2 rule doesn't apply in this case.Using presence or absence of a Foreign Key limits the meaning to -- well -- present or absent.  You don't have any way to represent \"why?\"Also, you have the following optionThis can be pricey because it does fetch all the A's rather than force the database to do the work.Edit: From the comment \"would require me to watch for user join / user leaving the pool events\".You don't \"watch\" anything -- you provide an API which does what you need.  That's the central benefit of the Django model.  Here's one way, with explict methods in the  class.You can also define a special  for this, and replace the default  Manager with your manager that counts references and updates .I assume that joining or leaving the pool may not happen as often as listing (showing) the pools. I also believe that it would be more efficient for the users join/leave actions to update the pool display status. This way, listing & showing the pools would require less time as you would just run a single query for SHOW_STATUS of the pool objects.How about just doing this way?"},
{"body": "How do I generate a unique session id in Python?You can use the  like so:A lot has happened in a the last ~5yrs.   has been updated and is now considered a high-entropy source of randomness on modern Linux kernels and distributions.  In the last 6mo we've seen entropy starvation on a Linux 3.19 kernel using Ubuntu, so I don't think this issue is \"resolved\", but it's sufficiently difficult to end up with low-entropy randomness when asking for any amount of randomness from the OS.I hate to say this, but none of the other solutions posted here are correct with regards to being a \"secure session ID.\" Neither  or  are good choices for generating session IDs. Both may generate  results, but random does not mean it is  due to poor . See \"\" by Haldir or . If you still want to use a UUID, then use a UUID that was generated with a good initial random number:or:M2Crypto is best OpenSSL API in Python atm as pyOpenSSL appears to be maintained only to support legacy applications.It can be as simple as creating a random number.  Of course, you'd have to store your session IDs in a database or something and check each one you generate to make sure it's not a duplicate, but odds are it never will be if the numbers are large enough.What's the session for? A web app? You might wanna look at . It is the default module for handling sessions in Pylons."},
{"body": "Suppose code like this:Now I want to call methods start and stop (maybe also others) for each object in the list all. Is there any elegant way for doing this except of writing a bunch of functions likeThe *_all() functions are so simple that for a few methods I'd just write the functions. If you have lots of identical functions, you can write a generic function:Or create a function factory:simple exampleand in python3It seems like there would be a more Pythonic way of doing this, but I haven't found it yet.I use \"map\" sometimes if I'm calling the same function (not a method) on a bunch of objects:This replaces a bunch of code that looks like this:But can also be achieved with a pedestrian \"for\" loop:The downside is that a) you're creating a list as a return value from \"map\" that's just being throw out and b) it might be more confusing that just the simple loop variant.You could also use a list comprehension, but that's a bit abusive as well (once again, creating a throw-away list):For methods, I suppose either of these would work (with the same reservations):orSo, in reality, I think the pedestrian (yet effective) \"for\" loop is probably your best bet.The approachis simple, easy, readable, and concise. This is the main approach Python provides for this operation. You can certainly encapsulate it in a function if that helps something. Defining a special function for this for general use is likely to be less clear than just writing out the for loop.maybe , but since you don't want to make a list, you can write your own...then you can do:by the way, all is a built in function, don't overwrite it ;-)Taking @Ants Aasmas answer one step further, you can create a wrapper that takes any method call and forwards it to all elements of a given list:That class can then be used like this:Which produces the following output:With some work and ingenuity it could probably be enhanced to handle attributes as well (returning a list of attribute values).There is an easier way:"},
{"body": "I want to insert the integers 188 and 90 in my MySQL database, but the following code doesn't work:Why doesn't it work? working for me:table in mysql;"},
{"body": "How can I extract the date from a string like \"monkey 2010-07-10 love banana\"? Thanks!If the date is given in a fixed form, you can simply use a regular expression to extract the date and \"datetime.datetime.strptime\" to parse the date:Otherwise, if the date is given in an arbitrary form, you can't extract it easily.Using :Invalid dates raise a :It can recognize dates in many formats:Note that it makes a guess if the date is ambiguous:But the way it parses ambiguous dates is customizable:For extracting the date from a string in Python; the best module available is the  module.You can use it in your Python project by following the easy steps given below. if you are expecting a large number of matches; then typecasting to list won't be a recommended way as it will be having a big performance overhead."},
{"body": "For example, the standard division symbol '/' rounds to zero:However, I want it to return 0.04. What do I use?There are three options:which is the same behavior as the C, C++, Java etc, or You can also activate this behavior by passing the argument  to the Python interpreter:The second option will be the default in Python 3.0. If you want to have the old integer division, you have to use the  operator. : added section about , thanks to !Other answers suggest how to get a floating-point value.  While this wlil be close to what you want, it won't be exact:If you actually want a  value, do this:That will give you an object that properly knows that 4 / 100 in  is \"0.04\".  Floating-point numbers are actually in base 2, i.e. binary, not decimal.Make one or both of the terms a floating point number, like so:Alternatively, turn on the feature that will be default in Python 3.0, 'true division', that does what you want. At the top of your module or script, do:You need to tell Python to use floating point values, not integers. You can do that simply by using a decimal point yourself in the inputs:You might want to look at Python's  package, also.  This will provide nice decimal results.Try 4.0/100A simple route 4 / 100.0 or4.0 / 100You cant get a decimal value by dividing one integer with another, you'll allways get an integer that way (result truncated to integer). You need at least one value to be a decimal number.Please consider following example"},
{"body": "Every so often on here I see someone's code and what looks to be a 'one-liner', that being a one line statement that performs in the standard way a traditional 'if' statement or 'for' loop works.I've googled around and can't really find what kind of ones you can perform? Can anyone advise and preferably give some examples?For example, could I do this in one line:Or:Well,Is this an improvement? . You could even add more statements to the body of the -clause by separating them with a semicolon. I recommend  that though.I've found that in the majority of cases doing block clauses on one line is a bad idea.It will, again as a generality, reduce the quality of the form of the code.  High quality code form is a key language feature for python.In some cases python will offer ways todo things on one line that are definitely more pythonic. Things such as what Nick D mentioned with the list comprehension: although unless you need a reusable list specifically you may want to consider using a generator insteadnote the biggest difference between the two is that you can't reiterate over a generator, but it is more efficient to use. There is also a built in ternary operator in modern versions of python that allow you to do things likeor Some people may find these more readable and usable than the similar  block.When it comes down to it, it's about code style and what's the standard with the team you're working on.  That's the most important, but in general, i'd advise against one line blocks as the form of the code in python is so very important.More generally, all of the following are valid syntactically:...etc.an example of a language feature that isn't just removing line breaks, although still not convinced this is clearer than the more verbose versionYou can rewrite the above as:Python lets you put the indented clause on the same line if it's only one line:You could do all of that in one line by omitting the  variable:Older versions of Python would only allow a single simple statement after   or similar block introductory statements.I see that one can have multiple simple statements on the same line as any of these.  However, there are various combinations that don't work.  For example we can:... but, on the other hand, we can't:We can:... but we can't:... and so on.In any event all of these are considered to be extremely NON-pythonic.  If you write code like this then experience Pythonistas will probably take a dim view of your skills.It's marginally acceptable to combine multiple statements on a line in some cases.  For example:... or even:... for simple   and even  statements or assigments.In particular if one needs to use a series of  one might use something like:... then you might not irk your colleagues too much.  (However, chains of  like that scream to be refactored into a dispatch table ... a dictionary that might look more like:Dive into python has a bit where he talks about what he calls the , which seems like an effective way to cram complex logic into a single line.Basically, it simulates the ternary operater in c, by giving you a way to test for truth and return a value based on that. For example:This is an example of \"if else\" with actions."},
{"body": "I've a python script that has to launch a shell command for every file in a dir:This works fine for the first file, but after the \"myscript\" command has ended, the execution stops and does not come back to the python script.How can I do? Do I have to  before ?Usage:You can use .  There's a few ways to do it:Or, if you don't care what the external program actually does:The subprocess module has come along way since 2008.  In particular  and  make simple subprocess stuff even easier.  The  family of functions are nice it that they raise an exception if something goes wrong.Any output generated by  will display as though your process produced the output (technically  and your python script share the same stdout).  There are a couple of ways to avoid this.The  functions  the current programm with the new one. When this programm ends so does your process. You probably want .use spawn"},
{"body": "How can I convert the result of a ConfigParser.items('section') to a dictionary to format a string like here:This is actually already done for you in .  Example:And then:  My solution to the same problem was downvoted so I'll further illustrate how my answer does the same thing without having to pass the section thru , because  is .Magic happening:So this solution is  wrong, and it actually requires one less step.  Thanks for stopping by!Have you tried?How I did it in just one line.No more than other answers but when it is not the real businesses of your method and you need it just in one place use less lines and take the power of dict compression could be useful. I know this was asked a long time ago and a solution chosen, but the solution selected does not take into account defaults and variable substitution. Since it's the first hit when searching for creating dicts from parsers, thought I'd post my solution which does include default and variable substitutions by using ConfigParser.items().A convenience function to do this might look something like:For an individual section, e.g. \"general\", you can do:"},
{"body": "In PHP, a string enclosed in \"double quotes\" will be parsed for variables to replace whereas a string enclosed in 'single quotes' will not. In Python, does this also apply?:Python is one of the few (?) languages where ' and \" have identical functionality. The choice for me usually depends on what is inside. If I'm going to quote a string that has single quotes within it I'll use double quotes and visa versa, to cut down on having to escape characters in the string.Examples:This is documented on the \"String Literals\" page of the python documentation: In some other languages, meta characters are not interpreted if you use single quotes. Take this example in Ruby:In Python, if you want the string to be taken literally, you can use raw strings (a string preceded by the 'r' character):Single and double quoted strings in Python are identical. The only difference is that single-quoted strings can contain unescaped double quote characters, and vice versa. For example:Then again, there are triple-quoted strings, which allow both quote chars and newlines to be unescaped.You can substitute variables in a string using named specifiers and the locals() builtin:The difference between \" and ' string quoting is just in style - except that the one removes the need for escaping the other inside the string content. recommends a consistent rule,  suggests that docstrings use triple double quotes.Widely used however is the practice to prefer double-quotes for natural language strings (including interpolation) - thus anything which is potentially candidate for I18N. And single quotes for technical strings: symbols, chars, paths, command-line options, technical REGEXes, ...(For example, when preparing code for I18N, I run a semi-automatic REGEX converting double quoted strings quickly for using e.g. )The interactive Python interpreter prefers single quotes:This could be confusing to beginners, so I'd stick with single quotes (unless you have different coding standards).There are 3 ways you can qoute strings in python:\n\"string\"\n'string'\n\"\"\"\nstring\nstring\n\"\"\"\nthey all produce the same result.There is no difference in Python, and you can really use it to your advantage when generating XML.  Correct XML syntax requires double-quotes around attribute values, and in many languages, such as Java, this forces you to escape them when creating a string like this:But in Python, you simply use the other quote and make sure to use the matching end quote like this:Pretty nice huh?  You can also use three double quotes to start and end multi-line strings, with the EOL's included like this:Yes. \nThose claiming single and double quotes are identical in Python are simply wrong.Otherwise in the following code, the double-quoted string would not have taken an extra 4.5% longer for Python to process:Output:So if you want fast clean respectable code where you seem to know your stuff, use single quotes for strings whenever practical. You will also expend less energy by skipping the shift key."},
{"body": "I tried running this piece of code:And it prints . I thought Python treats anything with value as . Why is this happening?From :The key phrasing here that I think you are misunderstanding is \"interpreted as false\" or \"interpreted as true\".  This does not mean that any of those values are identical to True or False, or even equal to True or False.The expression  will be treated as true where a Boolean expression is expected (like in an  statement), but the expressions  and  will evaluate to False for the reasons in Ignacio's answer. compares identity. A string will never be identical to a not-string. is equality. But a string will never be equal to either  or .You want neither."},
{"body": "I just want fixed width columns of text but the strings are all padded right, instead of left!!?producesbut we wantThis version uses the  method.Previously there was a statement in the docs about the % operator being removed from the language in the future. This statement has been .You can prefix the size requirement with  to left-justify:on a side note you can make the width variable with Use  instead of  They will be aligned to left..This one worked in my python script:A slightly more readable alternative solution:\n  Note that  uses fixed width characters and doesn't dynamically adjust like the other solutions."},
{"body": "If I have this string:what is the most efficient approach for creating this list:It just so happens that the tokens you want split are already Python tokens, so you can use the built-in  module.  It's almost a one-liner:You can use  from the  module.  Example code:\\DThis looks like a parsing problem, and thus I am compelled to present a solution based on parsing techniques.While it may seem that you want to 'split' this string, I think what you actually want to do is 'tokenize' it. Tokenization or lexxing is the compilation step before parsing. I have amended my original example in an edit to implement a proper recursive decent parser here. This is the easiest way to implement a parser by hand.Implementation of handling of brackets is left as an exercise for the reader. This example will correctly do multiplication before addition.Matches consecutive digits or consecutive non-digits.Each match is returned as a new element in the list.Depending on the usage, you may need to alter the regular expression. Such as if you need to match numbers with a decimal point.This is a parsing problem, so neither regex not split() are the \"good\" solution. Use a parser generator instead.I would look closely at . There have also been some decent articles about pyparsing in the .Regular expressions:You can expand the regular expression to include any other characters you want to split on.Another solution to this would be to avoid writing a calculator like that altogether. Writing an RPN parser is much simpler, and doesn't have any of the ambiguity inherent in writing math with infix notation.This will do the trick. I have encountered this kind of problem before.Why not just use ?  It should do what you're trying to achieve.This doesn't answer the question exactly, but I believe it solves what you're trying to achieve. I would add it as a comment, but I don't have permission to do so yet.I personally would take advantage of Python's maths functionality directly with exec:i'm sure Tim meant if you copy exactly what he has down you only get the  not the ."},
{"body": "Python is the language I know the most, and strangely I still don't know why I'm typing \"self\" and not \"this\" like in Java or PHP.I know that Python is older than Java, but I can't figure out where does this come from. Especially since you can use any name instead of \"self\": the program will work fine.So where does this convention come from?Smalltalk-80, released by Xerox in 1980, used .  Objective-C (early 1980s) layers Smalltalk features over C, so it uses  too.  Modula-3 (1988), Python (late 1980s), and Ruby (mid 1990s) also follow this tradition.C++, also dating from the early 1980s, chose  instead of .  Since Java was designed to be familiar to C/C++ developers, it uses  too.Smalltalk uses the metaphor of objects sending messages to each other, so \"self\" just indicates that the object is sending a message to itself.Check the  for user defined classes:It's a choice as good as any other.  You might ask why C++, Java, and C# chose \"this\" just as easily.Smalltalk, which predates Java of course.With respect to python, there is nothing special about . You can use  instead if you wanted:Here's an example:Although you could name it whatever you want,  is the convention for the first argument of a class function. Check out  in the python documentation, which says:As for the convention, it started out in Smalltalk, but is also used in Object Pascal, Python, Ruby, and Objective-C.  has a great explanation.Python follows Smalltalk's footsteps in the aspect - self is used in Smalltalk as well. I guess the  question should be 'why did Bjarne decide to use  in C++'...The primary inspiration was Modula-3, which Guido was introduced to at DEC: -- Guido, I think that since it's explicitly declared it makes more sense seeing an actual argument called \"self\" rather than \"this\". From the grammatical point of view at least, \"self\" is not as context dependent as \"this\".I don't know if I made myself clear enough, but anyway this is just a subjective appreciation. is not a keyword (*). represents by convention the address of the current objectYou can get more info on  .Why not  ? well it is a convention for a name. You can use  for your code if you like it better.(*) This answer has been ported and merged here from a question asking . As the clarification in this first line could be useful for others I keep it here."},
{"body": "I have the following DataFrame:I would like to add a column 'e' which is the sum of column 'a', 'b' and 'd'.Going across forums, I thought something like this would work:But no!I would like to realize the operation having the list of columns  and  as inputs.You can just  and set param  to sum the rows, this will ignore none numeric columns:If you want to just sum specific columns then you can create a list of the columns and remove the ones you are not interested in:If you have a just a few columns to sum, you can write: This creates new column  with the values:For longer lists of columns, EdChum's answer is preferred."},
{"body": "I recently encountered a scenario in which if a set only contained a single element, I wanted to do something with that element.  To get the element, I settled on this approach:But this isn't very satisfying, as it creates an unnecessary list.  It could also be done with iteration, but iteration seems unnatural as well, since there is only a single element.  Am I missing something simple?tuple unpacking works.(By the way, python-dev has explored but rejected the addition of myset.get() to return an arbitrary element from a set. , Guido van Rossum answers  and .)My personal favorite for getting arbitrary element is (when you have an unknown number, but also works if you have just one):: in Python 2.5 and before, you have to use Between making a tuple and making an iterator, it's almost a wash, but iteration wins by a nose...:Not sure why all the answers are using the older syntax  rather than the new one , which seems preferable to me (and also works in Python 3.1).However, unpacking wins hands-down over both:This of course is for single-item sets (where the latter form, as others mentioned, has the advantage of failing fast if the set you \"knew\" had just one item actually had several).  For sets with arbitrary N > 1 items, the tuple slows down, the iter doesn't:So, unpacking for the singleton case, and  for the general case, seem best.I reckon  is great. But if your set  contain more than one element, and you want a not-so-arbitrary element, you might want to use  or . E.g.:or:(Don't use , because that it has unnecessary overhead for this usage.)you can use  which is a bit more efficient, or, you can do something like  I guess constructing an iterator is more efficient than constructing a tuple/list.I suggest:"},
{"body": "when i give \nls -l /etc/fonts/conf.d/70-yes-bitmaps.conf so for a symbolic link or soft link, how to find the target file's full(absolute path) in python,If i use it outputs but i need the absolute path not the relative path, so my desired output must be,how to replace the  with the actual full path of the parent directory of the symbolic link or soft link file. returns the canonical path of the specified filename, eliminating any symbolic links encountered in the path.As unutbu says, os.path.realpath(path) should be the right answer, returning the canonical path of the specified filename, resolving any symbolic links to their targets.  But it's broken under Windows.I've created a patch for Python 3.2 to fix this bug, and uploaded it to:It fixes the realpath function in Python32\\Lib\\ntpath.pyI've also put it on my server, here:Unfortunately, the bug is present in Python 2.x, too, and I know of no fix for it there.also joinpath and normpath, depending on whether you're in the current working directory, or you're working with things elsewhere.  might be more direct for you.Update:specifically:"},
{"body": "I'm using python2.7's  to read from stdin.I want to let the user change a given default string.Code:Console:The user should be presented with  but can change (backspace) it to something else.The  argument would be the prompt for  and that part shouldn't be changeable by the user.You could do: This way, if user just presses return without entering anything, \"i\" will be assigned \"Jack\". In dheerosaur's answer If user press Enter to select default value in reality it wont be saved as python considers it as '' string so Extending a bit on what dheerosaur.Fyi .. The  of backspace is On platforms with , you can use the method described here: On Windows, you can use the msvcrt module:Note that arrows keys don't work for the windows version, when it's used, nothing will happen.Put this in a file called a.py:Run the program, it stops and presents the user with this:The cursor is at the end, user presses backspace until 'an insecticide' is gone, types something else, then presses enter:Program finishes like this, final answer gets what the user typed: I only add this because you should write a simple function for reuse.  Here is the one I wrote:Try this: The ASCII value of  is ."},
{"body": "What's the best way to count the number of occurrences of a given string, including overlap in python? is it the most obvious way:?or is there a better way in python?Well, this  be faster since it does the comparing in C:If you didn't want to load the whole list of matches into memory, which would never be a problem! you could do this if you really wanted:As a function ( makes sure the substring doesn't interfere with the regex):You can also try using the , which supports overlapping matches.Python's  counts non-overlapping substrings:Here are a few ways to count overlapping sequences, I'm sure there are many more :)My answer, to the bob question on the course:This function (another solution!) receive a pattern and a text. Returns a list with all the substring located in the and their positions.Function that takes as input two strings and counts how many times sub occurs in string, including overlaps. To check whether sub is a substring, I used the  operator. Here is my edX MIT \"find bob\"* solution (*find number of \"bob\" occurences in a string named s), which basicaly counts overlapping occurrences of a given substing:For a duplicated  i've decided to count it 3 by 3 and comparing the string e.g.An alternative very close to the accepted answer but using  as the  test instead of including  inside the loop:This avoids  and is a little cleaner in my opinionIf strings are large, you want to use , in summary:If you want to count permutation counts of length 5 (adjust if wanted for different lengths):"},
{"body": "By default, when running Flask application using the built-in server (), it monitors its Python files and automatically reloads the app if its code changes:Unfortunately, this seems to work for  files only, and I don't seem to find any way to extend this functionality to other files. Most notably, it would be extremely useful to have Flask restart the app when a  changes. I've lost count on how many times I was fiddling with markup in templates and getting confused by not seeing any changes, only to find out that the app was still using the old version of Jinja template.So, is there a way to have Flask monitor files in  directory, or does it require diving into the framework's source?: I'm using Ubuntu 10.10. Haven't tried that on any other platforms really.After further inquiry, I have discovered that changes in templates indeed  updated in real time, without reloading the app itself. However, this seems to apply only to those templates that are passed to .But it so happens that in my app, I have quite a lot of reusable, parametrized components which I use in Jinja templates. They are implemented as s, reside in dedicated \"modules\" and are ed into actual pages. All nice and DRY... except that those imported templates are apparently never checked for modifications, as they don't pass through  at all.(Curiously, this doesn't happen for templates invoked through . As for , I have no idea as I don't really use them.)So to wrap up, the roots of this phenomenon seems to lie somewhere between Jinja and Flask or Werkzeug. I guess it may warrant a trip to bug tracker for either of those projects :) Meanwhile, I've accepted the 's answer because that's the solution I actually used - and it works like a charm.In my experience, templates don't even need the application to restart to be refreshed, as they should be loaded from disk everytime  is called. Maybe your templates are used differently though.To reload your application when the templates change (or any other file), you can pass the  argument to , a collection of filenames to watch: any change on those files will trigger the reloader.Example:See here: you can use\nTEMPLATES_AUTO_RELOAD = TrueFrom Actually for me TEMPLATES_AUTO_RELOAD = True does not work (0.12 version). I use jinja2 and what i have done:What worked for me is just adding this:()When you are working with  templates, you need to set some parameters. In my case with python3, I solved it with the following code:Using the latest version of Flask on Windows, using the run command and debug set to true; Flask doesn't need to be reset for changes to templates to be brought in to effect. Try Shift+F5 (or Shift plus the reload button) to make sure nothing it being cached."},
{"body": "I have a list in python and I want to convert it to an array to be able to use  function.Use :I wanted a way to do this without using an extra module. First turn list to string, then append to an array:if variable b has a list then you can simply do the below:create a new variable \"a\" as: \n then assign the list to \"a\" as: now \"a\" has all the components of list \"b\" in array.so you have successfully converted list to array."},
{"body": "I have a large amount of data in a collection in mongodb which I need to analyze. How do i import that data to pandas?I am new to pandas and numpy.EDIT:\nThe mongodb collection contains sensor values tagged with date and time. The sensor values are of float datatype. Sample Data: might give you a hand, followings are some codes I'm using: does exactly that, and it's . ()See  which includes a quick tutorial and some timings.You can load your mongodb data to pandas DataFrame using this code. It works for me. Hopefully for you too.export to csv and use \nor JSON and use For dealing with out-of-core (not fitting into RAM) data efficiently (i.e. with parallel execution), you can try : Blaze / Dask / Odo.Blaze (and ) has out-of-the-box functions to deal with MongoDB.A few useful articles to start off:And an article which shows what amazing things are possible with Blaze stack:  (essentially, querying 975 Gb of Reddit comments in seconds).P.S. I'm not affiliated with any of these technologies.Using will consume a lot of memory if the iterator/generator result is largebetter to generate small chunks and concat at the endAs per PEP, simple is better than complicated:  You can include conditions as you would working with regular mongoDB database or even use find_one() to get only one element from the database, etc. and voila!"},
{"body": "I don't care if it's JSON, pickle, YAML, or whatever.All other implementations I have seen are not forwards compatible, so if I have a config file, add a new key in the code, then load that config file, it'll just crash.Are there any simple way to do this?There are several ways to do this depending on the file format required.I would use the standard  approach unless there were compelling reasons to use a different format.Write a file like so:The file format is very simple with sections marked out in square brackets:Values can be extracted from the file like so:JSON data can be very complex and has the advantage of being highly portable.Write data to a file:Read data from a file:A basic YAML example is provided . More details can be found on .If you want to use something like an INI file to hold settings, consider using  which loads key value pairs from a text file, and can easily write back to the file. INI file has the format:The file can be loaded and used like this:which outputsAs you can see, you can use a standard data format that is easy to read and write. Methods like getboolean and getint allow you to get the datatype instead of a simple string.Writing configurationresults inSeems not to be used at all for configuration files by the Python community. However, parsing / writing XML is easy and there are plenty of possibilities to do so with Python. One is BeautifulSoup:where the config.xml might look like thisSave and load a dictionary. You will have arbitrary keys, values and arbitrary number of key, values pairs."},
{"body": "The piece of code that I have looks some what like this:Is there a way to make sure (or encourage) that the different processes does not get a copy of glbl_array but shares it. If there is no way to stop the copy I will go with a memmapped array, but my access patterns are not very regular, so I expect memmapped arrays to be slower. The above seemed like the first thing to try. This is on Linux. I just wanted some advice from Stackoverflow and do not want to annoy the sysadmin. Do you think it will help if the the second parameter is a genuine immutable object like .You can use the shared memory stuff from  together with Numpy fairly easily:which prints\nHowever, Linux has copy-on-write semantics on , so even without using , the data will not be copied unless it is written to.The following code works on Win7 and Mac (maybe on linux, but not tested).For those stuck using Windows, which does not support  (unless using CygWin), pv's answer does not work.  Globals are not made available to child processes.Instead, you must pass the shared memory during the initializer of the / as such:(it's not numpy and it's not good code but it illustrates the point  ;-)If you are looking for an option that works efficiently on Windows, and works well for irregular access patterns, branching, and other scenarios where you might need to analyze different matrices based on a combination of a shared-memory matrix and process-local data, the mathDict toolkit in the  package was designed to handle this exact situation."},
{"body": "I have a function asNow I want to write unit test for the above function separately (without calling the view). \nSo how should I call the above in . Is it possible to create request ?See :If you are using django test client () you can access request from response object like this:or if you are using () you can access client instance in any testcase just by typing :Use   to create a dummy request.You mean  right?With Django unittest, you can use the  to make request.See here: @Secator's answer is prefect as it creates a mock object which is really preferred for a really good unittest. But depending on your purpose, it might be easier to just use Django's test tools.You can use django test client for more details\n"},
{"body": "I've got a class, located in a separate module, which I can't change.This doesn't change MyClass anywhere else but this file. However if I'll add a method like thisthis will work and foo method will be available everywhere else.How do I replace the class completely?Avoid the  (horrid;-) way to get barenames when what you need most often are  names.  Once you do things the right Pythonic way:This way, you're monkeypatching the  object, which is what you need and will work when that module is used for others.  With the  form, you just don't  the module object (one way to look at the glaring defect of most people's use of ) and so you're obviously worse off;-);The one way in which I recommend using the  statement is to import a module from within a package:so you're still getting the module object and will use qualified names for all the names in that module.I am but an egg . . . . Perhaps it is obvious to not-newbies, but I needed the  idiom.  I had to modify one method of GenerallyHelpfulClass.  This failed:The code ran, but didn't use the behaviors overloaded onto SpeciallyHelpfulClass.This worked:I speculate that the  idiom 'gets the module', as Alex wrote, as it will be picked up by other modules in the package.  Speculating further, the longer dotted reference seems to bring the module into the namespace with the import by long dotted reference, but doesn't change the module used by other namespaces.  Thus changes to the import module would only appear in the name space where they were made.  It's as if there were two copies of the same module, each available under slightly different references.  Its preferable not to change the name of class while replacing, because somehow someone may have referenced them using getattr - which will result in fail like below --> which will fail if you have replaced MyClass by ReplaceClass !"},
{"body": "Silly question:\nI have a simple for loop followed by a simple if statement:  and I was wondering if I can write this as a single line somehow.\nSo, yes, I can do this:  but it reads so silly and redundant (\"for airport in airport for airport in airports...\").\nIs there a better way?No, there is no shorter way. Usually, you will even break it into two lines :This is more flexible, easier to read and still don't consume much memory.You could doI'd use a negative guard on the loop. It's readable, and doesn't introduce an extra level of indentation.Mabe this, but it's more or less the same verbose...This is a design philosophy of python.  If it takes you too many words to put it on one line, it should be broken into a few lines to help the person who comes after you.  List and generator expressions are more for transforming iterables in-place -- making more readable forms of  and .Here's an alternative to some of the other filter versions:This has the advantages of being pretty concise and also letting you use dot notation attr('first_class.is_full').You could also put something like that (or a version using a list comprehension) into a utility function like filter_by_attr. Then you could do:I still think e-satis is right to put it in a new variable no matter the method you use, though. It is just clearer that way, especially if the use doesn't exactly match the name of the attribute in question (or the the criteria is more complex).My only note on that would be that if you find yourself using this in several places, perhaps you should make airports a special collection with 'important_airports' being a @property which returns the filtered collection. Or some sort other abstraction to hide away the filtering (like a service call).Using list comprehension (only if airports is a list of objects):"},
{"body": "I'm working on something like an online store. I'm making a form in which the customer buys an item, and she can choose how many of these item she would like to buy. But, on every item that she buys she needs to choose what its color would be. So there's a non-constant number of fields: If the customer buys 3 items, she should get 3  boxes for choosing a color, if she buys 7 items, she should get 7 such  boxes.I'll make the HTML form fields appear and disappear using JavaScript. But how do I deal with this on my Django form class? I see that form fields are class attributes, so I don't know how to deal with the fact that some form instance should have 3 color fields and some 7.Any clue?Jacob Kaplan-Moss has an extensive writeup on dynamic form fields:\nEssentially, you add more items to the form's fields member variable during instantiation.Here's another option: how about a ?\nSince your fields are all the same, that's precisely what formsets are used for.The django admin uses s + a bit of javascript to add arbitrary length inlines.you can do it likeand when you create form instance, you just doit's just the basic idea, you can change the code to whatever your want. :D"},
{"body": "I want a regular expression to extract the title from a HTML page. Currently I have this:Is there a regular expression to extract just the contents of  so I don't have to remove the tags?thanks!Use   in regexp and  in python to retrieve the captured string ( will return  if it doesn't find the result, so ):Try using capturing groups:Try: You can use any HTML parser like Beautiful Soup for that. Check out Also remember that May I recommend you to Beautiful Soup.  Soup is a very good lib to parse all of your html document.The provided pieces of code do not cope with \nMay I suggestThis returns an empty string by default if the pattern has not been found, or the first match.Don't use regular expressions for HTML parsing in Python.  Use an HTML parser!  (Unless you're going to write a full parser, which would be a of extra work when various HTML, SGML and XML parsers are already in the standard libraries.If your handling \"real world\"  HTML (which is frequently non-conforming to any SGML/XML validator) then use the  package.  It isn't in the standard libraries (yet) but is wide recommended for this purpose.Another option is:  ... which is written for properly structured (standards conformant) HTML.  But it has an option to fallback to using BeautifulSoup as a parser: ."},
{"body": "I think that I fully understand this, but I just want to make sure since I keep seeing people say to NEVER EVER test against , , or .  They suggest that routines should raise an error rather than return False or None.  Anyway, I have many situations where I simply want to know if a flag is set or not so my function returns True or False.  There are other situations where I have a function return None if there was no useful result.  From my thinking, neither is problematic so long as I realize that I should never use:and should instead use:since True, False, and None are all singletons and will always evaluate the way I expect when using \"is\" rather than \"==\".  Am I wrong here?Along the same lines, would it be more pythonic to modify the functions that sometimes return None so that they raise an error instead?  Say I have an instance method called \"get_attr()\" that retrieves an attribute from some file.  In the case where it finds that the attribute I requested does not exist is it appropriate to return None?  Would it be better to have them raise an error and catch it later?The advice isn't that you should never  , , or . It's just that you shouldn't use . is silly because  is just a binary operator! It has a return value of either  or , depending on whether its arguments are equal or not. And  will proceed if  is true. So when you write  Python is going to first evaluate , which will become  if  was  and  otherwise, and then proceed if the result of that is true. But if you're expecting  to be either  or , why not just use  directly!Likewise,  can usually be replaced by .There are some circumstances where you might want to use . This is because an  statement condition is \"evaluated in boolean context\" to see if it is \"truthy\" rather than testing exactly against . For example, non-empty strings, lists, and dictionaries are all considered truthy by an if statement, as well as non-zero numeric values, but none of those are equal to . So if you want to test whether an arbitrary value is  the value , not just whether it is truthy, when you would use . But I almost never see a use for that. It's so rare that if you  ever need to write that, it's worth adding a comment so future developers (including possibly yourself) don't just assume the  is superfluous and remove it.Using  instead is actually worse. You should never use  with basic built-in immutable types like booleans (, ), numbers, and strings. The reason is that for these types we care about , not .  tests that values are the same for these types, while  always tests identities.Testing identities rather than values is bad because an implementation could theoretically construct new boolean values rather than go find existing ones, leading to you having two  values that have the same value but are stored in different placed in memory and have different identities. In practice I'm pretty sure  and  are always re-used by the Python interpreter so this won't happen, but that's really an implementation detail. This issue trips people up all the time with strings, because short strings and literal strings that appear directly in the program source are recycled by Python so  always returns . But it's easy to construct the same string 2 different ways and have Python give them different identities. Observe the following: So it turns out that Python's equality on booleans is a little unexpected (at least to me):The rationale for this, as explained in , is that the old behaviour of using integers 1 and 0 to represent True and False was good, but we just wanted more descriptive names for numbers we intended to represent truth values.One way to achieve that would have been to simply have  and  in the builtins; then 1 and True really would be indistinguishable (including by ). But that would also mean a function returning  would show  in the interactive interpreter, so what's been done instead is to create  as a subtype of . The only thing that's different about  is  and ;  instances still have the same data as  instances, and still compare equality the same way, so .So it's wrong to use  when  might have been set by some code that expects that \"True is just another way to spell 1\", because there are lots of ways to construct values that are equal to  but do not have the same identity as it:And it's wrong to use  when  could be an arbitrary Python value and you only want to know whether it is the boolean value . The only certainty we have is that just using  is best when you just want to test \"truthiness\". Thankfully that is usually all that is required, at least in the code I write!A more sure way would be . But that's getting pretty verbose for a pretty obscure case. It also doesn't look very Pythonic by doing explicit type checking... but that really is what you're doing when you're trying to test precisely  rather than truthy; the duck typing way would be to accept truthy values and allow any user-defined class to declare itself to be truthy.If you're dealing with this extremely precise notion of truth where you not only don't consider non-empty collections to be true but also don't consider 1 to be true, then just using  is probably okay, because presumably then you know that  didn't come from code that considers 1 to be true. I don't think there's any pure-python way to come up with another  that lives at a different memory address (although you could probably do it from C), so this shouldn't ever break despite being theoretically the \"wrong\" thing to do.And I used to think booleans were simple!In the case of , however, the idiom is to use . In many circumstances you can use , because  is a \"falsey\" value to an  statement. But it's best to only do this if you're wanting to treat all falsey values (zero-valued numeric types, empty collections, and ) the same way. If you are dealing with a value that is either some possible other value or  to indicate \"no value\" (such as when a function returns  on failure), then it's  better to use  so that you don't accidentally assume the function failed when it just happened to return an empty list, or the number 0.My arguments for using  rather than  for immutable value types would suggest that you should use  rather than . However, in the case of  Python does explicitly guarantee that there is exactly one  in the entire universe, and normal idiomatic Python code uses .Regarding whether to return  or raise an exception, it depends on the context.For something like your  example I would expect it to raise an exception, because I'm going to be calling it like . The normal expectation of the callers is that they'll get the attribute value, and having them get  and assume that was the attribute value is a much worse danger than forgetting to handle the exception when you can actually continue if the attribute can't be found. Plus, returning  to indicate failure means that  is not a valid value for the attribute. This can be a problem in some cases.For an imaginary function like , that we provide a pattern to and it checks several places to see if there's a match, it could return a match if it finds one or  if it doesn't. But alternatively it could return a list of matches; then no match is just the empty list (which is also \"falsey\"; this is one of those situations where I'd just use  to see if I got anything back).So when choosing between exceptions and  to indicate failure, you have to decide whether  is an expected non-failure value, and then look at the expectations of code calling the function. If the \"normal\" expectation is that there will be a valid value returned, and only occasionally will a caller be able to work fine whether or not a valid value is returned, then you should use exceptions to indicate failure. If it will be quite common for there to be no valid value, so callers will be expecting to handle both possibilities, then you can use .Use  or , there is no need for either  or  for that.For checking against None,  and  are recommended. This allows you to distinguish it from False (or things that evaluate to False, like  and ).Whether  should return  would depend on the context. You might have an attribute where the value is None, and you wouldn't be able to do that. I would interpret  as meaning \"unset\", and a  would mean the key does not exist in the file.If checking for truth:for false:for none:for non-none:For  the correct behaviour is  to return  but raise an  error instead - unless your class is something like Concerning whether to raise an exception or return : it depends on the use case.  Either can be pythonic.  Look at python's  class for example,  hooks into  and it raises a  if key is not present.  But  method returns the second argument (which is defaulted to ) if key is not present.    The most important thing to consider is to document that behaviour in the docstring, and make sure that your  method does what it says it does. To address your other questions, use these conventions:Functions that return  or  should probably have a name that makes this obvious to improve code readabilityIn python3 you can \"type-hint\" that:You can directly check that variable contains value or not like  or .In the examples in , I have seen that  or  are being used instead of  or . Also using  is recommended in this document instead of  or . So I think if this is the official Python way, we Python guys should go on this way, too.Regards.In the case of your fictional  function, if the requested attribute always should be available but isn't then throw an error. If the attribute is optional then return .for True, not None:   for false, None:One thing to ensure is that nothing can reassign your variable. If it is not a boolean in the end, relying on truthiness will lead to bugs. The beauty of conditional programming in dynamically typed languages :).The following prints \"\".Now let's change .Now the statement prints \"\" because the string is truthy.This statement, however, correctly outputs \"\"."},
{"body": "I'm using flask for my application. I'd like to send an image (dynamically generated by PIL) to client without saving on disk.Any idea how to do this ? First, you can save the image to a  and remove the local file (if you have one):Second, set the temp file to the response (as per ):Here's a version without any temp files and the like (see ):To use in your code simply doIt turns out that flask provides a solution (rtm to myself!):I was also struggling in the same situation. Finally, I have found its solution using a WSGI application, which is an acceptable object for \"make_response\" as its argument.Please replace \"opening image\" operations with appropriate PIL operations."},
{"body": "I want to add a debug print statement test, if I enable  from the command line and if I have the following in the script.I went through the following questions, but couldn't get the answer...You need to combine the wisdom of the  with . Here's an example...Run the help:Running in verbose mode:Running silently:I find both  (for users) and  (for developers) useful.  Here's how I do it with  and :So if  is set, the logging level is set to .  If , logging is set to .  If neither, the lack of  sets the logging level to the default of .Here is a more concise method, that does bounds checking, and will list valid values in help:Usage:Another variant would be to count the number of  and use the count as an index to the a  with the actual levels from :This works for , , , ,  , etc,  If no  then  is selected if more  are provided it will step to  and You can explicity specify a level as an integer after the  flag:if you want to enable logging.DEBUG level for a script you don't want to (or cannot) edit, you can customize your startup:enter the following:then you can just:from Paul Ollis at Here's another take on having argparse count the -v option to increase verbosity up two levels from the default WARNING to INFO (-v) to DEBUG (-vv). This does not map to the constants defined by  but rather calculates the value directly, limiting the input:"},
{"body": "I feel like I spend a lot of time writing code in Python, but not enough time creating Pythonic code.  Recently I ran into a funny little problem that I thought might have an easy, idiomatic solution.  Paraphrasing the original, I needed to collect every sequential pair in a list.  For example, given the list , I wanted to compute .I came up with a quick solution at the time that looked like translated Java.  Revisiting the question, the best I could do waswhich has the side effect of tossing out the last number in the case that the length isn't even.Is there a more idiomatic approach that I'm missing, or is this the best I'm going to get?This will do it a bit more neatly:(but it's arguably less readable if you're not familiar with the \"stride\" feature of ranges).Like your code, it discards the last value where you have an odd number of values.The one often-quoted is:I usually copy the  recipe from the  documentation into my code for this.How about using the step feature of :The right thing is probably not to compute lists, but to write an iterator->iterator function. This is more generic -- it works on every iterable, and if you want to \"freeze\" it into a list, you can use the \"list()\" function.I'm surprised the itertools module does not already have a function for that -- perhaps a future revision. Until then, feel free to use the version above :) is a well-built library with many functional programming niceties overlooked in itertools.  solves this (with an option to pad the last entry for lists of odd length)try thisSo,\ngivesIf you don't want to lose elements if their number in list is not even try this:"},
{"body": "The first  works and then the second time around it comes up with an error:Does anyone know why this is?@Greg Hewgill: I've already tried that and I get the error:This is the problem:You are redefining what  means.  is the built-in Python name of the string type, and you don't want to change it. Use a different name for the local variable, and remove the  statement.While not in your code, another hard-to-spot error is when the  character is missing in an attempt of string formatting:but it should be:The missing  would result in the same .In my case I had a class that had a method and a string property of the same name, I was trying to call the method but was getting the string property.Another case of this: Messing with the  function of an object where a  call fails non-transparently.In our case, we used a  decorator on the  and passed that object to a .  The  decorator causes the  object to be turned into a string, which then results in the  object is not callable error.I had the same error. In my case wasn`t because of a variable named str. But because i named a function with a str parameter and the variable the same. .I run it in a loop. The first time it run ok. The second time i got this error. Renaming the variable to a name different from the function name fixed this. So I think it\u00b4s because Python once associate a function name in a scope, the second time tries to associate the left part ( same_name =) as a call to the function and detects that the str parameter is not present, so it's missing, then it throws that error.In my case, I had a Class with a method in it. The method did not have 'self' as the first parameter and the error was being thrown when I made a call to the method. Once I added 'self,' to the method's parameter list, it was fine.In my case if I had defined  as a variable and due to same faced . Post that I changed the function name to . Then I stopped the kernel and restarted. Still it didn't work out. Post that I restarted the kernel with option of Restart the Kernel and Clear all outputs. And immediately all codes ran successfully. Thanks for your input.  "},
{"body": "Why does this happen? If there are only 5 characters in that string, why is the count function returning one more?Also, is there a more effective way of counting characters in a string? returns how many times an object occurs in a list, so if you count occurrences of  you get 6 because the empty string is at the beginning, end, and in between each letter.Use the  function to find the length of a string.That is because there are six different substrings that are the empty string: Before the 1, between the numbers, and after the 5. If you want to count characters use  instead:How many pieces do you get if you cut a string five times?The same thing is happening here. It counts the empty string after the  also. is what you should use.The most common way is to use . It returns the number of characters in a given string - in this case 5.Count and Len are two very different things. Len simply prints the length of the string (hence the name 'Len'), while Count iterates through the string or list and gives you the number of times an object occurs, which counts the beginning and end of the string as well as in between each letter.It's the same reason why it makes sense for  to return , not ."},
{"body": "I made some weird observations that my GridSearches keep failing after a couple of hours and I initially couldn't figure out why. I monitored the memory usage then over time and saw that it it started with a few gigabytes (~6 Gb) and kept increasing until it crashed the node when it reached the max. 128 Gb the hardware can take. \nI was experimenting with random forests for classification of a large number of text documents. For simplicity -- to figure out what's going on -- I went back to naive Bayes.The versions I am using are I found some related discussion on the scikit-issue list on GitHub about this topic:  and\nAnd it sounds like it was already successfully addressed!So, the relevant code that I am using isJust out of curiosity, I later decided to do the grid search the quick & dirty way via nested for loopSo far so good. The grid search runs and writes the results to stdout. However, after some time it exceeds the memory cap of 128 Gb again. Same problem as with the GridSearch in scikit. After some experimentation, I finally found out that in the for loop solves the problem and the memory usage stays constantly at 6.5 Gb over the run time of ~10 hours.Eventually, I got it to work with the above fix, however, I am curious to hear your ideas about what might be causing this issue and your tips & suggestions!RandomForest in 0.15.2 does not support sparse inputs.Upgrade sklearn and try again...hopefully this will allow the multiple copies that end up being made to consume way less memory. (and speed things up)I can't see your exact code, but I faced similar problem nowadays.\nIt is worth a try.\nThe similar memory blow-up easily could happen when we copy values from a mutable array or list like object to an other variable creating a copy of the original one and then we modify the new array or list with append or something similar increasing the size of it and the same time increasing the original object too in the background. So this is an exponential process, so after some time we are out of memory. I was be able to and maybe you can avoid this kind of phenomenon with  the original object at a value passing.I had the similar problem, I blew-up the memory with a similar process, then I managed to stay at 10% memory load.\nNow I see the snippet of the code with pandas DataFrame. There would be such a valuecopy issue easily.I'm not familiar with GridSearch sir, but I'd suggest when memory and huge lists are an issue write a small custom generator. It can be reused for all your items, just use one that takes any list.  If implementing beyond the lower solution here first read this article, best generator article I've found. I typed it all in and went piece by piece, any questions you have after reading it I can try tooDon't need:\nTryThe 'yield' word (anywhere in the declaration) makes this a generator, not a regular function. This runs through and says i equals 0, while True I gotta do stuff, they want me to yield this_list[0], here you go I'll wait for you at  if you need me again. The next time it is called, it picks up and does , and notices it's still in a while loop and gives this_list[1], and records its location ( again...it will wait there until called again). Notice as I feed it the list once and make a generator (x here), it will exhaust your list. Let's see if we can use it in a for:Nope. Let's try to handle that:That works. Now it won't blindly try to return a list element even if it isn't there. From what you said, I can almost guarantee you'll need to be able to seed it (pick up from a certain place, or start freshly from a certain place): is a thing that looks for seed, but seed defaults to None so will usually just start at the logical place, 0, the beginning of the listHow can you use this beast without using (almost) any memory?that looks familiar huh? Now you can process a trillion values one by one, picking important ones to write to disk, and never blowing up your system. Enjoy!"},
{"body": "I have been trying to use both the Python implementation (opencv 2.4.11) and the Java implementation (opencv 2.4.10) of OpenCV's MSER algorithm. Interestingly, I've noticed that MSER's detect returns different types of output in Python vs Java. In Python, detect returns a list of lists of points, where each list of points represents a blob detected. In Java, a  is returned, where each row is a single point with an associated diameter representing a blob detected. I would like to reproduce the Python behavior in Java, where blobs are defined by a set of points, not one point. Anyone know what's going on?Python:  Python output:  Java:Java output:A mod on the answers.opencv.org forum provided a bit more information ():You are using two different interfaces to the MSER implementation. Python  gives you a wrapped , which exposes its  to Python as :This gives you the nice list of contours interface you are looking for.In contrast Java uses the  wrapper which calls  which is backed by  and uses the standard FeatureDetector interface: a list of KeyPoints.If you want to access the  in Java (in OpenCV 2.4) you will have to wrap it up in JNI."},
{"body": "My goal is to write an interactive variable viewer for IPython, i.e. which allows one to see, say nested dicts/lists as a tree, and drill down (a bit like the console.log in Javascript).I spent a lot of time trying to extract minimalistic code out of the directview example in IPython but still can't get my head around how it all works. My Python's ok but my jQuery experience is quite limited.So I got to stripping down directview.js to the followingThis works when I load the directview.ipynb. However I am still not clear how to make it completely standalone (i.e. not require the directview.py, and pass another callback than the standard  of ). One of the issues is the  which fails when called in a standalone way.I am lost on several aspects:I appreciate that the authors of the IPython notebook have done an incredible job creating such a beautiful front-end using jQuery/websockets, and that creating developer documentation that allows beginners like me to tweak with it is asking much, but if anyone can lend a hand that would be great!I can answer to your second question.  The fact is when JavaScript calls your callback, it makes it so without specifying the context, i.e. without setting  (Pythonistas call it ).  But it's possible to bound a function to  via , which you saw in:"},
{"body": "I'm trying to implement a custom scoring function for RidgeClassifierCV in scikit-learn.  This involves passing a custom scoring function as the  when initializing the  object.  I expected the score_func to take in categorical values as input for  and .  Instead, however, floating point values are passed in as  and .  The size of the y vectors is equal to the number of classes times the number of training examples, rather than simply having a y vector with length equivalent to the number of training examples. Can I somehow force categorical predictions to be passed into the custom scoring function, or do I have to deal with the raw weights?  If I do have to deal directly with the raw weights, is the index of the maximum value in a slice of the vector of outputs equivalent to the predicted class?This is a  that has been fixed."},
{"body": "I'd like to insert a couple small graphics (vector graphics but can be made raster if necessary) into the legend of a maplotlib plot.  There would be one graphic per item in the legend.I know I could manually draw the entire legend using  but that looks tedious, and any small change in the figure would require fixing it by hand.Is there any way to include graphics in the label in a call to  or later in the  call?So, the below is a little hacky, but it can get you most of the way there. Note: you need to replace  with the image you want (otherwise you get Grace Hopper for free!). You can also make the image larger than the default by passing the  parameter. This is the hacky way to fix your aspect ratio on the image. Use the  parameter if your images overlap from one series to the next.Here's what it produces:"},
{"body": "This is a follow up to . I have updated the setup and tests, and don't want to change the original question too much.The whole story (including scripts etc) is on Short version: while trying to verify the performance claims made in the 'Graph Database' book I came to the following results (querying a random dataset containing n people, with 50 friends each):\"*\": single run only\"*\": single run onlyUsing 1.9.2 on a 64bit ubuntu  I have setup neo4j.properties with these values:and neo4j-wrapper.conf with:My query to neo4j looks like this (using the REST api):Node_auto_index is in place, obviouslyAnd also there is  with same problem.I'm sorry you can't reproduce the results. However, on a MacBook Air (1.8 GHz i7, 4 GB RAM) with a 2 GB heap, GCR cache, but no warming of caches, and no other tuning, with a similarly sized dataset (1 million users, 50 friends per person), I repeatedly get approx 900 ms using the Traversal Framework on 1.9.2:Cypher is slower, but nowhere near as slow as you suggest: approx 3 seconds:Kind regardsianYes, I believe the REST API is significantly slower than the regular bindings and therein lies your performance problem."},
{"body": "I get different results (test accuracy) every time I run the  example from Keras framework ()\nThe code contains  in the top, before any keras imports. It should prevent it from generating different numbers for every run. What am I missing?  UPDATE: How to repro:  UPDATE2: I'm running it on Windows 8.1 with MinGW/msys, module versions:\ntheano 0.7.0\nnumpy 1.8.1\nscipy 0.14.0c1UPDATE3: I narrowed the problem down a bit. If I run the example with GPU (set theano flag device=gpu0) then I get different test accuracy every time, but if I run it on CPU then everything works as expected. My graphics card: NVIDIA GeForce GT 635)Theano's  talks about the difficulties of seeding random variables and why they seed each graph instance with its own random number generator. They also provide  on how to seed all the random number generators. I finally got reproducible results with my code. It's a combination of answers I saw around the web. The first thing is doing what @alex says:Then you have to solve the issue noted by @user2805751 regarding cuDNN by calling your Keras code with the following additional :And finally, you have to patch your Theano installation as per , which basically consists in:This should get you the same results for the same seed.Note that there might be a slowdown. I saw a running time increase of about 10%.I would like to add something to the previous answers. If you use  and you want to get reproducible results for every run, you have toI agree with the previous comment, but reproducible results sometimes needs the same environment(e.g. installed packages, machine characteristics and so on). So that, I recommend to copy your environment to other place in case to have reproducible results. Try to use one of the next technologies:I have trained and tested  kind of neural networks using Keras. I performed non linear regression on noisy speech data. I used the following code to generate random seed : I get the exact same results of  each time I train and test on the same data. "},
{"body": "I am trying to get Oauth working with the Google API using Python. I have tried different oauth libraries such as ,  and  but I cannot get it to work (including the provided examples).For debugging Oauth I use Google's  and I have studied the  and the With some libraries I am struggling with getting a right signature, with other libraries I am struggling with converting the request token to an authorized token. What would really help me if someone can show me a working example for the Google API using one of the above-mentioned libraries. EDIT: My initial question did not lead to any answers so I have added my code. There are two possible causes of this code not working:\n1) Google does not authorize my request token, but not quite sure how to detect this\n2) THe signature for the access token is invalid but then I would like to know which oauth parameters Google is expecting as I am able to generate a proper signature in the first phase.This is written using oauth2.py and for Django hence the HttpResponseRedirect.I have OAuth working in a python App Engine app:The app is running at:This work for me.Have you tried the official gdata python api ?\nIt ships with an oauth client and hides the complexity of oauth calls.\nThis may be the answer. When calling OAuthGetRequestToken you sign the base_string with your consumer_secret followed by an & (ampersand)When calling OAuthGetAccessToken you sign the base_string with your consumer_secret followed by an & (ampersand) followed by token_secret.You would sign the base_string using (consumer_secret + \"&\") for OAuthGetRequestToken and \nyou would sign the base_string using (consumer_secret + \"&\" + token_secret) for OAuthGetAccessTokenTornado has working code for Google oauth. Check it out here. . I 've used it and worked pretty well out of the box. All you need to do is pluck out the class and carefully put it into a django view. PS: Tornado makes use of async module for the user to return. Since you are using django you need to rely on some get variable to identify that a user has just granted access to your application.IIRC Google oauth is not quite following the standard, you  to specify what service you're requesting for (look at the examples provided in the google docs) in the request as an additional parameter, or it won't work."},
{"body": "I am trying to work on a script that manipulates another script in Python, the script to be modified has structure like:I use  to locate the  line number, and I use some code to change the original file with new description string base on the line number. So far so good.Now the only issue is  occasionally is a multi-line string, e.g.orand I only have the line number of the first line, not the following lines. So my one-line replacer would doand the code is broken. I figured that if I know both the lineno of start and end/number of lines of  assignment I could repair my code to handle such situation. How do I get such information with Python standard library?I looked at the other answers; it appears people are doing backflips to get around the problems of computing line numbers, when your real problem is one of modifying the code.   That suggests the baseline machinery is not helping you the way you really need.If you use a , you could avoid a lot of this nonsense.A good PTS will parse your source code to an AST, and then let you apply source-level rewrite rules to modify the AST, and will finally convert the modified AST back into source text.   Generically PTSes accept transformation rules of essentially this form:[A parser that builds an AST is NOT a PTS.  They don't allow rules like this; you can write ad hoc code to hack at the tree, but that's usually pretty awkward.   Not do they do the AST to source text regeneration.](My PTS, see bio, called) DMS is a PTS that could accomplish this.  OP's specific example would be accomplished easily by using the following rewrite rule:The one transformation rule is given an name  to distinguish it from all the other rule we might define.   The rule parameters (e: expression) indicate the pattern will allow an arbitrary expression as defined by the source language.    means the rule maps a statement in the source language, to a statement in the target language; we could use any other syntax category from the Python grammar provided to DMS.  The  used here is a , used to distinguish the syntax of the rule language form the syntax of the subject language.  The second   separates the source pattern  from the target pattern .You'll notice that there is no need to mention line numbers.  The PTS converts the rule surface syntax into corresponding ASTs by actually parsing the patterns with the same parser used to parse the source file.  The ASTs produced for the patterns are used to effect the pattern match/replacement.   Because this is driven from ASTs, the actual layout of the orginal code (spacing, linebreaks, comments) don't affect DMS's ability to match or replace.  Comments aren't a problem for matching because they are attached to tree nodes rather than being tree nodes; they are preserved in the transformed program.  DMS does capture line and precise column information for all tree elements; just not needed to implement transformations. Code layout is also preserved in the output by DMS, using that line/column information. Other PTSes offer generally similar capabilities.As a workaround you can change:to:etc. It is a simple change but indeed ugly code produced.Indeed, the information you need is not stored in the . I don't know the details of what you need, but it looks like you could use the  module from the standard library. The idea is that every logical Python statement is ended by a  token (also it could be a semicolon, but as I understand it is not your case). I tested this approach with such file:And here is what I propose to do:This prints out:This seems to be correct, you could tune this approach depending on what exactly you need.  is more verbose than  but also more flexible. Of course the best approach is to use them both for different parts of your task. I tried this in Python 3.4, but I think it should also work in other versions.My solution takes a different path: When I had to change code in another file I opened the file, found the line and got all the next lines which had a deeper indent than the first and return the line number for the first line which isn't deeper. \nI return None, None if I couldn't find the text I was looking for.\nThis is of course incomplete, but I think it's enough to get you through :)There is a new  library that addresses this well: produces"},
{"body": "What is the correct way to work with complex numbers in Cython?I would like to write a pure C loop using a numpy.ndarray of dtype np.complex128. In Cython, the associated C type is defined in \n as so it seems this is just a simple C double complex.However, it's easy to obtain strange behaviors. In particular, with these definitionsthe linecan be compiled by Cython but gcc can not compiled the C code produced (the error is \"testcplx.c:663:25: error: two or more data types in declaration specifiers\" and seems to be due to the line ). This error has already been reported (for example ) but I didn't find any good explanation and/or clean solution.Without inclusion of , there is no error (I guess because the  is then not included).However, there is still a problem since in the html file produced by , the line  is yellow, meaning that it has not been translated into pure C. The corresponding C code is:and the  and  are orange (Python calls).Interestingly, the linedoes not produce any error and is translated into pure C (just ), whereas it is very very similar to the first one.I can avoid the error by using intermediate variables (and it translates into pure C):or simply by casting (but it does not translate into pure C):So what happens? What is the meaning of the compilation error? Is there a clean way to avoid it? Why does the multiplication of a np.complex128_t and np.float64_t seem to involve Python calls?Cython version 0.22 (most recent version in Pypi when the question was asked) and GCC 4.9.2.I created a tiny repository with the example () and a tiny Makefile with 3 targets (, , ) so it is easy to test anything.The simplest way I can find to work around this issue is to simply switch the order of multiplication.If in  I changetoI change from the failing situation to described to one that works correctly.  This scenario is useful as it allows a direct diff of the produced C code.The order of the multiplication changes the translation, meaning that in the failing version the multiplication is attempted via  types, whereas in the working version it is done via  types.  This in turn introduces the typedef line , which is invalid. I am fairly sure this is a cython bug (Update: ).  Although , the response explicitly states (in saying that it is not, in fact, a  bug, but user code error):They conclude that  is a valid type specifier whereas  is not.   has the same type of response - trying to use  on a non fundamental type is outside spec, and the  indicates that  can only be used with ,  and So - we can hack the cython generated C code to test that: replace  with   and verify that it is indeed valid and can make the output code compile.Swapping the multiplication order only highlights the problem that we are told about by the compiler. In the first case, the offending line is the one that says  - it is trying to assign the type   use the keyword  to the type . or  is a valid type, whereas  is not. To see the effect, you can just delete  from that line, or replace it with  or  and the code compiles fine.  The next question is why that line is produced in the first place...This seems to be produced by  in the Cython source code.Why does the order of the multiplication change the code significantly - such that the type  is introduced, and introduced in a way that fails?In the failing instance, the code to implement the multiplication turns  into a  type, does the multiplication on real and imaginary parts and then reassembles the complex number.  In the working version, it does the product directly via the  type using the function I guess this is as simple as the cython code taking its cue for which type to use for the multiplication from the first variable it encounters.  In the first case, it sees a float 64, so produces () C code based on that, whereas in the second, it sees the (double) complex128 type and bases its translation on that.  This explanation is a little hand-wavy and I hope to return to an analysis of it if time allows...A note on this -  that the  for  is , so in this particular case, a fix might consist of modifying  to use  where  is , but this is getting beyond the scope of a SO answer and doesn't present a general solution.Creates this C code from the line `varc128 = varf64 * varc128Creates this C code from the line Which necessitates these extra imports - and the offending line is the one that says  - it is trying to assign the type   the type  to the type "},
{"body": "Assuming we have a polygon coordinates as polygon = [(x1, y1), (x2, y2), ...], the following code displays the polygon:By default it is trying to adjust the aspect ratio so that the polygon (or whatever other diagram) fits inside the window, and automatically changing it so that it fits even after resizing. Which is great in many cases, except when you are trying to estimate visually if the image is distorted. How to fix the aspect ratio to be strictly 1:1? (Not sure if \"aspect ratio\" is the right term here, so in case it is not - I need both X and Y axes to have 1:1 scale, so that (0, 1) on both X and Y takes an exact same amount of screen space. And I need to keep it 1:1 no matter how I resize the window.)Does it help to use:There is, I'm sure, a way to set this directly as part of your plot command, but I don't remember the trick.  To do it after the fact you can use the current axis and set it's aspect ratio with \"set_aspect('equal')\". In your example:I use this all the time and it's from the examples on the matplotlib website.The best thing to use is:As  Saullo Castro said. Because with equal you can't change one axis limit without changing the other so if you want to fit all non squered figures you will have a lot of white space.Better , it works better if you want to modify the axes with  and ."},
{"body": "I tried to install python dateutil for my django tastypie but unsuccessful,I downloaded the tar file in c:/python27 and unzipped it,I get the following error msg,This will install  and its dependencies:To get only dateutil:I am not sure if this is different on Windows, but it does not appear you are referencing an actual link (see the  line). Instead, try this:That will (hopefully) get the package you need. Also, see  post for what looks like a similar issue."},
{"body": "Say I have this list:As far as help showed me, there is not a builtin function that returns the last occurrence of a string (like the reverse of ). So basically, how can I find the last occurrence of  in the given list?If you are actually using just single letters like shown in your example, then  would work nicely.  It will return  if 'a' is not in the list.For the general case you could use: It will raise  if  is not in the list.  The slicing here creates a  of the entire list.  That's fine for short lists, but for the case where  is very large, performance can be better with a lazy approach:A one-liner that's like Ignacio's except a little simpler/clearer would beIt seems very clear and Pythonic to me: you're looking for the highest index that contains a matching value. No nexts, lambdas, reverseds or itertools required. Many of the other solutions require iterating over the entire list.  This does not.Edit: In hindsight this seems like unnecessary wizardry.  I'd do something like this instead:I like both  and  answers. However, I think  provides a slightly more readable alternative, lambda notwithstanding. (For Python 3; for Python 2, use  instead of ).This will raise a  exception if the item isn't found; you could catch that and raise a  instead, to make this behave just like .Defined as a function, avoiding the  shortcut:It works for non-chars too. Tested:Use a simple loop:"},
{"body": "Is there a way to slice only the first and last item in a list?For example; If this is my list:I  to do this (obviously  is not valid syntax):Some things I have tried:One way:A better way (Doesn't use slicing, but is easier to read):Just thought I'd show how to do this with numpy's fancy indexing:Note that it also supports arbitrary index locations, which the  method would not work for:As DSM points out, you can do something similar with :You can do it like this:What about this?Some people are answering the wrong question, it seems.  You said you want to do:Ie., you want to extract the first and last elements each into separate variables.In this case, the answers by Matthew Adams, pemistahl, and katrielalex are valid.  This is just a compound assignment:But later you state a complication: \"I am splitting it in the same line, and that would have to spend time splitting it twice:\"So in order to avoid two split() calls, you must only operate on the list which results from splitting once.In this case, attempting to do too much in one line is a detriment to clarity and simplicity.  Use a variable to hold the split result:Other responses answered the question of \"how to get a new list, consisting of the first and last elements of a list?\"  They were probably inspired by your title, which mentions slicing, which you actually don't want, according to a careful reading of your question.AFAIK are 3 ways to get a new list with the 0th and last elements of a list:The advantage of the list comprehension approach, is that the set of indices in the tuple can be arbitrary and programmatically generated.Actually, I just figured it out:You can use something like `if you really want to use slicing. The advantage of this is that it cannot give index errors and works with length 1 or 0 lists as well.This isn't a \"slice\", but it is a general solution that doesn't use explicit indexing, and works for the scenario where the sequence in question is anonymous (so you can create and \"slice\" on the same line, without creating twice and indexing twice): You could just inline it as (after  for brevity at time of use):but if you'll be reusing the getter a lot, you can save the work of recreating it (and give it a useful, self-documenting name) by creating it once ahead of time.Thus, for your specific use case, you can replace:with:and  only once without storing the complete  in a persistent name for  checking or double-indexing or the like.Note that  for multiple items returns a , not a , so if you're not just unpacking it to specific names, and need a true , you'd have to wrap the call in the  constructor.Python 3 only answer (that doesn't use slicing or throw away the rest of the , but might be good enough anyway) is use unpacking generalizations to get  and  separate from the middle:The choice of  as the catchall for the \"rest\" of the arguments is arbitrary; they'll be stored in the name  which is often used as a stand-in for \"stuff I don't care about\".Unlike many other solutions, this one will ensure there are at least two elements in the sequence; if there is only one (so  and  would be identical), it will raise an exception ().The answers work for the specific first and last, but some, like myself, may be looking for a solution that can be applied to a more general case in which you can return the top N points from either side of the list (say you have a sorted list and only want the 5 highest or lowest), i came up with the following solution:and an example to return bottom and top 3 numbers from list 1-10:"},
{"body": "I was running some dynamic programming code (trying to brute-force disprove the Collatz conjecture =P) and I was using a dict to store the lengths of the chains I had already computed. Obviously, it ran out of memory at some point. Is there any easy way to use some variant of a  which will page parts of itself out to disk when it runs out of room? Obviously it will be slower than an in-memory dict, and it will probably end up eating my hard drive space, but this could apply to other problems that are not so futile.I realized that a disk-based dictionary is pretty much a database, so I manually implemented one using sqlite3, but I didn't do it in any smart way and had it look up every element in the DB one at a time... it was about 300x slower.Is the smartest way to just create my own set of dicts, keeping only one in memory at a time, and paging them out in some efficient manner?Hash-on-disk is generally addressed with Berkeley DB or something similar - several options are listed in the . You can front it with an in-memory cache, but I'd test against native performance first; with operating system caching in place it might come out about the same.The 3rd party  module is also worth taking a look at. It's very similar to shelve in that it is a simple dict-like object, however it can store to various backends (such as file, SVN, and S3), provides optional compression, and is even threadsafe. It's a very handy moduleLast time I was facing a problem like this, I rewrote to use SQLite rather than a dict, and had a massive performance increase. That performance increase was at least partially on account of the database's indexing capabilities; depending on your algorithms, YMMV.A thin wrapper that does SQLite queries in  and  isn't much code to write.The  module may do it; at any rate, it should be simple to test.  Instead of:do:The only catch is that keys to shelves must be strings, so you'll have to replacewith(I'm assuming your keys are just integers, as per your comment to Charles Duffy's post)There's no built-in caching in memory, but your operating system may do that for you anyway.[actually, that's not quite true: you can pass the argument 'writeback=True' on creation.  The intent of this is to make sure storing lists and other mutable things in the shelf works correctly.  But a side-effect is that the whole dictionary is cached in memory.  Since this caused problems for you, it's probably not a good idea :-) ]With a little bit of thought it seems like you could get the  to do what you want.I've read you think shelve is too slow and you tried to hack your own dict using sqlite.Another did this too :It seems pretty efficient (and sebsauvage is a pretty good coder). Maybe you could give it a try ?read answer for this question from GvR ;)\nYou should bring more than one item at a time if there's some heuristic to know which are the most likely items to be retrieved next, and don't forget the indexes like Charles mentions.I did not try it yet but  is promising and has a Python interface."},
{"body": "I'm trying to center a tkinter window. I know I can programatically get the size of the window and the size of the screen and use that to set the geometry, but I'm wondering if there's a simpler way to center the window on the screen.You can try to use the methods  and , which return respectively the width and height (in pixels) of your  instance (window), and with some basic math you can center your window:I am calling  method before retrieving the width and the height of the window in order to ensure that the values returned are accurate.The general approach to centering a window is to calculate the appropriate screen coordinates for the window's top left pixel:  However, this is  sufficient for  centering a tkinter window (on Windows 7 at least);\nbecause the window's width and height returned by  method will not include the outermost frame, with the title and min/max/close buttons.\nIt will also not include a   (with File, Edit, etc.). Fortunately  to find the dimensions of these.Here is the most basic function, which does not consider the aforementioned issue:Alternatives: , First, and foremost, we want to call the window's  method\ndirectly before retrieving any geometry, to ensure that the values returned are accurate.It's important to understand the  used with the  method.\nThe first half is the window's width and height  the outer-frame,\nand the second half is the outer-frame's top left x and y coordinates.There are four methods that will allow us to determine the outer-frame's dimensions.\n will give us the window's top left x coordinate,  the outer-frame.\n will give us the outer-frame's top left x coordinate.\nTheir difference is the outer-frame's width.The difference between  and  will be our title-bar / menu-bar's height.Here is the complete function, in a working example:One way to prevent seeing the window move across the screen is to use\n to make the window fully transparent and then set it to  after the window has been centered. Using  or  later followed by  doesn't seem to work well, for this purpose, on Windows 7. Note that I use  as a trick to activate the window.Tk provides a helper function that can do this as , but I don't believe it has been exposed as a wrapped method in Tkinter. You would center a widget using the following:This function should deal with multiple displays correctly as well. It also has options to center over another widget or relative to the pointer (used for placing popup menus), so that they don't fall off the screen.I use frame and expand option. Very simple. I want some buttons in the middle of screen. Resize window and button stay in the middle. This is my solution."},
{"body": "I tried using , but some numbers were the same. Is there a method/module to create a list unique random numbers?This will return a list of 10 numbers selected from the range 0 to 99, without duplicates.With reference to your specific code example, you probably want to read all the lines from the file  and then select random lines from the saved list in memory. For example:This way, you only need to actually read from the file once, before your loop. It's much more efficient to do this than to seek back to the start of the file and call  again for each loop iteration.Why not create a list of 1..100 and shuffle it with  algorithm?'s works (+1), but it could become problematic with memory if the sample size is small, but the population is huge (eg ). To fix that, I would go with this:If the list of N numbers from 1 to N is randomly generated, then yes, there is a possibility that some numbers may be repeated.If you want a list of numbers from 1 to N in a random order, fill an array with inegeters 1 to N, and then use a .: as @Greg points out: since this is Python, use You can use the  function from the  module like this:Note here that the shuffle method doesn't return any list as one may expect, it only shuffle the list passed by reference.If you need to sample extremely large numbers, you cannot use  because it throws:Also, if  cannot produce the number of items you want due to the range being too smallit throws:This function resolves both problems:Usage with extremely large numbers:Sample result:Usage where the range is smaller than the number of requested items:Sample result:It also works with with negative ranges and steps:Sample results:If you wish to ensure that the numbers being added are unique, you could use a if using 2.7 or greater, or import the sets module if not.As others have mentioned, this means the numbers are not truly random.From the CLI in win xp: In Canada we have the 6/49 Lotto. I just wrap the above code in lotto.bat and run  or just .Because  often repeats a number, I use  with  and then shorten it to a length of 6.Occasionally if a number repeats more than 2 times the resulting list length will be less than 6.EDIT: However,  is the correct way to go."},
{"body": "I'm looking for a clean way to use variables within a Python multiline string. Say I wanted to do the followingIn a way I'm looking to see if there is a Perl like $ to indicate a variable in the Python syntax.If not - what is the cleanest way I can achieve this multiline string with variables.The common way is the  function:You can also pass a dictionary with variables:The closest thing to what you asked (in terms of syntax) are . For example:I should add though that the  function is more common because it's readily available and it does not require an import line.You probably could have answered this one with a little bit of Googling, but here's the code you were looking for. Note that I corrected your syntax on strings.Some reading to learn more about Python string formatting:That what you want:A dictionary can be passed to , each key name will become a variable for each associated value.\nAlso a list can be passed to , the index number of each value will be used as variables in this case.\nBoth solutions above will output the same:I'm will go there\nI will go now\ngreatI think that the answer above forgot the {}:   "},
{"body": "I have two numpy arrays that define the x and y axes of a grid.  For example:I'd like to generate the Cartesian product of these arrays to generate:In a way that's not terribly inefficient since I need to do this many times in a loop.  I'm assuming that converting them to a Python list and using  and back to a numpy array is not the most efficient form.See  for a general solution for computing the Cartesian product of N arrays.Another approach that tests a bit faster for me is to use  + : I did a few tests; see the end of this post for a very simple, general solution that performs very well, if not always optimally, for all inputs. Definitions: is a tad faster for small arrays:And a bit faster yet for large arrays:For smaller arrays it's also faster than :But very large arrays, it doesn't do quite as well:Then there's a generalized version that should work on arbitrary-dimensional products. This is as fast or faster than  for all inputs that I tried:It beats both  and  for very large products:Finally, here's a  simplified approach that performs similarly to the above -- sometimes a bit faster, sometimes a bit slower, but never different by more than 50%. (This is based on ideas from ):You can just do normal list comprehension in pythonwhich should give youMore generally, if you have two 2d numpy arrays a and b, and you want to concatenate every row of a to every row of b (A cartesian product of rows, kind of like a join in a database), you can use this method:"},
{"body": "can anyone please explain what is wrong with this code?The output I got is:but I expected it to print  instead.When you say  you are stripping the last element. Instead of slicing the string, you can apply  and  on the string object itself like thisSo the whole program becomes like this    Even simpler, with a conditional expression, like thisYou are testing against the string :Note how the last character, the , is not part of the output of the slice.I think you wanted just to test against the last character; use  to slice for just the last element.However, there is no need to slice here; just use  and  directly.You should either useorbut not slice and check startswith/endswith together, otherwise you'll slice off what you're looking for...When you set a string variable, it doesn't save quotes of it, they are a part of its definition.\nso you don't need to use :1"},
{"body": "I'm trying to remove an item from a list in python:But it doesn't remove  item. Any ideas?You can't remove items from a list while iterating over it.  It's much easier to build a new list based on the old one:hymloth and sven's answers work, but they do not modify the list (the create a new one). If you need the object modification you need to assign to a slice:However, for large lists in which you need to remove few elements, this is memory consuming, but it runs in O(n). glglgl's answer suffers from O(n\u00b2) complexity, because  is O(n). Depending on the structure of your data, you may prefer noting the indexes of the elements to remove and using the  keywork to remove by index:Now  is also O(n) because you need to copy all elements after index  (a list is a vector), so you'll need to test this against your data. Still this should be faster than using  because you don't pay for the cost of the search step of remove, and the copy step cost is the same in both cases. [edit] Very nice in-place, O(n) version with limited memory requirements, courtesy of . It uses  which was introduced in python 2.7:This stems from the fact that on deletion, the iteration skips one element as it semms only to work on the index.Workaround could be:The already-mentioned list comprehension approach is probably your best bet. But if you absolutely want to do it in-place (for example if  is really large), here's one way:"},
{"body": "Python Version 2.7I am looking for documentation  examples on how to extract text from a PDF file using PDFMiner with Python.It looks like PDFMiner updated their API and all the relevant examples I have found contain outdated code(classes and methods have changed).  The libraries I have found that make the task of extracting text from a PDF file easier are using the old PDFMiner syntax so I'm not sure how to do this.As it is, I'm just looking at source-code to see if I can figure it out. Here is a working example of extracting text from a PDF file using the current version of PDFMiner(September 2016) PDFMiner's structure changed recently, so this should work for extracting text from the PDF files. : Still working as of the 1st of February of 2017.terrific answer from DuckPuncher, this is for Python3:"},
{"body": "I have a list containing URLs with escaped characters in them. Those characters have been set by  when it recovers the html page:Is there a way to transform them back to their unescaped form in python?P.S.: The URLs are encoded in utf-8And then just decode.And if you are using  you could use:or You can use "},
{"body": "I am using Python to write chunks of text to files in a single operation:If the script is interrupted so a file write does not complete I want to have no file rather than a partially complete file. Can this be done?Write data to a temporary file and when data has been successfully written, rename the file to the correct destination file e.gAccording to doc alsoNote: A simple snippet that implements atomic writing using Python .or even reading and writing to and from the same file:using two simple context managersThere is a simple AtomicFile helper: I\u2019m using this code to atomically replace/write a file:Usage:It\u2019s based on .Since it is very easy to mess up with the details, I recommend using a tiny library for that. The advantage of a library is that it takes care all these nitty-gritty details, and is being  by a community.One such library is  by  which even has proper Windows support:From the README:"},
{"body": "I am trying to install and use the Evernote module () . I ran  and it says that the installation worked.I can confirm that the evernote module exists in . However, when I try to run  I get the following error:This is the contents of my :I am having this same problem with other modules installed with . Help?EDIT: I am a super newbie and have not edited that  file. EDIT:  Outputs the following: I seemed to have made progress towards a solution by adding  to my  file. However, now when I run  it tries to import oauth2, which fails with the same error. The ouath2 module is present in the module directory. is the executable for the python that comes with OS X.  is a location for user-installed programs only, possibly from Python.org or Homebrew. So you're mixing different Python installs, and changing the python path is only a partial workaround for different packages being installed for different installations.In order to make sure you use the  associated with a particular python, you can run , or go look at what the  on your path is, or is symlinked to.I figured it out! I added this line:to my  and now I can import modules stored in that directory. Thanks for everyone who  answered.None of this helped me with my similar problem. Instead, I had to fix the newly installed files  to be able to import. This is usually an obvious thing, but not so much when you use  when installing module/packages.I faced similar problem,its related to /usr/local/lib/python2.7/site-packages had no read or write permission for group and other, and they were owned by root. This means that only the root user could access them. Try this:Simply just type in terminal:and type  (whatever you like) or type  (whatever you like)  (whatever you like)."},
{"body": "I have made a custom profile model which looks like this:But when I run , I get:I also tried:But it gives the same error. Where I'm wrong and how to fix this?Change this:to this:Exactly in Django 1.5 the  setting , allowing using a custom user model with auth system.If you're writing an app that's intended to work with projects on Django 1.5 through 1.10 and later, this is the proper way to reference user model (which can now be different from ):In case you're writing a reusable app supporting Django 1.4 as well, then you should probably determine what reference to use by checking Django version, perhaps like this:"},
{"body": "I'm developing a website (in Django) that uses OpenID to authenticate users. As I'm currently only running on my local machine I can't authenticate using one of the OpenID providers on the web. So I figure I need to run a local OpenID server that simply lets me type in a username and then passes that back to my main app.Does such an OpenID dev server exist? Is this the best way to go about it?The libraries at  ship with examples that are sufficient to run a local test provider.  Look in the examples/djopenid/ directory of the python-openid source distribution.  Running that will give you an instance of .I have no problems testing with . I thought there would be a problem testing on my local machine but it just worked. (I'm using ASP.NET with DotNetOpenId library).The 'realm' and return url must contain the port number like ''.I assume it works OK because the provider does a client side redirect.I'm also looking into this.  I too am working on a Django project that might utilize Open Id.  For references, check out:Hopefully someone here has tackled this issue.I'm using  to authenticate at StackOverflow right now. Generates a standard HTTP auth realm and works perfectly. It should be exactly what you need.You could probably use the django OpenID library to write a provider to test against. Have one that always authenticates and one that always fails.Why not run an OpenID provider from your local machine?If you are a .Net developer there is an OpenID provider library for .Net at . This uses the standard .Net profile provider mechanism and wraps it with an OpenID layer. We are using it to add OpenID to our custom authentication engine.If you are working in another language/platform there are a number of OpenID implementation avalaiable from the OpenID community site .You shouldn't be having trouble developing against your own machine. What error are you getting?An OpenID provider will ask you to give your site (in this case  or similar) access to your identity. If you click ok then it will redirect you that url. I've never had problems with  and I expect that  will work too.If you're having problems developing locally I suggest that the problem you're having is unrelated to the url being localhost, but something else. Without an error message or problem description it's impossible to say more.: It turns out that Yahoo do things differently to other OpenID providers that I've come across and disallow redirections to ip address, sites without a correct tld in their domain name and those that run on ports other than 80 or 443. See  for a post from a Yahoo developer on this subject.  offers a work around, but I would suggest that for development myopenid.com would be far simpler than working around Yahoo, or running your own provider."},
{"body": "I'm trying to learn a little about python scripting in GVim, but I'm having trouble with starting. Elementary things, really.Are there any resources / tutorials concerting python scripting in Vim out there ? Simple examples which show the basic principles, would be welcomed also.Here is all I know of in terms of such docs::help python-vim is a good start. The best vim resource is always at your fingertips and the sooner you get used to referring to it the better you will get at vim overall.I got better at searching vim help with this..also :he vim-script-intro I'd also recommend looking straight at the source of existing plugins that do something similar to what you want to do. That way you cut out the middle man and dont have to deal with blog ads and other distractions.Have you checked out ?  It is an excellent read.vim 7.0 and above can be compiled with the  flag, which gives vim Python support.  This is a (albeit, somewhat extreme) path to getting the language support you want. Refer to the  chapter in ."},
{"body": "Look at all those backends!  Do I need to care which backend is in use?  e.g. if I develop and test my stuff using only  backend, and someone else using my code might be using  backend on their system, might my stuff break for them in a way that I won't have noticed - or are all backends required to more or less \"work\" the same way? The backend mainly matters if you're embedding matplotlib in an application, in which case you need to use a backend (GTK, Qt, TkInter, WxWindows) which matches the toolkit you're using to build your application. If you're also using matplotlib in a simple interactive way, you'll also want to use a backend which matches what is available on your machine (GTK if you're running Gnome, Qt if you're running KDE, etc) (although most libs are already installed on most machines)The drawing layer part of the backend (Cairo, Agg...) also matters in terms of functionalities: you can choose it depending on what that layer provides compared to what your application needs (anti aliasing, alpha channel, export formats...). So if you develop and test using TkAgg and other people run with e.g. TkCairo, some things might not work. OTOH, running with QtAgg would certainly work in a very similar way as long as you stick to the matplotlib API and don't reach in the wrapped toolkit layer. "},
{"body": "I recently started learning . I went through the tutorial, read some introductory articles, so far so good.Now I want to use it for Python development. From what I understand, there are two separate Python modes for Emacs: python-mode.el, which is part of the Python project; and python.el, which is part of Emacs 22.I read all information I could find but most of it seems fairly outdated and I'm still confused. The questions:Relevant links:If you are using GNU Emacs 21 or before, or XEmacs, use python-mode.el. The GNU Emacs 22 python.el won't work on them. On GNU Emacs 22, python.el does work, and ties in better with GNU Emacs's own symbol parsing and completion, ElDoc, etc. I use XEmacs myself, so I don't use it, and I have heard people complain that it didn't work very nicely in the past, but there are updates available that fix some of the issues (for instance, on the emacswiki page you link), and you would hope some were integrated upstream by now. If I were the GNU Emacs kind, I would use python.el until I found specific reasons not to.The python-mode.el's single biggest problem as far as I've seen is that it doesn't quite understand triple-quoted strings. It treats them as single-quoted, meaning that a single quote inside a triple-quoted string will throw off the syntax highlighting: it'll think the string has ended there. You may also need to change your auto-mode-alist to turn on python-mode for .py files; I don't remember if that's still the case but my init.el has been setting auto-mode-alist for many years now.As for other addons, nothing I would consider 'essential'. XEmacs's func-menu is sometimes useful, it gives you a little function/class browser menu for the current file. I don't remember if GNU Emacs has anything similar. I have a rst-mode for reStructuredText editing, as that's used in some projects. Tying into whatever VC you use, if any, may be useful to you, but there is builtin support for most and easily downloaded .el files for the others. has a description of how to get Python code completion in Emacs.\n is a way to get Rope to work in emacs.  I haven't had extensive experience with either, but they're worth looking into.Given the number of times I have several open buffers all called , I consider the  library essential for python development.Pyflakes also aids productivity."},
{"body": "I'm making a simple learning simulation, where there are multiple organisms on screen. They're supposed to learn how to eat, using their simple neural networks. They have 4 neurons, and each neuron activates movement in one direction (it's a 2D plane viewed from the bird's perspective, so there are only four directions, thus, four outputs are required). Their only input are four \"eyes\". Only one eye can be active at the time, and it basically serves as a pointer to the nearest object (either a green food block, or another organism).Thus, the network can be imagined like this:\nAnd an organism looks like this (both in theory and the actual simulation, where they really are red blocks with their eyes around them):And this is how it all looks (this is an old version, where eyes still didn't work, but it's similar):Now that I have described my general idea, let me get to the heart of the problem...But, there is a big problem. I think that this isn't a good approach, because they don't actually learn anything! Only those that had their initial weights randomly set to be beneficial will get a chance of eating something, and then only them will have their weights strengthened! What about those that had their connections set up badly? They'll just die, not learn.How do I avoid this? The only solution that comes to mind is to randomly increase/decrease the weights, so that eventually, someone will get the right configuration, and eat something by chance. But I find this solution to be very crude and ugly. Do you have any ideas?\nThank you for your answers! Every single one of them was very useful, some were just more relevant. I have decided to use the following approach:This is similar to issues with trying to find a , where it's easy to get stuck in a local minimum.  Consider trying to find the global minimum for the profile below: you place the ball in different places and follow it as it rolls down the hill to the minimum, but depending on where you place it, you may get stuck in a local dip.  The general solutions to this are to fluctuate the parameters (, weights, in this case) more vigorously (and usually reduce the size of the fluctuations as you progress the simulation -- like in simulated annealing), or just realize that a bunch of the starting points aren't going to go anywhere interesting.As mentioned by Mika Fischer, this sounds similar to artificial life problems, so that's one avenue you could look at.It also sounds a bit like you're trying to reinvent . I would recommend reading through , which is freely available in HTML form at that website, or purchasable in dead tree format. Example code and solutions are also provided on that page. Use of neural networks (and other function approximators) and planning techniques is discussed later in the book, so don't get discouraged if the initial stuff seems too basic or non-applicable to your problem.How do you want it to learn? You don't like the fact that randomly seeded organisms either die off or prosper, but the only time you provide feedback into your organism is if they randomly get food. Let's model this as hot and cold. Currently, everything feeds back \"cold\" except when the organism is right on top of food. So the only opportunity to learn is accidentally running over food. You can tighten this loop to provide more continuous feedback if you desire. Feedback warmer if there is movement toward food, cold if moving away.Now, the downside of this is that there is no input for anything else. You've only got a food-seeker learning technique. If you want your organisms to find a balance between hunger and something else (say, overcrowding avoidance, mating, etc), the whole mechanism probably needs to be re-thought.There are several algorithms that can be used to optimize the weights in a neural network, the most common of which is the . From reading your question I gather that you are trying to build neural network bots that will search for food. The way to achieve this with backpropogation would be to have an initial learning period, where the weights are initially randomly set (as you're doing) and gradually refined using the backpropogation algorithm until they reach a performance level you're happy with. At that point you can stop them learning and allow them to frolic freely in flatland. However I think there might be a few issues with your network design. Firstly, if there is only 1 eye active at any time, it would make more sense to have just one input node and keep track of orientation some other way (if I'm understanding that correctly). Simply, if there is only one active eye and four possible actions (forward, back, left, right) then the inputs from the inactive eyes (presumably zero) would have no bearing on the output decision, in fact I suspect the weights for each input to all outputs would converge, essentially duplicating the same function. Moreover, it needlessly increases the complexity of the network and increases the learning time. Secondly, you don't need that many output neurons to represent all possible actions. As you have it described there, your output would be {1,0,0,0} = right, {0,1,0,0} = left and so on. Depending on the type of neuron modeled, this can be done with 2 or even 1 output neuron. If using a binary neuron (each output is either 1 or 0), then do something like {0,0} = back, {1,1} = forward, {1,0} = left, {0,1} = right. Using a sigmoidal function neuron (the output can be a real number from 0..1), you could do {0} = back, {0.33} = left, {0.66} = right, {1} = forward. I can see a bunch of potential problems.First and foremost, I'm not clear about the algorithm that updates your weights.  I like the 1% decrease as a concept-- it looks like you're trying to discount distant memories, which is good in principle-- but the rest probably isn't sufficient.  You need to look at some of the standard update algorithms like backpropagation, but that's just a start, because.......You're only giving your network credit for the last stage of eating the food.  It doesn't seem like there's any direct mechanism for getting your network incrementally closer to the food, or to clumps of food.  Even taking the directionality of the eyes at face value, your eyes are very simple, and there's not much long term memory.Also, if your network diagram is accurate, it's probably not sufficient.  You really want to have a hidden layer (at least one) between the sensors and the actuators, if you use something related to backpropagation.  There are detailed mathematics behind that statement, but it boils down to, \"The hidden layers will allow good solutions of more problems.\"Now, notice that a lot of my comments are talking about the architecture of the network, but only in general terms without saying concretely, \"This will work,\" or \"that will work.\"  That's because I don't know either (although I think Kwatford's suggestion of reinforcement learning is a very good one.)  Sometimes, you can evolve the network parameters as well as the network instances.  One such technique is Neuroevolution of Augmenting Topologies, or \"NEAT\".  Might be worth a look. I think a more complex example of what you're doing is presented by .You can also see the Google Tech Talks presentation from 2007:  However, the fundamental idea is to take an evolutionary approach within your system: use small random mutations combined with genetic cross-over (as the main form of diversification) and select individuals which are \"better\" suited to survive the environment."},
{"body": "I want to change the value of the variable declared outside the loop within a loop. But always changing, it keeps the initial value outside the loop.This renders:So the only (bad) solution  have found so far was this:This renders:But, its is very ugly! Is there another more elegant solution?Try also dictionary-based approach. It seems to be less ugly.This also renders:You could do this to clean up the template codeAnd in the server code useThis could be generalized to the followingAnd in the server code use"},
{"body": "I am just wondering how come the code above doesn't really print \"you exited\". What am I doing wrong? if so, may someone point me out the correct way to exit gracefully? (I am not referring to process.terminate or kill)The reason you are not seeing this happen is because you are not communicating with the subprocess.  You are trying to use a local variable (local to the parent process) to signal to the child that it should shutdown.Take a look at the information on .  You need to setup a signal of some sort that can be referenced in both processes.  Once you have this you should be able to flick the switch in the parent process and wait for the child to die.Try the following code:"},
{"body": "I'm drawing a legend on an axes object in matplotlib but the default positioning which claims to place it in a smart place doesn't seem to work.  Ideally, I'd like to have the legend be draggable by the user.  How can this be done?Note: This is now built into matplotlibwill work as expectedWell, I found bits and pieces of the solution scattered among mailing lists. I've come up with a nice modular chunk of code that you can drop in and use... here it is:...and in your code...I emailed the Matplotlib-users group and John Hunter was kind enough to add my solution it to SVN HEAD.I hope this is helpful to people working with matplotlib.In newer versions of Matplotlib (v1.0.1), this is built-in.If you are using matplotlib interactively (for example, in IPython's pylab mode)."},
{"body": "I have a camera that will be stationary, pointed at an indoors area. People will walk past the camera, within about 5 meters of it. Using , I want to detect individuals walking past - my ideal return is an array of detected individuals, with bounding rectangles.I've looked at several of the built-in samples:Is anyone able to provide guidance or samples for doing this - preferably in ?The latest SVN version of OpenCV contains an (undocumented) implementation of HOG-based pedestrian detection.  It even comes with a pre-trained detector and a python wrapper.  The basic usage is as follows:So instead of tracking, you might just run the detector in each frame and use its output directly.See  for the implementation and  for a more complete python example (both in the OpenCV sources).Nick,What you are looking for is not people detection, but motion detection. If you tell us a lot more about what you are trying to solve/do, we can answer better. \nAnyway, there are many ways to do motion detection depending on what you are going to do with the results. Simplest one would be differencing followed by thresholding while a complex one could be proper background modeling -> foreground subtraction -> morphological ops -> connected component analysis, followed by blob analysis if required. Download the opencv code and look in samples directory. You might see what you are looking for. Also, there is an Oreilly book on OCV.Hope this helps,\nNand    This is clearly a non-trivial task. You'll have to look into scientific publications for inspiration ( is your friend here). Here's a paper about human detection and tracking: This is similar to a project we did as part of a Computer Vision course, and I can tell you right now that it is a hard problem to get right.You could use foreground/background segmentation, find all blobs and then decide that they are a person. The problem is that it will not work very well since people tend to go together, go past each other and so on, so a blob might very well consist of two persons and then you will see that blob splitting and merging as they walk along.You will need some method of discriminating between multiple persons in one blob. This is not a problem I expect anyone being able to answer in a single SO-post.My advice is to dive into the available research and see if you can find anything there. The problem is not unsolvavble considering that there exists products which do this: Autoliv has a product to detect pedestrians using an IR-camera on a car, and I have seen other products which deal with counting customers entering and exiting stores."},
{"body": "I am implementing a command line program which has interface like this:I have gone through the . I can implement  as optional argument using  in . And the  using .From the documentation it seems I can have only one sub-command. But as you can see I have to implement one or more sub-commands. What is the best way to parse such command line arguments useing ?@mgilson has a nice  to this question. But problem with splitting sys.argv myself is that i lose all the nice help message Argparse generates for the user. So i ended up doing this:Now after first parse all chained commands are stored in . I reparse it while it is not empty to get all the chained commands and create separate namespaces for them. And i get nicer usage string that argparse generates.I came up with the same qustion, and it seems i have got a better answer.the solution is we shall not simply nest subparser with anothor subparser, but we can add subparser following with a parser fllowing anothor subparser.Code tell you how: returns a Namespace and a list of unknown strings.  This is similar to the  in the checked answer.produces:An alternative loop would give each subparser its own namespace.  This allows overlap in positionals names.You can always split up the command-line yourself (split  on your command names), and then only pass the portion corresponding to the particular command to  -- You can even use the same  using the namespace keyword if you want.    Grouping the commandline is easy with :You could try .  This is an extension to argparse with explicit support for subcommands.Improving on the answer by @mgilson, I wrote a small parsing method which splits argv into parts and puts values of arguments of commands into hierarchy of namespaces:It behaves properly, providing nice argparse help:For :For :And creates a hierarchy of namespaces containing the argument values:you can use the package optparse"},
{"body": "The background:  I'm building a trie to represent a dictionary, using a minimal construction algorithm.  The input list is 4.3M utf-8 strings, sorted lexicographically.  The resulting graph is acyclic and has a maximum depth of 638 nodes.  The first line of my script sets the recursion limit to 1100 via .The problem:  I'd like to be able to serialize my trie to disk, so I can load it into memory without having to rebuild from scratch (roughly 22 minutes).  I have tried both  and , with both the text and binary protocols.  Each time, I get a stack-trace that looks like the following:My data structures are relatively simple:   contains a reference to a start state, and defines some methods.   contains a boolean field, a string field, and a dictionary mapping from label to state.I'm not very familiar with the inner workings of  - does my max recursion depth need to be greater/equal n times the depth of the trie for some n?  Or could this be caused by something else I'm unaware of?Update:  Setting the recursion depth to 3000 didn't help, so this avenue doesn't look promising.Update 2:  You guys were right; I was being short-sighted in assuming that pickle would use a small nesting depth due to default recursion limitations.  10,000 did the trick.From :Although your trie implementation may be simple, it uses recursion and can lead to issues when converting to a persistent data structure.  My recommendation would be continue raising the recursion limit to see if there is an upper bound for the data you are working with and the trie implementation you are using.  Other then that, you can try changing your tree implementation to be \"less recursive\", if possible, or write  that has data persistence built-in (use pickles and  in your implementation).  Hope that helpsPickle does need to recursively walk your trie. If Pickle is using just 5 levels of function calls to do the work your trie of depth 638 will need the level set to more than 3000.Try a much bigger number, the recursion limit is really just there to protect users from having to wait too long if the recursion falls in an infinite hole.Pickle handles cycles ok, so it doesn't matter even if your trie had a cycle in thereDouble-check that your structure is indeed acyclic.You could try bumping up the limit even further. There's a hard maximum that's platform dependent, but trying 50000 would be reasonable.Also try pickling a trivially small version of your trie. If pickle dies even though it's only storing a couple three-letter words, then you know there's some fundamental problem with your trie and not pickle. But if it only happens when you try storing 10k words, then it might be the fault of a platform limitation in pickle.If you use just , you can still segfault if you reach the maximum stack size allowed by the Linux kernel.This value can be increased with  as mentioned at: See also: The default maximum value for me is 8Mb.Tested on Ubuntu 16.10, Python 2.7.12."},
{"body": "Whats a simple code that does parallel processing in python 2.7? All the examples Ive found online are convoluted and include unnecessary codes.how would i do a simple brute force integer factoring program where I can factor 1 integer on each core (4)? my real program probably only needs 2 cores, and need to share information.I know that parallel-python and other libraries exist, but i want to keep the number of libraries used to a minimum, thus I want to use the  and/or  libraries, since they come with pythonA good simple way to start with parallel processing in python is just the pool mapping in mutiprocessing -- its like the usual python maps but individual function calls are spread out over the different number of processes.Factoring is a nice example of this - you can brute-force check all the divisions spreading out over all available tasks:This gives me is the simplest map/reduce implementation that I've found. Also, it's very light on dependencies - it's a single file and does everything with standard library.I agree that using  from  is probably the best route if you want to stay within the standard library.  If you are interested in doing other types of parallel processing, but not learning anything new (i.e. still using the same interface as ), then you could try , which provides several forms of parallel maps and has pretty much the same interface as  does.Also,  has a sister package with the same interface, called , which runs , but provides it with parallel maps that run in MPI and can be run using several schedulers.One other advantage is that  comes with a much better serializer than you can get in standard python, so it's much more capable than  at serializing a range of functions and other things.  And you can do everything from the interpreter.Get the code here: "},
{"body": "When saving a form I am getting this error:\n\"\" needs to have a value for field \"surveythread\" before this many-to-many relationship can be used.Models.py:views.py:forms.pyError Message:Traceback:I'm new to Django and Python. I can post the debug trace or migration file if needed, but I have a feeling it's a simple fix. Obviously the point is I want to save multiple survey thread for each survey result.Thanks!Ok, the code is slightly messy, I'm sure you'll be better off . Seems to me the problem actually is the line:because  object hasn't been written to the database yet, so accessing it's  ManyToMany field can yield problems. If you want to copy the same set of surveys from   to  you should do it iterating over them like this:If this yields the same error, then try to do  first and later copy the items:Your code is likely to remain like this:I can see you have a  to persist the object, so try to accommodate the code to make use of it. If the first version of my answer worked then you'll be fine with the  at the end, if the second is the one who worked, then you'll have to adjust the code a little to stick to the  value.Hope this helps!In this part of the code in , you're setting the  field on the  object to , yet it's not allowed to be  according to your :You have to set  to a  object before you can save it, or allow it to be  in the model.I think you want to change it to say:"},
{"body": "There are two (three, but I'm not counting , as its not \"official\") ways to define a persisting object with :I can see that while using the mapper objects, I separate completely the ORM definition from the business logic, while using the declarative syntax, whenever I modify the business logic class, I can edit right there the database class (which ideally should be edited little).What I'm not completely sure, is which approach is more maintainable for a business application?I haven't been able to find a comparative between the two mapping methods, to be able to decide which one is a better fit for my project.I'm leaning towards using the \"normal\" way (i.e. not the declarative extension) as it allows me to \"hide\", and keep out of the business view all the ORM logic, but I'd like to hear compelling arguments for both approaches.\"What I'm not completely sure, is which approach is more maintainable for a business application?\"Can't be answered in general.However, consider this.The Django ORM is strictly declarative -- and people like that.SQLAlchemy does several things, not all of which are relevant to all problems.The fundamental question is \"Do you want to see and touch the SQL?\"If you think that touching the SQL makes things more \"maintainable\", then you have to use explicit mappings.If you think that concealing the SQL makes things more \"maintainable\", then you have to use declarative style.In our team we settled on declarative syntax.Rationale:Regarding \"keeping out ORM from business view\": in reality your  class, defined in a \"normal\" way, gets seriously monkey-patched by SA when  function has its way with it. IMHO, declarative way is more honest because it screams: \"this class is used in ORM scenarios, and may not be treated just as you would treat your simple non-ORM objects\".I've found that using mapper objects are much simpler then declarative syntax if you use  to version your database schema (and this is a must-have for a business application from my point of view). If you are using mapper objects you can simply copy/paste your table declarations to migration versions, and use simple api to modify tables in the database. Declarative syntax makes this harder because you have to filter away all helper functions from your class definitions after copying them to the migration version.Also, it seems to me that complex relations between tables are expressed more clearly with mapper objects syntax, but this may be subjective."},
{"body": "I'm trying to get the post data in flaskI'm sending the data \"personId\" via POST in RESTClient. But i'm not getting the result. \nIs there an error in the code? If so how do I get specific data when the request data is via POST.I'm new to flask, so can anybody give a solution to this?You want to  your  value; printing it to the console won't help in a web application:The parenthesis around  are meaningless noise, btw, they are not needed in Python and are in this case ignored by the parser.I'm assuming you are doing something meaningful with the  value in the view; otherwise using  on the value is a little pointless.You can also use the  to have the value converted for you:Now  will be set to an integer,   if the field is not present in the form or cannot be converted to an integer."},
{"body": "I am starting with multi-threads in python (or at least it is possible that my script creates multiple threads). would this algorithm be the right usage of a Mutex? I haven't tested this code yet and it probably won't even work. I just want processData to run in a thread (one at time) and the main while loop to keep running, even if there is a thread in queue.  Edit: re-reading my code I can see that it is grossly wrong. but hey, that's why I am here asking for help.I don't know why you're using the Window's Mutex instead of Python's.  Using the Python methods, this is pretty simple:But note, because of the architecture of CPython (namely the ) you'll effectively only have one thread running at a time anyway--this is fine if a number of them are I/O bound, although you'll want to release the lock as much as possible so the I/O bound thread doesn't block other threads from running.An alternative, for Python 2.6 and later, is to use Python's  package.  It mirrors the  package, but will create entirely new processes which  run simultaneously.  It's trivial to update your example:This is the solution I came up with: Output:You have to unlock your Mutex at sometime..."},
{"body": "I want to remove any brackets from a string. Why doesn't this work properly?I did a time test here, using each method 100000 times in a loop.  The results surprised me.  (The results still surprise me after editing them in response to valid criticism in the comments.)Here's the script:Here are the results:Results on other runs follow a similar pattern.  If speed is not the primary concern, however, I still think  is not the most readable; the other three are more obvious, though slower to varying degrees.Because that's not what  does. It removes leading and trailing characters that are present in the argument, but not those characters in the middle of the string.You could do:or:or maybe use a regex: with table=None works fine.Because  only strips trailing and leading characters, based on what you provided. I suggest: only strips characters from the very front and back of the string.To delete a list of characters, you could use the string's  method:"},
{"body": "Is this possible? Doesn't have to be in place, just looking for a way to reverse a tuple so I can iterate on it backwards.There are two idiomatic ways to do this:or Since tuples are immutable, there is no way to reverse a tuple in-place.\nBuilding on @lvc's comment, the iterator returned by  would be equivalent toi.e. it relies on the sequence having a known length to avoid having to actually reverse the tuple.As to which is more efficient, i'd suspect it'd be the  if you are using all of it and the tuple is small, and  when the tuple is large, but performance in python is often surprising so measure it!You can use the  builtin function.If you just want to iterate over the tuple, you can just use the iterator returned by  directly without converting it into a tuple again.Similar to the way you would reverse a list, i.e. s[::-1]and"},
{"body": "Let's say I have this number .\nHow do I refer to it as to an unsigned variable?\nSomething like  in C.:then you just need to add  to the negative value.For example, apply this to -1:Assumption #1 means you want -1 to be viewed as a solid string of 1 bits, and assumption #2 means you want 32 of them.Nobody but you can say what your hidden assumptions are, though.  If, for example, you have 1's-complement representations in mind, then you need to apply the  prefix operator instead.  Python integers work hard to give the illusion of using an infinitely wide 2's complement representation (like regular 2's complement, but with an infinite number of \"sign bits\").And to duplicate what the platform C compiler does, you can use the  module:C's  happens to be 4 bytes on the box that ran this sample.To get the value equivalent to your C cast, just bitwise and with the appropriate mask. e.g. if  is 32 bit:or if it is 64 bit:Do be aware though that although that gives you the value you would have in C, it is still a signed value, so any subsequent calculations may give a negative result and you'll have to continue to apply the mask to simulate a 32 or 64 bit calculation.Python doesn't have builtin unsigned types.  You can use mathematical operations to compute a  int representing the value you would get in C, but there is no \"unsigned value\" of a Python int.  The Python int is an abstraction of an integer value, not a direct access to a fixed-byte-size integer."},
{"body": "I am studying python, and although I think I get the whole concept and notion of Python, today I stumbled upon a piece of code that I did not fully understand:Say I have a class that is supposed to define Circles but lacks a body:Since I have not defined any attributes, how can I do this:The weird part is that Python accepts the above statement. I don't understand why Python doesn't raise an . I do understand that via  I just bind variables to objects whenever I want, but shouldn't an attribute  exist in the  class to allow me to do this?: Lots of wonderful information in your answers!  It's a pity I only get to mark one as an answer.A leading principle is that . That is, you never declare \"this class has a method foo\" or \"instances of this class have an attribute bar\", let alone making a statement about the types of objects to be stored there. You simply define a method, attribute, class, etc. and it's added. As JBernardo points out, any  method does the very same thing. It wouldn't make a lot of sense to arbitrarily restrict creation of new attributes to methods with the name . And it's sometimes useful to store a function as  which don't actually have that name (e.g. decorators), and such a restriction would break that.Now, this isn't universally true. Builtin types omit this capability as an optimization. Via , you can also prevent this on user-defined classes. But this is merely a space optimization (no need for a dictionary for every object), not a correctness thing.If you want a safety net, well, too bad. Python does not offer one, and you cannot reasonably add one, and most importantly, it would be shunned by Python programmers who embrace the language (read: almost all of those you want to work with). Testing and discipline, still go a long way to ensuring correctness. Don't use the liberty to make up attributes outside of  , and do automated testing. I very rarely have an  or a logical error due to trickery like this, and of those that happen, almost all are caught by tests.Just to clarify some misunderstandings in the discussions here. This code:And this code:is . There really is no difference. It does exactly the same thing. This difference is that in the first case it's encapsulated and it's clear that the bar attribute is a normal part of Foo-type objects. In the second case it is not clear that this is so.In the first case you can not create a Foo object that doesn't have the bar attribute (well, you probably can, but not easily), in the second case the Foo objects will not have a bar attribute unless you set it.So although the code is programatically equivalent, it's used in different cases.Python lets you store attributes of any name on virtually on any instance. It's possible to block this (either by writing the class in C, like the built-in types, or by using  which allows only certain names).The reason it works is that most instances store their attributes in a dictionary. Yes, a regular Python dictionary like you'd define with . The dictionary is stored in an instance attribute called . In fact, some people say \"classes are just syntactic sugar for dictionaries.\" That is, you can do everything you can do with a class with a dictionary; classes just make it easier.You're used to static languages where you must define all attributes at compile time. In Python, class definitions are , not compiled; classes are objects just like any other; and adding attributes is as easy as adding an item to a dictionary. This is by design.No, python is flexible like that, it does not enforce what attributes you can store on user-defined classes.There is a trick however, using the  on a class definition will prevent you from creating additional attributes not defined in the  sequence:It creates a  data member of .If you had asked it for  it would have thrown an exception:Interestingly, this does not change the class; just that one instance. So:There are two types of attributes in Python -  and .Python gives you flexibility of creating  on the fly. Since an instance data attribute is related to an instance, you can also do that in  method or you can do it after you have created your instance..So, you see that we have created two instance attributes, one inside  and one outside, after instance is created..But a difference is that, the instance attribute created inside  will be set for all the instances, while if created outside, you can have different instance attributes for different isntances..This is unlike Java, where each Instance of a Class have same set of Instance Variables.."},
{"body": "This crops up every now and then for me: I have some C# code badly wanting the  function available in Python.I am aware of usingBut this brakes down in functional usages, as when I want to do a Linq  instead of writing the above loop.Is there any builtin? I guess I could always just roll my own with a  or such, but this would be  handy to just .You're looking for the  method:Just to complement everyone's answers, I thought I should add that  is closer to Python 2.x's  because it's an enumerable.If anyone requires specifically a list or an array:orare closer to Python's .Enumerable.Range(0,12);"},
{"body": "I have a script which uses the Django ORM features, amongst other external libraries, that I want to run outside of Django (that is, executed from the command-line).Edit: At the moment, I can launch it by navigating to a URL...How do I setup the environment for this?The easiest way to do this is to set up your script as a  subcommand. It's quite easy to do:Put this in a file, in any of your apps under management/commands/yourcommand.py (with empty  files in each) and now you can call your script with .All you need is importable settings and properly set python path. In the most raw form this can be done by setting up appropriate environment variables, like:There are other ways, like calling  and already mentioned  described by James Bennett in . that the suggestions around importing settings and using setup_environ have been deprecated with Django 1.4.  There's a thread on the Django Github describing .There are still some other options out there but many of them seem hackish to me.  My preferred method is often to include the scripted function as an Use  from : In order to do this, you need to:Refer to  and  for more details.I like to add the following command to my projects::Then, I can run custom scripts, e.g:"},
{"body": "Is there a way to group boxplots in matplotlib?Assume we have three groups \"A\", \"B\", and \"C\" and for each we want to create a boxplot for both \"apples\" and \"oranges\". If a grouping is not possible directly, we can create all six combinations and place them linearly side by side. What would be to simplest way to visualize the groupings? I'm trying to avoid setting the tick labels to something like \"A + apples\" since my scenario involves much longer names than \"A\".How about using colors to differentiate between \"apples\" and \"oranges\" and spacing to separate \"A\", \"B\" and \"C\"?  Something like this:A simple way would be to use .\nI adapted an example from the :Here is my version. It stores data based on categories.I am short of reputation so I cannot post an image to here.\nYou can run it and see the result. Basically it's very similar to what Molly did.Just to add to the conversation, I have found a more elegant way to change the color of the box plot by iterating over the dictionary of the object itselfCheers!Here's a function I wrote that takes Molly's code and some other code I've found on the internet to make slightly fancier grouped boxplots:You can use the function(s) like this:"},
{"body": "I'm developing C extensions from python ad I obtain some segfaults (inevitable during the development...).I'm searching a way to display at which line of code the segfault happens (an idea is like tracing every single line of code), how I can do that?Here's a way to output the filename and line number of every line of Python your code runs:Output:(You'd probably want to write the trace output to a file, of course.)If you are on linux, run python under gdbSegfaults from C extensions are very frequently a result of not incrementing a reference count when you create a new reference to an object. That makes them very hard to track down as the segfault occurs only after the last reference is removed from the object, and even then often only when some other object is being allocated.You don't say how much C extension code you have written so far, but if you're just starting out consider whether you can use either ctypes or . Ctypes may not be flexible enough for your needs, but you should be able to link to just about any C library with Cython and have all the reference counts maintained for you automatically.That isn't always sufficient: if your Python objects and any underlying C objects have different lifetimes you can still get problems, but it does simplify things considerably.There are somewhat undocumented python extensions for gdb.From the Python source grab  (it is not included in a normal install). Put this in Then:As you can see we now have visibility into the Python stack corresponding with the CPython call chain.Some caveats:If you cannot copy  into  then you can add it's location to  like this:This is somewhat poorly documented in the ,  and If you have an older  or just can't get this working there is also a  in the Python source that you can copy to  which add some similar functionality "},
{"body": "When I load a page, there is a link  that I want to append to it the  of its containing page.I have following URL: How can I do that?To capture the QUERY_PARAMS that were part of the request, you reference the dict that contains those parameters () and urlencode them so they are acceptable as part of an href.   returns a string that looks like  which you can put into a link on the page like so:I found that @Michael's answer didn't quite work when you wanted to update an existing query parameter.The following worked for me:If you register a templatetag like follows:you can modify the query string in your template:This will preserve anything already in the query string and just update the keys that you specify.Following on from @Prydie (thank you!) I wanted to do the same, but in Python 3 & Django 1.10, with the addition of being able to strip querystring keys as well as modify them. To that end, I use this:The python 3 bit being  over "},
{"body": "In Python, how to check if a string only contains certain characters?I need to check a string containing only a..z, 0..9, and . (period) and no other character.I could iterate over each character and check the character is a..z or 0..9, or . but that would be slow.I am not clear now how to do it with a regular expression.Is this correct? Can you suggest a simpler regular expression or a more efficient approach.Answer, wrapped up in a function, with annotated interactive session:Note: There is a comparison with using re.match() further down in this answer. Further timings show that match() would win with much longer strings; match() seems to have a much larger overhead than search() when the final answer is True; this is puzzling (perhaps it's the cost of returning a MatchObject instead of None) and may warrant further rummaging.The [previously] accepted answer could use a few improvements:(1) Presentation gives the appearance of being the result of an interactive Python session:but match() doesn't return (2) For use with match(), the  at the start of the pattern is redundant, and appears to be slightly slower than the same pattern without the (3) Should foster the use of raw string automatically unthinkingly for any re pattern(4) The backslash in front of the dot/period is redundant(5)  (6) Here's a simple, pure-Python implementation. It should be used when performance is not critical (included for future Googlers).Regarding performance, iteration will probably be the fastest method. Regexes have to iterate through a state machine, and the set equality solution has to build a temporary set. However, the difference is unlikely to matter much. If performance of this function is very important, write it as a C extension module with a switch statement (which will be compiled to a jump table).Here's a C implementation, which uses if statements due to space constraints. If you absolutely need the tiny bit of extra speed, write out the switch-case. In my tests, it performs very well (2 seconds vs 9 seconds in benchmarks against the regex).Include it in your setup.py:Use as:Simpler approach? A little more Pythonic?It certainly isn't the most efficient, but it's sure readable.Regular expression solution is the fastest pure python solution so farCompared to other solutions:If you want to allow empty strings then change it to:Under request I'm going to return the other part of the answer. But please note that the following accept A-Z range.You can use  Using isalnum is much more efficient than the set solution\n    John gave an example where the above doesn't work. I changed the solution to overcome this special case by using encodeAnd it is still almost 3 times faster than the set solutionIn my opinion using regular expressions is the best to solve this problemThis has already been answered satisfactorily, but for people coming across this after the fact, I have done some profiling of several different methods of accomplishing this. In my case I wanted uppercase hex digits, so modify as necessary to suit your needs.Here are my test implementations:And the tests, in Python 3.4.0 on Mac OS X:which gave the following results:where:The columns we actually care about are cumtime and percall, as that shows us the actual time taken from function entry to exit. As we can see, regex match and search are not massively different. It is faster not to bother compiling the regex if you would have compiled it every time. It is about 7.5% faster to compile once than every time, but only 2.5% faster to compile than to not compile.test_set was twice as slow as re_search and thrice as slow as re_matchtest_not_any was a full order of magnitude slower than test_set: Use re.match or re.search"},
{"body": "Currently I am hosting a Django app I developed myself for my clients, but I am now starting to look at selling it to people for them to host themselves.My question is this: How can I package up and sell a Django app, while protecting its code from pirating or theft? Distributing a bunch of .py files doesn't sound like a good idea as the people I sell it to too could just make copies of them and pass them on.I think for the purpose of this problem it would be safe to assume that everyone who buys this would be running the same (LAMP) setup.Don't try and obfuscate or encrypt the code - it will never work.I would suggest selling the Django application \"as a service\" - either host it for them, or sell them the code . Write up a contract that forbids them from redistributing it.That said, if you were determined to obfuscate the code in some way - you can distribute python applications entirely as .pyc (Python compiled byte-code).. It's how Py2App works.It will still be re-distributable,  it will be very difficult to edit the files - so you could add some basic licensing stuff, and not have it foiled by a few s..As I said, I don't think you'll succeed in anti-piracy via encryption or obfuscation etc.. Depending on your clients, a simple contract, and maybe some really basic checks will go a long much further than some complicated decryption system (And make the experience of using your application , instead of )The way I'd go about it is this:This way the user only has to contact the server when the hostname changes and on first install, but you get a small layer of security.  You could change the hostname to something more complex, but there's really no need -- anyone that wants to pirate this will do so, but a simple mechanism like that will keep honest people honest.You could package the whole thing up as an Amazon Machine Instance (AMI), and then have them run your app on .  The nice thing about this solution is that Amazon will , and since you're distributing the entire machine image, you can be certain that all your clients are using the same LAMP stack.  The AMI is an encrypted machine image that is configured however you want it.You can have Amazon bill the client with a one-time fee, usage-based fee, or monthly fee.Of course, this solution requires that your clients host their app at Amazon, and pay the appropriate fees.\"Encrypting\" Python source code (or bytecode, or really bytecode for any language that uses it -- not just Python) is like those little JavaScript things some people put on web pages to try to disable the right-hand mouse button, declaring \"now you can't steal my images!\"The workarounds are trivial, and will not stop a determined person.If you're really serious about selling a piece of Python software, you need to act serious. Pay an attorney to draw up license/contract terms, have people agree to them at the time of purchase, and then just let them have the actual software. This means you'll have to haul people into court if they violate the license/contract terms, but you'd have to do that no matter what (e.g., if somebody breaks your \"encryption\" and starts distributing your software), and having the actual proper form of legal words already set down on paper, with their signature, will be far better for your business in the long term.If you're really  paranoid about people \"stealing\" your software, though, just stick with a hosted model and don't give them access to the server. Plenty of successful businesses are based around that model.You'll never be able to keep the source code from people who really want it. It's best to come to grips with this fact now, and save yourself the headache later.One thing you might want to consider is what FogBugz does.  Simply include a small binary (perhaps a C program) that is compiled for the target platforms and contains the code to validate the license. This way you can keep the honest people honest with minimal headache on your part.May I speak frankly, as a friend? Unless your app is Really Amazing, you may not get many buyers. Why waste the time on lawyers, obfuscation, licensing and whatnot? You stand to gain a better reputation by open-sourcing your code...and maintaining it.Django comes from the open-source end of the spectrum from licensing (and obfuscating). Granted, the MIT license is more common than the GPL; still they are both very far removed from anything like Microsoft's EULA. A lot of Djangophiles will balk at closed source code, simply because that's what Microsoft does.Also, people will trust your code more, since they will be able to read it and verify that it contains no malicious code. Remember, \"obfuscating\" means \"hiding;\" and who will really know exactly what you've hidden?Granted, there's no easy way to monetize open-sourced code. But you could offer your services or even post a campaign on Pledgie.com, for those who are thankful for all your great work."},
{"body": "How can I print the next year if the current year is given in python using the simplest code, possibly in one line using datetime module. Both date and datetime objects have a  attribute, which is a number. Just add 1:If you have the current year in a variable, just add 1 directly, no need to bother with the datetime module:If you have the date in a string, just select the 4 digits that represent the year and pass it to :Year arithmetic is exceedingly simple, make it an integer and just add 1. It doesn't get much simpler than that.If, however, you are working with a whole date, and you need the same date but one year later, use the components to create a new  object with the year incremented by one:or you can use the  function, which returns a copy with the field you specify changed:Note that this can get a little tricky when  is February 29th in a leap year. The absolute, fail-safe correct way to work this one is thus:"},
{"body": "I am writing a scraper that downloads all the image files from a HTML page and saves them to a specific folder. all the images are the part of the HTML page.Here is some code to download all the images from the supplied URL, and save them in the specified output folder. You can modify it to your own needs. You can specify the output folder now.Ryan's solution is good, but fails if the image source URLs are absolute URLs or anything that doesn't give a good result when simply concatenated to the main page URL.  urljoin recognizes absolute vs. relative URLs, so replace the loop in the middle with:You have to download the page and parse html document, find your image with regex and download it.. You can use urllib2 for downloading and Beautiful Soup for parsing html file.And this is function for download one image:Use htmllib to extract all img tags (override do_img), then use urllib2 to download all the images.If the request need an authorization refer to this one:"},
{"body": "What is the safest way to run queries on mysql, I am aware of the dangers involved with MySQL and SQL injection. However I do not know how I should run my queries to prevent injection on the variables to which other users (webclients)  can manipulate. I used to write my own escape function, but apparently this is \"not-done\". What should I use and how should I use it to query and do inserts safely on a MySQL database through python without risking mysql injection? To avoid injections, use  with  in place of each variable, then pass the value via a list or tuple as the second parameter of . Here is an :Note that this is using a , not  (which would be a direct string substitution, not escaped). :In addition, you don't need the quotes around the position holder () if the parameter is a string.As an expansion of Bruno's answer, your MySQL client library may support any of several different formats for specifying named parameters. From , you could write your queries like:You can see which your client library supports by looking at the  module-level variable:Any of the above options should Do The Right Thing with regards to handling your possibly insecure data. As Bruno pointed out, please don't ever try to insert parameters yourself. The commonly-used client libraries are much better at processing data correctly than we mere mortals will ever be."},
{"body": "If the value is None, I'd like to change it to \"\" (empty string).I start off like this, but I forget:In a more general case, e.g. if you were adding or removing keys, it might not be safe to change the structure of the container you're looping on -- so using  to loop on an independent list copy thereof might be prudent -- but assigning a different value at a given existing index does  incur any problem, so, in Python 2.any, it's better to use .You could create a dict comprehension of just the elements whose values are None, and then update back into the original: - did some performance testsWell, after trying dicts of from 100 to 10,000 items, with varying percentage of None values, the performance of Alex's solution is across-the-board about twice as fast as this solution.Comprehensions are usually faster, and this has the advantage of not editing  during the iteration:"},
{"body": "I just updated my Aptana Studio3. When I open my python file it says that it can not find ,  and  and some other methods. but when I run my code, it'll run without any problem. my code completion doesn't work any more. The error for code completion when I use CTRL+SPACE is I don't know where the problem is ?!! I searched but I couldn't find a proper solution. I'm using windows 7. It seems that this is solved on PyDev and the problem is you can't upgrade PyDev on Aptana 3.6.0. Version 3.6.1 takes out the integration with PyDev and lets you upgrade PyDev.\nSo I installed Aptana 3.6.1 through Beta repository and then installed the latest PyDev.Aptana Beta link to add to \"Available Software Sites\" on Aptana:\nUpgrade to Aptana 3.6.1. This will uninstall PyDev.PyDev link to add to \"Available Software Sites\" on Aptana:\nInstall PyDev.And then, \"Port not bound\" will be solved.I did trace the differrence between Aptana 3.4 and 3.5, found a forked process to start pycompletionserver at specific ports:In 3.5, this process is not found at all, the version of pydev also changed.I managed to start the completion server with the same old ports, still Port not bound error occur.-Perhaps Aptana has changed the port numbers....I'm sorry to revert back to 3.4 :-( I've had the same issue for some time now. I originally downgraded to the previous release but have now decided to upgrade to the 3.6.1 beta release and this problem seems to be fixed. Instructions on how to upgrade to 3.6.1 .The PyDev interpreter may not get automatically installed when you upgrade so you'll have to manually install it again. If needed, there are instructions .Using info from idubs11, I was able to get my Aptana 3.6.1(downloaded 2016-03-01) working.  It exhibited the same problem, port not bound.  I was not able to uninstall pydev, until...Now it was in a weird state, no Pydev anymore and now using the beta version.  I then used the pydev.org install site to install pydev and now everything works just fine.I had this issue, even with Windows Firewall disabled. I even tried a clean reinstall (version 3.6) to no avail. In the end I was forced to downgrade to . It's now no longer an issue.I know that the OP asked for help in regards with a Windows OS, however if anyone wants to get this working on OS X 10.9+, then  follow the steps originally suggested by  , which are:, when installing PyDev uncheck  check box in Aptana's  window and make sure that you choose the latest 2.x.x version of PyDev.PyDev 3.x+ needs Java JDK 7+ in order to run, however for the latest versions of OS X any Java JDK 7+ is only available as the 64 bit version. Aptana Studio 3 for OS X seems to be available in 32 bit only, which requires a 32 bit JDK.Another option is to install a 64 bit Eclipse IDE and install Aptana Studio 3.6.1 as a plugin and the latest PyDev separately. I'm running Windows 10, I fixed this issue by running the installer again and picked \"repair\". Fixed this issue for me and did not lose any configuration in the process.Performing a clean Aptana reinstall had fixed this for me.but have you checked the windows firewall? as it tells the IDE? You should try to put an exception for Aptana and it should work .. in fact it seems that the method of python builtin does not read directly from python but from a db, most likely via a socket on a port closed by the firewall .."},
{"body": "I'm really curious about  format string in Python 3.  says  is  and that  is .It also says  will apply , but it doesn't say anything similar about . I think there's no significant difference between them, but I want to be sure. Can anyone clarify these?Some code example:It's still confusing, but let me wrap up with my own (layperson's) words.Is anything wrong here?, and its brethren  and  apply ,  and  respectively  interpolation and formatting. These are called , and are part of the , not the  applied to values when interpolating:Bold emphasis mine. only applies afterwards to the conversion result (or the original object if no conversion had been applied), and only if the  method for the type of object supports that formatting option. Usually, only objects of type  support this formatter; it's there as the default, mostly because the  allows for the existence of a type character and because the older   had a  format. If you tried to apply the  type to an object that doesn't support it, you'd get an exception.Use  (or  or ) when you have an object that is not itself a string and either doesn't support formatting otherwise (not all types do) or would format differently from their ,  or  conversions:Note:  formatting specified by the format spec are the responsibility of the  method; the last line does not apply the alignment operation in the  formatting spec, the  method only used it as literal text in a formatting operation (using a  conversion here).For the converted values, on the other hand, the  method is used and the output is aligned to the right in a 50 character wide field, padded with spaces on the left.You're unlucky you used strings as the value to be formatted. Using pretty much any other object you'd get how these aren't the same.In (as much as I can) layman's terms:I do think, though, that  is probably a confusing name to give this specifier.From the documentation, :So no, it's not the same.For example:If you want to print a float as a string, you need a conversion (float \u2192 string).If you don't use the conversion mark, you will get an error."},
{"body": "I've been looking around the internet for a Java scientific package that is \"similar\" to Scipy.  The only thing I have really found  is  but it seems not to offer plotting and such.  Does anyone know of a good scientific package for Java?I'm looking for similar packages .  Here is a summary of what's shown up there so far:My personal experience is focused primarily in Apache Commons Math where it is certainly working for our needs.We also use JFreechart at work.  That is less of a numerical method / scientific package and much more focused on just getting your values on the screen in some useful form.Your mileage may vary but hopefully this will aid your search.I had really good results with .You can try . It's a very good library, plots graphs and it's free :)\nSomeone also told me about  but i've never used it myself.Have a look at .  It is an open source project based on Eclipse.  It supports all kinds of different user interfaces.  It is easy to use and flexible.Even if it is not exactly what you need, keep reading down the page.  It mentions a lot of other technologies that might be an even closer fit.Both MathEcliipse and  look like incredibly outstanding tools for making powerful mathematics very easy and accessible for users of Java programs.You could also take a look at , which says you can interface it to your favorite plotting libraries.I have used  before and found it very simple to use.  Others must too.  I have found it embedded in some other well known Java projects.It may not be strictly connected to question, but whatever library you'll choose, I recommend using it with . Its state-of-art optimized, really stable and, what is important - usable in real-time apps. Interface has a lot in common with standard Java libraries, and new stuff is designed to be very understandable.You might want to check out  collection of Java numerics packages.If you have a Java library, you can script using anything like Groovy, Scala, Jyphon, JRuby, Javascript, etc. etc.Here is an example to script math in Groovy using SuanShu:For plotting you can use , a really nice open source library.  You'll have to combine it with another library to get all of the functionality you're asking for, though.If you must use the library and no Java equivalent exists, can you import the package from ?"},
{"body": "I'm debugging a python script, and I want to watch a variable for a change (much like you can watch a memory adress in gdb).  Is there a way to do this?Here is a really hacky way to do this with . These commands can be put in your  for automatic loading every time you use .This adds two commands,  and  which each take a variable name  as an argument. They will make a shallow copy of the current frame's local variable for  if possible, and keep executing  or  respectively until what that name points to changes.This works in CPython 2.7.2 but relies on some  internals so it will probably break elsewhere.For watching a variable when you are hitting a , you can use the  command. E.g. printing  when hitting breakpoint #1 ().For :you can use  functionality of pdbOnce you hit the breakpoint just type ipdb>  as you can see, each time you type display - it will print all of your watches (expressions). You can use undisplay function to remove certain watch.You can also use  to prettyprint the expression (very useful)A possible solution is to use :Then \"mark\" the object you want to watch with the decorator :Here  will break on any change of the attribute  on any Foo-object.\nOnly invocations of the underlying -method will trigger the breakpoint. This means that  and  will work, but manipulating the -object will not trigger the breakpoint:"},
{"body": "I want to test whether an object is an instance of a class, and only this class (no subclasses). I could do it either with:Are there reasons to choose one over another? (performance differences, pitfalls, etc)In other words: a) is there any practical difference between using  and ? b) are class objects always safe for comparison using ? Thanks all for the feedback. I'm still puzzled by whether or not class objects are singletons, my common sense says they are, but it's been really hard to get a confirmation (try googling for \"python\", \"class\" and \"unique\" or \"singleton\").I'd also like to clarify that, for my particular needs, the \"cheaper\" solution that just works is the best, since I'm trying to optimize the most out of a few, specialized classes (almost reaching the point where the sensible thing to do is to drop Python and develop that particular module in C). But the reason behind the question was to understand better the language, since some of its features are a bit too obscure for me to find that information easily. That's why I'm letting the discussion extend a little instead of settling for , so I can hear the opinion of more experienced people. So far it's been very fruitful!I ran a small test to benchmark the performance of the 4 alternatives. The profiler results were:Unsurprisingly,  performed better than  for all cases.  performed better in Python (2% faster) and  performed better in PyPy (6% faster). Interesting to note that  performed better in PyPy than . many people don't seem to understand what I mean with \"a class is a singleton\", so I'll ilustrate with an example:It doesn't matter if there are N objects of type , given an object, only one will be its class, hence it's safe to compare for reference in this case. And since reference comparison will always be cheaper than value comparison, I wanted to know whether or not my assertion above holds. I'm reaching the conclusion that it does, unless someone presents evidence in contrary.For old-style classes, there is a difference:The point of new-style classes was to unify class and type. Technically speaking,  is the only solution that will work both for new and old-style class instances, but it will also throw an exception on old-style class objects themselves. You can call  on any object, but not every object has . Also, you can muck with  in a way you can't muck with .Personally, I usually have an environment with new-style classes only, and as a matter of style prefer to use  as I generally prefer built-in functions when they exist to using magic attributes. For example, I would also prefer  to . should only be used for identity checks, not type checks (there is an exception to the rule where you can and should use  for check against singletons).  Note: I would generally not use  and  for type checks, either.  The preferable way for type checks is .  If you ever have a reason to check if something is not an subclass instance, it smells like a fishy design to me.  When , then   , and you should be avoiding any situations where some part of your code has to work on a  instance but breaks on a  instance. The result of  is equivalent to  in new style classes, and class objects are not safe for comparison using , use  instead. the preferable way here would be .As Michael Hoffman pointed out in his answer, there is a difference here between new and old style classes, so for backwards compatible code you may need to use .For those claiming that  is preferable, consider the following scenario:The OP wants the behavior of , where it will be false even though  is a base class of .I can confirm that  is a singleton. Here is the proof.As you can see both  and  print out the same value in memory even though  and  are at different values in memory. Here is the proof of that.The  method only exists if you used . If you use the classic style class, , than the  method doesn't exist.What this means is to be the most generic you probably want to use  unless you know for a fact instance does exist."},
{"body": "So I have these two problems for a homework assignment and I'm stuck on the second one. For the first one, this works perfectly: However, I'm pretty stumped on the second one. I think I may have to take the Cartesian                       product of the set r with something but I'm just not sure. This gets me somewhat close but I just want the consecutive pairs.Any help is greatly appreciated, thanks.I simplified the test a bit -  instead of I also limited y's range; there is no point in testing for divisors > sqrt(x). So max(x) == 100 implies max(y) == 10. For x <= 10, y must also be < x.Instead of generating pairs of primes and testing them, get one and see if the corresponding higher prime exists.You can get clean and clear solutions by building the appropriate predicates as helper functions.  In other words, use the Python set-builder notation the same way you would write the answer with regular mathematics set-notation.  The whole idea behind set comprehensions is to let us write and reason in code the same way we do mathematics by hand.With an appropriate predicate in hand, problem 1 simplifies to:And problem 2 simplifies to:Note how this code is a direct translation of the problem specification, \"A Prime Pair is a pair of consecutive odd numbers that are both prime.\"P.S. I'm trying to give you the correct problem solving technique without actually giving away the answer to the homework problem.You can generate pairs like this:Then all that is left to do is to get a condition to make them prime, which you have already done in the first example.A different way of doing it: (Although slower for large sets of primes)"},
{"body": "I was just pleasantly surprised to came across the documentation of , but noticed that it's gone in Python 3.0, without any clear replacement or explanation.I can't seem to find any discussion on python-dev about how this decision was made - does anyone have any insight inot this decision?I believe the functionality is now built in:"},
{"body": "I've recently started using git, and also begun unit testing (using Python's  module). I'd like to run my tests each time I commit, and only commit if they pass.I'm guessing I need to use  in , and I've managed to make it run the tests, but I can't seem to find a way to stop the commit if they tests fail. I'm running the tests with , which in turn is running . It seems like I don't get a different exit condition whether the tests pass or fail, but I may be looking in the wrong place. Is this something uncommon that I want to do here? I would have thought it was a common requirement... Just in case people can't be bothered to read the comments, the problem was that  doesn't exit with non-zero status, whether the test suite is successful or not. To catch it, I did:I would check to make sure that each step of the way, your script returns a non-zero exit code on failure. Check to see if your  returns a non-zero exit code if a test fails. Check to make sure your  command returns a non-zero exit code. And finally, check that your  hook itself returns a non-zero exit code on failure.You can check for a non-zero exit code by adding  to the end of a command; that will print out the exit code if the command failed.The following example works for me (I'm redirecting stderr to  to avoid including too much extraneous output here)::::Note the . This isn't necessary if  is the last command in the hook, as the exit status of the last command will be the exit status of the script. But if you have later checks in your  hook, then you need to make sure you exit with an error; otherwise, a successful command at the end of the hook will cause your script to exit with a status of .Could you parse the result of the python test session and make sure to exit your pre-commit hook with a non-zero status?So if your python script does not return the appropriate status for any reason, you need to determine that status directly from the  hook script.\nThat would ensure the commit does not go forward if the tests failed.\n(or you could call from the hook a python wrapper which would call the tests, and ensure a  according to the test results).Another option, if you don't want to handle manually pre-commit's:\nThere is nice tool to run tests and syntax checks for Python, Ruby and so on: "},
{"body": "The usual check to differentiate between running Python-application on Windows and on other OSes (Linux typically) is to use conditional:But I wonder is it safe to use today when 64-bit Python is more widely used in last years? Does 32 really means 32-bit, or basically it refers to Win32 API?If there is possibility to have one day sys.platform as 'win64' maybe such condition would be more universal?There is also another way to detect Windows I'm aware of:But I really never saw in other code the use of the latter.What is the best way then?: I'd like to avoid using extra libraries if I can. Requiring installing extra library to check that I'm work not in the Windows may be annoying for Linux users.  will be  regardless of the bitness of the underlying Windows system, as you can see in  (from the Python 2.6 source distribution):It's possible to find the  that introduced this on the web, which offers a bit more explanation:Notice that you cannot use either  or  for this on Jython:I think there's a plan in Jython project to change  to report the underlying OS similarly as CPython, but because people are using  to check are they on Jython this change cannot be done overnight. There is, however, already  on Jython 2.5.x:Personally I tend to use  with code that needs to run both on Jython and CPython, and both on Windows and unixy platforms. It's somewhat ugly but works.The caveats for Windows/32 and Windows/64 are the same, so they should use the same value. The only difference would be in e.g.  and . If you need to distinguish between 32 and 64 regardless then  is your best bet.Personally I use  for detecting the underlying platform.For 32-bit,  returns ."},
{"body": "Is there an easy way in Python to check whether the value of an optional parameter comes from its default value, or because the user has set it explicitly at the function call?Not really.  The standard way is to use a default value that the user would not be expected to pass, e.g. an  instance:Usually you can just use  as the default value, if it doesn't make sense as a value the user would want to pass.The alternative is to use :However this is overly verbose and makes your function more difficult to use as its documentation will not automatically include the  parameter.The following function decorator, , makes a set of parameter names of all the parameters given explicitly. It adds the result as an extra parameter () to the function. Just do  to check if parameter  is given explicitly.I sometimes use a universally unique string (like a UUID).This way, no user could even guess the default if they tried so I can be very confident that when I see that value for , it was not passed in.I agree with Volatility's comment. But you could check in the following manner:A little freakish approach would be:Which outputs:Now this, as I mentioned, is quite freakish, but it does the job. However this is quite unreadable and similarly to 's  won't be automatically documented."},
{"body": "I'm trying to mock something while testing a Django app using the imaginatively named . I can't seem to quite get it to work, I'm trying to do this:What am I doing wrong?Ah I was confused on where to apply that patch decorator. Fixed:To add onto Kit's answer above, specifying a 3rd argument to patch.object() allows the mocked object/method to be specified, otherwise a default MagicMock object is used.Note that, if you specify the mocking object, then the default MagicMock() is  passed into the patched object -- e.g. no longer:but instead:"},
{"body": "The Python docs clearly state that  calls .  However it seems that under many circumstances, the opposite is true.  Where is it documented when or why this happens, and how can I work out for sure whether my object's  or  methods are going to get called.Edit: Just to clarify, I know that  is called in preferecne to , but I'm not clear why  is called in preference to , when the latter is what the docs state will happen.Edit: From Mark Dickinson's answer and comment it would appear that:This explains the behaviour in the examples.   is a subclass of  but doesn't implement its own  so the  of  takes precedence in both cases (ie  calls  as an  because of rule 1).In the  examples,  is called in both instances because  is a subclass of  and so it is called in preference.In the  examples,  implements  and  doesn't so  gets called both times (rule 1).But I still don't understand the very first example with .   is not a subclass on  so AFAICT  should be called, but isn't.You're missing a key exception to the usual behaviour:  when the right-hand operand is an instance of a subclass of the class of the left-hand operand, the special method for the right-hand operand is called first.See the documentation at:and in particular, the following two paragraphs:Actually, in the , it states: is a rich comparison method and, in the case of , is not defined, hence the calling of As I know,  is a so-called \u201crich comparison\u201d method, and is called for comparison operators in preference to  below.  is called if \"rich comparison\" is not defined.So in A == B:\n    If  is defined in A it will be called\n    Else  will be called defined in 'str' so your  function was not called.The same rule is for  and  \"rich comparison\" methods.Is this not documented in the ? Just from a quick look there, it looks like  is ignored when , , etc are defined. I'm understanding that to include the case where  is defined on a parent class.  is already defined so  on its subclasses will be ignored.  etc are not defined so  on its subclasses will be honored.In response to the clarified question: Docs say  will be called first, but it has the option to return  in which case  is called. I'm not sure why you're confident something different is going on here.Which case are you specifically puzzled about? I'm understanding you just to be puzzled about the  and  cases, correct? In either case,  is being called. Since you don't override the  method in TestStrCmp, eventually you're just relying on the base string method and it's saying the objects aren't equal.Without knowing the implementation details of , I don't know whether  will return  and give tsc a chance to handle the equality test. But even if it did, the way you have TestStrCmp defined, you're still going to get a false result.So it's not clear what you're seeing here that's unexpected.Perhaps what's happening is that Python is preferring  to  if it's defined on  of the objects being compared, whereas you were expecting  on the leftmost object to have priority over  on the righthand object. Is that it?"},
{"body": "What Kivy tutorials and learning aids exist?   Where is the list for Kivy?This is besides the pong tutorial on their website?As others suggested, the  in the kivy_installation directory are there to help programmers understand how basic tasks can be done using kivy.There is also this  where we encourage people to put there snippets in the  section.You could also look at the  section in the I guess we need to be better at explaining that. There is an effort at a new  that aims at helping to introduce the different parts of the documentation that are otherwise not quite obvious to new comers, suggestions are welcome. You can open a issue on  for documentation, we will be also adding a suggestion section to the documentation."},
{"body": " to another thread suggests using  instead of . However, when I share axes between subplots, I usually use a syntax like the followingHow can I specify  and  when I use  ?First off, there's an easier workaround for your original problem, as long as you're okay with being slightly imprecise. Just reset the top extent of the subplots to the default  calling :However, to answer your question, you'll need to create the subplots at a slightly lower level to use gridspec.  If you want to replicate the hiding of shared axes like  does, you'll need to do that manually, by using the  argument to  and hiding the duplicated ticks with .As an example:Both Joe's choices gave me some problems: the former, related with direct use of  instead of  and, the latter, with some backends (). But Joe's answer definitely cleared my way toward another compact alternative. This is the result for a problem close to the OP's one:"},
{"body": "After googling and searching on here, couldn't figure out what this does.  Maybe I wasn't searching the right stuff but here it is.  Any input in debunking this shorthand is greatly appreciated.The current answers are good, but do not talk about how they are just  to some pattern that we are so used to.Let's start with an example, say we have 10 numbers, and we want a subset of those that are greater than, say, 5.For the above task, the below approaches below are totally identical to one another, and go from most verbose to concise, readable and :where:To understand the syntax in a slightly different manner, look at the Bonus section below.For further information, follow the tutorial all other answers have linked: (Slightly un-pythonic, but putting it here for sake of completeness)The example above can be written as:The general expression above can be written as:It's a  will be a filtered list of  containing the objects with the attribute occupants > 1 can be a , ,  or any other iterableHere is an example to clarifySo foo has 2  objects, but how do we check which ones they are? Lets add a  method to  so it is more informativeThis return a list which contains all the elements in bar which have occupants > 1.The way this should work as far as I can tell is it checks to see if the list \"bar\" is empty (0) or consists of a singleton (1) via x.occupants where x is a defined item within the list bar and may have the characteristic of occupants. So foo gets called, moves through the list and then returns all items that pass the check condition which is x.occupant.In a language like Java, you'd build a class called \"x\" where 'x' objects are then assigned to an array or similar. X would have a Field called \"occupants\" and each index would be checked with the x.occupants method which would return the number that is assigned to occupant. If that method returned greater than 1 (We assume an int here as a partial occupant would be odd.) the foo method (being called on the array or similar in question.) would then return an array or similar as defined in the foo method for this container array or what have you. The elements of the returned array would be the 'x' objects in the first array thingie that fit the criteria of \"Greater than 1\".Python has built-in methods via list comprehension to deal with this in a much more succinct and vastly simplified way. Rather than implementing two full classes and several methods, I write that one line of code."},
{"body": "I've been trying to wrap my head around how sockets work, and I've been trying to pick apart some sample code I found at  for a very simple client socket program. Since this is basic sample code, I assumed it had no errors, but when I try to compile it, I get the following error message. I've googled pretty much every part of this error, and people who've had similar problems seem to have been helped by changing the port number, using 'connect' instead of 'bind,' and a few other things, but none of them applied to my situation. Any help is greatly appreciated, since I'm very new to network programming and fairly new to python.By the way, here is the code in case that link doesn't work for whatever reason.It's trying to connect to the computer it's running on on port 5000, but the connection is being refused. Are you sure you have a server running?If not, you can use  for testing:Some implementations may require you to omit the  flag.Here is the simplest python socket example.Server side:Client Side:Here is a pretty simple socket program. This is about as simple as sockets get.for the client program(CPU 1)You have to replace the 111.111.0.11 in line 4 with the IP number found in the second computers network settings.For the server program(CPU 2)Run the server program and then the client one.It looks like your client is trying to connect to a non-existent server. In a shell window, run:before running your Python code. It will act as a server listening on port 5000 for you to connect to. Then you can play with typing into your Python window and seeing it appear in the other terminal and vice versa.You might be confusing compilation from execution. Python has no compilation step! :) As soon as you type  the program runs and, in your case, tries to connect to an open port 5000, giving an error if no server program is listening there. It sounds like you are familiar with two-step languages, that require compilation to produce an executable \u2014 and thus you are confusing Python's runtime compilaint that \u201cI can't find anyone listening on port 5000!\u201d with a compile-time error. But, in fact, your Python code is fine; you just need to bring up a listener before running it!"},
{"body": "I'm using Python 2.7.2 on Ubuntu 11.10. I got this error when importing the bz2 module:I thought the bz2 module is supposed to come with Python 2.7. How can I fix this problem?EDIT: I think I previously installed Python 2.7.2 by compiling from source. Probably at that point I didn't have libbz2-dev and so the bz2 module is not installed. Now, I'm hoping to install Python2.7 throughBut it will say it's already installed. Is there a way to uninstall the previous Python2.7 installation and reinstall?Okay, this is much easier to understand in answer form, so I'll move what I would write in my comment to this answer.Luckily for you, you didn't overwrite the system version of python, as Ubuntu 11.10 comes with 2.7.2 preinstalled.Your python binaries ( and ) are located in , which is a directory where user-specific stuff is usually installed. This is fine, it means your system python is still there.First, just try to run the system python. Type this from the command line:This should print out something like this:If so, means you're fine.So you just have to fix your , which tells the shell where to find commands.  is going to have priority over , so there are some ways to fix this, in order of difficulty/annoyance/altering your system:This will make it so that when you type , it should go back to executing , which is an alias for the system's python 2.7.2.Might not be desirable if you already have stuff in  that should have precedence over , but I'm adding this for completeness.In your shell profile (not sure what Ubuntu's default is, but I'm using , you can do this:This is extreme and the first option I presented should be your first option.Do you really need your own version of Python? If you want isolated python environments you probably really want . You can probably remove yours unless there's a reason not to.It's going to be a little annoying though, but basically:This part is not complete because I forget what else there is.I meet the same problem, here's my solution.The reason of import error is while you are building python, system couldn't find the bz2 headers and skipped building bz2 module.Install them on Ubuntu/Debian:Fedora:and then rebuild pythoncomes from @birryree's answer helps to back to the system's original python.In case, you must be used python2.7, you should run: (Centos 6.4)Maybe it will helps someone:For Ubuntu/Debian:For Fedora:And then recompile the python and install it.I used a symlink between Worked fine for me...matocnhoi's answer works for me in centOSand I used virtualenv, so the command isMake sure you bz2 installed, run . usually is , you need replace that if you install python in a another path."},
{"body": "In numpy/scipy I have an image stored in an array. I can display it, I want to save it using   any borders, axes, labels, titles,... Just pure image, nothing else.I want to avoid packages like  or , they are sometimes problematic (they do not always install well, only basic  for meAssuming : To make a figure without the frame :To make the content fill the whole figure Then draw your image on it :The  parameter changes the pixel size to make sure they fill the figure size specified in . To get a feel of how to play with this sort of things, read through , particularly on the subject of Axes, Axis and Artist.An easier solution seems to be:You can find the bbox of the image inside the axis (using ), and use the  parameter to save only that portion of the image:I learned this trick from Joe Kington .I've tried several options in my case, and the best solution was this:then save your figure with I will suggest heron13 answer with a slight addition borrowed from  to remove the padding left after setting the bbox to tight mode, therefore:I had the same problem while doing some visualization using  where I wanted to extract content of the plot without any other information. So this my approach. unutbu answer also helps me to make to work."},
{"body": "Thanks to some help from people here, I was able to get my code for Tasmanian camels puzzle working. However, it is horribly slow (I think. I'm not sure because this is my first program in python). The example run in the bottom of the code takes a long time to be solved in my machine:Here's the code:That is just for 3 camels each. I wanted to do this for 4 at least. That test case is still running (It's been about 5 minutes now :(). I'll update this if and when it finishes.What should I do to improve this code?(Mostly performance-wise, any other suggestions are welcome also).Thanks.I've been tripped up by this before too.  The bottleneck here is actually .The  statement is so easy to use, you forget that it's linear search, and when you're doing linear searches on lists, it can add up fast.  What you can do is convert closedlist into a  object.  This keeps hashes of its items so the  operator is much more efficient than for lists.  However, lists aren't hashable items, so you will have to change your configurations into tuples instead of lists.If the order of  is crucial to the algorithm, you could use a set for the  operator and keep an parallel list around for your results.I tried a simple implementation of this including aaronasterling's namedtuple trick and it performed in 0.2 sec for your first example and 2.1 sec for your second, but I haven't tried verifying the results for the second longer one.First let me tell you how to find the problem. Then I'll tell you where it is:I haven't even bothered to try to figure out your code. I just ran it and took 3 random-time stack samples. I did that by typing control-C and looking at the resulting stacktrace.One way to look at it is: if a statement appears on X% of random stack traces, then it is on the stack for about X% of the time, so that is what it's responsible for. If you could avoid executing it, that is how much you would save.OK, I took 3 stack samples. Here they are:Notice, in this case the stack samples are all identical. In other words, each one of these three lines is individually responsible for nearly all of the time. So look at them:Clearly line 87 is not one you can avoid executing, and probably not 85 either. That leaves 80, the  call. Now, you can't tell if the problem is in the  operator, the  call, the  call, or in the  call. You could find out if you could split those out onto separate lines.So I hope you pick up from this a quick and easy way to find out where your performance problems are.tkerwin is correct that you should be using a set for closedlist, which speeds things up a lot, but it is still kind of slow for 4 camels on each side. The next problem is that you are allowing a lot of solutions that aren't possible because you are allowing fCamels to go backwards and bCamels to go forward. To fix this, replace the lines,withthen I get solution to the 4 camels on each side problem in like .05 seconds rather than 10 seconds. I also tried 5 camels on each side and it took 0.09 seconds. I also am using a set for closedlist and heapq rather than Queue.You can get an additional speed-up by using your heuristic correctly. Currently, you are using the line(or the heapq version of that) but you should change it toThis doesn't factor in the number of moves that has been needed, but that is okay. With this puzzle (and the screening out of moves that move camels in the wrong direction), you don't need to worry about the number of moves it takes - either a move advances you towards the solution or it will come to a dead end. In other words, all possible solutions require the same number of moves. This one change takes the time to find the solution of the 12 camels on each side case from over 13 seconds (even using the heapq, set for closedlist, and the changes to find the neighbors above) to 0.389 seconds. That's not bad. By the way, a better way to find if you've found the solution is to check if the index of the first fCamel is equal to the length of the formation/2 + 1(using int division) and that the index before that is equal to the gap.Replacing with dropped the times from around 340-600 msecs to  1.89 msecs on the input . It yielded the same output.This obviously wont help with any algorithmic problems but as far as micro-optimizations go, it ain't bad.The code below using less than 1s to solve thiswell, I can't really say quite where your algorithm is running astray, but I just went ahead and made my own.  In the interest of doing the simplest thing that could possibly work, I used a bastardized version of Dijkstra's algorithm, where open nodes are visited in arbitrary order, without consideration of distance.  This means I don't have to come up with a heuristic.My other answer is rather long, so I decided to list this as a separate answer. This problem is really better suited to doing a depth-first search. I made a depth-first search solution and it is much, much faster than the optimized A-star method made with the changes outlined in my other answer (which is much, much faster than the OP code). For instance, here are the results for running both my A-star and my depth-first search methods on the 17 camels per side case.Here's my depth-first method code if you are interested:"},
{"body": "I would like to perform a bitwise exclusive or of two strings in python, but xor of strings are not allowed in python. How can I do it ?You can convert the characters to integers and xor those instead:Here's an updated function in case you need a string as a result of the XOR:See it working online: If you want to operate on bytes or words then you'll be better to use Python's array type instead of a string. If you are working with fixed length blocks then you may be able to use H or L format to operate on words rather than bytes, but I just used 'B' for this example:For bytearrays you can directly use XOR:Here is your string XOR'er, presumably for some mild form of encryption:Note that this is an  weak form of encryption.  Watch what happens when given a blank string to encode:the one liner for python3 is : where ,  and the returned value are  instead of  of coursecan't be easier, I love python3 :)(Based on Mark Byers answer.)Do you mean something like this:If the strings are not even of equal length, you can use thisBelow illustrates XORing string s with m, and then again to reverse the process:I've found that the ''.join(chr(ord(a)^ord(b)) for a,b in zip(s,m)) method is pretty slow. Instead, I've been doing this:Based on William McBrine's answer, here is a solution for fixed-length strings which is 9% faster for my use case:"},
{"body": "I want to find string similarity between two strings.  page has examples of some of them. Python has an implemnetation of . Is there a better algorithm, (and hopefully a python library), under these contraints.Would something other than Levenshtein distance(or Levenshtein ratio) be a better algorithm for my case?There's a great resource for string similarity metrics at the University of Sheffield. It has a list of various metrics (beyond just Levenshtein) and has open-source implementations of them. Looks like many of them should be easy to adapt into Python.Here's a bit of the list:I realize it's not the same thing, but this is close enough:You can make this as a functionThis snippet will calculate the difflib, Levenshtein, S\u00f8rensen, and Jaccard similarity values for two strings. In the snippet below, I was iterating over a tsv in which the strings of interest occupied columns  and  of the tsv. ( and ):I would use Levenshtein distance, or the so-called Damerau distance (which takes transpositions into account) rather than the difflib stuff for two reasons (1) \"fast enough\" (dynamic programming algo) and \"whoooosh\" (bit-bashing) C code is available and (2) well-understood behaviour e.g. Levenshtein satisfies the triangle inequality and thus can be used in e.g. a Burkhard-Keller tree.Threshold: you should treat as \"positive\" only those cases where distance < (1 - X) * max(len(string1), len(string2)) and adjust X (the similarity factor) to suit yourself. One way of choosing X is to get a sample of matches, calculate X for each, ignore cases where X < say 0.8 or 0.9, then sort the remainder in descending order of X and eye-ball them and insert the correct result and calculate some cost-of-mistakes measure for various levels of X.N.B. Your ape/apple example has distance 2, so X is 0.6 ... I would only use a threshold as low as 0.75 if I were desperately looking for something and had a high false-negative penaltyIs that what you mean?look at I know this isn't the same but you can adjust the ratio to filter out strings that are not similar enough and return the closest match to the string you are looking for.Perhaps you would be more interested in semantic similarity metrics.I realize you said speed is not an issue but if you are processing a lot of the strings for your algorithm the below is very helpful.Its about 20 times faster than difflib.import Levenshtein"},
{"body": "how can we XOR hex numbers in python eg. I want to xor 'ABCD' to '12EF'. answer should be B922.i used below code but it is returning garbage valueWhoa. You're really over-complicating it by a very long distance. Try:You seem to be ignoring these handy facts, at least:If you already have the numbers as strings, you can use the  function to convert to numbers, by providing the expected base (16 for hexadecimal numbers):So you can do two conversions, perform the XOR, and then convert back to hex:If the two hex strings are the same length and you want a hex string output then you might try this.If the strings are the same length, then I would go for  of the built-in xor (). Examples - If the strings are not the same length, truncate the longer string to the length of the shorter using a slice here's a better functionFor performance purpose, here's a little code to benchmark these two alternatives:Here are the results :Seems like  is faster then the zip version."},
{"body": "Anyone have any suggestions on how to make randomized colors that are all greenish? Right now I'm generating the colors by this:That mostly works, but I get brownish colors a lot. Simple solution:  instead of rgb (convert it to RGB afterwards if you need this). The difference is the meaning of the tuple: Where RGB means values for Red, Green and Blue, in HSL the H is the color (120 degree or 0.33 meaning green for example) and the S is for saturation and the V for the brightness. So keep the H at a fixed value (or for even more random colors you could randomize it by add/sub a small random number) and randomize the S and the V. See the  article.As others have suggested, generating random colours is much easier in the HSV colour space (or HSL, the difference is pretty irrelevant for this)So, code to generate random \"green'ish\" colours, and (for demonstration purposes) display them as a series of simple coloured HTML span tags:The output (when viewed in a web-browser) should look something along the lines of:: I didn't know about the colorsys module. Instead of the above  function, you could use , which makes the code much shorter (it's not quite a drop-in replacement, as my  function expects the hue to be in degrees instead of 0-1):Check out the  module:Use the HSL or HSV color space. Randomize the hue to be close to green, then choose completely random stuff for the saturation and V (brightness).If you stick with RGB, you basically just need to make sure the G value is greater than the R and B, and try to keep the blue and red values similar so that the hue doesn't go too crazy. Extending from Slaks, maybe something like (I know next to nothing about Python):So in this case you are lucky enough to want variations on a primary color, but for artistic uses like this it is better to specify color wheel coordinates rather than primary color magnitudes.You probably want something from the  module like:The solution with HSx color space is a very good one. However, if you need something extremely simplistic and have no specific requirements about the distribution of the colors (like uniformity), a simplistic RGB-based solution would be just to make sure that G value is greater than both R and BThis will give you \"greenish\" colors. Some of them will be ever so slightly greenish. You can increase the guaranteed degree of greenishness by increasing (absolutely or relatively) the lower bound in the last  call.What you want is to work in terms of  instead of RGB. You could find a range of hue that satisfies \"greenish\" and pick a random hue from it. You could also pick random saturation and lightness but you'll probably want to keep your saturation near 1 and your lightness around 0.5 but you can play with them.Below is some actionscript code to convert HSL to RGB. I haven't touched python in a while or it'd post the python version.I find that greenish is something like 0.47*PI to 0.8*PI.The simplest way to do this is to make sure that the red and blue components are the same, like this: (Forgive my Python)I'd go with with the HSV approach everyone else mentioned. Another approach would be to get a nice high resolution photo which some greenery in it, crop out the non-green parts, and pick random pixels from it using ."},
{"body": "I am trying to make a discrete colorbar for a scatterplot in matplotlibI have my x, y data and for each point an integer tag value which I want to be represented with a unique colour, e.g.typically tag will be an integer ranging from 0-20, but the exact range may changeso far I have just used the default settings, e.g.which gives a continuous range of colours. Ideally i would like a set of n discrete colours (n=20 in this example). Even better would be to get a tag value of 0 to produce a gray colour and 1-20 be colourful.I have found some 'cookbook' scripts but they are very complicated and I cannot think they are the right way to solve a seemingly simple problemYou can create a custom discrete colorbar quite easily by using a BoundaryNorm as normalizer for your scatter. The quirky bit (in my method) is making 0 showup as grey. For images i often use the cmap.set_bad() and convert my data to a numpy masked array. That would be much easier to make 0 grey, but i couldnt get this to work with the scatter or the custom cmap. As an alternative you can make your own cmap from scratch, or read-out an existing one and override just some specific entries.I personally think that with 20 different colors its a bit hard to read the specific value, but thats up to you of course.You could follow this :which produces the following image:To set a values above or below the range of the colormap, you'll want to use the  and  methods of the colormap.  If you want to flag a particular value, mask it (i.e. create a masked array), and use the  method.  (Have a look at the documentation for the base colormap class:  )It sounds like you want something like this:The above answers are good, except they don't have proper tick placement on the colorbar. I like having the ticks in the middle of the color so that the number -> color mapping is more clear. You can solve this problem by changing the limits of the matshow call:I think you'd want to look at  to generate your colormap, or if you just need a static colormap I've been working on  that might help."},
{"body": "In Python, How can one subtract two non-unique, unordered lists? Say we have  and  I'd like to do something like  and have  be  or  order doesn't matter to me. This should throw an exception if a does not contain all elements in b. I'm not interested in finding the difference of the sets of elements in a and b, I'm interested in the difference between the actual collections of elements in a and b.I can do this with a for loop, looking up the first element of b in a and then removing the element from b and from a, etc. But this doesn't appeal to me, it would be very inefficient (order of  time) while it should be no problem to do this in  time.Python 2.7 and 3.2 will add the  class which is a dictionary that maps elements to the number of occurrences of the element.  This can be used as a multiset.According to the docs you should be able to do something like this (untested, since I do not have either version installed).:Since you are stuck with 2.5 you could try importing it and define your own version if that fails.  That way you will be sure to get the latest version if it is available, and fall back to a working version if not. You will also benefit from speed improvements if if gets converted to a C implementation in the future.i.e.You can find the current Python source . I know \"for\" is not what you want, but it's simple and clear:Or if members of  might not be in  then use:..as wich wrote, this is wrong - it works only if the items are unique in the lists. And if they are, it's better to useI'm not sure what the objection to a for loop is: there is no multiset in Python so you can't use a builtin container to help you out.Seems to me anything on one line (if possible) will probably be helishly complex to understand.  Go for readability and KISS.  Python is not C :)to use list comprehension:would do the trick. It would change b in the process though. But I agree with jkp and Dyno Fu that using a for loop would be better.Perhaps someone can create a better example that uses list comprehension but still is KISS? Python 2.7+ and 3.0 have  (a.k.a. multiset).  The documentation links to  for Python 2.5:Then you can writeTo prove jkp's point that 'anything on one line will probably be helishly complex to understand', I created a one-liner. Please do not mod me down because I understand this is not a solution that you should actually use. It is just for demonstrational purposes.The idea is to add the values in a one by one, as long as the total times you have added that value does is smaller than the total number of times this value is in a minus the number of times it is in b:The horror! But perhaps someone can improve on it? Is it even bug free?Edit: Seeing Devin Jeanpierre comment about using a dictionary datastructure, I came up with this oneliner:Better, but still unreadable.You can try something like this:You have to define what [1, 2, 3] - [5, 6] should output though, I guess you want [1, 2, 3] thats why I ignore the ValueError.Edit:\nNow I see you wanted an exception if  does not contain all elements, added it instead of passing the ValueError.I attempted to find a more elegant solution, but the best I could do was basically the same thing that Dyno Fu said:Here's a relatively long but efficient and readable solution. It's O(n).You can use the  construct to do this. It looks quite ok, but beware that the  line itself will return a list of s."},
{"body": "The list can be all  or one of it is an re.Match instance.\nWhat one liner check can I do on the returned list to tell me that the contents are all ?will return  if all of the elements of  are Note that  is a lot faster but requires that  be an actual  and not just an iterable.returns  if all items of  are falsy.:  Since match objects are always trucy and  is falsy, this will give the same result as  for the case at hand.  As demonstrated in , using  is by far the faster alternative.Since Match objects are never going to evaluate to false, it's ok and much faster to just use Or a bit weird but:OR:EDITED:"},
{"body": "I am reading a code. There is a class in which  method is defined. I figured out that this method is used to destroy an instance of the class. However, I cannot find a place where this method is used. The main reason for that is that I do not know how this method is used, probably not like that: . So, my questions is how to call the  method? Thank you for any help. is a . It is called when an object is  which happens after all references to the object have been deleted. In a  this could be right after you say  or, if  is a local variable, after the function ends. In particular, unless there are circular references, CPython (the standard Python implementation) will garbage collect immediately. However, this is the  of CPython. The only  property of Python garbage collection is that it happens  all references have been deleted, so this might not necessary happen  and . Even more, variables can live for a long time for , e.g. a propagating exception or module introspection can keep variable reference count greater than 0. Also, variable can be a part of  \u2014 CPython with garbage collection turned on breaks most, but not all, such cycles, and even then only periodically.Since you have no guarantee it's executed, one should  put the code that you need to be run into  \u2014 instead, this code belongs to  clause of the  block or to a context manager in a  statement. However, there are  for : e.g. if an object  references  and also keeps a copy of  reference in a global  () then it would be polite for  to also delete the cache entry. that the destructor provides (in violation of the above guideline) a required cleanup, you might want to , since there is nothing special about it as a method: . Obviously, you should you do so only if you know that it doesn't mind to be called twice. Or, as a last resort, you can redefine this method usingI wrote up the answer for another question, though this is a more accurate question for it.  Here is a slightly opinionated answer.Don't use . This is not C++ or a language built for destructors. The  method really should be gone in Python 3.x, though I'm sure someone will find a use case that makes sense. If you need to use , be aware of the basic limitations per :But, on the other hand:And my pesonal reason for not liking the  function.So, find a reason not to use .The  method (note spelling!) is called when your object is finally destroyed.  Technically speaking (in cPython) that is when there are no more references to your object, ie when it goes out of scope.If you want to delete your object and thus call the  method usewhich will delete the object (provided there weren't any other references to it).I suggest you write a small class like thisAnd investigate in the python interpreter, egNote that jython and ironpython have different rules as to exactly when the object is deleted and  is called.  It isn't considered good practice to use  though because of this and the fact that the object and its environment may be in an unknown state when it is called.  It isn't absolutely guaranteed  will be called either - the interpreter can exit in various ways without deleteting all objects.The  method, it will be called when the object is garbage collected.  Note that it isn't necessarily guaranteed to be called though.  The following code by itself won't necessarily do it:The reason being that  just decrements the reference count by one.  If something else has a reference to the object,  won't get called.There are a few caveats to using  though.  Generally, they usually just aren't very useful.  It sounds to me more like you want to use a close method or maybe a .See the .One other thing to note:   methods can inhibit garbage collection if overused.  In particular, a circular reference that has more than one object with a  method won't get garbage collected.  This is because the garbage collector doesn't know which one to call first.  See the documentation on the  for more info."},
{"body": "I have two vectors as Python lists and an angle. E.g.:What is the best/easiest way to get the resulting vector when rotating the v vector around the axis?The rotation should appear to be counter clockwise for an observer to whom the axis vector is pointing. This is called the Take a look at . It provides a  class which has a method . It also provides a helper function  if you don't want to call the method on .Using the :A one-liner, with numpy/scipy functions.We use the following:  computes the taylor series of the exponential:\n\n, so it's time expensive, but readable and secure.\nIt can be a good way if you have few rotations to do but a lot of vectors.I just wanted to mention that if speed is required, wrapping unutbu's code in scipy's weave.inline and passing an already existing matrix as a parameter yields a 20-fold decrease in the running time.The code (in rotation_matrix_test.py):The timing:I made a fairly complete library of 3D mathematics for Python{2,3}. It still does not use Cython, but relies heavily on the efficiency of numpy. You can find it here with pip:Or have a look at my gitweb   and now also on github:  .Once installed, in python you may create the orientation object which can rotate vectors, or be part of transform objects. E.g. the following code snippet composes an orientation that represents a rotation of 1 rad around the axis [1,2,3], applies it to the vector [4,5,6], and prints the result:The output would beThis is more efficient, by a factor of approximately four, as far as I can time it, than the oneliner using scipy posted by B. M. above. However, it requires installation of my math3d package."},
{"body": "When describing a python package in  in  in Python, is there a way to make it so automatically get every directory that has a  in it and include that as a subpackage?ie if the structure is: I want to avoid doing:and instead just do:and have it automatically find things like  and  since they have an init file. thanks.The easiest way (that I know of) is to use  to yield the packages:I would recommend using the find_packages() function available with  such as:and then do Same as dm76 answer, just that I also have tests in my repo, so I use:"},
{"body": "null=True\nblank=True\ndefault = 0What's the difference? When do you use what?Direct from :  Do you not understand all this?From :You can use \"\" to set the value that will be used for the field in question should your code not explicitly set it to a value.Use \"\" for form validation purposes - blank=True will allow the field to be set to an empty valueUse \"\" if you would like to store an empty value as \"null\" in the DB.  Often it's preferred, however, to set blank values to an empty string or to 0 as appropriate for a given field.In implementation terms:The 'blank' field corresponds to all forms. Specifies if this value is required in form and corresponding form validation is done.\n'True' allows empty values.The 'null' field corresponds to DB level. It will be set as NULL or NOT NULL at the DB.Hence if leave a field empty in admin with blank=true, NULL is fed into the DB. Now this might throw an error if that particular column in the DB is specified as NOT NULL.Is probably as laymen as it gets.Blank implies that it is NOT NULL and instead has some value that represents \"blank\" for that datatype, which in the case of a string is probably an empty string. For numbers it might be 0."},
{"body": "I'm confused about how  searches for the executable when using .  It works if given absolute paths to the child process, but I'm trying to use relative paths.  I've found that if I set the environment variable PYTHONPATH then I can get imported modules from that path ok, and PYTHONPATH is there in , but it doesn't seem to help with the behaviour of .  I've also tried editing the  file adding PYTHONPATH to , like so and verified that when starting up python , either interactively, with ipython, or by running a script from the command line, that PYTHONPATH is successfully appearing in .  However,   doesn't search there for the executable.   I thought it was supposed to inherit the parents environment, if no  kwarg is specified?  Next I tried giving  explicitly, first by making a copy of  and secondly just by giving , and it still does not find the executable.  Now I'm stumped.  Hopefully an example will help explain my problem more clearly:/dir/subdir1/some_executable\n/dir/subdir2/some_script.pyIf I'm in  and I run  it works, but if I'm in  and I run  even though  is in the , then subprocess will throw .  (filling in details from a comment to make a separate answer)First off, relative paths (paths containing slashes) never get checked in any PATH, no matter what you do.  They are relative to the  only.  If you need to resolve relative paths, you will have to search the PATH manually, or munge the PATH to include the subdirectories and then just use the command name as in my suggestion below.If you want to run a program , use  and go from there to find the absolute path of the program, and then use the absolute path in .Secondly, there is  about how Python deals with bare commands (no slashes).  Basically, on Unix/Mac  uses  when invoked with , which means it looks at the value of   and no amount of changing  will help you fix that.  Also, on Windows with , it pays no attention to PATH at all, and will only look in relative to the current working directory.If you JUST need path evaluation and don't really want to run your command line through a shell, and are on UNIX, I advise using  instead of , as in .  This lets you pass a different PATH to the  process, which will use it to find the program.  It also avoids issues with shell metacharacters and potential security issues with passing arguments through the shell.  Obviously, on Windows (pretty much the only platform without a ) you will need to do something different.You appear to be a little confused about the nature of  and . is an environment variable that tells the OS shell where to search for executables. is an environment variable that tells the Python interpreter where to search for modules to import. It has nothing to do with  finding executable files.Due to the differences in the underlying implementation,  will only search the path by default on non-Windows systems (Windows has some system directories it always searches, but that's distinct from  processing). The only reliable cross-platform way to scan the path is by passing  to the subprocess call, but that has its own issues (as detailed in the )However, it appears your main problem is that you are passing a path fragment to  rather than a simple file name. As soon as you have a directory separator in there, you're going to disable the  search, even on a non-Windows platform (e.g. see the Linux documentation for the ).A relative path in subprocess.Popen acts relative to the current working directory, not the elements of the systems PATH.  If you run  from  than the expected executable location will be , a.k.a .If you would definitely like to use relative paths from a scripts own directory to a particular executable the best option would be to first construct an absolute path from the directory portion of the  global variable.The pythonpath is set to the path from where the python interpreter is executed. So, in second case of your example, the path is set to /dir and not /dir/subdir2\nThat's why you get an error."},
{"body": "I know how to convert a numpy array into a tensor object with the function . My problem is that after I apply some preprocessing to this tensors in terms of brightness, contrast, etc, I would like to view the resulting transformations to evaluate and tweak my parameters.How can I convert a tensor into a numpy array so I can show it as an image with PIL?To convert back from tensor to numpy array you can simply run  on the transformed tensor.Any tensor returned by  or  is a NumPy array.Or:Or, equivalently:You need to: Code:This worked for me. You can try it in a ipython notebook. Just don't forget to add the following line:Maybe you can try\uff0cthis method:"},
{"body": "I'm fairly new when it comes to programming, and have started out learning python.What I want to do is to recolour sprites for a game, and I am given the original colours,\nfollowed by what they are to be turned into. Each sprite has between 20 and 60 angles, so\nlooping through each one in the folder for each colour is probably the way to go for me.\nMy code goes thusly;But I keep getting an error on the pathname, and on pix being a string value (They're all pictures)Any help appreciated. returns a list of file names. Thus,  is a string. You need to open the file before iterating on it, I guess.Also, be careful with backslashes in strings. They are mostly used for special escape sequences, so you need to escape them by doubling them. You could use the constant  to be more portable, or even use  :iterates over the letters of the filename. So that's certainly not what you want. You'll probably want to replace that line by:(assuming Python 2.6) and indent the rest of the loop accordingly. However, I'm not sure that the new  loop does what you want unless by  you mean a line of text in the current file. If the files are binary picture files, you'll first need to read their contents correcty - not enough info in your post to guess what's right here. The path is wrong because the backslashes need to be doubled up - backslash is an escape for special characters. does not return open files, it returns filenames. You need to open the file using the filename."},
{"body": "I love how beautiful python looks/feels and I'm hoping this can be cleaner (readability is awesome).What's a clean way to accept an optional keyword argument when overriding a subclassed  where the optional  has to be used  the  call? I have a django form where I'd like to accept an optional user argument, but if I define it as one of the arguments , then the usual form call  assumes the positional argument refers to the keyword argument .Code speaks better than (my) words:I can make this the cleanest by looking into the actual  class to see what arguments it's looking for, but one of the greatest things about subclassing anything in python is collecting all  and  and passing it into whatever you subclassed. Also this doesn't hold up to any changes in the source.But unfortunately I have:Thanks for any input!I usually just do essentially what you're doing here. However, you can shorten/clean up your code by supplying a default argument to :"},
{"body": "Without going through with the installation, I want to quickly see all the packages that  would install.The closest you can get with pip directly is by using the  argument:For example, this is the output when installing celery:Admittedly, this does leave some cruft around in the form of temporary files, but it does accomplish the goal. If you're doing this with virtualenv (which you should be), the cleanup is as easy as removing the  directory.The accepted answer is no longer relevant for more current versions of pip and does not give an immediate answer without perusing multiple comments so I am providing an updated answer.This was tested with pip versions  and .To get the output without cluttering your current directory on Linux use tells pip the directory that download should put files in.Better, just use this script with the argument being the package name to get only the dependencies as output:Also available .If and only if the package is install, you can use . Look for the  filed at the end of the output.  Clearly, this breaks your requirement but might be useful nonetheless.For example:The command  should be used, as mentioned in comments by @radtek, since as of 7.0.0 (2015-05-21), --no-install is  from . This will download the dependencies needed into ."},
{"body": "When I call  it will generate  but there are times where I'd like it to generate  instead. I can't find in the documentation where I would specify this. Do I just need to do ? takes an  keyword argument that will return an absolute (rather than relative) URL.  I believe you will need to set a  config key with your root domain to make it work correctly."},
{"body": "I have a very large dataset were I want to replace strings with numbers. I would like to operate on the dataset without typing a mapping function for each key (column) in the dataset. (similar to the fillna method, but replace specific string with assosiated value).\nIs there anyway to do this?Here is an example of my datasetThe desired result:very bad=1, bad=2, poor=3, good=4, very good=5//JonasUse "},
{"body": "In Python 2, we could reassign  and  (but not ), but all three (, , and ) were considered builtin variables.  However, in Py3k all three were changed into keywords as per .From my own speculation, I could only guess that it was to prevent shenanigans like  which derive from the old  prank.  However, in Python 2.7.5, and perhaps before, statements such as  which reassigned  raised .Semantically, I don't believe , , and  are keywords, since they are at last semantically literals, which is what Java has done.  I checked PEP 0 (the index) and I couldn't find a PEP explaining why they were changed.Are there performance benefits or other reasons for making them keywords as opposed to literals or special-casing them like  in python2?Possibly because Python 2.6 not only allowed  but also allowed you to say funny things like:which would reset  to  for the entire process.  It can lead to really funny things happening:: As pointed out by , the  also states the following under :For two reasons, mainly:This was discussed some months ago on python-dev. \nHaving tons of links to the definition of True would be annoying, contrary to links to e.g. the nonlocal or with statements doc.And things I conclude why True and False will make things \"even finer\"."},
{"body": "I'd like to generate video using  feature.I've followed instructions found  and .And I now have the following process to build my  function:Fire a  instance and run this as root on it:I scp the resulting tarball to my laptop. And then run this script to build a zip archive. script is at the moment only a test to see if the stack is ok:Then I upload the resulting archive to S3 to be the source of my  function.\nWhen I test the function I get the following :I cant understand why python does not found the core directory that is present in the folder structure.EDIT:Following @jarmod advice I've reduced the function to:I now have the following error:I was also following your first link and managed to import  and  in a Lambda function this way (on Windows):With this, I could import numpy and pandas. I'm not familiar with moviepy, but scipy might already be tricky as Lambda has a  for unzipped deployment package size at 262 144 000 bytes. I'm afraid numpy and scipy together are already over that.With the help of all posts in this thread here is a solution for the records:To get this to work you'll need to:You can also download the  .The posts here help me to find a way to statically compile NumPy with libraries files that can be included in the AWS Lambda Deployment package. This solution does not depend on LD_LIBRARY_PATH value as in @rouk1 solution.The compiled NumPy library can be downloaded from Here are the steps to custom compile NumPyPrepare a fresh AWS EC instance with AWS Linux.Install compiler dependenciesInstall NumPy dependencies Create /var/task/lib to contain the runtime libraries/var/task is the root directory where your code will reside in AWS Lambda thus we need to statically link the required library files in a well known folder which in this case /var/task/libCopy the following library files to the /var/task/libGet the latest numpy source code from Go to the numpy source code folder e.g numpy-1.10.4\nCreate a site.cfg file with the following entries-lgfortran -lquadmath flags are required to statically link gfortran and quadmath libraries with files defined in runtime_library_dirsBuild NumPyInstall NumPyCheck whether the libraries are linked to the files in /var/task/libYou should seeAnother, very simple method that's possible these days is to build using the awesome docker containers that LambCI made to mimic Lambda: The  container resembles AWS Lambda with the addition of a mostly-complete build environment. To start a shell session in it:Inside the session:Or, with virtualenv:Later on you can use the main lambci/lambda container to test your build.I like @Vito Limandibhrata's answer but I think it's not enough to build numpy with runtime_library_dirs in numpy==1.11.1. If anybody think site-cfg is ignored, do the following:*.a files under atlas-sse3 are needed to build numpy. Also, you might need to run the following:to check numpy configuration. If it requires something more, you will see the following message:then site-cfg is going to be ignored.Tip: If pip is used to build numpy with runtime_library_dirs, you would better create  and add the following:then numpy recognizes .numpy-site.cfg file. It's quite simple and easy way.As of 2017, NumPy and SciPy have wheels that work on Lambda (the packages include precompiled  and ).\nAs far as I know, MoviePy is a pure Python module, so basically you could do:Then copy your handler into the  directory and zip it. Except, that you'll most likely exceed the 50/250 MB size limits. There are a couple of things that can help:Here's an example  that automates the above points."},
{"body": "In the 2nd case below, Python tries to look for a local variable. When it doesn't find one, why can't it look in the outer scope like it does for the 1st case? This looks for x in the local scope, then outer scope:This gives  error:I am not allowed to modify the signature of function f2() so I can not pass and return values of x. However, I do need a way to modify x. Is there a way to explicitly tell Python to look for a variable name in the outer scope (something similar to the  keyword)?Python version: 2.7Workaround is to use a mutable object and update members of that object. Name binding is tricky in Python, sometimes.In Python 3.x this is possible:The problem and a solution to it, for Python 2.x as well, are given in  post. Additionally, please read  for more information on this subject."},
{"body": "I've been searching for the accurate answer to this question for a couple of days now but haven't got anything good. I'm not a complete beginner in programming, but not yet even on the intermediate level.When I'm in the shell of Python, I type:  and I can see all the names of all the objects in the current scope (main block), there are 6 of them:Then, when I'm declaring a variable, for example , it automatically adds to that lists of objects under built-in module , and when I type  again, it shows now:The same goes for functions, classes and so on.  How do I delete all those new objects without erasing the standard 6 which where available at the beginning?I've read here about \"memory cleaning\", \"cleaning of the console\", which erases all the text from the command prompt window:  But all this has nothing to do with what I'm trying to achieve, it doesn't clean out all used objects.You can delete individual names with :or you can remove them from the  object:This is just an example loop; it defensively only deletes names that do not start with an underscore, making a (not unreasoned) assumption that you only used names without an underscore at the start in your interpreter. You could use a hard-coded list of names to keep instead (whitelisting) if you really wanted to be thorough. There is no built-in function to do the clearing for you, other than just exit and restart the interpreter.Modules you've imported () are going to remain imported because they are referenced by ; subsequent imports will reuse the already imported module object. You just won't have a reference to them in your current global namespace.Yes. There is a simple way to remove everything in iPython. \nIn iPython console, just type:Then system will ask you to confirm. Press y.\nIf you don't want to see this prompt, simply type:This should work..Actually python will reclaim the memory which is not in use anymore.This is called  which is automatic process in python. But still if you want to do it then you can delete it by . You can also do it by assigning the variable to  The only way to truly reclaim memory from unreferenced Python objects is via the garbage collector. The del keyword simply unbinds a name from an object, but the object still needs to be garbage collected. You can force garbage collector to run using the gc module, but this is almost certainly a premature optimization but it has its own risks. Using  has no real effect, since those names would have been deleted as they went out of scope anyway."},
{"body": "I've two pandas data frames which have some rows in common.Suppose dataframe2 is a subset of dataframe1.One method would be to store the result of an inner merge form both dfs, then we can simply select the rows when one column's values are not in this common:Another method as you've found is to use  which will produce  rows which you can drop:However if df2 does not start rows in the same manner then this won't work:will produce the entire df:Suppose you have two dataframes, df_1 and df_2 having multiple fields(column_names) and you want to find the only those entries in df_1 that are not in df_2 on the basis of some fields(e.g. fields_x, fields_y), follow the following steps.Step1.Add a column key1 and key2 to df_1 and df_2 respectively.Step2.Merge the dataframes as shown below. field_x and field_y are our desired columns.Step3.Select only those rows from df_1 where key1 is not equal to key2.Step4.Drop key1 and key2.This method will solve your problem and works fast even with big data sets. I have tried it for dataframes with more than 1,000,000 rows.As already hinted at, isin requires columns and indices to be the same for a match. If match should only be on row contents, one way to get the mask for filtering the rows present is to convert the rows to a (Multi)Index:If index should be taken into account, set_index has keyword argument append to append columns to existing index. If columns do not line up, list(df.columns) can be replaced with column specifications to align the data.could alternatively be used to create the indices, though I doubt this is more efficient.you can do it using  method:Explanation:a bit late, but it might be worth checking the \"indicator\" parameter of pd.merge.See this other question for an example:\nAssuming that the indexes are consistent in the dataframes (not taking into account the actual col values):My way of doing this involves adding a new column that is unique to one dataframe and using this to choose whether to keep an entryThis makes it so every entry in df1 has a code - 0 if it is unique to df1, 1 if it is in both dataFrames. You then use this to restrict to what you want"},
{"body": "The python documentation frequently speaks of \"containers\".  :But I can't find any official definition of containers, neither a list of them.For Python 2.7.3:( returns ) ( returns ):Tell me which other builtin types you have checked for   and I'll add them to the list.Containers are any object that holds an arbitrary number of other objects.  Generally, containers provide a way to access the contained objects and to iterate over them.Examples of containers include , , , ; these are the .  More container types are available in the  module.Strictly speaking, the  abstract base class ( in Python2) holds for any type that supports the  operator via the  magic method; so if you can write  then  is  a container, but not always: an important point of difference between  and general  is that when iterated over, containers will return existing objects that they hold a reference to, while generators and e.g.  objects will create a new object each time.  This has implications for garbage collection and deep object traversal (e.g.  and serialisation).As an example,  supports the  operator, but it is certainly  a container!The intent of the  abstract base class in only considering the  magic method and not other ways of supporting the  operator is that a true container should be able to test for containment in a single operation and without observably changing internal state.  Since  defines  as an abstract method, you are guaranteed that if  then  supports the  operator.In practice, then, all containers will have the  magic method. However, when testing whether an object is a container you should use  for clarity and for forward compatibility should the  subclass check ever be changed.According to , the most general definition of a container is simply an object that implements .  In general, Python concepts like \"container\" or \"sequence\" are not defined abstractly; they are \"duck-typed\" by their behavior.  That is, a container is something that you can use the  operator on.The Python builtin container types are tuple, list, dict, set, frozenset and str and unicode (or bytes and str in Python 3), as well as a couple other constructs that are technically types but are not commonly used outside of specific contexts (e.g., buffer objects and xrange objects).  Additional container types are provided in the  module.Container are all python objects that contain other object like  or . The  type is an ABC, it behave like an interface. A Container is a class that implements the  method.Here is the "},
{"body": "I went through the  in the  documentation, but it wasn't clear to me how I can make a plot that fills the area between two specific vertical lines.For example, say I want to create a plot between  and  (for the full  range of the plot). Should I use ,  or ? Can I use the  condition for this?It sounds like you want , rather than one of the fill between functions. The differences is that  (and ) will fill up the entire y (or x) extent of the plot regardless of how you zoom.For example, let's use  to highlight the x-region between 8 and 14:You could use  to do this, but the extents (both x and y) of the rectangle would be in .  With , the y-extents of the rectangle default to 0 and 1 and are in  (in other words, percentages of the height of the plot).  To illustrate this, let's make the rectangle extend from 10% to 90% of the height (instead of taking up the full extent).  Try zooming or panning, and notice that the y-extents say fixed in display space, while the x-extents move with the zoom/pan:"},
{"body": "Where should utility functions live in Django? Functions like custom encrypting/decrypting a number, sending tweets, sending email, verifying object ownership, custom input validation, etc. Repetitive and custom stuff that I use in a number of places in my app. I'm definitely breaking DRY right now.I saw some demos where functions were defined in models.py, although that didn't seem conceptually right to me. Should they go in a \"utilities\" app that gets imported into my project? If so, where do they go in the utilities app? The models.py file there?Thanks for helping this n00b out. Let me be even more specific. Say I need a function \"light_encrypt(number)\" which takes the param \"number\", multiplies it by 7, adds 10 and returns the result, and another function \"light_decrypt(encr_number) which takes the param \"encr_number\", subtracts 10, divides by 7 and returns the results. Where in my Django tree would I put this? This is not middleware, right? As Felix suggests, do I create a python package and import it into the view where I need these functions?Different  but same answer:OK, after reading the comments and answer here I've decided to create a directory called \"common/util/\" inside my project directory. Within that I have a file \"__ init__.py\" where I have my little helper functions. I guess if the file gets too big, I'll then split out the functions into individual .py files in common. So now, my project structure looks like this. Please correct if I'm making any poor choices, I'm early enough in development that I can fix it now while it is still easy to do so!Here is another way to do it:If the utility functions can be a stand-alone module\nand you are using a virtualenv environment for your django app\nthen you can bundle the functionality as a package and install it in your virtualenv.This makes it easy to import where ever you need it in your django app."},
{"body": "An Android/Iphone app will be accessing application data from the server. \n[Django-Python]How can I secure the communication with the mobile app ? : Secure enough for sensitive information like passwords, there shall be no direct way of decryption except brute-forcing. :: :So, how can/should I move forward on this ?\nShould I implement casual approach :Conclusion was to use AES, since if I can keep the key secure then I am as good as SSL.\nPlus I can keep changing the key over-time for better security.\nContribute if you think there is a better way, do read the entire post before posting.You're working on bad information. SSL can absolutely authenticate the client, it's just not something that is done for the bulk of SSL as the protocol is (or, atleast was) typically used to protect e-commerce sites where authentication of the server was important but doing so with the client was not important and/or not feasible. What you want to do is employ mutually-authenticated SSL, so that your server will only accept incoming connections from your app and your app will only communicate with your server.Here's the high-level approach. Create a self-signed server SSL certificate and deploy on your web server. If you're using Android, you can use the keytool included with the Android SDK for this purpose; if you're using another app platform like iOS, similar tools exist for them as well. Then create a self-signed client and deploy that within your application in a custom keystore included in your application as a resource (keytool will generate this as well). Configure the server to require client-side SSL authentication and to only accept the client certificate you generated. Configure the client to use that client-side certificate to identify itself and only accept the one server-side certificate you installed on your server for that part of it.If someone/something other than your app attempts to connect to your server, the SSL connection will not be created, as the server will reject incoming SSL connections that do not present the client certificate that you have included in your app.A step-by-step for this is a much longer answer than is warranted here. I would suggest doing this in stages as there are resources on the web about how to deal with self-signed SSL certificate in both Android and iOS, both server and client side. There is also a complete walk-through in my book, , published by O'Reilly.SSL does have two way authentication as already mentioned by the other commenters. \nBut, I do not think you even should try to authenticate the client, aka the app. You only authenticate the user (resource owner in Oauth terms) not the agent or client.It is a fact that mobile apps cannot hold any secrets. So never put certificates/ passwords on the device. Typical bad example would be to save the username and password in some system keystore, such as IOS keychain. If the app user does not set password on the phone, the entire keystore is saved in plain text and anyone can dump all information. Embed a certificate in the app is almost equally bad as unlike a server, mobile phone is not locked in a computer room. People do lose them.On that basis, you need a token based solution, so that the app does not need to hold secrets. You pass on the secrets (user login credentials) and clear them out from memory immediately. You only need to hold the token, which will be short lived (expires in 30 mins etc.)Oauth 2.0 Implicit flow is designed to solve this problem. However, its a far from perfect. And there are some fundamental issues with the Oauth 2.0 spec.  Especially, implementing this flow requires the app to use UIWebView (embeded browser), which itself can be insecure and bad user experience.  So this pretty much eliminates all redirection based flows.  The only well known app that uses OAuth 2 redirection flow is facebook, and its done badly. OAuth 2.0 Resource Owner flow is one option.   With this flow, your entire systems security level can be as high as B2C solution -- browser based online banking solution as an example. This means anyone with the username password will be able to access the resources on the server -- the same level of security for a browser based solution.However, you still need to be careful, as mentioned before, the OAuth 2 spec has some fundamental issues -- in this case, you cannot follow its spec to implement the token refresh logic -- that typically involves using a never-expire refreshing token-- which can be seen as Google's OAuth 2 implementation. That token then becomes a secret itself -- defeats the purpose of using OAuth. One workaround is to auto-renew the token based on last activity. Anyway, mobile app security is not a new topic at all but sadly we still do not have any standard tools/mechinisms to solve those unique challenges. Thats why banks pay millions to do their mobile banking solution and yet they still fail() \n;-)Use client authentication with SSL or just layer your own client authentication (username/password, token, etc) on top of server-authentication SSL. (Edit: Moving the comment here, since it won't fit as a comment)To elaborate a bit, any authentication info needs to be stored or entered in the app. If you have people enter the password each time, you don't need to save it, but that's clearly inconvenient. You can encrypt it with a device-specific key, so it's not visible on rooted devices. With a private key, you need to either protect it with a user entered password (see above) or have it protected by the system. That is only available since Android 4.0 (ICS) with the public API to the system keystore, the  class. In this case, the user needs to unlock (using pattern/password or PIN) the phone to access the keystore."},
{"body": "\nLast year I made some C++ code to compute posterior probabilities for a particular type of model (described by a Bayesian network). The model worked pretty well and some other people started to use my software. Now I want to improve my model. Since I'm already coding slightly different inference algorithms for the new model, I decided to use python because runtime wasn't critically important and python may let me make more elegant and manageable code.Usually in this situation I'd search for an existing Bayesian network package in python, but the inference algorithms I'm using are my own, and I also thought this would be a great opportunity to learn more about good design in python.I've already found a great python module for network graphs (networkx), which allows you to attach a dictionary to each node and to each edge. Essentially, this would let me give nodes and edges properties. For a particular network and it's observed data, I need to write a function that computes the likelihood of the unassigned variables in the model. For instance, in the classic \"Asia\" network (), with the states of \"XRay Result\" and \"Dyspnea\" known, I need to write a function to compute the likelihood that the other variables have certain values (according to some model). \nI'm going to try a handful of models, and in the future it's possible I'll want to try another model after that. For instance, one model might look exactly like the Asia network. In another model, a directed edge might be added from \"Visit to Asia\" to \"Has Lung Cancer.\" Another model might use the original directed graph, but the probability model for the \"Dyspnea\" node given the \"Tuberculosis or Cancer\" and \"Has Bronchitis\" nodes might be different. All of these models will compute the likelihood in a different way.All the models will have substantial overlap; for instance, multiple edges going into an \"Or\" node will always make a \"0\" if all inputs are \"0\" and a \"1\" otherwise. But some models will have nodes that take on integer values in some range, while others will be boolean. In the past I've struggled with how to program things like this. I'm not going to lie; there's been a fair amount of copied and pasted code and sometimes I've needed to propagate changes in a single method to multiple files. This time I  want to spend the time to do this the right way.Some options:Thanks a lot for your help.\nObject oriented ideas help a lot here (each node has a designated set of predecessor nodes of a certain node subtype, and each node has a likelihood function that computes its likelihood of different outcome states given the states of the predecessor nodes, etc.). OOP FTW!I've been working on this kind of thing in my spare time for quite a while.  I think I'm on my third or fourth version of this same problem right now.  I'm actually getting ready to release another version of Fathom (https://github.com/davidrichards/fathom/wiki) with dynamic bayesian models included and a different persistence layer.As I've tried to make my answer clear, it's gotten quite long.  I apologize for that.  Here's how I've been attacking the problem, which seems to answer some of your questions (somewhat indirectly):I've started with Judea Pearl's breakdown of belief propagation in a Bayesian Network. That is, it's a graph with prior odds (causal support) coming from parents and likelihoods (diagnostic support) coming from children.  In this way, the basic class is just a BeliefNode, much like what you described with an extra node between BeliefNodes, a LinkMatrix.  In this way, I explicitly choose the type of likelihood I'm using by the type of LinkMatrix I use.  It makes it easier to explain what the belief network is doing afterwards as well as keeps the computation simpler.Any subclassing or changes that I'd make to the basic BeliefNode would be for binning continuous variables, rather than changing propagation rules or node associations.I've decided on keeping all data inside the BeliefNode, and only fixed data in the LinkedMatrix.  This has to do with ensuring that I maintain clean belief updates with minimal network activity.  This means that my BeliefNode stores:The LinkMatrix can be constructed with a number of different algorithms, depending on the nature of the relationship between the nodes.  All of the models that you're describing would just be different classes that you'd employ.  Probably the easiest thing to do is default to an or-gate, and then choose other ways to handle the LinkMatrix if we have a special relationship between the nodes.  I use MongoDB for persistence and caching.  I access this data inside of an evented model for speed and asynchronous access.  This makes the network fairly performant while also having the opportunity to be very large if it needs to be.  Also, since I'm using Mongo in this way, I can easily create a new context for the same knowledge base.  So, for example, if I have a diagnostic tree, some of the diagnostic support for a diagnosis will come from a patient's symptoms and tests.  What I do is create a context for that patient and then propagate my beliefs based on the evidence from that particular patient.  Likewise, if a doctor said that a patient was likely experiencing two or more diseases, then I could change some of my link matrices to propagate the belief updates differently.If you don't want to use something like Mongo for your system, but you are planning on having more than one consumer working on the knowledge base, you will need to adopt some sort of caching system to make sure that you are working on freshly-updated nodes at all times.My work is open source, so you can follow along if you'd like.  It's all Ruby, so it would be similar to your Python, but not necessarily a drop-in replacement.  One thing that I like about my design is that all of the information needed for humans to interpret the results can be found in the nodes themselves, rather than in the code.  This can be done in the qualitative descriptions, or in the structure of the network.So, here are some important differences I have with your design:One big caveat: some of what I'm talking about hasn't been released yet.  I worked on the stuff I am talking about until about 2:00 this morning, so it's definitely current and definitely getting regular attention from me, but isn't all available to the public just yet.  Since this is a passion of mine, I'd be happy to answer any questions or work together on a project if you'd like.The  solves a similar problem: you describe your problem in terms of constraints on finite domain variables, constraint propagators and distributors, cost functions. When no more inference is possible but there are still unbound variables, it uses your cost functions to split the problem space on the unbound variable that most likely reduces search costs: that is, if X is between [a,c] is such a variable, and c (a < b < c) is the point most likely to reduce search cost, you end up with two problem instances where X is between [a,b] and, in the other instance, X is between [b,c]. Mozart does this rather elegantly since it reifies variable binding as a first class object (this is very useful, since Mozart is pervasively concurrent and distributed, to move a problem space to a different node). In its implementation, I suspect that it employs a copy-on-write strategy.You can surely implement a copy-on-write scheme in a graph-based library (tip: numpy uses various strategies to minimize copying; if you base your graph representation on it, you may get copy-on-write semantics for free) and reach your goals.I'm not too familiar with Bayesian Networks, so I hope the following is usefull:In the past I had a seemingly similar problem with a Gaussian Process regressor, instead of a\nbayesian classifier.I ended up using inheritance, which worked out nicely. All model-specific paremeters are set with the constructor. The calculate() functions are virtual.\nCascading different methods (e.g. a sum-method which combines an arbitrary number of other methods) also works nicely that way.I think you need to ask a couple questions that influence the design.If most of the time will be spent with existing models and new models will be less common, then inheritance is probably the design I would use.  It makes the documentation easy to structure and the code that uses it will be easy to understand.If the major purpose of the library is to provide a platform for experimenting with different models, then I would take the graph with properties that map to functors for computing things based on parents.  The library would be more complex and graph creation would be more complex, but it would be far more powerful as it would allow you to do hybrid graphs that change the computation functor based on the nodes.Regardless of what final design you work towards, I would start with a simple one class-one implementation design.  Get it passing a set of automated tests, then refactor into the more full design after that is done.  Also, don't forget version control ;-)"},
{"body": "I'm trying to understand the relationship between decision_function and predict, which are instance methods of SVC  (). So far I've gathered that decision function returns pairwise scores between classes. I was under the impression that predict chooses the class that maximizes its pairwise score, but I tested this out and got different results. Here's the code I was using to try and understand the relationship between the two. First I generated the pairwise score matrix, and then I printed out the class that has maximal pairwise score which was different than the class predicted by clf.predict.Does anyone know the relationship between these predict and decision_function?I don't fully understand your code, but let's go trough the example of the documentation page you referenced:Now let's apply both the decision function and predict to the samples:The output we get is:And that is easy to interpret: The desion function tells us on which side of the hyperplane generated by the classifier we are (and how far we are away from it). Based on that information, the estimator then label the examples with the corresponding label.When you call , you get the output from each of the pairwise classifiers (n*(n-1)/2 numbers total). See .Each classifier puts in a vote as to what the correct answer is (based on the sign of the output of that classifier);  returns the class with the most votes.For those interested, I'll post a quick example of the  function translated from C++ () to python:There are a lot of input arguments for  and , but note that these are all used internally in by the model when calling . In fact, all of the arguments are accessible to you inside the model after fitting:They probably have a bit complicated mathematical relation. But if you use the  in  classifier, the relation between those two will be more clear! Because then  will give you scores for each class label (not same as SVC) and predict will give the class with the best score. "},
{"body": "There is almost no documentation for it here:\nMaybe someone could, by example, show me how to populate a  in Django?-thanks.Looking at the source for django...Check out that regex validator.  Looks like as long as you give it a list of integers and commas, django won't complain.You can define it just like a charfield basically:And populate it like this:I just happened to have dealt with CommaSeparatedIntegerField in my latest project. I was using MySQL and it seemed like supplying a string of comma separated integer values is very DB friendly e.g. '1,2,3,4,5'. If you want to leave it blank, just pass an empty string.It does act like a CharField, and beaved in a weird way for me.. I ended up with values like \"[1, 2, 3, 4, 5]\" including the brackets in my database! So watch out!"},
{"body": "Lets say I have a MultiIndex Series :and I want to apply a function which uses the index of the row:How can I do  for such a function? What is the recommended way to make this kind of operations? I expect to obtain a new Series with the values resulting from this function applied on each row and the same MultiIndex.I don't believe  has access to the index; it treats each row as a numpy object, not a Series, as you can see:To get around this limitation, promote the indexes to columns, apply your function, and recreate a Series with the original index.Other approaches might use , which often gets a little ugly in my opinion, or , which is likely to be slower -- perhaps depending on exactly what  does.Make it a frame, return scalars if you want (so the result is a series)SetupPrinting functionSince you can return anything here, just return the scalars (access the index via the  attribute)You  find it faster to use  rather than  here:Also you can use numpy-style logic/functions to any of the parts:You can access the whole row as argument inside the fucntion if you use DataFrame.apply() instead of Series.apply(). "},
{"body": "I'm confused about the rules Pandas uses when deciding that a selection from a dataframe is a copy of the original dataframe, or a view on the original.If I have, for example,I understand that a  returns a copy so that something likewill have no effect on the original dataframe, . I also understand that scalar or named slices return a view, so that assignments to these, such as or will change . But I'm lost when it comes to more complicated cases. For example, changes , butdoes not.Is there a simple rule that Pandas is using that I'm just missing? What's going on in these specific cases; and in particular, how do I change all values (or a subset of values) in a dataframe that satisfy a particular query (as I'm attempting to do in the last example above)?Note: This is not the same as ; and I have read , but am not enlightened by it. I've also read through the \"Related\" questions on this topic, but I'm still missing the simple rule Pandas is using, and how I'd apply it to \u2014 for example \u2014\u00a0modify the values (or a subset of values) in a dataframe that satisfy a particular query.Here's the rules, subsequent override:Your example of is not guaranteed to work (and thus you shoulld  do this). Instead do:as this is  and will always workThe chained indexing is 2 separate python operations and thus cannot be reliably intercepted by pandas (you will oftentimes get a , but that is not 100% detectable either). The , which you pointed, offer a much more full explanation."},
{"body": "This  of @Dunes states, that due to pipeline-ing there is (almost) no difference between floating-point multiplication and division. However, from my expience with other languages I would expect the division to be slower.My small test looks as follows:For different commands and  I get the following times on my machine:The most interesting part: dividing by  is almost twice as fast as dividing by . One could assume, it is due to some smart optimization, e.g. replacing division by . However the timings of  and  are too far off to support this claim.In general, the division by floats with values  is faster:It gets even more interesting, if we look at :Now,  there is no difference between division by  and by !I tried it out for different numpy versions and different machines. On some machines (e.g. Intel Xeon E5-2620) one can see this effect, but not on some other machines  - and this does not depend on the numpy version.With the script of @Ralph Versteegen (see his great answer!) I get the following results: What is the reason for higher cost of the division by  compared to division by  for some processors, if the array sizes are large (>10^6).  The @nneonneo's answer states, that for some intel processors there is an optimization when divided by powers of two, but this does not explain, why we can see the benefit of it only for large arrays.The original question was \"How can these different behaviors (division by  vs. division by ) be explained?\"Here also, my original testing script, which produced the timings:At first I suspected that numpy is invoking BLAS, but at least on my machine (python 2.7.13, numpy 1.11.2, OpenBLAS), it doesn't, as a quick check with gdb reveals:In fact, numpy is running exactly the same generic loop regardless of the constant used. So all timing differences are purely due to the CPU.Actually, division is an instruction with a highly variable execution time. The amount of work to be done depends on the bit patterns of the operands, and special cases can also be detected and sped up. According to  (whose accuracy I do not know), on your E5-2620 (a Sandy Bridge) DIVPD has a latency and an inverse throughput of 10-22 cycles, and MULPS has latency 10 cycles and inverse throughput of 5 cycles. Now, as for  being slower than . gdb shows that exactly the same function is being used for multiplication, except now the output  differs from first input . So it has to be purely an artifact of the extra memory being drawn into cache slowing down the non-inplace operation for the large input (even though MULPS is producing only 2*8/5 = 3.2 bytes of output per cycle!). When using the 1e4-sized buffers, everything fits in cache, so that doesn't have a significant effect, and other overheads mostly drown out the difference between  and .Still, there are lots of weird effects in those timings, so I plotted some a graph (code to generate this is below)I've plotted size of the A array against the number of CPU cycles per DIVPD/MULPD/ADDPD instruction. I ran this on a 3.3GHz AMD FX-6100. Yellow and red vertical lines are L2 and L3 cache size. The blue line is the supposed maximum throughput of DIVPD according to those tables, 1/4.5cycles (which seems dubious). As you can see, not even  gets anywhere near this, even when the \"Overhead\" of performing an numpy operation falls close to zero. So there is about 24 cycles of overhead just looping and reading and writing 16 bytes to/from L2 cache! Pretty shocking, maybe the memory accesses aren't aligned.Lots of interesting effects to note:Unfortunately, on another machine with a CPU with different FPU speed, cache size, memory bandwidth, numpy version, etc, these curves could look quite different.My take-away from this is: chaining together multiple arithmetic operations with numpy is going to be many times slower than doing the same in Cython while iterating over the inputs just once, because there is no \"sweet spot\" at which the cost of the arithmetic operations dominates the other costs.Intel CPUs have special optimizations when dividing by powers of two. See, for example, , where it statesAlthough this applies to FDIV and not DIVPD (as @RalphVersteegen's answer notes), it would be quite surprising if DIVPD did not also implement this optimization.Division is normally a very slow affair. However, a division by a power of two is just an exponent shift, and the mantissa usually doesn't need to change. This makes the operation very fast. Furthermore, it's easy to detect a power of two in floating-point representation as the mantissa will be all zeros (with an implicit leading 1), so this optimization is both easy to test for and cheap to implement."},
{"body": "I have encountered an interesting case in my programming job that requires me to implement a mechanism of dynamic class inheritance in python. What I mean when using the term \"dynamic inheritance\" is a class that doesn't inherit from any base class in particular, but rather chooses to inherit from one of several base classes at instantiation, depending on some parameter.My question is thus the following: in the case I will present, what would be the best, most standard and \"pythonic\" way of implementing the needed extra functionality via dynamic inheritance.To summarize the case in point in a simple manner, I will give an example using two classes that represent two different image formats:  and  images. I will then try to add the ability to support a third format: the  image. I realize my question isn't that simple, but I hope you are ready to bare with me for a few more lines.This script contains two classes:  and , both inheriting\nfrom the  base class. To create an instance of an image object, the user is asked to call the  function with a file path as the only parameter.This function then guesses the file format ( or ) from the path and\nreturns an instance of the corresponding class.Both concrete image classes (and ) are able to decode\nfiles via their  property. Both do this in a different way. However,\nboth ask the  base class for a file object in order to do this.Building on the first image example case, one would like to\nadd the following functionality:An extra file format should be supported, the  format. Instead of\nbeing a new image file format, it is simply a compression layer that,\nonce decompressed, reveals either a  image or a  image.The  function keeps its working mechanism and will\nsimply try to create an instance of the concrete image class \nwhen it is given a  file. Exactly in the same way it would\ncreate an instance of  when given a  file.The  class just wants to redefine the  property.\nIn no case does it want to redefine the  property. The crux\nof the problem is that, depending on what file format is hiding\ninside the zip archive, the  classes needs to inherit\neither from  or from  dynamically. The correct class to\ninherit from can only be determined upon class creation when the \nparameter is parsed.Hence, here is the same script with the extra  class\nand a single added line to the  function.Obviously, the  class is non-functional in this example.\nThis code requires Python 2.7.I have found a way of getting the wanted behavior by intercepting the  call in the  class and using the  function. But it feels clumsy and I suspect there might be a better way using some Python techniques or design patterns I don't yet know about.Bear in mind if you want to propose a solution that the goal is not to change the behavior of the  function. That function should remain untouched. The goal, ideally, is to build a dynamic  class.I just don't really know what the best way to do this is. But this is a perfect occasion for me to learn more about some of Python's \"black magic\". Maybe my answer lies with strategies like modifying the  attribute after creation or maybe using the  class attribute? Or maybe something to do with the special  abstract base classes could help here? Or other unexplored Python territory?What about defining the  class on function-level ?\nThis will enable your .: Without need to change I would favor composition over inheritance here.  I think your current inheritance hierarchy seems wrong.  Some things, like opening the file with or gzip have little to do with the actual image format and can be easily handled in one place while you want to separate the details of working with a specific format own classes.  I think using composition you can delegate implementation specific details and have a simple common Image class without requiring metaclasses or multiple inheritance.I only tested this on a few local png and jpeg files but hopefully it illustrates another way of thinking about this problem.If you ever need \u201cblack magic\u201d, first try to think about a solution that doesn't require it. You're likely to find something that works better and results in needs clearer code.It may be better for the image class constructors to take an  instead of a path.\nThen, you're not limited to files on the disk, but you can use file-like objects from urllib, gzip, and the like.Also, since you can tell JPG from PNG by looking at the contents of the file, and for gzip file you need this detection anyway, I recommend not looking at the file extension at all.For black magic, go to , but think twice before using that, especially at work.You should use composition in this case, not inheritance. Take a look at the . The  class should decorate other image classes with the desired functionality.With decorators, you get a very dynamic behavior depending on the composition that you create:It's more flexible also, you can have other decorators:Each decorator just encapsulates the functionality it adds and delegates to the composed class as needed."},
{"body": "If  is not set, the Flask framework will not allow you to set or access the session dictionary. This is all that the flask user guide has to say on the subject.I am very new to web development and I have no idea how/why any security stuff works. I would like to understand what flask is doing under the hood.\nWhy does flask force us to set this secret_key property? How does flask use the secret_key property? Anything that requires encryption (for safe-keeping against tampering by attackers) requires the secret key to be set. For  Flask itself, that 'anything' is the  object, but other extensions can make use of the same secret. is merely the value set for the  configuration key, or you can set it directly.The  has good, sane advice on what kind of server-side secret you should set.Encryption relies on secrets; if you didn't set a server-side secret for the encryption to use, everyone would be able to break your encryption; it's like the password to your computer. The secret plus the data-to-sign are used to create a signature string, a hard-to-recreate value using a ; only if you have the exact same secret  the original data can you recreate this value, letting Flask detect if anything has been altered without permission. Since the secret is never included with data Flask sends to the client, a client cannot tamper with session data and hope to produce a new, valid signature. Flask uses the  to do all the hard work; sessions use the  with a customized JSON serializer."},
{"body": "Python's NOSE testing framework has the concept of . The purpose of this is not to test concurrency in the code, but to make tests for code that has \"no side-effects, no ordering issues, and no external dependencies\" run faster. The performance gain comes from concurrent I/O waits when they are accessing different devices, better use of multi CPUs/cores, and by running time.sleep() statements in parallel.I believe the same thing could be done with Python's unittest testing framework, by having a plugin Test Runner.Has anyone had any experience with such a beast, and can they make any recommendations?Python unittest's builtin testrunner does not run tests in parallel.  It probably wouldn't be too hard write one that did.  I've written my own just to reformat the output and time each test.  That took maybe 1/2 a day.  I think you can swap out the TestSuite class that is used with a derived one that uses multiprocess without much trouble.The  package is an extension of unittest which supports running tests concurrently. It can be used with your old test classes that inherit .For example:Please use , if you want parallel run.More info: "},
{"body": "I have a triangle (A, B, C) and am trying to find the angle between each pair of the three points.The problem is that the algorithms I can find online are for determining the angle between vectors. Using the vectors I would compute the angle between the vector that goes from (0, 0) to the point I have, and that doesn't give me the angles inside the triangle.OK, here's some code in Python after the method on the Wikipedia page and after subtracting the values:That gives me 60.2912487814, 60.0951900475 and 120.386438829, so what am I doing wrong?There are two errors here.If I make these modifications I get a result that makes sense:which prints outI'd probably make things more symmetrical and use A, B, C vectors that are cyclic and sum to zero:which prints outThe minus signs in the dot product come because we're trying to get the inside angles.I'm sorry we drove you away in your time of need, by closing the question.I would use the , since you can easily calculate the length of each side of the triangle and then solve for each angles individually.Alternatively, if you only know the length of the sides of the triangle, you can use the .Suppose that you want the angle at A.  Then you need to find the angle between the vector from A to B and the vector from A to C.  The vector from A to B is just B-A.  (Subtract the coordinates.)Create three vectors, one from v2 to v1 (v2-v1), one from v3 to v1 (v3-v1), and one from v3 to v2 (v3-v2). Once you have these three vectors, you can use the algorithms you already found along with the fact that all the angles will add to 180 degrees."},
{"body": "This question likely betrays a misconception, but I'm curious what the \"Tomcat\" of the Python world is.All of my web programming experience is in Java (or Groovy) so I think in Java terms. And when I think of making a basic web-app, I think of writing some servlets, building a WAR file, and deploying it in Tomcat or another servlet container.In Python, suppose I wrote some code that was capable of responding to HTTP requests, what would I do with it? How would I deploy it?Specifically: What is the most commonly used container in Python? And is there an equivalent of a WAR file, a standard packaging of a web-app into one file that works in the various containers?There are different approaches which have one thing in common: They usually communicate via WSGI with their \"container\" (the server receiving the HTTP requests before they go to your Python code).There are various containers:That's nice, but irrelevant.  That's just a Java-ism, and doesn't apply very widely outside Java.That depends.There isn't one.There isn't one.HTTP is a protocol for producing a response to a request.  That's it.  It's really a very small thing.You have CGI scripts which can respond to a request.  The Python  library can do that.  .This is relatively inefficient because the simple CGI rule is \"fire off a new process for each request.\"  It can also be insecure if the script allows elevated privileges or badly planned-out uploads.You have the  framework to connect Apache to Python.  This can behave like CGI, or it can have a dedicated Python \"daemon\" running at the end of a named pipe.  The WSGI standard defines a format for request and response processing that's very handy and very extensible.  Most of the frameworks -- in one way or another -- are WSGI compatible.Finally, there are more complete frameworks that include class definitions for requests and responses, URL parsing, Authentication, Authorization, etc., etc.Here's a list: Maybe 'uwsgi' will help. Here is the link:There are many web-servers available for python. Some of the webservers like CherryPy was written in Python itself. The most coolest part of the answer is that tomcat server itself support Python-based applications.For further details look into this site: "},
{"body": "Why python 2.7 doesn't include Z character (Zulu or zero offset) at the end of UTC datetime object's isoformat string unlike JavaScript?Whereas in javascriptPython  objects don't have time zone info by default, and without it, Python actually violates the ISO 8601 specification (). You can use the  to get some default time zones, or directly subclass  yourself:Then you can manually add the time zone info to :Note that this DOES conform to the ISO 8601 format, which allows for either  or  as the suffix for UTC. Note that the latter actually conforms to the standard better, with how time zones are represented in general (UTC is a special case.)Python datetimes are a little clunky. Use .Arrow has essentially the same api as datetime, but with timezones and some extra niceties that should be in the main library.Python's  does not support the  suffixes like 'Z' suffix for UTC. The following simple string replacement does the trick: is essentially the same as See: Or you could use  to achieve the same effect:Note: This option works only when you know the date specified is in UTC. Going further, you may be interested in displaying human readable timezone information,  with   timezone flag: "},
{"body": "I've been playing with various ways of doing literate programming in Python.  I like , but I have two main problems with it: first, it is hard to build on Windows, where I spend about half my development time; and second, it requires me to indent each chunk of code as it will be in the final program --- which I don't necessarily know when I write it.  I don't want to use Leo, because I'm very attached to Emacs.Is there a good literate programming tool that:Thanks!Correction:   allow me to indent later --- I misread the paper I found on it.  That leaves me with only the \"Runs on Windows\" problem.I did this:You can get any number of web/weave products that will help you construct a document and code in one swoop.You can -- pretty easily -- write your own.  It's not rocket science to yank the Python code blocks out of RST source and assemble it.  Indeed, I suggest you write your own Docutils directives to assemble the Python code from an RST source document.You run the RST through docutils rst2html (or Sphinx) to produce your final HTML report.You run your own utility on the same RST source to extract the Python code blocks and produce the final modules.I have written Pweave , that is aimed for dynamic report generation and uses noweb syntax. It is a pure python script so it also runs on Windows. It doesn't fix your indent problem, but maybe you can modify it for that, the code is really quite simple. The de-facto standard in the community is IPython notebooks.Excellent example in which Peter Norvig demonstrates algorithms to solve the Travelling Salesman Problem: More examples listed at You could use org-mode and babel-tangle.That works quite well, since you can give :noweb-ref to source blocks.Here\u2019s a minimal example: , then put this into the file :You can also use properties of headlines for giving the noweb-ref. It can then even automatically concatenate several source blocks into one noweb reference.Add  to the  line of the second block to see the print results under that block when you hit  in the block.You might find  easier to build on Windows.   It was designed to be more portable than standard noweb.See also my last LP tool: . It does not requires special input format, supports Markdown/MultiMarkdown, reStructuredText, OpenOffice/LibreOffice, Creole, TeX/LaTeX and has super light and clean syntax - no more cryptic literate programs.Found this tool to be useful: "},
{"body": "Using py.test, two tests called the same in different directory causes py.test to fail. Why is that? How can I change this without renaming all the tests?To duplicate do:Putting an  is one way of resolving the conflict.  Unlike nose, current pytest does not try to unload test modules in order to import test modules with the same import name.  I used to think it's a bit magic to do this auto-unimporting and might mess up people's expectation from what the import mechanism does; sometimes people rely on the global state of a test module and with auto-unloading you loose it (a test module importing from another test module might then do unexpected things).  But maybe it's not a practical issue and thus pytest could add a similar hack ...This is an actual feature of py.test. You can find the reason for this behavior stated in :As that's the recommended workflow of working with py.test: install the package under development with , then test it.Because of this, I myself opt for unique test names, in the convention over configuration manner. It also ensures that you don't get ambiguous test names in the various test run output.If you need to keep the test names and don't care about the above mentioned functionality, you should be ok with putting an ."},
{"body": "In python is there an easy way to tell if something is not a sequence? I tried to just do:\n but python did not like that will raise a  if  cannot be iterated on -- but that check \"accepts\" sets and dictionaries, though it \"rejects\" other non-sequences such as  and numbers.On the other hands, strings (which most applications want to consider \"single items\" rather than sequences)  in fact sequences (so, any test, unless specialcased for strings, is going to confirm that they are).  So, such simple checks are often not sufficient.In Python 2.6 and better,  were introduced, and among other powerful features they offer more good, systematic support for such \"category checking\".You'll note strings are  considered \"a sequence\" (since they ), but at least you get dicts and sets out of the way.  If you want to exclude strings from your concept of \"being sequences\", you could use  (but that also excludes tuples, which, like strings, are sequences, but are not mutable), or do it explicitly:Season to taste, and serve hot!-)The  describes the following sequence types: string, Unicode string, list, tuple, buffer, and xrange.Why are you doing this? The normal way here is to require a certain type of thing (A sequence or a number or a file-like object, etc.) and then use it without checking anything. In Python, we don't typically use classes to carry semantic information but simply use the methods defined (this is called \"duck typing\"). We also prefer APIs where we know exactly what to expect; use keyword arguments, preprocessing, or defining another function if you want to change how a function works.Since Python \"adheres\" duck typing, one of the approach is to check if an object has some member (method).A sequence has length, has sequence of items, and support slicing []. So, it would be like this:They are all special methods,  should return number of items,  should return an item (in sequence it is -th item,  not with mapping),  should return subsequence, and  and  like you expect. This is such a contract, but whether the object really do these or not depends on whether the object adheres the contract or not. that, the function above will also return  for mapping, e.g. , since mapping also has these methods. To overcome this, you can do a  work: most of the time you don't need this, just do what you want as if the object is a sequence and catch an exception if you wish. This is more pythonic.I think the below code snippet does what you want:why ask whytry getting a length and if exception return false"},
{"body": "I have the following data frame in IPython, where each row is a single stock:I want to apply a groupby operation that computes cap-weighted average return across everything, per each date in the \"yearmonth\" column.This works as expected:But then I want to sort of \"broadcast\" these values back to the indices in the original data frame, and save them as constant columns where the dates match.I realize this naive assignment should not work. But what is the \"right\" Pandas idiom for assigning the result of a groupby operation into a new column on the parent dataframe?In the end, I want a column called \"MarketReturn\" than will be a repeated constant value for all indices that have matching date with the output of the groupby operation.One hack to achieve this would be the following:But this is slow, bad, and unPythonic.While I'm still exploring all of the incredibly smart ways that  concatenates the pieces it's given, here's another way to add a new column in the parent after a groupby operation.May I suggest the  method (instead of aggregate)? If you use it in your original example it should do what you want (the broadcasting).As a general rule when using groupby(), if you use the .transform() function pandas will return a table with the same length as your original. When you use other functions like .sum() or .first() then pandas will return a table where each row is a group. I'm not sure how this works with apply but implementing elaborate lambda functions with transform can be fairly tricky so the strategy that I find most helpful is to create the variables I need, place them in the original dataset and then do my operations there. If I understand what you're trying to do correctly (I apologize if I'm mistaken) first you can calculate the total market cap for each group: This will add a column called \"group_MarketCap\" to your original data which would contain the sum of market caps for each group. Then you can calculate the weighted values directly: And finally you would calculate the weighted average for each group using the same transform function:I tend to build my variables this way. Sometimes you can pull off putting it all in a single command but that doesn't always work with groupby() because most of the time pandas needs to instantiate the new object to operate on it at the full dataset scale (i.e. you can't add two columns together if one doesn't exist yet). Hope this helps :) Does this work?I use  for this:"},
{"body": "I am quite new to programing so I hope this question is simple enough.I need to know how to convert a string input of numbers separated by spaces on a single line:and convert this to a float listHow can this be done?Try a list comprehension:In Python 2.x it can also be done with map:Note that in Python 3.x the second version returns a map object rather than a list. If you need a list you can convert it to a list with a call to , or just use the list comprehension approach instead."},
{"body": "I have an array like this:And I am trying to get an array like this:Where each row (of a fixed arbitrary width) is shifted by one. The array of A is 10k records long and I'm trying to find an efficient way of doing this in Numpy. Currently I am using vstack and a for loop which is slow. Is there a faster way?Edit:Actually, there's an even more efficient way to do this... The downside to using  etc, is that you're making a copy of the array.  Incidentally, this is effectively identical to @Paul's answer, but I'm posting this just to explain things in a bit more detail...There's a way to do this with just views so that  memory is duplicated.I'm directly borrowing this from , who in turn, borrowed it from Keith Goodman's  (Which is quite useful!).The basic trick is to directly manipulate the  (For one-dimensional arrays):Where  is your input array and  is the length of the window that you want (3, in your case).This yields:However, there is absolutely no duplication of memory between the original  and the returned array.  This means that it's fast and scales  better than other options.For example (using  and ):If we generalize this to a \"rolling window\" along the last axis for an N-dimensional array, we get Erik Rigtorp's \"rolling window\" function:So, let's look into what's going on here... Manipulating an array's  may seem a bit magical, but once you understand what's going on, it's not at all.  The strides of a numpy array describe the size in bytes of the steps that must be taken to increment one value along a given axis.  So, in the case of a 1-dimensional array of 64-bit floats, the length of each item is 8 bytes, and  is .Now, if we reshape this into a 2D, 3x3 array, the strides will be , as we would have to jump 24 bytes to increment one step along the first axis, and 8 bytes to increment one step along the second axis.Similarly a transpose is the same as just reversing the strides of an array:Clearly, the strides of an array and the shape of an array are intimately linked.  If we change one, we have to change the other accordingly, otherwise we won't have a valid description of the memory buffer that actually holds the values of the array.Therefore, if you want to change  the shape and size of an array simultaneously, you can't do it just by setting  and , even if the new strides and shape are compatible.  That's where  comes in.  It's actually a very simple function that just sets the strides and shape of an array simultaneously.  It checks that the two are compatible, but not that the old strides and new shape are compatible, as would happen if you set the two independently.  (It actually does this through , which allows arbitrary classes to describe a memory buffer as a numpy array.)So, all we've done is made it so that steps one item forward (8 bytes in the case of a 64-bit array) along one axis, but .  In other words, in case of a \"window\" size of 3, the array has a shape of , but instead of stepping a full  for the second dimension, it , effectively making the rows of new array a \"moving window\" view into the original array. (This also means that  will not be the same as  for your new array.)At any rate, hopefully that makes things slightly clearer..This solution is not efficiently implemented by a python loop since it comes with all kinds of type-checking best avoided when working with numpy arrays.  If your array is exceptionally tall, you will notice a large speed up with this:This gives a  of the array A.  If you want a new array you can edit, do the same but with  at the end.Details on strides:The  tuple in this case will be (4,4) because the array has 4-byte items and you want to continue to step thru your data in single-item steps in the i-dimension.  The second value '4' refers to the strides in the j-dimension (in a normal 4x4 array it would be 16).  Because in this case you want to also also increment your read from the buffer in 4-byte steps in the j-dimension.Joe give a nice, detailed description and makes things crystal-clear when he says that all this trick does is change strides and shape simultaneously. Which approach are you using?If your width is relatively low (3) and you have a big  (10000 elements), then the difference is even more important: 32.4ms for the first and 44\u00b5s for the second.Just to further go with the answer of @Joe generalwhich outputs:To generalize further for the 2d case,i.e use it for patch extraction from an imagewhich outputs:I think this might be faster than looping, when the width is fixed at a low number...EDIT Clearly, the solutions using strides are superior to this, with the only major disadvantage being that they are not yet well documented...Take a look at: .Around 1 second.I'm using a more generalized function similar to that of @JustInTime but applicable to An example,"},
{"body": "How do I execute the following shell command using the Python  module?The input data will come from a string, so I don't actually need .  I've got this far, can anyone explain how I get it to pipe through  too?:  Note that while the accepted answer below doesn't actually answer the question as asked, I believe S.Lott is right and it's better to avoid having to solve that problem in the first place!You'd be a little happier with the following.Delegate part of the work to the shell.  Let it connect two processes with a pipeline.You'd be a lot happier rewriting 'script.awk' into Python, eliminating awk and the pipeline..  Some of the reasons for suggesting that awk isn't helping.[There are too many reasons to respond via comments.]Bottom line: awk can't add significant value.  In this case, awk is a net cost; it added enough complexity that it was necessary to ask this question.  Removing awk will be a net gain. Why building a pipeline () is so hard.When the shell is confronted with  it has to do the following.I think that the above can be used recursively to spawn , but you have to implicitly parenthesize long pipelines, treating them as if they're .Since Python has ,  and , and you can replace  and , there's a way to do the above in pure Python.  Indeed, you may be able to work out some shortcuts using  and .However, it's easier to delegate that operation to the shell.To emulate a shell pipeline:without invoking the shell (see ): provides some syntax sugar:The analog of:is: covered this pretty well. Is there some part of this you didn't understand?Your program would be pretty similar, but the second  would have stdout= to a file, and you wouldn't need the output of its .Inspired by @Cristian's answer. I met just the same issue, but with a different command. So I'm putting my tested example, which I believe could be helpful:This is tested. I like this way because it is natural pipe conception gently wrapped with  interfaces.  is available on Windows but, crucially, doesn't appear to actually  on Windows. See comments below.The Python standard library now includes the  module for handling this:, I'm not sure how long this module has been around, but this approach appears to be vastly simpler than mucking about with ."},
{"body": "I would like to convert a python variable name into the string equivalent as shown. Any ideas how?There is an usage scenario where you might need this. I'm not implying there are not better ways or achieving the same functionality.This would be useful in order to 'dump' an arbitrary list of dictionaries in case of error, in debug modes and other similar situations.What would be needed, is the reverse of the  function:which would take an identifier name ('variable','dictionary',etc) as an argument, and return a \nstring containing the identifier\u2019s name.Consider the following current state of affairs:If one is passing an identifier name ('function','variable','dictionary',etc)  to a  (another identifier name), one actually passes an identifier (e.g.: ) to another identifier  (e.g.: ):From my understanding, only the memory address is passed to the function:Therefore, one would need to pass a string as an argument to  in order for that function to have the argument's identifier name:Inside the random_function(), one would use the already supplied string  to:Unfortunately, this doesn't work in all cases. It only works if the  can resolve the  string to an actual identifier. I.e. If  identifier name is available in the 's namespace.This isn't always the case:Expected results would be:Because  identifier name is not available in the 's namespace, this would yield instead:Now, consider the hypotetical usage of a  which would behave as described above.Here's a dummy Python 3.0 code: .Expected results would be:Unfortunately,  would not see the 'original' identifier names (,,). It would only see the  identifier name.Therefore the real result would rather be:So, the reverse of the  function won't be that useful in this case.Currently, one would need to do this:This can be achieved by passing both the  and  to the called function at the same time. I think this is the most 'general' way of solving this egg-chicken problem across arbitrary functions, modules, namespaces, without using corner-case solutions. The only downside is the use of the  function which may easily lead to unsecured code. Care must be taken to not feed the  function with just about anything, especially unfiltered external-input data.Technically the information is available to you, but as others have asked, how would you make use of it in a sensible way?This shows that the variable name is present as a string in the globals() dictionary.In this case it happens to be the third key, but there's no reliable way to know where a given variable name will end upYou could filter out system variables like this, but you're still going to get all of your own items. Just running that code above created another variable \"k\" that changed the position of \"x\" in the dict.But maybe this is a useful start for you. If you tell us what you want this capability for, more helpful information could possibly be given.This is not possible.In Python, there really isn't any such thing as a \"variable\".  What Python really has are \"names\" which can have objects bound to them.  It makes no difference to the object what names, if any, it might be bound to.  It might be bound to dozens of different names, or none.Consider this example:Now, suppose you have the integer object with value 1, and you want to work backwards and find its name.  What would you print?  Three different names have that object bound to them, and all are equally valid.In Python, a name is a way to access an object, so there is no way to work with names directly.  There might be some clever way to hack the Python bytecodes or something to get the value of the name, but that is at best a parlor trick.If you know you want  to print , you might as well just execute  in the first place.EDIT: I have changed the wording slightly to make this more clear.  Also, here is an even better example:In practice, Python reuses the same object for integers with common values like 0 or 1, so the first example should bind the same object to all three names.  But this example is crystal clear: the same object is bound to foo, bar, and baz.I searched for this question because I wanted a Python program to print assignment statements for some of the variables in the program. For example, it might print \"foo = 3, bar = 21, baz = 432\". The print function would need the variable names in string form. I could have provided my code with the strings \"foo\",\"bar\", and \"baz\", but that felt like repeating myself. After reading the previous answers, I developed the solution below.The globals() function behaves like a dict with variable names (in the form of strings) as keys. I wanted to retrieve from globals() the key corresponding to the value of each variable. The method globals().items() returns a list of tuples; in each tuple the first item is the variable name (as a string) and the second is the variable value. My variablename() function searches through that list to find the variable name(s) that corresponds to the value of the variable whose name I need in string form.The function itertools.ifilter() does the search by testing each tuple in the globals().items() list with the function . In that function x is the tuple being tested; x[0] is the variable name (as a string) and x[1] is the value. The lambda function tests whether the value of the tested variable is the same as the value of the variable passed to variablename(). In fact, by using the  operator, the lambda function tests whether the name of the tested variable is bound to the exact same object as the variable passed to variablename(). If so, the tuple passes the test and is returned by ifilter().The itertools.ifilter() function actually returns an iterator which doesn't return any results until it is called properly. To get it called properly, I put it inside a list comprehension . The list comprehension saves only the variable name , ignoring the variable value. The list that is created contains one or more names (as strings) that are bound to the value of the variable passed to variablename().In the uses of variablename() shown below, the desired string is returned as an element in a list. In many cases, it will be the only item in the list. If another variable name is assigned the same value, however, the list will be longer.You somehow have to refer to the variable you want to print the name of. So it would look like:There is no such function, but if there were it would be kind of pointless. You have to type out , so you can as well just type quotes to the left and right of it to print the name as a string:What are you trying to achieve? There is absolutely no reason to ever do what you describe, and there is likely a much better solution to the problem you're trying to solve..The most obvious alternative to what you request is a dictionary. For example:Mostly as a.. challenge, I implemented your desired output. Do not use this code, please!Does Django not do this when generating field names?Seems reasonable to me.I think this is a cool solution and I suppose the best you can get. But do you see any way to handle the ambigious results, your function may return?\nAs  shows, low integers and strings of the same value get cached by python so that your variablename-function might priovide ambigous results with a high probability. \nIn my case, I would like to create a decorator, that adds a  new variable to a class by the varialbename i pass it:But if your method returns ambigous results, how can I know the name of the variable I added?Maybe if I will also check the local list I could at least remove the \"dependency\"-Variable from the list, but this will not be a reliable result.as long as it's a variable and not a second class, this here works for me:this happens for class members:ans this for classes (as example):So for classes it gives you the name AND the properteriesOr did you mean something_else?This will work for simnple data types (str, int, float, list etc.)"},
{"body": "I can't figure out how to make  add a scrip to the the user's  or  or whatever.E.g., I'd like to add a  to  so that the user can call  from any directory.The Python documentation explains it under the  section.Consider using :Where  is a main function in your main module.\ncommand-name is a name under which it will be saved in /usr/local/bin/ (usually)If you're willing to build and install the entire python package, this is how I would go about it:There are two ways in order to get a working command line tool from setuptools and PyPI infrastructure:"},
{"body": "How do I remove  chars from the following dictionary?  This data is coming from   query  so that it looks like  Some databases such as Sqlite3 let you define  and  functions so you can retrieve text as  rather than .  Unfortunately, MongoDB doesn't provide this option for any of the commonly needed types such as str, decimal or datetime:Having eliminated Mongo options, that leaves writing Python code to do the conversion after the data is retrieved.  You could write a recursive function that traverses the result to convert each field.As a quick-and-dirty alternative, here is a little hack that may be of use:The  characters that you are seeing simply mean that they are unicode strings.If you do not want them to be unicode, you can encode them as something else, such as ASCII.You need to let psycopg2 encode your strings, not try to insert Python-syntax strings into your queries raw \u2014 you are putting yourself in danger of a SQL injection problem if some of the strings contain characters that SQL will interpret as ending the string. You should pass parameters to psycopg2 like this:Because psycopg2 knows SQL syntax very, very well, it will leave off the  characters as it gets your  and  strings and quotes and escapes them in exactly the way that this SQL statement needs.As sven mentions in his comment, the  is an indication of the types represented in mongodb (actually it's because json is defined to use unicode).  This fact should be totally transparent to you, in fact you can use  and  values interchangeably in the dicts.If you simply want to convert the dict to json data string you can do:You cannot simply remove the u from the strings, as it symbolizes, that the strings are in Unicode.One solution is to use the encode function:This would just ignore non ascii characters."},
{"body": "I'm trying to do an animation of a scatter plot where colors and size of the points changes at different stage of the animation. For data I have two numpy ndarray with an x value and y value:Now I want to plot a scatter plot of the typeand create an animation over the index . How do I do this?Here's a quick example using the new animation module.  It's slightly more complex than it has to be, but this should give you a framework to do fancier things.  If you're on OSX and using the OSX backend, you'll need to change  to  in the  initialization below.  The OSX backend doesn't fully support blitting.  The performance will suffer, but the example should run correctly on OSX with blitting disabled.For a simpler example, have a look at the following:Here is the thing. I used to a user of Qt and Matlab and I am not quite familiar with the animation system on the matplotlib.But I do have find a way that can make any kind of animation you want just like it is in matlab. It is really powerful. No need to check the module references and you are good to plot anything you want. So I hope it can help.The basic idea is to use the time event inside PyQt( I am sure other Gui system on the Python like wxPython and TraitUi has the same inner mechanism to make an event response. But I just don't know how). Every time a PyQt's Timer event is called I refresh the whole canvas and redraw the whole picture, I know the speed and performance may be slowly influenced but it is not that much. Here is a little example of it:You can adjust the refresh speed in the I am just like you who want to use the Animated scatter plot to make a sorting animation. But I just cannot find a so called \"set\" function. So I refreshed the whole canva.Hope it helps.."},
{"body": "I have a list containing data as such:I'd like to print out the ranges of consecutive integers:Is there a built-in/fast/efficient way of doing this?From :You can adapt this fairly easily to get a printed set of ranges.Built-In: No, as far as I'm aware.You have to run through the array. Start off with putting the first value in a variable and print it, then as long as you keep hitting the next number do nothing but remember the last number in another variable. If the next number is not in line, check the last number remembered versus the first number. If it's the same, do nothing. If it's different, print \"-\" and the last number. Then put the current value in the first variable and start over.\nAt the end of the array you run the same routine as if you had hit a number out of line.I could have written the code, of course, but I don't want to spoil your homework :-)This will print exactly as you specified:If the list has any single number ranges, they would be shown as n-n:Here is another basic solution without using any module, which is good for interview, generally in the interview they asked without using any modules:Output:I had a similar problem and am using the following for a sorted list. It outputs a dictionary with ranges of values listed in a dictionary. The keys separate each run of consecutive numbers and are also the running total of non-sequential items between numbers in sequence. Your list gives me an output of Using set operation, the following algorithm can be executedOutput for the above list \"a\"  \n1-4 \n6-8 \n14-15 \n21 "},
{"body": "I am trying to build lxml for Python 2.7 on Windows 64 bit machine. I couldn't find lxml egg for Python 2.7 version. So I am compiling it from sources. I am following instructions on this siteunder static linking section. I am getting errorCan any one help me with this? I tried setting the path to have Microsoft Visual Studio..\nI can run vcvarsall.bat from the commandline.. but python is having problemsI bet you're not using VS 2008 for this :)  There's  function (guess what, it looks for vcvarsall.bat) in distutils with the following commentIf you're not using VS 2008 then you have neither the registry key nor suitable environment variable and that's why distutils can't find vcvarsall.bat file. It does  check if the bat file is reachable through the PATH environment variable.  The solution is to define VS90COMNTOOLS variable to point to Tools directory of Visual Studio.That being said take a look at  section in Python's docs which statesMartin v. Loewis in the email titled  on python-list mailing list states the sameIn the light of above statements you  use VS 2008 if you want to build lxml for Python 2.7 so although setting VS90COMNTOOLS takes care of finding vcvarsall.bat file it's not  solution.That being said :) people do try to use older CRT with newer compiler:\n\n\n  I'd like to thank Kev Dwyer (for pointing out importance of version of VS which is used) and Stefan Behnel (for pointing me to distutils as a place dealing with compiler's configuration) in the thread  on lxml mailing list. I'd like to also thank  from freenode #distutils IRC channel for confirmation that distutils does contain code which looks for vcvarsall.bat file.After following the recommended solution:my problem still existed (want to build a Python extension in C).I had to do the following 2 unbelievably dirty tweaks, before everything now is indeed working:I cannot tell who was doing something wrong here - probably I.EDIT. Moving directories works because of the issue described in .The described workaround is using the Visual C++ command prompt:Jorj McKie was almost correct: indeed installing  isn't enough, and yes there is an issue in distutils which prevent it from finding find_vcvarsall. In fact the issue is not directly in distutils, but in how VCForPython27.msi was packaged and where vcvarsall.bat is placed (the folders layout is different from the VS2008 SDK).A simple workaround meanwhile this gets patched maybe in Python 2.7.11: use setuptools instead of distutils.Another manual workaround if you're stuck with distutils:Bug report and workaround by Gregory Szorc:\nMore info and a workaround for using %%cython magic inside IPython: I had similar issues. This worked instantly without anything other than using an install wizard and setting one preference."},
{"body": "I am using Sphinx for generating docs for a python project.\nThe output html is not preserving the line breaks which are present in the docstring.\nExample:CodeSphinx O/P:Any idea how to fix it ?In general in restructured text useto keep line breaksIf you add the following to your main .rst file:Then in your markup you can add in  to create linebreaks just for HTML. From: This answer comes late, but maybe it'll still be useful to others.You could use  in your docstrings. This would look something likeFrom the looks of your example however it seems you're using the Google Style for docstrings ().Sphinx does not natively support those. There is however an extension named  that parses Google and Numpy style docstrings at .To use the extension you have to append  to the -list in your Sphinx  (usually ), so it becomes something likeIn your case you can write:In my particular case, I was trying to get autodoc to read a doc string ().  I ended up using  everywhere I needed to add a line break:"},
{"body": "I just upgraded to Django 1.9 and noticed these changes. What is the logic behind them?Is anything different because of these changes?It is explained in issue  (emphasis mine):Also see  for a more up-to-date discussion.Another  (not directly related to this issue) demonstrates that  is actually faster with a . It is correct that the above issue was closed years ago, but I included it because it explained the rationale behind the decision and many similar discussions refer to the same ticket. The actual implementation decision was triggered after the  started by core Django developer :And the switch to lists actually happened in  which also referred to the above discussion.In , there is:So it appears that it was just done for consistency. Both tuples and lists should work fine. If you use a tuple with 1 element, remember the comma  because otherwise it's not a tuple but simply an expression in parens.As for urlpatterns, those used to be defined using a  function, but that was deprecated in Django 1.8, as a list of url instances works fine. As the function will be removed in the future, it shouldn't be used in new apps and projects."},
{"body": "How can i receive and send email in python? A 'mail server' of sorts.I am looking into making an app that listens to see if it recieves an email addressed to foo@bar.domain.com, and sends an email to the sender.Now, am i able to do this all in python, would it be best to use 3rd party libraries? Here is a very simple example:For more options, error handling, etc, look at the .I do not think it would be a good idea to write a real mail server in Python. This is certainly possible (see mcrute's and Manuel Ceron's posts to have details) but it is a lot of work when you think of everything that a real mail server must handle (queuing, retransmission, dealing with spam, etc).You should explain in more detail what you need. If you just want to react to incoming email, I would suggest to configure the mail server to call a program when it receives the email. This program could do what it wants (updating a database, creating a file, talking to another Python program).To call an arbitrary program from the mail server, you have several choices:Found a helpful example for reading emails by connecting using IMAP: Python has an SMTPD module that will be helpful to you for writing a server. You'll probably also want the SMTP module to do the re-send. Both modules are in the standard library at least since version 2.3.poplib and smtplib will be your friends when developing your app.The sending part has been covered, for the receiving you can use  or Yes, you can do pretty much everything with the built-in libraries.  Do a search here looking for the tags  and  and you'll see how it's done.Depending on the amount of mail you are sending you might want to look into using a real mail server like postifx or sendmail (*nix systems) Both of those programs have the ability to send a received mail to a program based on the email address. The best way to do this would be to create a windows service in python that receives the emails using imaplib2Below is a sample python script to do the same.You can install this script to run as a windows service by  running the following command on the command line \"python THENAMEOFYOURSCRIPTFILE.py install\"."},
{"body": "On a django site, I want to generate an excel file based on some data in the database.I'm thinking of using , but it only has a method to save the data to a file. How can get the file to the HttpResponse object? Or maybe do you know a better library?I've also found this  but it doesn't do what I need. All I want is a way to get the stream from the xlwt object to the response object (without writing to a temporary file)neat package! i didn't know about thisAccording to the doc, the  method takes either a filename to save on, or a file-like stream to write on.And a Django response object happens to be a file-like stream!  so just do .  Look the Django docs about  with ReportLab to see a similar situation. (adapted from ShawnMilo's comment):then, from your view function, just create the  object and finish with ***UPDATE: django-excel-templates no longer being maintained, instead try Marmir Still in development as I type this but  Django excel templates project aims to do what your asking.Specifically look at the tests. Here is a simple case:You can save your XLS file to a  object, which is file-like.You can return the StringIO object's  in the response.  Be sure to add headers to mark it as a downloadable spreadsheet.You might want to check  which comes fith a function called  do convert a queryset into an downloadable Excel Sheet.Use If your data result doesn't need formulas or exact presentation styles, you can always use CSV.  any spreadsheet program would directly read it.  I've even seen some webapps that generate CSV but name it as .XSL just to be sure that Excel opens it"},
{"body": "Anyone could recommend a good guide/tutorial/article with tips/guidelines in how to organize and partition a large Django project? I'm looking for advices in what to do when you need to start factorizing the initial unique files (models.py, urls.py, views.py) and working with more than a few dozens of entities.Each \"application\" should be small -- a single reusable entity plus a few associated tables.  We have about 5 plus/minus 2 tables per application model.  Most of our half-dozen applications are smaller than 5 tables.  One has zero tables in the model.  Each application should be designed to be one reusable concept.  In our case, each application is a piece of the overall site; the applications could be removed and replaced separately.Indeed, that's our strategy.  As our requirements expand and mature, we can remove and replace applications independently from each other.It's okay to have applications depend on each other.  However, the dependency has to be limited to the obvious things like \"models\" and \"forms\".  Also, applications can depend on the names in each other's URL's.  Consequently, your named URL's must have a form like \"application-view\" so the  function or the  tag can find them properly.Each application should contain it's own batch commands (usually via a formal Command that can be found by the  script.Finally, anything that's more complex than a simple model or form that's shared probably doesn't belong to either application, but needs to be a separate shared library.  For example, we use , but wrap parts of it in our own class so it's more like the built-in  module.  This wrapper for XLRD isn't a proper part of any one application, to it's a separate module, outside the Django applications.I've found it to be helpful to take a look at large open-source Django projects and take note of how that project does it. Django's site has a good list of open-source projects:As does Google (although most of these are smaller add-in template tags and Middleware:Of course, just because one project does it one way does not mean that that way is The Right Way (or The Wrong Way). Some of those projects are more successful than others.In the end, the only way to really learn what works and doesn't work is to try it out yourself. All the tips and hints in the world wont help unless you try it out yourself, but they may help you get started in the right direction."},
{"body": "I am interested in taking an arbitrary dict and copying it into a new dict, mutating it along the way.One mutation I would like to do is swap keys and value. Unfortunately, some values are dicts in their own right. However, this generates a \"unhashable type: 'dict'\" error. I don't really mind just stringifying the value and giving it the key. But, I'd like to be able to do something like this:Is there a clean way to do this that  involve trapping an exception and parsing the message string for \"unhashable type\" ?Since Python 2.6 you can use the abstract base class :This approach is also mentioned briefly in the documentation for .All hashable built in python objects have a  method. You can check for that.outputWhy not use duck typing?"},
{"body": "I'm currently in the process of making my Nintendo Wiimote (Kinda sad actually) to work with my computer as a mouse. I've managed to make the nunchuk's stick control actually move the mouse up and down, left and right on the screen! This was so exciting. Now I'm stuck.I want to left/right click on things via python when I press A, When I went to do a search, All it came up with was tkinter?So my question is, What do I call to make python left/right click on the desktop, and if it's possible, maybe provide a snippet?Thank you for your help!NOTE: I guess I forgot to mention that this is for Linux.python-uinput is very easy to use.Here's an example You can use  which has now merged with . I installed it via pip:apt-get install python-pippip install pymouseIn some cases it used the cursor and in others it simulated mouse events without the cursor.You can also specify which mouse button you want used. Ex left button:Keep in mind, on Linux it requires Xlib.The  package provides bindings to parts of the input handling subsystem in Linux. It also happens to include a pythonic interface to uinput. Example of sending a relative motion event and a  with :PyAutoGui works superb.. Thanks to Al Sweigart...An example of mine...you might find this helpful:Good luck!You can try to interface  program from the Python script.Open your terminal and goto \nmay this   python script will work for you,check them out.You can install the PyAutoGUI GUI automation module from PyPI (run ) and then call the  to click on a certain X and Y coordinates of the screen:PyAutoGUI works on Windows, Mac, and Linux, and on Python 2 and 3. It also can emulate the keyboard, do mouse drags, take screenshots, and do simple image recognition of the screenshots.Full docs are at I didn't see this mentioned, so here it goes - there is also ; see:It requires \"Enable assistive technologies\" in the Gnome Desktop - but can in principle obtain e.g. names of GUI buttons of an application, and allow virtual clicks on them (rather than via x/y coordinates)."},
{"body": "I fear that this is a messy way to approach the problem but... let's say that I want to make some imports in Python based on some conditions.For this reason I want to write a function:Now how can I have the imported modules globally available?For example:Imported modules are just variables - names bound to some values.  So all you need is to import them and make them global with  keyword.Example:You can make the imports global within a function like this:You could have this function return the names of the modules you want to import, and then useYou can use the built-in function  to conditionally import a module with global scope.To import a top level module (think: ):Import from a hierarchy (think: ):Import from a hierarchy and alias (think: ):"},
{"body": "I have the  object and i need to pass that along many pages to store data in single itemLIke my item isNow those three description are in three separate pages. i want to do somrething likeNow this works good for But i want something likeNo problem. Instead ofDoIn order to guarantee an ordering of the requests/callbacks and that only one item is ultimately returned you need to chain your requests using a form like:Each callback function returns an iterable of items or requests, requests are scheduled and items are run through your item pipeline. If you return an item from each of the callbacks, you'll end up with 4 items in various states of completeness in your pipeline, but if you return the next request, then you can guaruntee the order of requests and that you will have exactly one item at the end of execution.The accepted answer returns a total of three items [with desc(i) set for i=1,2,3].If you want to return a single item, Dave McLain's item does work, however it requires , , and  to succeed and run without errors in order to return the item.For my use case, some of the subrequests MAY return HTTP 403/404 errors at random, thus I lost some of the items, even though I could have scraped them partially.Thus, I currently employ the following workaround: Instead of only passing the item around in the  dict, pass around a  that knows what request to call next. It will call the next item on the stack (so long as it isn't empty), and returns the item if the stack is empty.The  request parameter is used to return to the dispatcher method upon errors and simply continue with the next stack item.This solution is still synchronous, and will still fail if you have any exceptions within the callbacks.For more information, ."},
{"body": "I have a set of data which I want plotted as a line-graph.  For each series, some data is missing (but different for each series).  Currently matplotlib does not draw lines which skip missing data: for example results in a plot with gaps in the lines.  How can I tell matplotlib to draw lines through the gaps? (I'd rather not have to interpolate the data). You can mask the NaN values this way: This leads toQouting @Rutger Kassies () :A solution if you are using , :Without interpolation you'll need to remove the None's from the data.  This also means you'll need to remove the X-values corresponding to None's in the series.  Here's an (ugly) one liner for doing that:The lambda function returns False for None values, filtering the x,series pairs from the list, it then re-zips the data back into its original form.For what it may be worth, after some trial and error I would like to add one clarification to Thorsten's solution. Hopefully saving time for users who looked elsewhere after having tried this approach.I was unable to get success with an identical problem while usingand attempting to plot withIt seemed it was required to use  to get the proper NaNs handling, though I cannot say why.Perhaps I missed the point, but I believe Pandas now .  The example below is a little involved, and requires internet access, but the line for China has lots of gaps in the early years, hence the straight line segments.  "},
{"body": "In my code I use the  from  like thisHowever, all of the sudden I get the following error when I load the cursor:Maybe something is dorked in my installation but I have no clue where to start looking. I made some updates with pip, but as far as I know no dependencies of .You need to explicitly import :"},
{"body": "Simple models just to ask my question.I wonder how can i query blogs using tags in two different ways.Tag and Blog is just used for an example.You could use Q objects for #1:Unions and intersections, I believe, are a bit outside the scope of the Django ORM, but its possible to to these.  The following examples are from a Django application called called  that provides the functionality. : For part two, you're looking for a union of two queries, basicallyFor part #3 I believe you're looking for an intersection. See  I've tested these out with Django 1.0:The \"or\" queries:or you could use the Q class:The \"and\" query:I'm not sure about the third one, you'll probably need to drop to SQL to do it.Please don't reinvent the wheel and use  which was made exactly for your use case. It can do all queries you describe, and much more.If you need to add custom fields to your Tag model, you can also take a look at .This will do the trick for you"},
{"body": "I am currently working on a wrapper for a dedicated server running in the shell. The wrapper spawns the server process via subprocess and observes and reacts to its output.The dedicated server must be explicitly given a command to shut down gracefully. Thus, CTRL-C must not reach the server process.If I capture the KeyboardInterrupt exception or overwrite the SIGINT-handler in python, the server process still receives the CTRL-C and stops immediately.So my question is:\nHow to prevent subprocesses from receiving CTRL-C / Control-C / SIGINT?Somebody in the #python IRC-Channel (Freenode) helped me by pointing out the  parameter of :Thus, the following code solves the problem (UNIX only): The signal is actually not prevented from reaching the subprocess. Instead, the  above overwrites the signal's default handler so that the signal is ignored. Thus, this solution  not work if the subprocess overwrites the  handler again. This solution works for all sorts of subprocesses, i.e. it is not restricted to subprocesses written in Python, too. For example the dedicated server I am writing my wrapper for is in fact written in Java.Combining some of other answers that will do the trick - no signal sent to main app will be forwarded to the subprocess.Try setting SIGINT to be  before spawning the subprocess (reset it to default behavior afterward).If that doesn't work, you'll need to read up on  and learn how to put a process in its own background process group, so that  doesn't even cause the kernel to send the signal to it in the first place.  (May not be possible in Python without writing C helpers.)See also .you can do something like this to make it work in windows and unix:"},
{"body": "It's a thing that bugged me for a while. Why can't I do:...while I can do the following?What's the rule here? Could you please point me to some description?You can add attributes to any object that has a .If an object is using  / doesn't have a , it's usually to save space. For example, in a  it would be overkill to have a dict - imagine the amount of bloat for a very short string.If you want to test if a given object has a , you can use . might also be interesting to read:Another interesting article about Python's data model including , , etc. is  from the python reference."},
{"body": "I have some values in a Python Pandas Series (type: pandas.core.series.Series)I would like to get values of histogram (not necessary plotting histogram)... I just need to get the frequency for each interval.Let's say that my intervals are going from [-200; -150] to [950; 1000]so lower bounds areand upper bounds areI don't know how to get frequency (the number of values that are inside each interval) now...\nI'm sure that defining lwb and upb is not necessary... but I don't know what\nfunction I should use to perform this!\n(after diving in Pandas doc, I think  function can help me because it's a discretization problem... but I'm don't understand how to use it)After being able to do this, I will have a look at the way to display histogram (but that's an other problem)You just need to use the histogram function of numpy:where division is the automatically calculated border for your bins and count is the population inside each bin.If you need to fix a certain number of bins, you can use the argument bins and specify a number of bins, or give it directly the boundaries between each bin.to plot the results you can use the matplotlib function hist, but if you are working in pandas each Series has its own handle to the hist function, and you can give it the chosen binning:Inorder to get the frequency counts of the values in a given interval binned range, we could make use of  which returns indices of half open bins for each element along with  for computing their respective counts. To plot their counts, a bar plot can be then made.Frequency for each interval sorted in descending order of their counts:To modify the plot to include just the lower closed interval of the range for aesthetic purpose, you could do:If you say you want to get values of histogram, you are simply looking for frequency of each unique value in your series, if I'm not mistaken. In that case, you can simply do , which will give you:"},
{"body": "I have a Python Pandas  object containing textual data. My problem is, that when I use  function, it truncates the strings in the output.For example:The output is truncated at There is a related question on SO, but it uses placeholders and search/replace functionality to postprocess the HTML, which I would like to avoid:Is there a simpler solution to this problem? I could not find anything related from the .What you are seeing is pandas truncating the output for display purposes only. The default  value is 50 which is what you are seeing.You can set this value to whatever you desire or you can set it to -1 which effectively turns this off:Although I would advise against this, it would be better to set it to something that can be displayed easily in your console or ipython.A list of the options can be found here: it seems that  is indeed the only option. To prevent irreversible global changes of how dataframes are presented in the console, you may save the previous setting in a variable and restore it immediately after the usage, as follows:"},
{"body": "I know  is an old school way of defining a class. But I would like to understand in more detail the difference between these two.Prior to python 2.2 there were essentially two different types of class: Those defined by C extensions and C coded builtins (types) and those defined by python class statements (classes).  This led to problems when you wanted to mix python-types and builtin types.  The most common reason for this is subclassing.  If you wanted to subclass the list type in python code, you were out of luck, and so various workarounds were used instead, such as subclassing the pure python implementation of lists (in the UserList module) instead.This was a fairly ugly, so in 2.2 there was a  to unify python and builtin types, including the ability to  from them. The result is \"new style classes\".  These do have some incompatible differences to old-style classes however, so for backward compatability the bare class syntax creates an old-style class, while the new behaviour is obtained by inheriting from object.  The most visible behaviour differences are: is the 'new' way of declaring classes.This change was made in python 2.2, see  of the differences. Subclassing  yields a new-style class. Two well known advantages of new-style classes are:"},
{"body": "Profiling shows this is the slowest segment of my code for a little word game I wrote:Notes:Results:Thanks everyone, with combinations of different suggestions I got the program running twice as fast now (on top of the optimizations I did on my own before asking, so 4 times speed increase approx from my initial implementation)I tested with 2 sets of inputs which I'll call A and B ... fromto Got execution time from 11.92 to 9.18 for input A, and 79.30 to 74.59 for input BOptimization2:\nAdded a separate method for differs-by-one in addition to the distance-method (which I still needed elsewhere for the A* heuristics)Got execution time from 9.18 to 8.83 for input A, and 74.59 to 70.14 for input BOptimization3:\nBig winner here was to use  instead of Got execution time from 8.83 to 5.02 for input A, and 70.14 to 41.69 for input BI could probably do better writing it in a lower level language, but I'm happy with this for now.  Thanks everyone!Edit again: More results\nUsing Mark's method of checking the case where the first letter doesn't match got it down from 5.02 -> 3.59 and 41.69 -> 29.82Building on that and , I ended up with this:Which squeezed a little bit more, bringing the times down from 3.59 -> 3.38 and 29.82 -> 27.88Even more results!Trying Sumudu's suggestion that I , instead of the is_neighbor function I ended up with this:Which ended up being slower (3.38 -> 3.74 and 27.88 -> 34.40) but it seemed promising.  At first I thought the part I'd need to optimize was \"one_letter_off_strings\" but profiling showed otherwise and that the slow part was in fact I thought if there'd be any difference if I switched \"oneoff\" and \"wordlist\" and did the comparison the other way when it hit me that I was looking for the intersection of the 2 lists.  I replace that with :Bam! 3.74 -> 0.23 and 34.40 -> 2.25This is truely amazing, total speed difference from my original naive implementation:\n23.79 -> 0.23 and 180.07 -> 2.25, so approx 80 to 100 times faster than the original implementation.If anyone is interested, I made blog post  and  made including one that isn't mentioned here (because it's in a different section of code).Ok, me and Unknown are having a big debate which you can read in the comments of . He claims that it would be faster using the original method (using is_neighbor instead of using the sets) if it was ported to C.  I tried for 2 hours to get a C module I wrote to build and be linkable without much success after trying to follow  and  example, and it looks like the process is a little different in Windows? I don't know, but I gave up on that.  Anyway, here's the full code of the program, and the text file come from the  using the \"2+2lemma.txt\" file.  Sorry if the code's a little messy, this was just something I hacked together.  Also I forgot to strip out commas from the wordlist so that's actually a bug that you can leave in for the sake of the same comparison or fix it by adding a comma to the list of chars in cleanentries.I left the is_neighbors method in even though it's not used.  This is the method that is proposed to be ported to C.  To use it, just replace getchildren with this:As for getting it to work as a C module I didn't get that far, but this is what I came up with:I profiled this using:And the time recorded was the total time of the AStar method call.  The fast input set was \"verse poets\" and the long input set was \"poets verse\".  Timings will obviously vary between different machines, so if anyone does end up trying this give result comparison of the program as is, as well as with the C module.If your wordlist is very long, might it be more efficient to generate all possible 1-letter-differences from 'word', then check which ones are in the list?  I don't know any Python but there should be a suitable data structure for the wordlist allowing for log-time lookups.I suggest this because if your words are reasonable lengths (~10 letters), then you'll only be looking for 250 potential words, which is probably faster if your wordlist is larger than a few hundred words.Your function  is calculating the total distance, when you really only care about distance=1. The majority of cases you'll know it's >1 within a few characters, so you could return early and save a lot of time.Beyond that, there might be a better algorithm, but I can't think of it. Another idea.You can make 2 cases, depending on whether the first character matches. If it doesn't match, the rest of the word has to match exactly, and you can test for that in one shot. Otherwise, do it similarly to what you were doing. You could even do it recursively, but I don't think that would be faster. I've deleted the check to see if the strings are the same length, since you say it's redundant. Running  on my own code and on the is_neighbors function , I get the following: (Probably getting into community wiki territory here, but...)Trying your final definition of is_neighbors() with izip instead of zip: 2.9 seconds.Here's my latest version, which still times at 1.1 seconds:Or maybe in-lining the  code:And a rewritten :People are mainly going about this by trying to write a quicker function, but there might be another way..Why is this? Perhaps a better way to optimise is to try and reduce the number of calls to , rather than shaving milliseconds of  execution time. It's impossible to tell without seeing the full script, but optimising a specific function is generally unnecessary.If that is impossible, perhaps you could write it as a C module?How often is the distance function called with the same arguments? A simple to implement optimization would be to use . You could probably also create some sort of dictionary with frozensets of letters and lists of words that differ by one and look up values in that. This datastructure could either be stored and loaded through pickle or generated from scratch at startup.Short circuiting the evaluation will only give you gains if the words you are using are very long, since the hamming distance algorithm you're using is basically O(n) where n is the word length. I did some experiments with timeit for some alternative approaches that may be illustrative.Well you can start by having your loop break if the difference is 2 or more.Also you can changeto Because xrange generates sequences on demand instead of generating the whole range of numbers at once.You can also try comparing word lengths which would be quicker. Also note that your code doesn't work if word1 is greater than word2There's not much else you can do algorithmically after that, which is to say you'll probably find more of a speedup by porting that section to C.Attempting to explain my analysis of Sumudu's algorithm compared to verifying differences char by char.When you have a word of length L, the number of \"differs-by-one\" words you will generate will be 25L. We know from implementations of sets on modern computers, that the  search speed is approximately , where n is the number of elements to search for. Seeing that most of the 5 million words you test against is  in the set, most of the time, you will be traversing the entire set, which means that it really becomes  instead of only log(25L)/2. (and this is assuming best case scenario for sets that comparing string by string is equivalent to comparing char by char)Now we take a look at the time complexity for determining a \"differs-by-one\". If we assume that you have to check the entire word, then the number of operations per word becomes . We know that most words differ by 2 very quickly. And knowing that most prefixes take up a small portion of the word, we can logically assume that you will break most of the time by , or half the word (and this is a conservative estimate).So now we plot the time complexities of the two searches, L/2 and log(25L), and keeping in mind that  (highly in favor of sets). You have the equation log(25*L) > L/2, which can be simplified down to log(25) > L/2 - log(L). As you can see from the graph, it should be quicker to use the char matching algorithm until you reach  large numbers of L.I was the first person in this question to suggest breaking on a difference of 2 or more. The thing is, that Mark's idea of string slicing (if word1[0] != word2[0]: return word1[1:] == word2[1:]) is simply putting what we are doing into C. How do you think  is calculated? The same way that we are doing.As for producing the C code, I am a bit busy. I am sure you will be able to do it since you have written in C before. You could also try C#, which probably has similar performance characteristics.Here is a more indepth explanation to Davy8Your one_letter_off_strings function will create a set of 25L strings(where L is the number of letters). Creating a set from the wordlist will create a set of D strings (where D is the length of your dictionary). By creating an intersection from this, you  iterate over each  and see if it exists in . The time complexity for this operation is detailed above. This operation is less efficient than comparing the  you want with each word in . Sumudu's method is an optimization in C rather than in algorithm.To create the intersection of 125 one-letter-off words with a dictionary of 4500, you need to make 125 * 4500 comparisons. This is not log(125,2). It is at best 125 * log(4500, 2) assuming that the dictionary is presorted.  You are also doing a string by string instead of char by char comparison here.For such a simple function that has such a large performance implication, I would probably make a C library and call it using .  One of reddit's founders claims they made the website 2x as fast using this technique.You can also use  on this function, but beware that it can eat up a lot of memory.I don't know if it will significantly affect your speed, but you could start by turning the list comprehension into a generator expression.  It's still iterable so it shouldn't be much different in usage:toThe main problem would be that a list comprehension would construct itself in memory and take up quite a bit of space, whereas the generator will create your list on the fly so there is no need to store the whole thing.Also, following on unknown's answer, this may be a more \"pythonic\" way of writing distance():But it's confusing what's intended when len (word1) != len (word2), in the case of zip it will only return as many characters as the shortest word. (Which could turn out to be an optimization...)Try this:Also, do you have a link to your game? I like being destroyed by word gamesFirst thing to occur to me:which has a decent chance of going faster than other functions people have posted, because it has no interpreted loops, just calls to Python primitives. And it's short enough that you could reasonably inline it into the caller.For your higher-level problem, I'd look into the data structures developed for similarity search in metric spaces, e.g.  or , neither of which I've read (they came up in a search for a paper I have read but can't remember).for this snippet:i'd use this one:the same pattern would follow all around the provided code...Everyone else focused just on explicit distance-calculation without doing anything about constructing the distance-1 candidates.\nYou can improve by using a well-known data-structure called a  to merge the  with the task of . A Trie is a linked-list where each node stands for a letter, and the 'next' field is a dict with up to 26 entries, pointing to the next node.Here's the pseudocode: walk the Trie iteratively for your given word; at each node add all distance-0 and distance-1 neighbors to the results; keep a counter of distance and decrement it. You don't need recursion, just a lookup function which takes an extra distance_so_far integer argument.A minor tradeoff of extra speed for O(N) space increase can be gotten by building separate Tries for length-3, length-4, length-5 etc. words. "},
{"body": "I'm using Python logging, and for some reason, all of my messages are appearing twice.I have a module to configure logging:Later on, I call this method to configure logging:And then within say, the buy_ham module, I'd call:And for some reason, all the messages are appearing twice. I commented out one of the stream handlers, still the same thing. Bit of a weird one, not sure why this is happening...lol. Assuming I've missed something obvious.Cheers,\nVictorYou are calling  twice (maybe in the  method of ) :  will return the same object, but  does not check if a similar handler has already been added to the logger. Try tracing calls to that method and eliminating one of these. Or set up a flag  initialized to  in the  method of  and change  to do nothing if  is , and to set it to  after you've initialized the logger. If your program creates several  instances, you'll have to change the way you do things with a global  function adding the handlers, and the  method only initializing the  attribute.Another way of solving this is by checking the handlers attribute of your logger:The handler is added each time you call from outside. Try Removeing the Handler after you finish your job:I'm a python newbie, but this seemed to work for me (Python 2.7)If you are seeing this problem and you're not adding the handler twice then see abarnert's answer From the :So, if you want a custom handler on \"test\", and you don't want its messages also going to the root handler, the answer is simple: turn off its propagate flag:logger.propagate = False"},
{"body": "I have write a python library app(which contains several *.py files). And several of my python projects need to reuse the code in the library app. What's the recommended best practice for reusing python code? Currently I have thought out three options:Among the three options above which one do you prefer? What advantage does it have compared to the other two options?  Do you have any other better options? It is much appreciated that if some one with years of python development experiences could answer this question. Allow me to propose a fourth alternative: take the time to learn how to package your library and install it in your site-packages; it's easier than one may think and I'm convinced it's time well spent. This is a very good starting point: UPDATE:\nThe original URL is not valid anymore. Read the  instead.Of your three options, PYTHONPATH is the way to go.  Copy & paste is clearly out, and adding code to your other projects to modify sys.path simply pollutes those files with knowledge about their environment.A fourth option is, create a true installable package from your common code, and install it into your Python installation.  Then you can simply import those modules like any other 3rd-party install code.The first way is, as you noted yourself, hardly acceptable as it has .The other two have their own problems. For starters, they require manual work when files move (particular bad if you mix it with the application logic, i.e. put it in *.py files instead of leaving it to the computer it runs on) and requires either fixed installation locations (absolute paths) or at least a certain directory structure (relative paths). IMHO, these ways are only acceptable if the applications aren't ever going to move anywhere else. As soon as that becomes required, you should give up and use the existing solution as the one reason not to use it, being overkill for small and local scripts, doesn't apply any more:Make the common parts, which you apparently already treat as a free-standing library (good!), a fully-fledged project in its own right, with a  that allows installing and adding to PYTHONPATH in a cross-platform way with a single command. You don't need to actually publish it at PyPI, but it makes doing so easier if you should change your mind in the future. Should you do so and also publish some of your projects on PyPI, you also made installing the project in question and its dependencies easier for every potential user.If its a shared library you should package it up and add it to site-packages and then you wont have to worry about setting anything up.  This is the best option.If you dont want to use site-packages, then use PYTHONPATH.  Its why it exists, and it is  to do what you want.You might want to look into using , path.append does not prevent duplicates.  It will also allow you to leverage .pth files.Dynamically setting/adding things to PYTHONPATH via  achieves the same result, but it can complicate things if you choose to reuse your new library.  It also causes issues if your paths change, you have to change  versus an environment variable. Unless it is completely necessary, you should not dynamically setup your python path.  Copy & Paste is not a re-use pattern.  You aren't reusing anything you are duplicating and increasing maintenance. "},
{"body": "I had a very difficult time with understanding the root cause of a problem in an algorithm. Then, by simplifying the functions step by step I found out that evaluation of default arguments in Python doesn't behave as I expected.The code is as follows:The problem is that every instance of Node class shares the same  attribute, if the attribute is not given explicitly, such as:I don't understand the logic of this design decision? Why did Python designers decide that default arguments are to be evaluated at definition time? This seems very counter-intuitive to me.The alternative would be quite heavyweight -- storing \"default argument values\" in the function object as \"thunks\" of code to be executed over and over again every time the function is called without a specified value for that argument -- and would make it much harder to get early binding (binding at def time), which is often what you want.  For example, in Python as it exists:...writing a memoized function like the above is quite an elementary task.  Similarly:...the simple , relying on the early-binding (definition time) of default arg values, is a trivially simple way to get early binding.  So, the current rule is simple, straightforward, and lets you do all you want in a way that's extremely easy to explain and understand: if you want late binding of an expression's value, evaluate that expression in the function body; if you want early binding, evaluate it as the default value of an arg.The alternative, forcing late binding for both situation, would not offer this flexibility, and would force you to go through hoops (such as wrapping your function into a closure factory) every time you needed early binding, as in the above examples -- yet more heavy-weight boilerplate forced on the programmer by this hypothetical design decision (beyond the \"invisible\" ones of generating and repeatedly evaluating thunks all over the place).In other words, \"There should be one, and preferably only one, obvious way to do it [1]\": when you want late binding, there's already a perfectly obvious way to achieve it (since all of the function's code is only executed at call time, obviously everything evaluated  is late-bound); having default-arg evaluation produce early binding gives you an obvious way to achieve early binding as well (a plus!-) rather than giving TWO obvious ways to get late binding and no obvious way to get early binding (a minus!-).[1]: \"Although that way may not be obvious at first unless you're Dutch.\"The issue is this.It's too expensive to evaluate a function as an initializer .The construct  is literal, like , that means \"this exact object\".The problem is that some people hope that it to means  as in \"evaluate this function for me, please, to get the object that is the initializer\".It would be a crushing burden to add the necessary  statement to do this evaluation all the time.  It's better to take all arguments as literals and not do any additional function evaluation as part of trying to do a function evaluation.Also, more fundamentally, it's technically  to implement argument defaults as function evaluations.Consider, for a moment the recursive horror of this kind of circularity.  Let's say that instead of default values being literals, we allow them to be functions which are evaluated each time a parameter's default values are required.[This would parallel the way  works.]What is the value of ?  To get the default for , it must evaluate , which requires an eval of .  Oops.Of course in your situation it is difficult to understand. But you must see, that evaluating default args every time would lay a heavy runtime burden on the system.Also you should know, that in case of container types this problem may occur -- but you could circumvent it by making the thing explicit:The workaround for this,  (and very solid), is:As for why look for an answer from von L\u00f6wis, but it's likely because the function definition makes a code object due to the architecture of Python, and there might not be a facility for working with reference types like this in default arguments.I thought this was counterintuitive too, until I learned how Python implements default arguments.A function's an object.  At load time, Python creates the function object, evaluates the defaults in the  statement, puts them into a tuple, and adds that tuple as an attribute of the function named .  Then, when a function is called, if the call doesn't provide a value, Python grabs the default value out of .For instance:So all calls to  that don't provide an argument will use the same instance of , because that's the default value.As far as why Python does it this way:  well, that tuple  contain functions that would get called every time a default argument value was needed.  Apart from the immediately obvious problem of performance, you start getting into a universe of special cases, like storing literal values instead of functions for non-mutable types to avoid unnecessary function calls.  And of course there are performance implications galore.The actual behavior is really simple.  And there's a trivial workaround, in the case where you  a default value to be produced by a function call at runtime:This comes from python's emphasis on syntax and execution simplicity.  a def statement occurs at a certain point during execution.  When the python interpreter reaches that point, it evaluates the code in that line, and then creates a code object from the body of the function, which will be run later, when you call the function.  It's a simple split between function declaration and function body.  The declaration is executed when it is reached in the code.  The body is executed at call time.  Note that the declaration is executed every time it is reached, so you can create multiple functions by looping.Five separate functions have been created, with a separate list created each time the function declaration was executed.  On each loop through , the same function is executed twice on each pass through, using the same list each time.  This gives the results:Others have given you the workaround, of using param=None, and assigning a list in the body if the value is None, which is fully idiomatic python.  It's a little ugly, but the simplicity is powerful, and the workaround is not too painful.Edited to add: For more discussion on this, see effbot's article here: , and the language reference, here: Python function definitions are just code, like all the other code; they're not \"magical\" in the way that some languages are. For example, in Java you could refer \"now\" to something defined \"later\":but in PythonSo, the default argument is evaluated at the moment that that line of code is evaluated!Because if they had, then someone would post a question asking why it wasn't the other way around :-pSuppose now that they had. How would you implement the current behaviour if needed? It's easy to create new objects inside a function, but you cannot \"uncreate\" them (you can delete them, but it's not the same)."},
{"body": "Is it possible to use strings as indices in an array in python?For example:What you want is called an . In python these are called .Alternative way to create the above dict:Accessing values:Getting the keys (in Python v2):If you want to learn about python dictionary internals, I recommend this ~25 min video presentation: . It's called the \"The Mighty Dictionary\".Even better, try an  (assuming you want something like a list).  Closer to a list than a regular dict since the keys have an order just like list elements have an order.  With a regular dict, the keys have an arbitrary order.Note that this is available in Python 3 and 2.7.  If you want to use with an earlier version of Python you can find installable modules to do that."},
{"body": "I have a range of points x and y stored in numpy arrays.\nThose represent x(t) and y(t) where t=0...T-1I am plotting a scatter plot usingI would like to have a colormap representing the time (therefore coloring the points depending on the index in the numpy arrays) What is the easiest way to do so?Here is an exampleHere you are setting the color based on the index, , which is just an array of .\nPerhaps an easier-to-understand example is the slightly simplerNote that the array you pass as  doesn't need to have any particular order or type, i.e. it doesn't need to be sorted or integers as in these examples.  The plotting routine will scale the colormap such that the minimum/maximum values in  correspond to the bottom/top of the colormap.You can change the colormap by addingImporting  is optional as you can call colormaps as  just as well.  There is a  of colormaps showing what each looks like.  Also know that you can reverse a colormap by simply calling it as .  So eitherwill work.  Examples are  or .  Here's an example with the new 1.5 colormap viridis:You can add a colorbar by usingNote that if you are using figures and subplots explicitly (e.g.  or ), adding a colorbar can be a bit more involved. Good examples can be found  and .To add to wflynny's answer above, you can find the available colormaps Example:\nor alternatively,\n"},
{"body": "My code1st file:2nd file:in the above code, only keys of \"data\" dictionary were get passed to , but i want key-value pairs to pass. How to correct this ?I want the  to get modified like this and this is my requirement, give answers according to this  dictionary key  is changed to  If you want to use them like that, define the function with the variable names as normal (but use  for , you can't use reserved words):Now (as long as you rename class to klass in your dictionary) you can use  when you  the function:and it will work as you want.*data interprets arguments as tuples, instead you have to pass **data which interprets the arguments as dictionary. You can call the function like this:You can just pass itor if you really want to"},
{"body": "I am going through Zed Shaw's Python Book. I am currently working on the opening and reading files chapters. I am wondering why we need to do a truncate, when we are already opening the file in a 'w' mode?Its redundant since, as you noticed, opening in write mode will overwrite the file, more info at  section of Python documentation.So Zed Shaw calls truncate() on a file that is already truncated. OK, that's pretty pointless. Why does he do that? Who knows!? Ask him! Maybe he does it to show that the method exists? Could be, but that would be pretty daft, since I've never needed to truncate a file in my 15 years as a programmer so it has no place in a newbie book.Maybe he does it because he thinks he has to truncate the file, and he simply isn't aware that it's pointless?Maybe he does it intentionally to confuse newbies? That would fit with his general modus operandi, which seems to be to intentionally piss people off for absolutely no reason. The reason he does this is now clear. In later editions he lists this question as a \"common question\" in the chapter, and tells you to go read the docs. It's hence there to:You can debate if this is good teaching style or not, I wouldn't know.The number of \"Help I don't understand Zed Shaws book\"-questions on SO had dwindled, so I can't say that it's any worse than any other book out there, which probably means it's better than many. :-)If you would READ the questions before asking it, he answers it for you: He explicitly wants you to find these things out for yourself, this is why his extra credit is important.He also EXPLICITLY states that he wants you to PAY ATTENTION TO DETAIL. Every little thing matters. With , you can declare how much of the file you want to remove, based on where you're currently at in the file. Without parameters,  acts like w, whereas w always just wipes the whole file clean. So, these two methods  act identically, but they don't necessarily.While it's not useful to truncate when opening in 'w' mode, it is useful in 'r+'. Though that's not the OP's question, I'm going to leave this here for anyone who gets lead here by Google as I did.Let's say you open (with mode 'r+', ) a 5 line indented json file and modify the -ed object to be only 3 lines. If you  before writing the data back to the file, you will end up with 2 lines of trailing garbage. If you  it you will not.I know this seems obvious, but I'm here because I am fixing a bug that occurred after an object that stayed the exact same size for years... shrank because of a signing algorithm change. (What is not obvious is the unit tests I had to add to prevent this is the future. I wrote my longest docstring ever explaining why I'm testing signing with 2 ridiculously contrived algorithms.)Hope this helps someone.That's just a reflection of the standard posix semantics. see man fopen(3). Python just wraps that."},
{"body": "In Python, I'd like to write a function  which returns another function. That returned function should be callable with a parameter , and return the volume of a cylinder with height  and radius .I know how to return  from functions in Python, but how do I return ?Try this, using Python:Use it like this, for example with  and :Notice that returning a function was a simple matter of defining a new function inside the function, and returning it at the end - being careful to pass the appropriate parameters for each function. FYI, the technique of returning a function from another function is known as .Just want to point out that you can do this with pymonad"},
{"body": "given the following dataframe in pandas:where  is an id for each point consisting of an  and  value, how can I bin  and  into a specified set of bins (so that I can then take the median/average value of  and  in each bin)?   might have  values for  or  (or both) for any given row in . thanks.Here's a better example using Joe Kington's solution with a more realistic df. The thing I'm unsure about is how to access the df.b elements for each df.a group below:There may be a more efficient way (I have a feeling  would be useful here), but here's how I'd do it:Edit: As the OP was asking specifically for just the means of  binned by the values in , just do Also if you wanted the index to look nicer (e.g. display intervals as the index), as they do in @bdiamante's example, use  instead of .  (Kudos to bidamante. I didn't realize  existed.)This results in:Not 100% sure if this is what you're looking for, but here's what I think you're getting at:Of course, I don't know what bins you had in mind, so you'll have to swap mine out for your circumstance.Joe Kington's answer was very helpful, however, I noticed that it does not bin all of the data. It actually leaves out the row with a = a.min(). Summing up  gave 99 instead of 100.To guarantee that all data is binned, just pass in the number of bins to cut() and that function will automatically pad the first[last] bin by 0.1% to ensure all data is included.In this case, summing up groups.size() gave 100.I know this is a picky point for this particular problem, but for a similar problem I was trying to solve, it was crucial to obtain the correct answer."},
{"body": "I have values like this:I want to sort the values in each  in increasing order. I don't want to sort between the sets, but the values in each set.From a comment:That's easy. For any set  (or anything else iterable),  returns a list of the elements of  in sorted order:Note that  is giving you a , not a . That's because the whole point of a set, both in  and in ,* is that it's not ordered: the sets  and  are the same set.You probably don't really want to sort those elements as strings, but as numbers (so 4.918560000 will come before 10.277200999 rather than after).The best solution is most likely to store the numbers as numbers rather than strings in the first place. But if not, you just need to use a  function:For more information, see the  in the official docs.* See the comments for exceptions."},
{"body": "I'm using the pdb module to debug a program. I'd like to understand how I can exit pdb and allow the program to continue onward to completion. The program is computationally expensive to run, so I don't want to exit without the script attempting to complete.  doesn't seems to work. How can I exit pdb and continue with my program? should \"Continue execution, only stop when a breakpoint is encountered\", so you've got a breakpoint set somewhere. To remove the breakpoint (if you inserted it manually):Or, if you're using , you can try this (although if you're using pdb in more fancy ways, this may break things...)A simple - will break out of pdb. If you want to continue rather than breaking, just press  rather than the whole  commandIf you really wish to exit the debugger then you need to run something like  which allows you to detach from the process and then exit the debugger, (N.B. It is multi-platform).If you would like to continue debugging but no longer stop at a given breakpoint then you need to:For more detail on the above see ."},
{"body": "I am trying to find if the process is running based on process id. The code is as follows based on one of the post on the forum. I cannot consider process name as there are more than one process running with the same name. Output :It always return true as it can find the process id in the output string. Any suggestions? Thanks for any help.Try:Should succeed (and do nothing) if the process exists, or throw an exception (that you can catch) if the process doesn't exist.The simplest answer in my opinion (albiet maybe not ideal), would be to change yourTo:This will ignore the process listing for the grep search containing the PID of the process you are trying to find. It seems user9876's answer is far more \"pythonic\" however. This is a bit of a kludge, but on *nix you can use  or  to test the existence of the process ID.EDIT: Note that  works on Windows (as of Python 2.7), while  .   the Windows version calls , which will \"unconditionally cause a process to exit\", so I predict that it won't safely return the information you want without actually killing the process if it does exist.If you're using Windows, please let us know, because none of these solutions are acceptable in that scenario.You could check if the folder /proc/[process_id] exists.See this SO: If you don't mind using external module I'd suggest . It is cross-platform and easier to use than spawning subshell only for purpose of finding a running process.If that process belongs to the same user the checking process, you can just try to  it. If you use signal 0, kill will not send anything but still allow you to tell if the process is available.From :This should propagate appropriately to python's methods.I know this is old, but I've used this and it seems to work; you can do a quick adaptation to convert from process name to process id:On Windows another option is to use tasklist.exe:Syntax: tasklist.exe /NH /FI \"PID eq processID\"On Windows, you can use WMI. You can also use other filters.  For example, I'm much more likely to just want to tell if a process is running by name and take action.  For example, if DbgView isn't running, then start it.You can also iterate and do other interesting things.  Complete list of fields is .Recently I had to list the running processes and did so:You have to find it twice..Like this :If this returns  then this is actually running !!"},
{"body": "Can someone tell me how to install sqlite3 package to the very recent version of Python?\nI use Macbook, and on the command line I tried:but an error pops up.You don't need to install  module. It is included in the standard library (since Python 2.5).I have python 2.7.3 and this solved my problem:"},
{"body": "Any gotchas I should be aware of?  Can I store it in a text field, or do I need to use a blob?\n(I'm not overly familiar with either pickle or sqlite, so I wanted to make sure I'm barking up the right tree with some of my high-level design ideas.)If you want to store a pickled object, you'll need to use a blob, since it is binary data. However, you can, say, base64 encode the pickled object to get a string that can be stored in a text field.Generally, though, doing this sort of thing is indicative of bad design, since you're storing opaque data you lose the ability to use SQL to do any useful manipulation on that data. Although without knowing what you're actually doing, I can't really make a moral call on it.I needed to achieve the same thing too.  I turns out it caused me quite a headache before I finally figured out, , how to actually make it work in a binary format.You must specify the second argument to dumps to force a binary pickling.\nAlso note the  to make it fit in the BLOB field.When retrieving a BLOB field, sqlite3 gets a 'buffer' python type, that needs to be strinyfied using  before being passed to the loads method.I wrote a blog about this idea, except instead of a pickle, I used json, since I wanted it to be interoperable with perl and other programs.  Architecturally, this is a quick and dirty way to get persistence, transactions, and the like for arbitrary data structures.  I have found this combination to be really useful when I want persistence, and don't need to do much in the sql layer with the data (or it's very complex to deal with in sql, and simple with generators).  The code itself is pretty simple:Then, when you wnat to dump it into the db, Pickle has both text and binary output formats. If you use the text-based format you can store it in a TEXT field, but it'll have to be a BLOB if you use the (more efficient) binary format.Since Pickle can dump your object graph to a string it should be possible. Be aware though that TEXT fields in SQLite uses database encoding so you might need to convert it to a simple string before you un-pickle.If a dictionary can be pickled, it can be stored in text/blob field as well.Just be aware of the dictionaries that can't be pickled (aka that contain unpickable objects).Yes, you can store a pickled object in a TEXT or BLOB field in an SQLite3 database, as others have explained.Just be aware that some object .  The built-in container types can (dict, set, list, tuple, etc.).  But some objects, such as file handles, refer to state that is external to their own data structures, and other extension types have similar problems.Since a dictionary can contain arbitrary nested data structures, it might not be pickle-able.I have to agree with some of the comments here.  Be careful and make sure you really want to save pickle data in a db, there's probably a better way.In any case I had trouble in the past trying to save binary data in the sqlite db.\nApparently you have to use the sqlite3.Binary() to prep the data for sqlite.Here's some sample code:SpoonMeiser is correct, you need to have a strong reason to pickle into a database.  It's not difficult to write Python objects that implement persistence with SQLite.  Then you can use the SQLite CLI to fiddle with the data as well.  Which in my experience is worth the extra bit of work, since many debug and admin functions can be simply performed from the CLI rather than writing specific Python code.In the early stages of a project, I did what you propose and ended up re-writing with a Python class for each business object (note: I didn't say for each table!) This way the body of the application can focus on \"what\" needs to be done rather than \"how\" it is done.The other option, considering that your requirement is to save a dict and then spit it back out for the user's \"viewing pleasure\", is to use the  module which will let you persist any pickleable data to file. The python docs are .Depending on what you're working on, you might want to look into the  module. It does something similar, where it auto-stores Python objects inside a sqlite database (and all sorts of other options) and pretends to be a dictionary (just like the  module).It is possible to store object data as pickle dump, jason etc but it is also possible to index, them, restrict them  and run select queries that use those indices. Here is example with tuples, that can be easily applied for any other python class. All that is needed is explained in python sqlite3 documentation (somebody already posted the link). Anyway here it is all put together in the following example:See this solution at SourceForge:y_serial.py module :: warehouse Python objects with SQLite\"Serialization + persistance :: in a few lines of code, compress and annotate Python objects into SQLite; then later retrieve them chronologically by keywords without any SQL. Most useful \"standard\" module for a database to store schema-less data.\""},
{"body": "In python how would I pass an argument from the command line to a unittest function.  Here is the code so far\u2026 I know it's wrong.To extend with the above comment about unit tests.  Unit tests should stand alone in that they have no dependencies outside of their setup and tear down requirements, such as in your case setting up a email.  This makes sure that each tests has very specific side effects and reactions to the test. Passing in a parameter defeats this property of unit tests and thus makes them in a sense invalid. Using a test configuration would be the easiest way and-also more proper because again a unit test should never rely on outside information to perform the test. That is for integration tests.So the doctors here that are saying \"You say that hurts?  Then don't do that!\" are probably right.  But if you really want to, here's one way of passing arguments to a unittest test:You need the s so your command line params don't mess with unittest's own...[update]  The other thing you might want to look into is .  All the cool kids are saying it's the best python testing framework these days, and it :  Another method for those who really want to do this in spite of the correct remarks that you shouldn't: is meant for testing the very basic functionality (the lowest level functions of the application) to be sure that your  work correctly.  There is probably no formal definition of what does that exactly mean, but you should consider other kinds of testing for the  -- see . The unit testing framework may not be ideal for the purpose.If you want to use  with , you can modify the original test loader (see ):Have a same problem. My solution is after you handle with parsing arguments using argparse or other way, remove arguments from sys.argvIf you need you can filter unittest arguments from main.parseArgs()"},
{"body": "I'm using a builder pattern to seperate a bunch of different configuration possibilities. Basically, I have a bunch of classes that are named an ID (something like ID12345). These all inherit from the base builder class. In my script, I need to instantiate an instance for each class (about 50) every time this app runs. So, I'm trying to see if instead of doing something like this:Can I do something like this (I know this doesn't work):That way, when I need to add a new one in the future, all I have to do is add the id to the IDS list, rather than peppering the new ID throughout the code. Looks like there are some different opinions based on where the data is coming from. These IDs are entered in a file that no one else has access to. I'm not reading the strings from the command line, and I'd like to be able to do as little alteration when adding a new ID in the future. Not totally sure this is what you want, but it seems like a more Python'y way to instantiate a bunch of classes listed in a string:If you wanted to avoid an eval(), you could just do:Provided that the class is defined in (or imported into) your current scope. use  if you can help it. Python has  many better options (dispatch dictionary, , etc.) that you should never have to use the security hole known as .Simplest way is to just create a dict.Then use it (your code example):There's some stuff missing from your question, so I'm forced to guess at the omitted stuff.  Feel free to edit your question to correct the omissions.This works very nicely.  It doesn't instantiate from a string -- in practice you don't often want this.  Sometimes, but rarely.  More commonly, you a list of class objects from which you want instance objects.If you actually are getting your class names from the command line, then you'd make the following small change.Everything else remains the same.[Also, if possible, try to start instance variable names with lowercase letters.  Names which start with Uppercase Letters  are usually class names.]Removed .  In spite of the fact that  is absolutely   a security hole.  Eval (and  and ) are only a problem if someone specifically grants access to malicious users."},
{"body": "I'm debugging some Python that takes, as input, a list of objects, each with some attributes.I'd like to hard-code some test values -- let's say, a list of four objects whose \"foo\" attribute is set to some number.Is there a more concise way than this?Ideally, I'd just like to be able to say something like:(Obviously, that is made-up syntax. But is there something similar that really works?)Note: This will never be checked in. It's just some throwaway debug code. So don't worry about readability or maintainability.I like Tetha's solution, but it's unnecessarily complex.Here's something simpler:I found this: , and in my limited testing it seems like it works:Have a look at this:Non classy:Another obvious hack:But for your exact usecase, calling a function with anonymous objects directly, I don't know any one-liner less verbose thanUgly, does the job, but not really.So brief, such Python! O.oEDIT: Well okay, technically this creates a class object, not an object object. But you can treat it like an anonymous object or you modify the first line by appending a pair of parenthesis to create an instance immediately:"},
{"body": "I notice there is a comparison operator . Should I literally translate it into instead of To expand on what Ignacio said: and  test whether two objects have the same . You can override an object's  and  methods to determine what that means. and  test whether two objects are the same thing. It's like doing It's not relational comparison, it's identity. And it translates to .:and about operator  in the same chapter:means that \"A  B\", not \"A  B\"."},
{"body": "I'm looking for how to turn the frequency axis in a fft (taken via scipy.fftpack.fftfreq) into a frequency in Hertz, rather than bins or fractional bins.I tried to code below to test out the FFT:The sampling rate should be 4000 samples / 120 seconds = 33.34 samples/sec.The signal has a 2.0 Hz signal, a 8.0 Hz signal, and some random noise.I take the FFT, grab the frequencies, and plot it.  The numbers are pretty nonsensical.  If I multiply the frequencies by 33.34 (the sampling frequency), then I get peaks at about 8 Hz and 15 Hz, which seems wrong (also, the frequencies should be a factor of 4 apart, not 2!).Any thoughts on what I'm doing wrong here?I think you don't need to do fftshift(), and you can pass sampling period to fftfreq():from the graph you can see there are two peak at 2Hz and 8Hz. gives you the frequencies directly.  If you set , this will tell you the frequency in Hz for each point of the fft.The frequency width of each bin is (sampling_freq / num_bins).Your equation is messed up. This gives you 4000 samples of your function, sampled at 33.33 Hz, representing 120 seconds of data.Now take your FFT. Bin 0 will hold the DC result. Bin 1 will be 33.33, bin 2 will be 66.66, etc..Edit: I forget to mention that, since your sampling rate is 33.33 Hz, the maximum frequency that can be represented will be fs/2, or 16.665 Hz."},
{"body": "I am trying to develop an sample project in Django and getting errors when I run the syncdb command.This is how my project structure looks like:/Users/django_demo/godjango/bookings:And my manage.py file is as follows:And my PYTHONPATH and DJANGO_SETTINGS_MODULE are set as belowAnd my WSGI.py file looks like below:When I run the python manage.py syncdb command, I am getting the following error.Could someone suggest what I am missing?The error says So, is your path  within the python-sys.path?Check it in your shell with:If not - you need to add it or simply move your  app into one of the paths represented in your Modify your wsgi.py file from to The significant part of the traceback here is right at the very end. It says \"No module named unipath\". You've referred to that somewhere in your code, but you don't seem to have it in your project - it's not part of the standard library, so you'll need to install it somewhere that Python can see it.Alternatively, you can even pass the settings path at run time like so:This should override the environment variable Review hour /etc/apache2/httpd.conf file; you must include the WSGIPythonPath directive, to indicate the folder which contains your Django project (manage.py file), like:WSGIPythonPath /home/user/Projects/Django/MyProjectalso if you used some weird port in your VirtualHost, specify if for listening:Listen 90Hope this helps somebodyIn my case I was using  to import other files and there was an error in it, so settings might actually exist but be flawed. "},
{"body": "In the Python console, when I type:Gives:Though I'd expect to see such an output:What am I missing here?The console is printing the representation, not the string itself.If you prefix with , you'll get what you expect.See  for details about the difference between a string and the string's representation. Super-simplified, the representation is what you'd type in source code to get that string.You forgot to  the result. What you get is the  in  and not the actual printed result.In Py2.x you should so something like and in Py3.X, print is a function, so you should doNow that was the short answer. Your Python Interpreter, which is actually a REPL, always displays the representation of the string rather than the actual displayed output. Representation is what you would get with the  statementYou need to  to get that output.\nYou should doYou have to print it:When you print it with this  you would get:The  is a new line character specially used for marking END-OF-TEXT. It signifies the end of the line or text. This characteristics is shared by many languages like C, C++ etc."},
{"body": "When i run the following code in Python - 3.3:I get the following error:I did this too to verify:What am i doing wrong? Thanks in advance!Import  instead of .Interestingly, I noticed some IDE-depending behavior.Both Spyder and PyCharm use the same interpreter on my machine : in PyCharm I need to do while in Spyder, does fineIf this is on PyCharm, as was mine, make sure your file name isn't urllib.py."},
{"body": "I'd like to write a Python script for Amarok in Linux to automatically copy the stackoverflow podcast to my player. When I plug in the player, it would mount the drive, copy any pending podcasts, and eject the player. How can I listen for the \"plugged in\" event? I have looked through hald but couldn't find a good example.: As said in comments, Hal is not supported in recent distributions, the standard now is udev, Here is a small example that makes use of glib loop and , I keep the Hal version for historical reasons.This is basically the , adapted to work with older versions, and with the glib loop, notice that the filter should be customized for your specific needing:You can use D-Bus bindings and listen to  and  signals.\nYou will have to check the capabilities of the Added device in order to select the storage devices only.Here is a small example, you can remove the comments and try it.You need to connect to Hal Manager using the System Bus.And you need to connect a listener to the signals you are interested on, in this case .I'm using a filter based on capabilities. It will accept any  and will call  with if, you can read Hal documentation to find the more suitable queries for your needs, or more information about the properties of the Hal devices.Example function that shows some information about the volume:I haven't tried writing such a program myself, however I've just looked at the following two links (thanks Google!), which I think will be of help:In particular, read about the  interface, and its  and  events. :-)Hope this helps!I think D-Bus would work as Chris mentioned, but if you're using KDE4, you might use the Solid framework in a manner similar to the KDE4 \"New Device Notifier\" applet.The C++ source for that applet is , which shows how to use Solid to detect new devices.  Use PyKDE4 for Python bindings to these libraries, as shown .Here is a solution in 5 lines.Save this to a file say , run . Plug any usb and it will print device detailsTested on Python 3.5 with . "},
{"body": " If you get the character encoding wrong than your output will be messed up.People usually use some rudimentary technique to detect the encoding. They either use the charset from the header or the charset defined in the meta tag or they use an  (which does not care about meta tags or headers).\nBy using only one these techniques, sometimes you will not get the same result as you would in a browser.Browsers do it this way:(Well... at least that is the way I believe most browsers do it. Documentation is really scarce.) I'm sure I'm not the first who needs a proper solution to this problem.According to .Beautiful Soup tries the following encodings, in order of priority, to turn your document into Unicode:I would use  for this.When you download a file with urllib or urllib2, you can find out whether a charset header was transmitted:You can use BeautifulSoup to locate a meta element in the HTML:If neither is available, browsers typically fall back to user configuration, combined with auto-detection. As rajax proposes, you could use the chardet module. If you have user configuration available telling you that the page should be Chinese (say), you may be able to do better.Use the :The other option would be to just use wget:It seems like you need a hybrid of the answers presented:I honestly don't believe you're going to find anything better than that.  In fact if you read further into the FAQ you linked to in the comments on the other answer, that's what the author of detector library advocates.If you believe the FAQ, this is what the browsers do (as requested in your original question) as the detector is a port of the firefox sniffing code.Scrapy downloads a page and detects a correct encoding for it, unlike requests.get(url).text or urlopen. To do so it tries to follow browser-like rules - this is the best one can do, because website owners have incentive to make their websites work in a browser. Scrapy needs to take HTTP headers,  tags, BOM marks and differences in encoding names in account. Content-based guessing (chardet, UnicodeDammit) on its own is not a correct solution, as it may fail; it should be only used as a last resort when headers or  or BOM marks are not available or provide no information.You don't have to use Scrapy to get its encoding detection functions; they are released (among with some other stuff) in a separate library called w3lib: . To get page encoding and unicode body use  function, with a content-based guessing fallback:instead of trying to get a page then figuring out the charset the browser would use, why not just use a browser to fetch the page and check what charset it uses.. BeautifulSoup dose this with UnicodeDammit : "},
{"body": "I'm trying to install couchapp, which uses easy_install - and is quite explicit in stating a particular version of easy_install/setuptools is needed: 0.6c6. I seem to have easy_install already on my Mac, but there's no command-line arguments to check the version. Instead of just installing a new version over the top, I'd like to see whether it's necessary first.So: Any pointers on how I can see what version of setuptools/easy_install I have installed on my machine?I'm not a Python developer, so I'm assuming this is a simple question. However, I've not found anything via Google or here on SOF.One way would be to look at the actual source file for easy_install. Doto see where it's located, and then use that path inThe second line in my easy_install script says:which suggests that I have easy_install version 0.6c11.This seems to get a lot of hits from google, so I thought I'd update for those folks.  I was able to do:which produced the outputI believe this only works for some (newer?) versions of setuptoolsOn Windows, where Python is in the Path:For me, I get\nsetuptools 7.0\nas the response."},
{"body": "I am using Python to parse entries from a log file, and display the entry contents using Tkinter and so far it's been excellent. The output is a grid of label widgets, but sometimes there are more rows than can be displayed on the screen. I'd like to add a scrollbar, which looks like it should be very easy, but I can't figure it out.The documentation implies that only the List, Textbox, Canvas and Entry widgets support the scrollbar interface. None of these appear to be suitable for displaying a grid of widgets. It's possible to put arbitrary widgets in a Canvas widget, but you appear to have to use absolute co-ordinates, so I wouldn't be able to use the grid layout manager?I've tried putting the widget grid into a Frame, but that doesn't seem to support the scrollbar interface, so this doesn't work:Can anyone suggest a way round this limitation? I'd hate to have to rewrite in PyQt and increase my executable image size by so much, just to add a scrollbar!You can only associate scrollbars with a few widgets, and the root widget and  aren't part of that group of widgets. The most common solution is to create a canvas widget and associate the scrollbars with that widget. Then, into that canvas embed the frame that contains your label widgets. Determine the width/height of the frame and feed that into the canvas  option so that the scrollregion exactly matches the size of the frame.Drawing the text items directly on the canvas isn't very hard, so you might want to reconsider that approach if the frame-embedded-in-a-canvas solution seems too complex. Since you're creating a grid, the coordinates of each text item is going to be very easy to compute, especially if each row is the same height (which it probably is if you're using a single font).For drawing directly on the canvas, just figure out the line height of the font you're using (and there are commands for that). Then, each y coordinate is . The x coordinate will be a fixed number based on the widest item in each column. If you give everything a tag for the column it is in, you can adjust the x coordinate and width of all items in a column with a single command.Here's an example of the frame-embedded-in-canvas solution, using an object-oriented approach:Here is a solution that doesn't use objects: to make this work in python 2.x, use  rather than  in the import statement"},
{"body": "I am using Numpy to store data into matrices. Coming from R background, there has been an extremely simple way to apply a function over row/columns or both of a matrix.Is there something similar for python/numpy combination? It's not a problem to write my own little implementation but it seems to me that most of the versions I come up with will be significantly less efficient/more memory intensive than any of the existing implementation. I would like to avoid copying from the numpy matrix to a local variable etc., is that possible?The functions I am trying to implement are mainly simple comparisons (e.g. how many elements of a certain column are smaller than number x or how many of them have absolute value larger than y).Almost all numpy functions operate on whole arrays, and/or can be told to operate on a particular axis (row or column). As long as you can define your function in terms of numpy functions acting on numpy arrays or array slices, your function will automatically operate on whole arrays, rows or columns.It may be more helpful to ask about how to implement a particular function to get more concrete advice.Numpy provides  and  to turn Python functions which operate on numbers into functions that operate on numpy arrays.For example,(The elements of the first array get replaced by the corresponding element of the second array when the second is bigger.)But don't get too excited;  and  are . They don't actually make your code any faster. If your underlying Python function is operating on one value at a time, then  will feed it one item at a time, and the whole \noperation is going to be pretty slow (compared to using a numpy function which calls some underlying C or Fortran implementation).To count how many elements of column  are smaller than a number , you could use an expression such as:For example:Selecting elements from a NumPy array based on one or more conditions is straightforward using NumPy's  beautifully dense syntax:\n\n\nNumPy's indexing syntax is pretty close to R's; given your fluency in R, here are the key differences between R and NumPy in this context:NumPy , in R, indexing begins with 1NumPy (like Python) allows you to  using negative indices--e.g.,NumPy uses , e.g., in R, to\n   get the first three rows in A, you would use, A[1:3, ]. In NumPy, you\n   would use A[0:2, :] (in NumPy, the \"0\" is not necessary, in fact it\n   is preferable to use A[:2, :]I also come from a more R background, and bumped into the lack of a more versatile apply which could take short customized functions. I've seen the forums suggesting using basic numpy functions because many of them handle arrays. However, I've been getting confused over the way \"native\" numpy functions handle array (sometimes 0 is row-wise and 1 column-wise, sometimes the opposite). My personal solution to more flexible functions with apply_along_axis was to combine them with the implicit lambda functions available in python. Lambda functions should very easy to understand for the R minded who uses a more functional programming style, like in R functions apply, sapply, lapply, etc.So for example I wanted to apply standardisation of variables in a matrix. Tipically in R there's a function for this (scale) but you can also build it easily with apply:(R code)You see how the body of the function inside apply (x-mean(x))/sd(x) is the bit we can't type directly for the python apply_along_axis. With lambda this is easy to implement FOR ONE SET OF VALUES, so:(Python)Then, all we need is to plug this inside the python apply and pass the array of interest through apply_along_axisObviously, the lambda function could be implemented as a separate function, but I guess the whole point is to use rather small functions contained within the line where apply originated.I hope you find it useful ! is very useful for this. For instance,  and  should help you."},
{"body": "I have a Python regular expression that contains a group which can occur zero or many times - but when I retrieve the list of groups afterwards, only the last one is present. Example:()this returns the list ('g',)I need it to return ('a','b','c','d','e','f','g',)Is that possible? How can I do it? In addition to , here is the explanation:In regular expressions the group count is fixed. Placing a quantifier behind a group does not increase group count (imagine all other group indexes increment because an eralier group matched more than once).Groups with quantifiers are the way of making a complex sub-expression atomic, when there is need to match it more than once. The regex engine has no other way than saving the last match only to the group. In short: There is no way to achieve what you want with a single \"unarmed\" regular expression, and you have to find another way."},
{"body": "I want to compare UTC timestamps from a log file with local timestamps. When creating the local  object, I use something like:I want to find an automatic tool that would replace the with the current local time zone.Any ideas?Try , which has a  type that does what you need.It is  in a portable manner. Fortunately, you don't need it to perform the comparison. returns a pytz timezone corresponding to the local timezone:Unlike other solutions presented so far the above code avoids the following issues:Note: to get timezone-aware datetime object from a naive datetime object, you should use:instead of: forces an exception if given local time is ambiguous or non-existent.If you are certain that all local timestamps use the same (current) utc offset for the local timezone then you could perform the comparison using only stdlib: and  can be compared directly.Note: I was asking the same to myself, and I found the answer in 1:Take a look at section 8.1.7: the format \"%z\" (lowercase, the Z uppercase returns also the time zone, but not in the 4-digit format, but in the form of timezone abbreviations, like in [3]) of strftime returns the form \"+/- 4DIGIT\" that is standard in email headers (see section 3.3 of RFC 2822, see [2], which obsoletes the other ways of specifying the timezone for email headers).So, if you want your timezone in this format, use: [1] [2] [3] Timezone abbreviations:  , only for reference.In Python 3.x, local timezone may can be figure out like this:It's a tricky use of 's .First get pytz and tzlocal modulespip install pytz tzlocalthenthen you can do things likeAvoiding non-standard module (seems to be a missing method of datetime module):Based on Thoku's answer above, here's an answer that resolves the time zone to the nearest half hour (which is relevant for some timezones eg South Australia's) :Here's a way to get the local timezone using only the standard library, (only works in a *nix environment):You can use this to create a  timezone:...which you can then apply to a :Based on J. F. Sebastian's comment, you can do this with the standard library:Tested in 3.4, should work on 3.4+For simple things, the following  implementation can be used, which queries the OS for time zone offsets:First, note that the question presents an incorrect initialization of an aware datetime object:creates an invalid instance.  One can see the problem by computing the UTC offset of the resulting object:(Note the result which is an odd fraction of an hour.)To initialize an aware datetime properly using pytz one should use the  method as follows:Now, if you require a local pytz timezone as the new tzinfo, you should use the tzlocal package as others have explained, but if all you need is an instance with a correct local time zone offset and abbreviation then tarting with Python 3.3, you can call the  method with no arguments to convert an aware  instance to your local timezone: from .Code example follows. Last string suitable for use in filenames."},
{"body": "I want to store a datetime object with a localized UTC timezone. The method that stores the datetime object can be given a non-localized datetime (naive) object or an object that already has been localized. How do I determine if localization is needed?Code with missing if condition:From :Though if  is a datetime object representing time in UTC timezone then you could use in both cases:It works regardless  is timezone-aware or naive.Note:  (it is ok to use it with UTC timezone but otherwise you should use  method).If you want to support arbitrary timezones for aware datetime objects i.e., only naive datetime objects are required to represent time in UTC timezone in your application then you could use  to convert  to  timezone:Note: you don't need to call  method after  here because the destination timezone is UTC.It works even if  uses non-pytz timezones.if you want to check if a datetime object 'd' is localized, check the d.tzinfo, if it is None, no localization.Here's a more complete function to convert or coerce a timestamp obj to utc. If it reaches the exception this means the timestamp is not localized. Since it's good practice to always work in UTC within the code, this function is very useful at the entry level from persistence. The small addition from the 'try catch' in the answer by J.F. Sebastian is the additional catch condition, without which not all naive cases will be caught by the function. "},
{"body": "I just solved some problems in my Django 1.3 app by using PyMySQL instead of MySQLdb. I followed this tutorial on how to make the switch: Now I want to know what PyMySQL actually is and how it is different from MySQLdb.I am using it on localhost and will then upload it to some hosting.Is it fine to use PyMySQL on localhost and on hosting whatever they provide? Since I have changed \"MySQLdb\" in base.py and introspection.py to \"PyMySQL\", will I need to upload it to the server after changing these files? Or as it is Django's files, since Django will be uploaded there already, does it not matter much?PyMySQL and MySQLdb are both database connectors for Python, libraries to enable Python programs to talk to a MySQL server.You would normally never upload core Django files when deploying an app. If Django is working fine on your deployment server, you definitely don't need to change anything there. The DB driver is a step or two below the ORM even, and certainly none of the code you have written depends on which of these is in use.PyMySQL and MySQLdb provide the same functionality - they are both database connectors. The difference is in the implementation where MySQLdb is a C extension and PyMySQL is pure Python.There are a few reasons to try PyMySQL:The proper way to use it with Django is to import it and tell it to impersonate MySQLdb in your top-level file, usually manage.py. Put the following code at the very top of your manage.py (or whatever file you call when starting your server):Your first point:According to  page:Your second point:"},
{"body": "I'm trying to understand the following piece of code:Specifically, I don't understand what the index  refers to. If the index  refers to the first element, then what does  refer to?Negative numbers mean that you count from the right instead of the left. So,  refers to the last element,  is the second-last, and so on.List indexes of -x mean the xth item from the end of the list, so  means the last item in the list . Any good Python tutorial should have told you this.It's an unusual convention that few languages other than Python have adopted, but it is extraordinarily useful; in any other language you'll spend a lot of time writing  to access the last item of a list."},
{"body": "The code below produces gaps between the subplots.  How do I remove the gaps between the subplots and make the image a tight grid?You can use  to control the spacing between axes. There's more  here. The problem is the use of , which prevents the subplots from stretching to an arbitrary aspect ratio and filling up all the empty space.Normally, this would work:The result is this:However, with , as in the following code:This is what we get:The difference in this second case is that you've forced the x- and y-axes to have the same number of units/pixel. Since the axes go from 0 to 1 by default (i.e., before you plot anything), using  forces each axis to be a square. Since the figure is not a square, pyplot adds in extra spacing between the axes horizontally.To get around this problem, you can set your figure to have the correct aspect ratio. We're going to use the object-oriented pyplot interface here, which I consider to be superior in general:Here's the result:Have you tried ?with \n\nwithout it:\nOr: something like this (use )If you don't need to share axes, then simply \n"},
{"body": "Python's built-in unittest module makes assertions with  methods: I have generally used a testrunner such as  or  which allow use of the built-in  keyword when making assertions:What is the motivation for unittest's  approach and what are the strengths and weaknesses of this vs asserting with the built-in assert keyword? Are there reasons why unittest's syntax should be favoured?The problem with the  keyword is that it is optimized out, and thus , when Python is run in 'optimized' mode (with the  argument or with the  environment variable set.) If tests were to use  then testing with  would be impossible.Additionally, the use of the assert methods makes it trivial to report about what the values involved actually were, without having to dig into the stack and the source and figure out what they were supposed to be (which, I believe, is the technique  and  use for this.)I don't see a concrete design decision for this. Looking at the  it states \nthat So I would say it is an implementation decision to help produce more meaningful errors etc.The main strength of this approach is providing several built in tests that are commonly performed so that one doesn't have to write them over and over again. Additionally, assertRaises lets you customize the exact behavior of the assert by throwing an exception."},
{"body": "I have a signal_handler connected through a decorator, something like this very simple one:What I want to do is to mock it  in a test, to check how many times django calls it. My code at the moment is something like:The problem here is that the original signal handler is called even if mocked, most likely because the  decorator is storing a copy of the signal handler somewhere, so I'm mocking the wrong code.So the question: how do I mock my signal handler to make my test work?Note that if I change my signal handler to:and I mock  instead, everything works as expected.So, I ended up with a kind-of solution: mocking a signal handler simply means to connect the mock itself to the signal, so this exactly is what I did:Notice that  in  is required in order to make  to correctly work on a , otherwise django will raise some exceptions and the connection will fail.Possibly a better idea is to mock out the functionality  the signal handler rather than the handler itself. Using the OP's code:Then mock :There is a way to mock django signals with a small class.You should keep in mind that this would only mock the function as a django signal handler and not the original function; for example, if a m2mchange trigers a call to a function that calls your handler directly, mock.call_count would not be incremented. You would need a separate mock to keep track of those calls.Here is the class in question:Example usagetake a look at mock_django . It has support for signalsYou can mock a django signal by mocking the ModelSignal class at  like this:That should do the trick. Note that this will mock ALL signals no matter which object you are using.If by any chance you use the  library instead, it can be done like this:It's more lines but it works pretty well too :)In django 1.9 you can mock all receivers with something like thisThis replaces all receivers with mocks, eg ones you've registered, ones pluggable apps have registered and ones that django itself has registered. Don't be suprised if you use this on  and things start breaking. You may want to inspect the receiver to determine if you actually want to mock it."},
{"body": "Alright, I have a fairly simple design.Is there an easy-ish way to allow a user to create an update all on one page?What I  is for a user to be able to go to the admin interface, add a new Update, and then while editing an Update add one or more Posts, with each Post having one or more Media items.  In addition, I want the user to be able to reorder Posts within an update.My current attempt has the following in admin.py:This let's the user add a new Post item, select the relevant Update, add the Media items to it, and hit save - which is fine.  But there's no way to see all the Posts that belong to a given Update in a single place, which in turn means you can't roderder Posts within an update.  It's really quite confusing for the end user.Help?As of now there is no \"built-in\" way to have nested inlines (inline inside inline) in django.contrib.admin. Pulling something like this off is possible by having your own ModelAdmin and InlineModelAdmin subclasses that would enable this kind of functionality. See the patches on this ticket  for ideas on how to implement this. You'd also need to provide your own templates that would have nested iteration over both the top level inline and it's child inline.There is now this egg available, which is a collation of the relevant patches mentioned in the other answer:however, it appears that it is not stable yet (they mention some known issues, and are looking for bugfix pull requests).  They don't say what the issues are.Edit: I'm using it, it seems to be working, no issues so far.I have just ran into this issue as well... Seems this thread which contains the request for the nested inlines feature () has been updated with further information. A custom made app called \"django-super-inline\" has been released. More details here: Installation and usage instructions below. Hope this is useful for whomever comes across this.I ran into a similar issue to this.  My approach was to make an UpdateAdmin that held inlines for both Media and Post... it basically just makes it so you have a list of all of the media entries followed by all of the posts in an update.It isn't an ideal solution but it works for a quick and dirty work around.I have done this using , for the following Data structure:My file: appears to be much more actively maintained than the other apps already mentioned ( and )"},
{"body": "I have the following code in my template:It works as expected when there is more than one item in the friendslist. But if there is just 1 item, then the content inside the forloop.last\nconditional does not display. I guess this is because the loop in that case is the first, but I mean it's also the last right? So why dosn't both contents inside first and last\nconditional show?In my code they both execute if there is only one element in friendslist. Here is a test you can run in the shell where maybe you can figure out what isn't working:couldn't you just use an \"or\" tag like {% if forloop.last or friendlist.count == 1 %}"},
{"body": "I'm trying to extract named entities from my text using NLTK. I find that NLTK NER is not very accurate for my purpose and I want to add some more tags of my own as well. I've been trying to find a way to train my own NER, but I don't seem to be able to find the right resources. \nI have a couple of questions regarding NLTK-I would really appreciate help in this regardAre you committed to using NLTK/Python?  I ran into the same problems as you, and had much better results using Stanford's named-entity recognizer: .  The process for training the classifier using your own data is very well-documented in the FAQ.  If you really need to use NLTK, I'd hit up the mailing list for some advice from other users: .  Hope this helps!You can easily use the Stanford NER alongwith nltk.\nThe python script is likeTo train your own data and to create a model you can refer to the first question on Stanford NER FAQ.The link is "},
{"body": "As a self-development exercise, I want to develop a simple classification algorithm that, given a particular cell of a Dilbert cartoon, is able to identify which characters are present in the cartoon (Dilbert, PHB, Ratbert etc.). I assume the best way to do this is to (1) apply some algorithm to the image, which converts it into a set of features, and (2) use a training set and one of many possible machine learning algorithms to correlate the presence/absence of certain features with a particular character being present in the cell.So my questions are - (a) is this the correct approach, (b) since there's a number of classification algorithms and ML algorithms to test, what is a good methodology for finding the right one, and (c) which algorithms would you start with, given that we're essentially conducting a classification exercise on a cartoon.So i think you are on the right track w/r/t your step 1 (. This project is more challenging that most ML problems because here you will actually have to create your training data set from the raw data (the individual frames comprising the cartoons). For instance, grab a frame, identify two characters in that frame, Dilbert and the character with horns (Dilbert's boss i believe, don't know his name), extract those two characters from that frame and append to each the appropriate class label (e.g., \"1\" for Dlibert).\n\nTo extract the individual characters from each of the frames comprising the a Dilbert cartoon, i would suggest  a  of each frame. If you are not familiar with this technique, at its core, it's just an eigenvector decomp. If you like python (or R, given that you can use python-to-R bindings like ) then i would strongly encourage you to look at . In particular, this excellent library (which was originally developed under the  project umbrella, and therefore uses NumPy + SciPy for matrix computation) has several algorithms for image segmentation, one of which is based on . For this step in your Project, you would most likely want to look at these two scikits.learn modulesIncluded with these two modules are two good example scripts, one  and the  segmenting an image comprised of three partially super-imposed circles with minimal contrast w/r/t each other and w/r/t the background--both, i suspect are more difficult problems that the decompositions you will need to perform. In other words,  has two complete, well-documented example scripts included in the source distribution, both of which process data similar to yours. Either or both would be an excellent template for this step.\nSo that's the first step; here's the second:  all of the components of the decomposed images into groups, . Next, assign a class label to each Group, e.g., if there are four characters from your decomposition step, then a decent choice for class labels is \"0\", \"1\", \"2\", and \"3.\" Append those class labels to the component matrices (the decomposition products from step 1) so each character matrix is mapped to its corresponding class (Dilbert character).\nSelect a suitable ML technique. You have many choices for this step; the only criteria are that the technique is in the  category (because you have assigned class labels to your data) and that it function as a  (i.e., it returns a class label, versus a regressor which outputs a numerical value). Given this is a personal project, i would chose the one that seems most interesting to you. A few that satisfy the criteria i just mentioned are:  (neural network),  (SVM), and  (kNN).\ntrain, validate, and test your classifier: Once Step 1 is completed (each image is decomposed into a set of objects, some of which will no doubt represent the characters) you can manually sift through these decomposition products and collect exemplars for each character in the cartoon. The are the .Next you compare objects segmented from an image with this set of unique templates. In , another scipy scikit, you can use the method , to which you pass in a template image and a candidate image, and this method returns a 2D array showing the pixel-by-pixel correlation (between -1 and 1).  I think, in general, this is the correct approach and that there are two techniques you could have a look at.This question was asked 5 years back, therefore the answers provided above are outdated given the fact that deep learning has changed the face of computer vision over the past 3-4 years. A deep learning based solution would involve training a Convolutional Neural Network, which would learn features and perform classification in an end-to-end learning framework. However, since multiple cartoons may be present in the same image, the standard softmax cross entropy loss used in image classification may not be appropriate. Hence, independent logistic regression should be used as a loss function. Threshold for each class can be obtained based on accuracy obtained over a held-out validation set. Even for cartoons, it is better to use a pre-trained model initialized using imagenet instead of training from scratch (, although the final task in this paper is different, they still do processing on cartoons). If you have abundant data, pre-training may not be that important. This task can be performed using standard libraries like / etc.You can try building a model by uploading your training data (images of comics) to  (free to use)Then query the API using the following (Python Code):the response looks like:"},
{"body": "What is the current status of easy_install, pip and the repository (pypi.python.org) with regards to Python 3.x?Are there versions of easy_install and/or pip that can install the right versions of packages from there? Else, are they expected soon?PyPi itself supports Python 3.  The  package provides a version of easy_install that works with Python 3.According to , pip support Python 3 since v 1.0.There is ongoing effort to provide support for Python 3 in pip.\nSee  thread on virtualenv mailing list or head directly to Vinay Sajip's  on bitbucket.orgAs of version 1.0,  supports Python 3.Notice that easy_install is shipped with Python 3.4 and higher"},
{"body": "I think the best way to ask this question is with some code... can I do this? (: ANSWER: no)\n: you can keep the column name as 'foo' in the database by passing a db_column to the model field. This is very helpful when you are working on an existing system and you don't want to have to do db migrations for no reasonA model field is already property, so I would say you have to do it the second way to avoid a name clash.When you define foo = property(..) it actually overrides the foo = models.. line, so that field will no longer be accessible.You will need to use a different name for the property and the field. In fact, if you do it the way you have it in example #1 you will get an infinite loop when you try and access the property as it now tries to return itself.EDIT: Perhaps you should also consider not using _foo as a field name, but rather foo, and then define another name for your property because properties cannot be used in QuerySet, so you'll need to use the actual field names when you do a filter for example.As mentioned, a correct alternative to implementing your own django.db.models.Field class, one should use -  argument and a custom (or hidden) class attribute. I am just rewriting the code in the edit by @Jiaaro following more strict conventions for OOP in python (e.g. if _foo should be actually hidden): will be resolved into  (as seen by ) thus hidden (). Note that this form also permits using of  which would be ultimately a nicer way to write readable code.Again, django will create *_MyModel table with two fields  and .The previous solutions suffer because @property causes problems in admin, and .filter(_foo).  A better solution would be to override  except that this can cause problems initializing the ORM object from the DB.  However, there is a trick to get around this, and it's universal.The secret is ''.  When the model initializes either from new or hydrated from the !"},
{"body": "What is the most elegant way to solve this:The built-in functions work like thisscr.txt now contains '111'.scr.txt was overwritten and now contains '222' (on Windows, Python 2.4).The solution should work inside the same process (like in the example above) as well as when another process has opened the file.\nIt is preferred, if a crashing program will not keep the lock open.I don't think there is a fully crossplatform way.  On unix, the fcntl module will do this for you.  However on windows (which I assume you are by the paths), you'll need to use the win32file module.Fortunately, there is a portable implementation () using the platform appropriate method at the python cookbook.To use it, open the file, and then call:where flags are portalocker.LOCK_EX for exclusive write access, or LOCK_SH for shared, read access.If by 'another process' you mean 'whatever process' (i.e. not your program), in Linux there's no way to accomplish this relying only on system calls ( & friends). What you want is , and the Linux way to obtain it is a bit more involved:Remount the partition that contains your file with the  option:Set the  flag for your file:In your Python code, obtain an exclusive lock on that file:Now even  will not be able to read the file until you release the lock. Here's a start on the win32 half of a portable implementation, that does not need a seperate locking mechanism.Requires the  to get down to the win32 api, but that's pretty much mandatory for python on windows already, and can alternatively be done with . The code could be adapted to expose more functionality if it's needed (such as allowing  rather than no sharing at all). See also the MSDN documentation for the  and  system calls, and the .As has been mentioned, you can use the standard  module to implement the unix half of this, if required.Here's how your example from above behaves:EDIT:  By using  & age as a locking mechanism! Locking by file is safe only on Windows (because Linux silently overwrites), but locking by directory works perfectly both on Linux and Windows. See my GIT where I created an easy to use class  for that:At the bottom of the readme, you find 3 GITplayers where you can see the code examples execute live in your browser! Quite cool, isn't it? :-)Thanks for your attentionI would like to answer to parity3 () but I can neither comment directly ('You must have 50 reputation to comment'), nor do I see any way to contact him/her directly. What do you suggest to me, to get through to him?My question:I have implemented something similiar to what parity3 suggested here as an answer:  (\"Assuming your Python interpreter, and the ...\")And it works brilliantly - on Windows. (I am using it to implement a locking mechanism that works across independently started processes.  )But other than parity3  says, it does NOT work the same on Linux:The silent replacing is the problem. On Linux.\nThe \"if dst already exists, OSError will be raised\" is great for my purposes. But only on Windows, sadly.I guess parity3's example still works most of the time, because of his if conditionBut then the whole thing is not atomic anymore. Because the if condition might be true in two parallel processes, and then both will rename, but only one will win the renaming race. And no exception raised (in Linux).Any suggestions? Thanks! P.S.: I know this is not the proper way, but I am lacking an alternative. PLEASE don't punish me with lowering my reputation. I looked around a lot, to solve this myself. How to PM users in here? And  why can't I?To make you safe when opening files within one application, you could try something like this:That way you subclass the  class. Now just do:If you open it first with 'w' mode, it won't allow anymore opens, even in read mode, just as you wanted...Assuming your Python interpreter, and the underlying os and filesystem treat os.rename as an atomic operation and it will error when the destination exists, the following method is free of race conditions.  I'm using this in production on a linux machine.  Requires no third party libs and is not os dependent, and aside from an extra file create, the performance hit is acceptable for many use cases.  You can easily apply python's function decorator pattern or a 'with_statement' contextmanager here to abstract out the mess.You'll need to make sure that lock_filename does not exist before a new process/task begins.It has come to light that os.rename silently overwrites the destination on a non-windows OS. Thanks for pointing this out @ akrueger!Here is a workaround, gathered from :Instead of using os.rename you can use:@ akrueger You're probably just fine with your directory based solution, just giving you an alternate method."},
{"body": "Suppose I have a module  and a package . If I callwhich one will be loaded? How can I specify I wand to load the module, or the package?I believe the package will always get loaded. You can't work around this, as far as I know. So change either the package or the module name. Docs: Actually, it is possible (this code is not well tested, but seems to work).File File File File RunningMaybe you want to move your classes from  module to .This way you'll be able to import them from the package as well as importing optional subpackages:File :File :Nonetheless I would like somebody to double-check this approach and let me know if it's correct or the  module shouldn't be used like that."},
{"body": "I'm working with some rather large, dense numpy float arrays that currently reside on disk in PyTables s. I need to be able to perform efficient dot products using these arrays, for example , where  is a huge (~1E4 x 3E5 float32) memory-mapped array, and  and  are smaller numpy arrays that are resident in core memory.What I'm doing at the moment is copying the data into memory-mapped numpy arrays using , then calling  directly on the memory-mapped arrays. This works, but I suspect that the standard  (or rather the underlying BLAS functions it calls) is probably not very efficient in terms of the number of I/O operations required in order to compute the result.I came across an interesting example in . A naive dot product computed using 3x nested loops, like this:requires  I/O operations to compute.However, by processing the arrays in appropriately-sized blocks:where  is the maximum number of elements that will fit into core memory, the number of I/O operations is reduced to .How smart is  and/or ? Does calling  perform an I/O-efficient blockwise dot product? Does  do any fancy caching that would improve the efficiency of this type of operation?If not, is there some pre-existing library function that performs I/O efficient dot products, or should I try and implement it myself?I've done some benchmarking with a hand-rolled implementation of  that operates on blocks of the input array, which are explicitly read into core memory. This data at least a partially addresses my original question, so I'm posting it as an answer.I've implemented a function for applying  to blocks that are explicitly read into core memory from the memory-mapped array: I then did some benchmarking to compare my  function to the normal  function applied directly to a memory-mapped array (see below for the benchmarking script). I'm using numpy 1.9.0.dev-205598b linked against OpenBLAS v0.2.9.rc1 (compiled from source). The machine is a quad-core laptop running Ubuntu 13.10, with 8GB RAM  and an SSD, and I've disabled the swap file.As @Bi Rico predicted, the time taken to compute the dot product is beautifully  with respect to the dimensions of . Operating on cached blocks of  gives a huge performance improvement over just calling the normal  function on the whole memory-mapped array:It's surprisingly insensitive to the size of the blocks being processed - there's very little difference between the time taken to process the array in blocks of 1GB, 2GB or 4GB. I conclude that whatever caching  arrays natively implement, it seems to be very suboptimal for computing dot products.It's still a bit of a pain to have to manually implement this caching strategy, since my code will probably have to run on machines with different amounts of physical memory, and potentially different operating systems. For that reason I'm still interested in whether there are ways to control the caching behaviour of memory-mapped arrays in order to improve the performance of .I noticed some odd memory handling behaviour as I was running the benchmarks - when I called  on the whole of  I never saw the resident set size of my Python process exceed about 3.8GB, even though I have about 7.5GB of RAM free. This leads me to suspect that there is some limit imposed on the amount of physical memory an  array is allowed to occupy - I had previously assumed that it would use whatever RAM the OS allows it to grab. In my case it might be very beneficial to be able to increase this limit.Does anyone have any further insight into the caching behaviour of  arrays that would help to explain this?I don't think numpy optimizes dot product for memmap arrays, if you look at the code for matrix multiply, which I got , you'll see that the function  (as currently implemented) computes the values of the result matrix in c memory order:In the above code,  is the return matrix,  is the 1d dot product function and  and  are iterators over the input matrices.That being said, it looks like your code might already be doing the right thing. In this case the optimal performance is actually much better than O(n^3/sprt(M)), you can limit your IO to only reading each item of A once from disk, or O(n). Memmap arrays naturally have to do some caching behind the scene and inner loop operates on , so if A is in C-order and the memmap cache is big enough, your code might already be working. You can enforce caching of rows of A explicitly by doing something like:I recomend you to use  instead of numpy.memmap. Also read their presentations about compression, it sounds strange to me but seems that sequence .Also use np.dot with MKL. And I don't know how numexpr() can be used for matrix multiplication, but for example for calculating euclidean norm it's the fastest way(comparing with numpy).Try to benchmark this sample code:The problem that I don't know how to tune chunk size and compression rate to current machine, so I think performance can be dependent on parameters.Also please note that all matrices in sample code are stored on disk, if some of them will be stored in RAM I think it will be faster.By the way I'm using x32 machine and with numpy.memmap I have some limitations on matrix size(I'm not sure but it seems that view size can be only ~2Gb) and PyTables have no limitations."},
{"body": "I am having an issue deploying a flask app on apache2 using wsgi. I have posted the error logs and config files below. I have tried moving things around, renaming them, etc, but all give me an internal server error. Not sure why I'm getting the import error. Any input / suggestions are appreciated, thanks!Here is my apache error.logwsgi.pyapp.pyHere is the basics of the directory tree, to give you an idea.Here is the apache virtualhost fileThanks to  and  on irc.freenode.org at #pocoo, they were able to help me get this fixed. The problem was the PythonPath was not correct. We fixed this by using the following wsgi.pyI used your solution to get it working but it kept duplicating the path in sys.path (you can write it out to see if it happens to you) so I made a little modification:That way it's only included once"},
{"body": "I have some classes looking like this:Now, this fails because Sub1 and Sub3 are not defined when Base.subs is. But obviously I can't put the subclasses before Base either. Is there a way to forward-declare classes in Python? I want to work with  so the types in subs actually have to be the same as the later declared subclasses, it's not enough that they have the same name and other properties.One workaround is to do:   the subclasses have been defined, but I don't like having to split my class in that way.Edit: Added information about orderThis is a hybrid version of @Ignacio Vazquez-Abrams and @aaronasterling's answers which preserves the order of the subclasses in the list. Initially the subclass names are manually placed in the  list in the desired order. Then as each subclass is defined, a class decorator causes the corresponding string to be replaced with the actual subtype. It important to note that there's a perhaps subtle difference between my original answer above and the update which follows below -- namely which is that the while the former will work with  class name that is registered with , whether  or not its a subclass of . I'm pointing this out for two reasons. First because in your comments you said that  was a \"bunch of classes in a list, some of which might be its subclasses\", and more importantly because that is  the case with the code in my update, which only works for  subclasses since they are effectively \"registered\" automatically via the metaclass -- but leave anything other in the list alone.\nExactly the same thing can also be done using a metaclass -- which has the advantage that it eliminates the need to explicitly decorate each subclass as shown in the accepted answer above and instead makes it happen automagically. Note that even though the metaclass's  is called for the creation of every subclass, it only updates the  list if that subclass's name appears in it -- so the initial Base class definition of the contents of  still controls what gets put in it and in what order.Write a decorator that adds it to the registry in . Because of the added requirement of order I completely reworked my answer. I also make use of a class decorator, which was used here first by @Ignacio Vazquez-Abrams.  code now tested and some stupidity slightly correctedI'm pretty sure this should work for you. Just assign the depended class attribute afterwards.\nThis is also a lot less complicated.There is no way to directly declare forward-references in Python, but there are several workarounds, a few of which are reasonable: Add the subclasses manually after they are defined.Example: Create  with  values, and use a  to substitute the actual subclasses (this can be a class method on Base or a function -- I'm showing the function version, although I would probably use the method version).Example:) Create  with  values, and have the method that uses  do the substitution.Example:I would use option  myself, as it keeps the functionality and the data all in one place.  The only thing you have to do is keep  up to date (and write the appropriate subclasses, of course).I would just define the subclasses as strings and have the inevitable decorator replace the strings with the classes that they name. I would also define the decorator on a metaclass because I think that that's more in line with the objective: we're modifying class behavior and just like you modify object behavior by modifying its class, you modify class behavior by modifying its metaclass.This outputs:"},
{"body": "My production system occasionally exhibits a memory leak I have not been able to reproduce in a development environment.  I've used a  (specifically, Heapy) with some success in the development environment, but it can't help me with things I can't reproduce, and I'm reluctant to instrument our production system with Heapy because it takes a while to do its thing and its threaded remote interface does not work well in our server.What I think I want is a way to dump a snapshot of the production Python process (or at least gc.get_objects), and then analyze it offline to see where it is using memory.    Once I have one, how do I do something useful with it?Using Python's  garbage collector interface and  it's possible to dump all the python objects and their sizes. Here's the code I'm using in production to troubleshoot a memory leak:Note that I'm only saving data from objects that have a  attribute because those are the only objects I care about. It should be possible to save the complete list of objects, but you will need to take care choosing other attributes. Also, I found that getting the referrers for each object was extremely slow so I opted to save only the referents. Anyway, after the crash, the resulting pickled data can be read back like this:Could you record the traffic (via a log) on your production site, then re-play it on your development server instrumented with a python memory debugger? (I recommend dozer: ), then clone an instance of the program on a sufficiently similar box using .  There are  to help with debugging python programs within gdb, but if you can get your program to concurrently , you could just continue the program's execution, and query it with python.I have never had to do this, so I'm not 100% sure it'll work, but perhaps the pointers will be helpful. looks promising:I don't know how to dump an entire python interpreter state and restore it. It would be useful, I'll keep my eye on this answer in case anyone else has ideas.If you have an idea where the memory is leaking, you can add checks the refcounts of your objects. For example:You could also check for reference counts higher than some number that is reasonable for your app. To take it further, you could modify the python interpreter to do these kinds of check by replacing the  and  macros with your own. This might be a bit dangerous in a production app, though.Here is an essay with more info on debugging these sorts of things. It's more geared for plugin authors but most of it applies.The  has some functions that might be useful, like listing all objects the garbage collector found to be unreachable but cannot free, or a list of all objects being tracked.If you have a suspicion which objects might leak, the  module could be handy to find out if/when objects are collected."},
{"body": "For now, when I try to save the m2m data, it just dies and says I should use the ClientGroupe Manager...so what's missing?:and here's the ClientForm code::\nhere's the error message:If you use the save method right now, Django will try to save using the manager (which Django doesn't allow).  Unfortunately, the behavior you want is a little bit trickier than what  does by default.  What you need to do is create a .First of all, you will need to change the options of your  so that it doesn't display the  attribute.Next, you must change the view to display the formset:And obviously, you must also tweak your template to render the formset.If you need any other advice on formsets, see these articles:\nYou probably need to remove the ManyToMany field from your Client model, or else carefully exclude it from your form.  Unfortunately, the default widget for the ManyToMany field cannot populate the ClientGroupe Model properly (even if the missing field, dt, had been set to autonow=True).  This is something you'll either need to break out into another form, or handle in your view.When you save your form, you save Client object. Now if you want to assign client to the group you should do this:where client_instance and groupe_instance your client and groupe objets.I'm providing an alternative solution due to issues I encountered with forms_valid not being called:I've copied the logic from djangos forms/models.py, my field Genres is a manytomany with an intermediary table - I exclude it from the save_instance and then save it separately."},
{"body": "I successfully built/installed NumPy on my mac os x for python 2.7.3.\nNow I would like to build/install scipy as well. I downloaded it from git hub.\nWent into the directory. Ran python setup.py build and it seemed to be working until it came across this error:I thought that I had Fortran installed for NumPy...guess not? How would I download it?Your problem is that you need to install a Fortran compiler to build .Also, if you already have a  that's built with Fortran support disabled, you may have to replace it. Some of Apple's pre-installed Python versions have such a  build pre-installed.The easiest way to get Fortran is with . As  say, you need to install Xcode and its Command Line Tools first. (The way to install the Command Line Tools changes with almost each major version of Xcode, so see the linked docs for an up-to-date explanation.) Then install Homebrew. The installation URL has changed a few times, so see the Homebrew home page or installation instructions(), but it will be something like:Then:(Note that until some time in 2014,  was a separate recipe from , so the command was . But if you try that now, you'll get an error saying \"GNU Fortran is now provided as part of GCC, and can be installed with: \".)You really want to use  to install , so if you don't have that, get it first. Apple's pre-installed Python, at least in 10.7 and 10.8, includes  but not , so the easiest way to do that is:However, you may want to consider using a  instead of a global install (in which case you also want to remove the  on the following commands).Now that you've got  and , all you have to do is this:Caveats:If you're using a non-Apple build of Python 2.7, and you want to avoid the PATH problems, you have to do two things:First, do not, ever, install any Python packages that include scripts or binaries (including  itself) in more than one Python. For example, if you install  for both Apple 2.7 and Homebrew 2.7, both will attempt to create scripts named  and . If you're lucky, one install will fail. Otherwise, they'll both succeed, one will end up overwriting the other, and you will have no way of running the overwritten version.Second, make sure the path to the alternate Python's scripts and binaries comes before Apple's in the PATH. Depending on which alternate Python you've installed and which instructions you followed, this could be:Whatever the path is, you need to edit your PATH variable.If you want to affect GUI apps (and LaunchAgents, etc.), there is apparently no longer a supported way to do this, but the deprecated  does seem to still work in Lion. It's also what the Homebrew  and  suggest.If you only care about command-line sessions (both Terminal.app and remote ssh), you can instead just do the standard Unix thing of editing the appropriate profile file. Which profile file is appropriate depends on what you want to affect. (All users or just one user? bash or any shell? And so on.) If you don't know which one you want, you really should do some research. If you don't want to bother learning, just do  and then don't complain if it wasn't what you wanted.Either way, you need to make sure that the appropriate path comes before  in the PATH. So, for example, you could add the following to :(You will of course need to either create a new Terminal shell, or source the script, before it takes effect.)If you're using ,  will tell you if you got it right.It looks like your actual problem was just an intermittent download failure from Sourceforge:Homebrew should just recover automatically from this error if you try  again. So, that's the first thing to try.If that doesn't work, see if  finds any problems, then  to see where it's storing the partially-downloaded file so you can delete it manually and try again.If all else fails, you can force it to not use the bottle by using . Of course building from source takes a lot longer than installing a binary bottle, but it should give the same result.As of 5/20/2014, if you're using brew, Fortran is installed as part of gcc.  There is no separate Fortran package required.  Here's what worked for me to install numpy:I don't think this problem is too complicated at all.1) Install pipcd /Library/Frameworks/Python.framework/Versions/\nln -s 2.7 Currentcd /usr/bin\nrm -f python\nln -s /Library/Frameworks/Python.framework/Versions/Current/bin/python python2) download egg from \nsudo sh setuptools-0.6c11-py2.7.egg3) Simply download and install the gfotran from this link:\n4) After that, you need to type:\nsudo pip install -U scipy\nsudo pip install -U numpy\nsudo pip install -U matplotlibHopefully, you should have everything you want to have.I met the same problem, using following steps:\n1. brew install gfortran\n2. pip install scipy  then it's ok.I had  installed and the binary was . So, everytime I was trying to install SciPy the compiler couldn't be found because it was trying to use .What I did was create a symlink of  to .To find where you should create the symlink run:Then symlink it:i have homebree installed so I'm going to try: $brew install gfortran\nhopefully this worksFor OSX yosemite and possibly older osx releases too, you may want to download and install the fortran compiler before running . To download the latest fortran compiler, just go to the following link:I think your . I had Xcode: 6.4 and Os X 10.11.1 and was still getting the same error messages (could not locate gfortran, etc). I tried many things, including the thorough response by @abarnert, but the solution for me was upgraded Xcode (to 7.1.1).Here are detailed instructions for a clean install on OSX 10.7. If I were you, I'd go this route instead of trying to download and build the sources yourself."},
{"body": "how would I stop a while loop after 5 minutes if it does not achieve what I want it to achieve.This code throws me in an endless loop.Try the following:You may also want to add a short sleep here so this loop is not hogging CPU (for example  at the beginning or end of the loop body).Try this module:  RegardsYou do not need to use the  loop in this case. There is a much simpler way to use the time condition directly:"},
{"body": "How would I parse the string  (one million) into it's integer value in Python?There's also a simple way to do this that should handle internationalization issues as well:I found though that I have to explicitly set the locale first otherwise it doesn't work for me and I end up with an ugly traceback instead:So if that happens to you:Replace the ',' with '' and then cast the whole thing to an integer."},
{"body": "Am i able to overload the print function? and call the normal function? What i want to do is after a specific line i want print to call my print which will call the normal print and write a copy to file.Also i dont know how to overload print. I dont know how to do variable length arguments. i'll look it up soon but   just told me i cant overload print in 2.x which is what i am using.Overloading  is a design feature of python 3.0 to address your lack of ability to do so in python 2.x.However, you can override sys.stdout. (.) Just assign it to another file-like object that does what you want.Alternatively, you could just pipe your script through the the unix  command.  will print to both stdout and to output.txt, but this will capture all output.For those reviewing the previously dated answers, as of version release \"Python 2.6\" there is a new answer to the original poster's question.In Python 2.6 and up, you can disable the print statement in favor of the print function, and then override the print function with your own print function:Of course you'll need to consider that this print function is only module wide at this point. You could choose to override , but you'll need to save the original ; likely mucking with the  namespace.I came across the same problem.How about this:It worked like a charm for me. \nIt was taken from  I don't think you can overload print, but Python has a robust logging package that is highly customizable.Though you can't replace the  keyword (in Python 2.x  is a keyword), it's common practice to replace  to do something similar to  overriding; for example, with an instance of . This will capture all of the printed data in the  instance, after which you can manipulate it.In Python 2.x you can't, because print isn't a function, it's a statement. In Python 3 print is a function, so I suppose it could be overridden (haven't tried it, though).I answered the same question Essentially, simplest solution is to just redirect the output to stderr as follows, in the wsgi config file.just thought I'd add my idea... suited my purposes of being able to run sthg in Eclipse and then run from the (Windows) CLI without getting encoding exceptions with each print statement.\nWhatever you do don't make EncodingStdout a subclass of class file: the line \"self.encoding = encoding\" would then result in the encoding attribute being None!NB one thing I found out during a day grappling with this stuff is that the encoding exception gets raised BEFORE getting to \"print\" or \"write\": it is when the parameterised string (i.e. \"mondodod %s blah blah %s\" % ( \"blip\", \"blap\" )) is constructed by... what??? the \"framework\"?For a very simple example, as of Python3.4 (haven't tested with older versions) this works well for me (placed at top of module):Note, this only works if the string parameter is a str...  YMMV"},
{"body": "This is a slightly weird request but I am looking for a way to write a list to file and then read it back some other time.I have no way to remake the lists so that they are correctly formed/formatted as the example below shows.My lists have data like the following:If you don't need it to be human-readable/editable, the easiest solution is to just use .To write:To read:If you  need them to be human-readable, we need more information.If  is guaranteed to be a list of strings with no embedded newlines, just write them one per line:If they're Unicode strings rather than byte strings, you'll want to  them. (Or, worse, if they're byte strings, but not necessarily in the same encoding as your system default.)If they might have newlines, or non-printable characters, etc., you can use escaping or quoting. Python has a variety of different kinds of escaping built into the stdlib.Let's use  here to solve both of the above problems at once:You can also use the 3.x-style solution in 2.x, with either the  module or the  module:*As long as your file has consistent formatting (i.e. line-breaks), this is easy with just basic file IO and string operations:That will store your data file as a list of items, one per line. To then put it into a file, you would do the opposite:Hopefully that fits what you're looking for."},
{"body": "I want to get a fixed length list from another list like:And I want to get a list like this: . In other words, if , i want to fill up list  with values from list  up to length of the list , somewhat similar to what  does.This is my code:But it shows error:What can i do?Why not just:Use itertools repeat.Why not just  Can't you just do:If you want to fill with mutable values, for example with s:Where  is the number of elements in the array.Populating without the  would cause every element to be !To be more explicit, this solution replaces the first elements of  with all of the elements of  regardless of the values in  or :This will also work with:If you do not want to the result to be longer than :Just another solution:I like some of the other solutions better."},
{"body": "sqlautocode - has issues with many-to-many relationssqlsoup - doesn't support relationselixir - it's note auto-generateIs there something else I could try?In theory reflection in sqlalchemy should work for you.  In this case I'm using an mssql database with two tables which have a simple Many-to-one relation:\"Tests\" with fields:\"Users\" with fields:So the following should reflect the database:So this appears to work with table relations.  Although you still haven't given much detail to exactly what you are trying to do.Well I went through that, tried on Northwind database and it looks promising. Although, I had to add relationship field to be able to follow database relations. Let's consider that I don't know relations between tables at the moment of starting the application so I need is a way to generate automatically. You could use  to generate all the models from the database. However you need to take care of the foreign key manually."},
{"body": "So, lets say I have 100,000 float arrays with 100 elements each.  I need the highest X number of values, BUT only if they are greater than Y.  Any element not matching this should be set to 0.   What would be the fastest way to do this in Python?  Order must be maintained.  Most of the elements are already set to 0.sample variables:expected result:This is a typical job for , which is very fast for these kinds of operations:Now, if you only need the highCountX largest elements, you can even \"forget\" the small elements (instead of setting them to 0 and sorting them) and only sort the list of large elements:Of course, sorting the whole array if you only need a few elements might not be optimal.  Depending on your needs, you might want to consider the standard  module.:)Using :Where  could be:The expression  can be written without  as follows:There's a special MaskedArray class in NumPy that does exactly that.  You can \"mask\" elements based on any precondition.  This better represent your need than assigning zeroes: numpy operations will ignore masked values when appropriate (for example, finding mean value).As an addded benefit, masked arrays are well supported in matplotlib visualisation library if you need this.The simplest way would be:In pieces, this selects all the elements greater than :This array only contains the number of elements greater than the threshold. Then, sorting it so the largest values are at the start:Then a list index takes the threshold for the top  elements:Finally, the original array is filled out using another list comprehension:There is a boundary condition where there are two or more equal elements that (in your example) are 3rd highest elements. The resulting array will contain that element more than once.There are other boundary conditions as well, such as if . Handling such conditions is left to the implementor.Settings elements below some threshold to zero is easy:(plus the occasional abs() if needed.)The requirement of the N highest numbers is a bit vague, however. What if there are e.g. N+1 equal numbers above the threshold? Which one to truncate?You could sort the array first, then set the threshold to the value of the Nth element:Note: this solution is optimized for readability not performance.You can use map and lambda, it should be fast enough.Use a .This works in time .deletemin works in heap  and insertion  or  depending on which heap type you use.Using a heap is a good idea, as egon says.  But you can use the  function to cut down on some effort:"},
{"body": "I need to convert (0, 128, 64) to something like this #008040. I'm not sure what to call the latter, making searching difficult. Use the format operator :Note that it won't check bounds...This uses the preferred method of string formatting, as . It also uses  and  to ensure that . added the clamp function as suggested below. From the title of the question and the context given, it should be obvious that this expects 3 ints in [0,255] and will always return a color when passed 3 such ints. However, from the comments, this may not be obvious to everyone, so let it be explicitly stated: orpython3 is slightly differentThis is an old question but for information, I developed a package with some utilities related to colors and colormaps and contains the rgb2hex function you were looking to convert triplet into hexa value (which can be found in many other packages, e.g. matplotlib). It's on pypiand thenValidity of the inputs is checked (values must be between 0 and 255).Hello i have created a full python program for it the following functions can convert rgb to hex and vice versa.You can see the full code and tutorial at the following link : "},
{"body": "What is the simple basic explanation of what the return statement is, how to use it in Python?And what is the difference between it and the  statement?The  function writes, i.e., \"prints\", a string in the console. The  statement causes your function to exit and hand back a value to its caller. The point of functions in general is to take in inputs and return something. The  statement is used when a function is ready to return a value to its caller. For example, here's a function utilizing both  and :Now you can run code that calls foo, like so:If you run this as a script (e.g. a  file) as opposed to in the Python interpreter, you will get the following output:I hope this makes it clearer. The interpreter writes return values to the console so I can see why somebody could be confused.Here's another example from the interpreter that demonstrates that:You can see that when  is called from , 1 isn't written to the console. Instead it is used to calculate the value returned from .   is a function that causes a side effect (it writes a string in the console), but execution resumes with the next statement.  causes the function to stop executing and hand a value back to whatever called it.I think the  is your best reference here and In short:  or  to the caller of the function while  Think of the print statement as causing a , it makes your function write some text out to the user, but it can't be I'll attempt to explain this better with some examples, and a couple definitions from Wikipedia.Here is the definition of a function from WikipediaThink about that for a second. What does it mean when you say the function has a value?What it means is that you can actually substitute the value of a function with a normal value! (Assuming the two values are the same type of value)Why would you want that you ask?What about other functions that may accept the same type of value as an ?There is a fancy mathematical term for functions that only depend on their inputs to produce their outputs: Referential Transparency. Again, a definition from Wikipedia.It might be a bit hard to grasp what this means if you're just new to programming, but I think you will get it after some experimentation.\nIn general though, you can do things like print in a function, and you can also have a return statement at the end.Just remember that when you use return you are basically saying \"A call to this function is the same as writing the value that gets returned\"Python will actually insert a return value for you if you decline to put in your own, it's called \"None\", and it's a special type that simply means nothing, or null. means, \"output this value from this function\". means, \"send this value to (generally) stdout\"In the Python REPL, a function return will be output to the screen by default (this isn't quite the same as print).This is an example of print:\n\n\n\n\nThis is an example of return:\n\n\n\n\n\n\n\n\n\nIn python, we start defining a function with \"def\" and end the function with \"return\".A function of variable x is denoted as f(x). What this function does? Suppose, this function adds 2 to x. So, f(x)=x+2Now, the code of this function will be:After defining the function, you can use that for any variable and get result. Such as:We could just write the code slightly differently, such as:That would also give \"4\".Now, we can even use this code:That would also give 4. See, that the \"x\" beside return actually means (x+2), not x of \"A_function(x)\".I guess from this simple example, you would understand the meaning of return command.Just to add to @Nathan Hughes's excellent answer:The  statement can be used as a kind of control flow. By putting one (or more)  statements in the middle of a function, we can say: \"stop executing this function. We've either got what we wanted or something's gone wrong!\"Here's an example:See the  of the Python Guide for more advice on this way of using .Difference between \"return\" and \"print\" can also be found in the following example:RETURN:The above code will give correct results for all inputs.PRINT:NOTE: This will fail for many test cases.ERROR:        Here is my understanding. (hope it will help someone and it's correct). So this simple piece of code counts number of occurrences of something. The placement of return is significant. It tells your program where do you need the value. So when you print, you send output to the screen. When you return you tell the value to go somewhere. In this case you can see that count = 0 is indented with return - we want the value (count + 1) to replace 0. \nIf you try to follow logic of the code when you indent the return command further the output will always be 1, because we would never tell the initial count to change.\nI hope I got it right. \nOh, and return is always inside a function. should be used for  functions/methods or you want to use the returned value for later applications in your algorithm. should be used when you want to display a meaningful and desired output to the user and you don't want to clutter the screen with intermediate results that the user is not interested in, although they are helpful for debugging your code.The following code shows how to use  and  properly:This explanation is true for all of the programming languages not just . is part of a function definition, while  outputs text to the standard output (usually the console).A function is a procedure accepting parameters and returning a value.  is how the latter, while the former is done with .Example:Best thing about  function is you can return a value from function but you can do same with  so whats the difference ?\nBasically  not about just returning it gives output in object form so that we can save that return value from function to any variable but we can't do with  because its same like  in .We are now doing our own math functions for  and . The important thing to notice is the last line where we say return  (in ). What this does is the following:"},
{"body": "I'm writing an application in Python (2.6) that requires me to use a dictionary as a data store.I am curious as to whether or not it is more memory efficient to have one large dictionary, or to break that down into many (much) smaller dictionaries, then have an \"index\" dictionary that contains a reference to all the smaller dictionaries.I know there is a lot of overhead in general with lists and dictionaries. I read somewhere that python internally allocates enough space that the dictionary/list # of items to the power of 2.I'm new enough to python that I'm not sure if there are other unexpected internal complexities/suprises like that, that is not apparent to the average user that I should take into consideration.One of the difficulties is knowing how the power of 2 system counts \"items\"? Is each key:pair counted as 1 item? That's seems important to know because if you have a 100 item monolithic dictionary then space 100^2 items would be allocated. If you have 100 single item dictionaries (1 key:pair) then each dictionary would only be allocation 1^2 (aka no extra allocation)?Any clearly laid out information would be very helpful!Three suggestions:If you're using Python, you really shouldn't be worrying about this sort of thing in the first place.  Just build your data structure the way it best suits  needs, not the computer's.This smacks of premature optimization, not performance improvement.  Profile your code if something is actually bottlenecking, but until then, just let Python do what it does and focus on the actual programming task, and not the underlying mechanics.\"Simple\" is generally better than \"clever\", especially if you have no tested reason to go beyond \"simple\". And anyway \"Memory efficient\" is an ambiguous term, and there are tradeoffs, when you consider persisting, serializing, cacheing, swapping, and a whole bunch of other stuff that someone else has already thought through so that in most cases you don't need to.Think \"Simplest way to handle it properly\" optimize much later.Premature optimization bla bla, don't do it bla bla.I think you're mistaken about the  of two extra allocation does.  I think its just a  of two.  x*2, not x^2.I've seen this question a few times on various python mailing lists.With regards to memory, here's a paraphrased version of one such discussion (the post in question wanted to store hundreds of millions integers):So, the fewer objects you have, the less memory you're going to be using, and the fewer lookups you're going to do (since you'll have to lookup in the index, then a second lookup in the actual value).Like others, said, profile to see your bottlenecks.  Keeping an membership set() and value dict() might be faster, but you'll be using more memory.I'd also suggest reposting this to a python specific list, such as comp.lang.python, which is full of much more knowledgeable people than myself who would give you all sorts of useful information.If your dictionary is so big that it does not fit into memory, you might want to have a look at , a very mature object database for Python.The 'root' of the db has the same interface as a dictionary, and you don't need to load the whole data structure into memory at once e.g. you can iterate over only a portion of the structure by providing start and end keys.It also provides transactions and versioning.Honestly, you won't be able to tell the difference either way, in terms of either performance or memory usage.  Unless you're dealing with tens of millions of items or more, the performance or memory impact is just noise.From the way you worded your second sentence, it sounds like the one big dictionary is your first inclination, and matches more closely with the problem you're trying to solve.  If that's true, go with that.  What you'll find about Python is that the solutions that everyone considers 'right' nearly always turn out to be those that are as clear and simple as possible.Often times, dictionaries of dictionaries are useful for other than performance reasons.  ie, they allow you to store context information about the data without having extra fields on the objects themselves, and make querying subsets of the data faster.In terms of memory usage, it would stand to reason that one large dictionary will use less ram than multiple smaller ones.  Remember, if you're nesting dictionaries, each additional layer of nesting will roughly double the number of dictionaries you need to allocate.In terms of query speed, multiple dicts will take longer due to the increased number of lookups required.So I think the only way to answer this question is for you to profile your own code.  However, my suggestion is to use the method that makes your code the cleanest and easiest to maintain.  Of all the features of Python, dictionaries are probably the most heavily tweaked for optimal performance."},
{"body": "I am trying to print out Python path folders using this:The output is like this:How do I print them into separate lines so I can parse them properly?It should be like this:(The outer parentheses are included for Python 3 compatibility and are usually omitted in Python 2.)Use the print function (Python 3.x) or import it (Python 2.6+):Another good option for handling this kind of option is the  module, which (among other things) pretty prints long lists with one element per line:Sven Marnach's answer is pretty much it, but has one generality issue... It will fail if the list being printed doesn't just contain strings.So, the more general answer to \"How to print out a list with elements separated by newlines\"...Then again, the print function approach JBernardo points out is superior.  If you can, using the print function instead of the print statement is almost always a good idea."},
{"body": "I have a Spark DataFrame (using PySpark 1.5.1) and would like to add a new column.I've tried the following without any success:Also got an error using this:So how do I add a new column (based on Python vector) to an existing DataFrame with PySpark?You cannot add an arbitrary column to a  in Spark. New columns can be created only by using literals (other literal types are described in )transforming an existing column:included using :or generated with function / udf:Performance-wise, built-in functions (), which map to Catalyst expression, are usually preferred over Python user defined functions.If you want to add content of an arbitrary RDD as a column you can To add a column using a UDF:For You can define a new  when adding a :"},
{"body": "[Using Python 3.2]\nI was wondering if it was possible to remove items you have printed in python, not from the python gui but from the command propt.\ne.g.so it printsBut, My problem is I want this all on one line, and for it it remove it self when somthing else comes along.  so instead of going, \"loading\", \"loading.\", \"loa.... I want it to got \"Loading.\", then it removes what is on the line and replaces it with \"Loading..\" and then removes \"Loading..\" and replaces it (on the same line) with \"Loading...\".  Its kinda hard to discribe.p.s\nI have tried to use the Backspace charactor but it doesnt seem to work (\"\\b\")ThanksJust use CR to go to beginning of the line.One way is to use :Also sometimes useful (for example if you print something shorter than before):"},
{"body": "I am playing in Python a bit again, and I found a neat book with examples. One of the examples is to plot some data. I have a .txt file with two columns and I have the data. I plotted the data just fine, but in the exercise it says: Modify your program further to calculate and plot the running average of the data, defined by:where  in this case (and the  is the second column in the data file). Have the program plot both the original data and the running average on the same graph.So far I have this:So how do I calculate the sum? In Mathematica it's simple since it's symbolic manipulation (Sum[i, {i,0,10}] for example), but how to calculate sum in python which takes every ten points in the data and averages it, and does so until the end of points?I looked at the book, but found nothing that would explain this :\\heltonbiker's code did the trick ^^ :DAnd I got this: Thank you very much ^^ :) One common way to apply a moving/sliding average (or any other sliding window function) to a signal is by using .Here, interval is your  array, and  is the number of samples to consider. The window will be centered on each sample, so it takes samples before and after the current sample in order to calculate the average. Your code would become:Hope this helps!A moving average is a convolution, and numpy will be faster than most pure python operations. This will give you the 10 point moving average. I would also  suggest using the great pandas package if you are working with timeseries data. There are some nice . As  is pretty slow, those who need a fast performing solution might prefer an easier to understand  approach. Here is the code:where  contains your data, and  will contain moving averages of  length.On average,  is about 30-40 times faster than .This isn't the most efficient approach but it will give your answer and I'm unclear if your window is 5 points or 10. If its 10, replace each 5 with 10 and the 4 with 9.There is a problem with the accepted answer. I think we need to use  instead of  here   -  .As an Example try out the MA of this data-set =  - the result should be , but having \"same\" gives us an incorrect output of Rusty code to try this out -:Try this with valid & same and see whether the math makes sense. See also -: I think something like:But I always have to double check the indices are doing what I expect. The range you want is (0, 5, 10, ...) and data[0:6] will give you data[0]...data[5]ETA: oops, and you want ave rather than sum, of course. So actually using your code and the formula:My Moving Average function, without numpy function:"},
{"body": "I need to make an export like this in Python :I've tried to do :But when I list export, \"MY_DATA\" not appear :How I can do an export with Python without saving \"my_export\" into a file ?You actually want to do is a command that you give directly to the shell (e.g. ), to tell it to add or modify one of its environment variables. You can't change your shell's environment from a child process (such as Python), it's just not possible.Here's what's happening with you try ...When the bottom-most  subprocess finishes running your  command, then it's discarded, along with the environment that you have just changed.Not that simple:But:Another way to do this, if you're in a hurry and don't mind the hacky-aftertaste, is to execute the output of the python script in your bash environment and print out the commands to execute setting the environment in python. Not ideal but it can get the job done in a pinch. It's not very portable across shells, so YMMV.(you can also enclose the statement in backticks in some shells ``)You could try os.environ[\"MY_DATA\"] instead.os.system ('/home/user1/exportPath.ksh')exportPath.ksh:"},
{"body": "I have a pandas dataframe and I wish to divide it to 3 separate sets. I know that using  from , one can divide the data in two sets (train and test). However, I couldn't find any solution about splitting the data into three sets. Preferably, I'd like to have the indices of the original data. I know that a workaround would be to use  two times and somehow adjust the indices. But is there a more standard / built-in way to split the data into 3 sets instead of 2?Numpy solution (thanks to  for the randomizing hint) - we will split our data set into the following parts: (60% - train set, 20% - validation set, 20% - test set):PS [int(.6*len(df)), int(.8*len(df))] - is an  array for Here is a small demo for  usage - let's split 20-elements array into the following parts: 90%, 10%, 10%:Function was written to handle seeding of randomized set creation.  You should not rely on set splitting that doesn't randomize the sets.However, one approach to dividing the dataset into , ,  with , ,  would be to use the  method twice."},
{"body": "From :\"A file name like exercise_1.py is better than the name execise-1.py. We can run both programs equally well from the command line, but the name with the hyphen limits our ability to write larger and more sophisticated programs.\"Why?The issue here is that importing files with dashes in their name doesn't work since dashes are minus signs in python. So, if you had your own module you wanted to import, it couldn't have a dash in its name:Larger programs tend to be logically separated into many different modules, hence the quoteFrom that very document (p.368, Section 30.2 'Module Definition'):"},
{"body": "I've been using string.join() method in python 2 but it seems like it has been removed in python 3. What is the equivalent method in python 3? string.join() method let me combine multiple strings together with a string in between every other string. For example, string.join((\"a\", \"b\", \"c\"), \".\") would result \"a.b.c\".  or .. So any string  has the method  works fine in Python 3, you just need to get the order of the arguments correctThere are method  for string objects: Visit ab cd efab.cd.ef"},
{"body": "I have a program that writes a list to a file. \nThe list is a list of pipe delimited lines and the lines should be written to the file like this:BUT it wrote them line this ahhhh:This program wrote all the lines into like one line without any line breaks.. This hurts me a lot and I gotta figure-out how to reverse this but anyway, where is my program wrong here? I thought write lines should write lines down the file rather than just write everything to one line..This is actually a pretty common problem for newcomers to Python\u2014especially since, across the standard library and popular third-party libraries, some reading functions strip out newlines, but almost no writing functions (except the -related stuff) add them.So, there's a lot of Python code out there that does things like:orEither one is correct, and of course you could even write your own writelinesWithNewlines function that wraps it up\u2026But you should only do this if you can't avoid it.It's better if you can create/keep the newlines in the first place\u2014as in Greg Hewgill's suggestions:And it's even better if you can work at a higher level than raw lines of text, e.g., by using the  module in the standard library, as esuaro suggests. For example, right after defining , you might do this:Then, instead of this:You do this:And at the end, instead of this:You do this:Finally, your design is \"open a file, then build up a list of lines to add to the file, then write them all at once\". If you're going to open the file up top, why not just write the lines one by one? Whether you're using simple writes or a , it'll make your life simpler, and your code easier to read. (Sometimes there can be simplicity, efficiency, or correctness reasons to write a file all at once\u2014but once you've moved the  all the way to the opposite end of the program from the , you've pretty much lost any benefits of all-at-once.)The  states:So you'll need to add them yourself. For example:whenever you append a new item to . does not add line separators. You can alter the list of strings by using  to add a new  (line break) at the end of each string.As others have mentioned, and counter to what the method name would imply,  does not add line separators. This is a textbook case for a generator. Here is a contrived example:Benefits: adds newlines explicitly without modifying the input or output values or doing any messy string concatenation. And, critically, does not create any new data structures in memory. IO (writing to a file) is when that kind of thing tends to actually matter. Hope this helps someone!I kept ending up on this thread when looking to solve a similar problem when writing to files in R. So for those who have had the same issue, here is my solution:"},
{"body": "Is there an easy way to export a data frame (or even a part of it) to LaTeX?  DataFrames have a  method:You can simply write this to a tex file.By default latex will render this as:Just write to a textfile. It's no magic:"},
{"body": "say df is a pandas DataFrame.\nI would like to find all columns of numeric type.\nsomething like:You could use  method of DataFrame. It includes two parameters include and exclude. So isNumeric would look like:You can use the following command to filter only numeric columnsExampleAdapting , you could doHere,  shows whether every cell in the data frame is numeric, and  checks if all values in a column are True and returns a series of Booleans that can be used to index the desired columns."},
{"body": "Is there a way to map a signal number (e.g. signal.SIGINT) to its respective name (i.e. \"SIGINT\")?I'd like to be able to print the name of a signal in the log when I receive it, however I cannot find a map from signal numbers to names in Python, i.e.:For some dictionary sig_names, so when the process receives SIGINT it prints:There is none, but if you don't mind a little hack, you can generate it like this:With the addition of the   in Python 3.5 this is now as easy as:The Python Standard Library By Example shows this function in the chapter on signals:You can then use it like this:I found this article when I was in the same situation and figured the handler is only handling one signal at a time, so I don't even need a whole dictionary, just the name of one signal:there's probably a notation that doesn't need the tuple(...)[0] bit, but I can't seem to figure it out.Well,  says at the bottom:So this should work:Building on :"},
{"body": "I have a simple image that I'm showing with imshow in matplotlib. I'd like to apply a custom colormap so that values between 0-5 are white, 5-10 are red (very simple colors), etc.  I've tried following this tutorial:  with the following code:But this ends up showing strange colors, and I only need 3-4 colors that I want to define.  How do I do this?You can use a  to specify the white and red as the only colors in the color map, and the bounds determine where the transition is from one color to the next:The resulting figure has only two colors:I proposed essentially the same thing for a somewhat different question: The solution is inspired by a . The example explains that the  must be one more than the number of colors used. The  is a normalization that maps a series of values to integers, which are then used to assign the corresponding colors. , in the example above, just defines the number of colors. for why  shows strange color, I think this link would be helpful."},
{"body": "I have several  files that look like this:I would like to add a new column to all CSV files so that it would look like this:The script I have so far is this:(Python 3.2)But in the output, the script skips every line and the new column has only Berry in it: This should give you an idea of what to do:Edit, note in py3k you must use Thanks for accepting the answer. Here you have a bonus (your working script):Please note I'm surprised no one suggested Pandas. Although using a set of dependencies like Pandas might seem more heavy-handed than is necessary for such an easy task, it produces a very short script and Pandas is a great library for doing all sorts of CSV (and really all data types) data manipulation. Can't argue with 4 lines of code:Check out  for more information!Contents of :Maybe something like that is what you intended?Also, csv stands for comma separated values.  So, you kind of need commas to separate your values like this I think:I don't see where you're adding the new column, but try this:"},
{"body": "Is it possible to add some meta-information/metadata to a pandas DataFrame?For example, the instrument's name used to measure the data, the instrument responsible, etc.Sure, like most Python objects, you can attach new attributes to a :Note, however, that while you can attach attributes to a DataFrame, operations performed on the DataFrame (such as , ,  or  to name just a few) may return a new DataFrame  the metadata attached. Pandas does not yet have a robust method of  .Preserving the metadata  is possible. You can find an example of how to store metadata in an HDF5 file .Not really. Although you could add attributes containing metadata to the DataFrame class as @unutbu mentions, many DataFrame methods return a new DataFrame, so your meta data would be lost. If you need to manipulate your dataframe, then the best option would be to wrap your metadata and DataFrame in another class. See this discussion on GitHub: There is currently an open  to add a MetaDataFrame object, which would support metadata better. Just ran into this issue myself. As of pandas 0.13, DataFrames have a _metadata attribute on them that does persist through functions that return new DataFrames. Also seems to survive serialization just fine (I've only tried json, but I imagine hdf is covered as well).Coming pretty late to this, I thought this might be helpful if you need metadata to persist over I/O. There's a relatively new package called  that I've been using to accomplish this.It should let you do a quick read/write from HDF5 for a few common formats, one of them being a dataframe. So you can, for example, put a dataframe in a dictionary and include metadata as fields in the dictionary. E.g.:Another option would be to look into a project like , which is more complex in some ways, but I think it does let you use metadata and is pretty easy to convert to a DataFrame.As mentioned in other answers and comments,  is not a part of public API, so it's definitely not a good idea to use it in a production environment. But you still may want to use it in a research prototyping and replace it if it stops working. And right now it works with /, which is helpful. This is an example (which I couldn't find in other answers):Output:"},
{"body": "I have a pandas dataframe with the following columns;How do I combine data['Date'] & data['Time']  to get the following? Is there a way of doing it using ?It's worth mentioning that you may have been able to read this in  e.g. if you were using  using .Assuming these are just strings you could simply add them together (with a space), allowing you to apply :The accepted answer works for columns that are of datatype . For completeness: I come across this question when searching how to do this when the columns are of datatypes: date and time. I don't have enough reputation to comment on  so:I had to amend  for it to work:This might help others.Also, I have tested a different approach, using  instead of :which in the OP's case would be:I have timed both approaches for a relatively large dataset (>500.000 rows), and they both have similar runtimes, but using  is faster (59s for  vs 50s for )."},
{"body": "Suppose I have code that maintains a parent/children structure. In such a structure I get circular references, where a child points to a parent and a parent points to a child. Should I worry about them? I'm using Python 2.5.I am concerned that they will not be garbage collected and the application will eventually consume all memory.\"Worry\" is misplaced, but if your program turns out to be slow, consume more memory than expected, or have strange inexplicable pauses, the cause is indeed likely to be in those garbage reference loops -- they need to be garbage collected by a different procedure than \"normal\" (acyclic) reference graphs, and that collection is occasional and may be slow if you have a lot of objects tied up in such loops (the cyclical-garbage collection is also inhibited if an object in the loop has a  special method).So, reference loops will not affect your program's correctness, but may affect its performance and/or footprint.If and when you want to remove unwanted loops of references, you can often use the  module in Python's standard library.If and when you want to exert more direct control (or perform debugging, see what exactly is happening) regarding cyclical garbage collection, use the  module in Python's standard library.Experimentally: you're fine:It consistently stays at using 3.6\u00a0MB of RAM.Python will detect the cycle and release the memory when there are no outside references.Circular references are a normal thing to do, so I don't see a reason to be worried about them.  Many tree algorithms require that each node have links to its children and its parent.  They're also required to implement something like a doubly linked list.I don't think you should worry. Try the following program and will you see that it won't consume all memory:There seems to be a issue with references to methods in lists in a variable. Here are two examples. The first one does not call . The second one with weakref is ok for . However, in this later case the problem is that you cannot weakly reference methods: "},
{"body": "Say I have a python file in directory  like this:Under directory  I have a few folders I want to access but if file.py is executed from anywhere else rather than from folder  the relative path won't work for me. Also folder  could be located anywhere but always with the a set of sub folders so absolute path will not work.First, is there any function to get the absolute path in relation to the source files location?If not, any ideas how to sort this out? Grab the command line used and add the CWD together?My problem here is that this folder are being installed on 20 different machines and OS's and I want it to be as dynamic as possible with little configurations and \"rules\" where it has to be installed etc.Here is my solution which (a) gets the .py file rather than the .pyc file, and (b) sorts out symlinks.Working on Linux, the  files are often symlinked to another place, and the  files are generated in the directory next to the symlinked py files. To find the  path of the source file, here's part of a script that I use to find the source path."},
{"body": "I have a system (developed in Python) that accepts  and i have to   them..Currently datetime string formats are : Now i want a  that can convert any of these datetime formats in appropriate datetime object...Otherwise, i have to go with parsing them individually. So please also provide method for parsing them individually (if there is no generic parser)..!!As @TimPietzcker suggested, the dateutil package is the way to go, it handles the first 3 formats correctly and automatically:The unixtime format it seems to hick up on, but luckily the standard  is up for the task:It is rather easy to make a function out of this that handles all 4 formats:You should look into the  package."},
{"body": "I'm writing a Python script that needs to write some data to a temporary file, then create a subprocess running a C++ program that will read the temporary file.  I'm trying to use  for this, but according to the docs,And indeed, on Windows if I flush the temporary file after writing, but don't close it until I want it to go away, the subprocess isn't able to open it for reading.I'm working around this by creating the file with , closing it before spawning the subprocess, and then manually deleting it once I'm done:This seems inelegant.  Is there a better way to do this?  Perhaps a way to open up the permissions on the temporary file so the subprocess can get at it?At least if you open a temporary file using existing Python libraries, accessing it from multiple processes is not possible in case of Windows. According to  you can specify a 3rd parameter () shared mode flag  to  function which:So, you can write a Windows specific C routine to create a custom temporary file opener function, call it from Python and then you can make your sub-process access the file without any error. But I think you should stick with your existing approach as it is the most portable version and will work on any system and thus is the most elegant implementation.EDIT: Turns out it is possible to open & read the temporary file from multiple processes in Windows too. See Piotr Dobrogost's .Since nobody else appears to be interested in leaving this information out in the open... does expose a function, , which can trivialize this problem:I leave the implementation of the intermediate functions up to the reader, as one might wish to do things like use  to tighten up the security of the temporary file itself, or overwrite the file in-place before removing it. I don't particularly know what security restrictions one might have that are not easily planned for by perusing the source of .Anyway, yes, using  on Windows might be inelegant, and my solution here might also be inelegant, but you've already decided that Windows support is more important than elegant code, so you might as well go ahead and do something readable. to Richard Oudkerk and he gives an example of how to do this in Python 3.3+Because there's no  parameter in the built-in  in Python 2.x, we have to combine lower level  and  functions to achieve the same effect:You can always go low-level, though am not sure if it's clean enough for you:"},
{"body": "My work recently involves programmatically making videos. In python, the typical workflow looks something like this:This workflow creates an image for each frame in the video and saves it to disk. After all images have been saved, ffmpeg is called to construct a video from all of the images.Saving the images to disk (not the creation of the images in memory) consumes the majority of the cycles here, and does not appear to be necessary. Is there some way to perform the same function, but without saving the images to disk? So, ffmpeg would be called and the images would be constructed and fed to ffmpeg immediately after being constructed.Ok I got it working. thanks to LordNeckbeard suggestion to use image2pipe. I had to use jpg encoding instead of . The first script is essentially the same as your question's code except I implemented a simple image creation that just creates images going from black to red. I also added some code to time the execution.the results are interesting, I ran each script 3 times to compare performance:\n So it seems the parallel version is faster about 1.5 times faster."},
{"body": "I have functions that contribute to small parts of a figure generation. I'm trying to use these functions to generate multiple figures? So something like this:If anyone could help, that'd be great!There are several ways to do this, and the simplest is to use the figure numbers.   The code below makes two figures, #0 and #1, each with two lines.  #0 has the points 1,2,3,4,5,6, and #2 has the points 10,20,30,40,50,60.For a more general answer to this question and to questions you may soon have, I would recommend the .The best way to show multiple figures is use matplotlib or pylab. (for windows)\nwith matplotlib you can prepare the figures and then when you finish the process with them you can show with the comand \"matplotlib.show()\" and all figures should be shown.(on linux) you don\u00b4t have problems adding changes to figures because the interactive mode is enable (on windows the interactive mode don't work OK). "},
{"body": "Python has an identifier  that allows for storing the result of the  which makes it great for speeding up data exploration and introspection.Is there a similar command in R?Tis a faff to type, but .Last.value:"},
{"body": "Which is it better to use generally?Is there any advantage to writing:over:Or is it a direct alias? Is there any difference? Which is more pythonic/django-nic?There is a difference between the two: In the case of  the first argument can only be a .  which will  return a  can accept a , , or  as it's \"to\" argument. So it is a little more flexible in what it can \"redirect\" to. I also like how  is shorter. So I'd use  over . Both are fine to use though.From documentation - from the definition its the same. Thats what shortcuts are for. Both are one and the same.shortcuts generally are written one level above the actual API's. So  encapsulates  and  with the arg . There is no major downside to using  over . Hope this clears it."},
{"body": "I have developed a python C-extension that receives data from python and compute some cpu intensive calculations.\nIt's possible to profile the C-extension?The problem here is that writing a sample test in C to be profiled would be challenging because the code rely on particular inputs and data structures (generated by python control code).Do you have any suggestions?After the comment by pygabriel I decided to upload a package to pypi that implements a profiler for python extensions using the cpu-profiler from google-perftools: I've found my way using . The trick was to wrap the functions StartProfiler and StopProfiler in python (throught cython in my case).To profile the C extension is sufficient to wrap the python code inside the StartProfiler and StopProfiler calls.Then to analyze for example you can export in callgrind format and see the result in kcachegrind:With , you can profile any program that was  and linked ( etc, in 's case).  If you're using a Python version not built with  (e.g., the Windows precompiled version the PSF distributes), you'll need to research what equivalent tools exist for that platform and toolchain (in the Windows PSF case, maybe  can help).  There may be \"irrelevant\" data there (internal C functions in the Python runtime), and, if so, the percentages shown by  may not be applicable -- but the absolute numbers (of calls, and durations thereof) are still valid, and you can post-process 's output (e.g., with a little Python script;-) to exclude the irrelevant data and compute the percentages you want."},
{"body": "I'm trying to create a super-simplistic Virtual In / Out Board using wx/Python.  I've got the following code in place for one of my requests to the server where I'll be storing the data:Nothing special going on there.  The problem I'm having is that, based on how I read the docs, this should perform a Post Request because I've provided the data parameter and that's not happening.  I have this code in the index for that url:And every time I run my Python App I get the 'No action specified' text printed to my console.  I'm going to try to implement it using the Request Objects as I've seen a few demos that include those, but I'm wondering if anyone can help me explain why I don't get a Post Request with this code.  Thanks!-- EDITED --This code does work and Posts to my web page properly:I am still unsure why the urllib2 library doesn't Post when I provide the data parameter - to me the docs indicate that it should.Using the path  without a trailing  doesn't fetch . Instead the server will issue a  redirect to the version with the trailing .Doing a 302 will typically cause clients to convert a POST to a GET request."},
{"body": "Both list comprehensions and map-calculations should -- at least in theory -- be relatively easy to parallelize: each calculation inside a list-comprehension could be done independent of the calculation of all the other elements. For example in the expressioneach x*x-Calculation could (at least in theory) be done in parallel.My question is: Is there any Python-Module / Python-Implementation / Python Programming-Trick to parallelize a list-comprehension calculation (in order to use all 16 / 32 / ... cores or distribute the calculation over a Computer-Grid or over a Cloud)?As Ken said, no, it can't, but with 2.6's  module, it's pretty easy to parallelize computations.There are also examples in the  that shows how to do this using Managers, which should allow for distributed computations as well. Using the  and  functions from the new 3.2  package could help.It's also available as a 2.6+ .IMHO,  automatic parallisation of list comprehension would be impossible without additional information (such as those provided using directives in OpenMP), or limiting it to expressions that involve only built-in types/methods.Unless there is a guarantee that the processing done on each list item has no side effects, there is a possibility that the results will be invalid (or at least different) if done out of order.There is also the issue of task distribution. How should the problem space be decomposed? If the processing of each element forms a task (~ task farm), then when there are many elements each involving trivial calculation, the overheads of managing the tasks will swamps out the performance gains of parallelisation. One could also take the data decomposition approach where the problem space is divided equally among the available processes. The fact that list comprehension also works with generators makes this slightly tricky, however this is probably not a show stopper if the overheads of pre-iterating it is acceptable. Of course, there is also a possibility of generators with side-effects which can change the outcome if subsequent items are prematurely iterated. Very unlikely, but possible.A bigger concern would be load imbalance across processes. There is no guarantee that each element would take the same amount of time to process, so statically partitioned data may result in one process doing most of the work while the idle your time away. Breaking the list down to smaller chunks and handing them as each child process is available is a good compromise, however, a good selection of chunk size would be application dependent hence not doable without more information from the user.As mentioned in several other answers, there are many approaches and  to choose from depending on one requirements.Having used only MPI (in C) with no experience using Python for parallel processing, I am not in a position to vouch for any (although, upon a quick scan through,\n, ,  and  stand out).If a requirement is to stick as close as possible to list comprehension, then  seems to be the closest match. From the , distributing tasks across multiple instances can be as simple as:While that does something similar to ,  can use different backends for synchronising process and storing intermediate results (redis, filesystem, in-memory) which means the processes can span across nodes in a cluster.No, because list comprehension itself is a sort of a C-optimized macro. If you pull it out and parallelize it, then it's not a list comprehension, it's just a good old fashioned .But you can easily parallelize your example. Here's a good tutorial on using MapReduce with Python's parallelization library:For shared-memory parallelism, I recommend :Not within a list comprehension AFAIK.You could certainly do it with a traditional for loop and the multiprocessing/threading modules.There is a comprehensive list of parallel packages for Python here:I'm not sure if any handle the splitting of a list comprehension construct directly, but it should be trivial to formulate the same problem in a non-list comprehension way that can be easily forked to a number of different processors. I'm not familiar with cloud computing parallelization, but I've had some success with mpi4py on multi-core machines and over clusters. The biggest issue that you'll have to think about is whether the communication overhead is going to kill any gains you get from parallelizing the problem.  The following might also be of interest:"},
{"body": "I need to create a Python REST/JSON web service for an iOS app to interact with. There will be no front end on the web.What will be the fastest, most lightweight framework to use for this? Learning curve to implement also considered?From the research I've done Django-Tastypie or Djanjo-Piston look like the best options, with Tastypie winning because the codebase is being maintained actively?In general, I think you'll find  to be one of the easiest frameworks to set up, learn, and use. web2py makes it very easy to  (just add a .json extension), and it now includes new functionality to automatically create  to access database models. In particular, check out the  and  functionality.If you need any help, ask on the .At Pycon Australia, Richard Jones compared the most popular lightweight web frameworks.   came out on top.  Here is the .When it comes to lightweight,  is pretty up there.If I were you I would use  that is really convenient to do that kind of rapid prototyping of .\nCheck out this snippet from the home page:Take a look to  and its extension You might also want to check out . They are free to use right now, and will give you a nice  for you mobile apps. However, as @iksnar points out, you don't write anything in Python, or anything at all for the backend. If you need to have the backend running in Python on your own servers I am a big fan of TastyPie if you are using Django already and the Django ORM already. "},
{"body": "I am trying to assign a new value to a tensorflow variable in python.But the output I get isSo the value has not changed. What am I missing?The statement  does not actually assign the value  to , but rather creates a  that you have to explicitly  to update the variable.* A call to  or  can be used to run the operation:(* In fact, it returns a , corresponding to the updated value of the variable, to make it easier to chain assignments.)First of all you can assign values to variables/constants just by feeding values into them the same way you do it with placeholders. So this is perfectly legal to do:Regarding your confusion with the  operator. In TF nothing is executed before you run it inside of the session. So you always have to do something like this:  and then inside of the session you run . Using assign as an example you will do something like this:There is an easier approach: "},
{"body": "This is the code that's giving me trouble. If I comment out the lines with the Label, the Frame displays with the right width.  However, adding the Label seems to shrink the Frame down to the Label's size.  Is there a way to prevent that from happening? By default, tk frames , which is what you want 99% of the time. The term that describes this feature is \"geometry propagation\". There is a  to turn geometry propagation on or off.Since you are using pack, the syntax would be:or maybe , depending on which widget(s) you actually want to affect.That being said, the vast majority of the time you should let tkinter compute the size. When you turn geometry propagation off your GUI won't respond well to changes in resolution, changes in fonts, etc. tkinter's geometry managers (,  and ) are remarkably powerful. Learn to take advantage of that power. "},
{"body": "Is there any way to set the directory where you want to start a SimpleHTTPServer or BaseHTTPServer?If you're using  directly from command line, you can simply use shell features:"},
{"body": "Is it possible to define a recursive list comprehension in Python?Possibly a simplistic example, but something along the lines of:Is anything like this possible?No, there's no (documented, solid, stable, ...;-) way to refer to \"the current comprehension\".  You could just use a loop:of course this is very costly (O(N squared)), so you can optimize it with an auxiliary  (I'm assuming that keeping the order of items in  congruent to that of the items in , otherwise  would do you;-)...:this is enormously faster for very long lists (O(N) instead of N squared).: in Python 2.5 or 2.6,  might actually work in the role you want for  (for a non-nested listcomp)... which is why I qualified my statement by clarifying there's no  way to access \"the list being built up\" -- that peculiar, undocumented \"name\"  (deliberately chosen not to be a valid identifier;-) is the apex of \"implementation artifacts\" and any code relying on it deserves to be put out of its misery;-).Actually you can! This example with an explanation hopefully will illustrate how. define recursive example to get a number only when it is 5 or more and if it isn't, increment it and call the 'check' function again. Repeat this process until it reaches 5 at which point return 5.result:essentially the two anonymous functions interact in this way:make g,f the 'same' function except that in one or both add a clause where the parameter is modified so as to cause the terminal condition to be reached and then go\nf(g,x) in this way g becomes a copy of f making it like:You need to do this because you can't access the the anonymous function itself upon being executed. i.e  so in this example let A = the first function and B the second. We call A passing B as f and i as v. Now as B is essentially a copy of A and it's a parameter that has been passed you can now call B which is like calling A. This generates the factorials in a listno. it won't work, there is no  to refer to while list comprehension is being executed.And the main reason of course is that list comprehensions where not designed for this use.Do this:or even this:Not sure if this is what you want, but you can write nested list comprehensions:From your code example, you seem to want to simply eliminate duplicates, which you can do with sets:No.But it looks like you are trying to make a list of the unique elements in nums.You could use a :Note that items in nums need to be hashable.You can also do the following. Which is a close as I can get to your original idea. But this is not as efficient as creating a ."},
{"body": "Consider the following code:This will raise . I expected it to raise  (need I explain why?). Why does the final  raise the original exception rather than (what I thought) was the last exception raised?On python2.6I guess, you are expecting the finally block to be tied with the \"try\" block where you raise the exception \"B\". The finally block is attached to the first \"try\" block.If you added an except block in the inner try block, then the finally block will raise exception B.Output:Another variation that explains whats happening hereOutput:If you see here, replacing the finally block with except does raise the exception B.(reposted from comments for clarity)"},
{"body": "How can I reverse the results of a ? That is, how can I obtain a quoted string that would , given a  of strings I wish quoted?I've located a Python bug, and made corresponding feature requests .We now (3.3) have a  function.  It\u2019s none other that  moved and documented (code using  will still work).  See  for the whole discussion. is a private function that should not be used.  It could however be moved to  and made officially public.  See also .How about using ?. uses .  It's not an official public API, but it's mentioned in the  documentation and I think it's pretty safe to use.  It's more sophisticated than  (for better or worse).Both  and  are transformed by  to the same list:So I don't think it is possible to reverse  in all cases, but  might get you close:"},
{"body": "I have a Python application which opens a simple TCP socket to communicate with another Python application on a separate host. Sometimes the program will either error or I will directly kill it, and in either case the socket may be left open for some unknown time.The next time I go to run the program I get this error:Now the program always tries to use the same port, so it appears as though it is still open. I checked and am quite sure the program isn't running in the background and yet my address is still in use.SO, how can I manually (or otherwise) close a socket/address so that my program can immediately re-use it?Based on Mike's answer I checked out the  page and looked at SO_REUSEADDR:Assume your socket is named ... you need to set  on the server's socket before binding to an interface...  this will allow you to immediately restart a TCP server...You might want to try using Twisted for your networking.  Mike gave the correct low-level answer, , but he didn't mention that this isn't a very good option to set on Windows.  This is the sort of thing that Twisted takes care of for you automatically.  There are many, many other examples of this kind of boring low-level detail that you have to pay attention to when using the socket module directly but which you can forget about if you use a higher level library like Twisted.You are confusing sockets, connections, and ports. Sockets are endpoints of connections, which in turn are 5-tuples {protocol, local-ip, local-port, remote-ip, remote-port}. The killed program's socket has been closed by the OS, and ditto the connection. The only relic of the connection is the peer's socket and the corresponding port at the peer host. So what you should really be asking about is how to reuse the local  To which the answer is SO_REUSEADDR as per the other answers."},
{"body": "I'm using  to manage asynchronous tasks. Occasionally, however, the celery process goes down which causes none of the tasks to get executed. I would like to be able to check the status of celery and make sure everything is working fine, and if I detect any problems display an error message to the user. From the Celery Worker documentation it looks like I might be able to use  or  for this, but ping feels hacky and it's not clear exactly how inspect is meant to be used (if inspect().registered() is empty?).Any guidance on this would be appreciated. Basically what I'm looking for is a method like so:EDIT: It doesn't even look like registered() is available on celery 2.3.3 (even though the 2.1 docs list it). Maybe ping is the right answer.EDIT: Ping also doesn't appear to do what I thought it would do, so still not sure the answer here.Here's the code I've been using.   returns a dict containing lots of details about the currently available workers, None if there are no workers running, or raises an  if it can't connect to the message broker.  I'm using RabbitMQ - it's possible that other messaging systems might behave slightly differently.  This worked in Celery 2.3.x and 2.4.x; I'm not sure how far back it goes.To check the same using command line in case celery is running as daemon,Source:\nThe following worked for me:"},
{"body": "I want output from execute Test_Pipe.py, I tried following code on Linux but it did not work. The line  was blocked, so no data prints out.You obviously can use subprocess.communicate but I think you are looking for real time input and output.readline was blocked because the process is probably waiting on your input. You can read character by character to overcome this like the following:Nadia's snippet does work but calling read with a 1 byte buffer is highly unrecommended. The better way to do this would be to set the stdout file descriptor to nonblocking using fcntland then using select to test if the data is readyShe was correct in that your problem must be different from what you posted as Caller.py and Test_Pipe.py do work as provided.To avoid the many problems that can always arise with buffering for tasks such as \"getting the subprocess's output to the main process in real time\", I always recommend using  for all non-Windows platform,  on Windows, instead of , when such tasks are desired. buffers its stdout by default so  in  don't see any output until the child's buffer is full (if the buffer size is 8KB then it takes around a minute to fill 's stdout buffer).To make the output unbuffered (line-buffered for text streams) you could pass  to the child Python script. It allows to read subprocess' output line by line in \"real-time\":See links in  on how to solve the block-buffering issue for non-Python child processes."},
{"body": "I use the following method to break the double loop in Python.Is there a better way to break the double loop?Probably not what you are hoping for, but usually you would want to have a  after setting  to Another way is to use a generator expression to squash the  into a single loopYou may also consider using The recommended way in Python for breaking nested loops is... ExceptionMost times you can use a number of methods to make a single loop that does the same thing as a double loop.In your example, you can use  to replace your code snippet withThe other itertools functions are good for other patterns too.Refactor using functions so you can return when you find your \"bingo\".The proposal to allow explicit breaking out of nested loops has been rejected:\n"},
{"body": "I went through this example here:All my tasks are in files called tasks.py.After updating celery and adding the file from the example django is throwing the following error, no matter what I try:Is the problem possibly caused by the following?Because it goes through all tasks.py files which all have the following import.::The error happens, when I try to access a url that is not valid, like /foobar.:Adding the following lines to cloud/celery.py:gave me the file itself and not the celery module from the library. After renaming celery.py to celeryapp.py and adjusting the imports all errors were gone.Note:That leads to a change in starting the worker:For those running celery==3.1.2 and getting this error:Apply the patch mentioned here: With Django 1.7.5, Celery 3.1.17, and Python 2.7.6 I found that I was still getting these . But only when running tests under PyCharm 4.0.4.I found that a solution was  to rely on  as described in . Instead I renamed  to  and then changed the content of  to match: . No more multiple instances of files named  to cause import confusion seemed to be a simpler approach. Work for me ( some bug after deploy in server ):\nRemove all *.pyc files from project and restart him. :\nI have meet this problem just now, then I found the problem --- sys.path.\nMaybe you add some path to sys.path like me, I add below code in manage.py,so,  would search celery in  and  first, that's the problem.change  to   It would search in python's  and  first. Solved perfectly.Did you add the line:to the top of your  module?Read the breakdown of the example here:\nNote that older Django projects have the  script in the same directory as the project directory. That is, the structure looks like this:instead of this:In this case, you will just have to rename the  file to something different, like  as suggested in the accepted answer above.I got the same error. It turns out there was a problem with my Celery version. I upgraded to 3.1 and  is now deprecated for this version (). So I had to downgrade to the version 3.0.19 that was the previous stable version used for the project, and it works good so far. Anyway, if you don't want to downgrade, the replacement for celeryd in the version 3.1 is . Check here for more info: .Hope this helps! :)I got the same error.Seems that  DOES NOT work for Python 2.6.1, still not raising an error.Upgraded to Python 2.7.5 and it just worked."},
{"body": "I want to add multiple values to a specific key. How can I do that?Make the value a list, e.g.UPDATE:There are a couple of ways to add values to key, and to create a list if one isn't already there. I'll show one such method in little steps.Results:Next, try:Results:The magic of  is that it initializes the value for that key  that key is not defined, otherwise it does nothing. Now, noting that  returns the key you can combine these into a single line:Results:You should look at the  methods, in particular the  method, and do some experiments to get comfortable with this.How aboutThis will result in:Is that what you were looking for?"},
{"body": "I wonder if it is bad manner to skip , when it is not needed.Example:In both cases, when condition is false, function will return with .Like you said,  is (almost) never needed.But you should consider that the  of your code is much clearer with an explicit . Remember: a piece of code also needs to be readable by human-beings, and being explicit usually helps.To expound on what others have said, I use a  if the function is supposed to return a value.  In Python, all functions return a value, but often we write functions that only ever return None, because their return value is ignored.  In some languages, these would be called procedures.So if a function is supposed to return a value, then I make sure all code paths have a return, and that the return has a value, even if it is None.If a function \"doesn't\" return a value, that is, if it is never called by someone using its return value, then it's ok to end without a return, and if I need to return early, I use the bare form, .Yes and No. \"return None\" because it returns None in only single negative condition. But if there are nested condition evaluation and multiple scenarios where a function could return None. I tend to include them as .[Editing: Based on comment below] as It is explicit and later, no one will be in doubt if the return meant returning None or was it an error as something was missing.Yes, if you do not return any value from a Python function, it returns None. So, whether to explicitly return None is a stylistic decision.Personally, I prefer to always return a value for clarity.or I do not believe in half defined function, but this False can be usefull in search type failure pruning.The more I think about it, the less I think the case you describe shows good practice. It forces the client to discriminate, so client code would almost always look like:You couldn't even write:since, if  is overwritten, b could evaluate to False, even if it's not None. It would be better to have a  instance (AKA ), e.g.:The point is: help the client (who might be yourself!) to keep  low. The fewer branches, the better. "},
{"body": "I've just recently started using SQLAlchemy and am still having trouble wrapping my head around some of the concepts.Boiled down to the essential elements, I have two tables like this (this is through Flask-SQLAlchemy):How would I go about querying for a list of users and their newest post (excluding users with no posts). If I was using SQL, I would do:So I know exactly the \"desired\" SQL to get the effect I want, but no idea how to express it \"properly\" in SQLAlchemy.Edit: in case it's important, I'm on SQLAlchemy 0.6.6.This should work (different SQL, same result):the previous answer works, but also the exact sql you asked for is written much as the actual statement:I guess the \"concept\" that isn't necessarily apparent is that as_scalar() is currently needed to establish a subquery as a \"scalar\" (it should probably assume that from the context against ==).Edit:  Confirmed, that's buggy behavior, completed ticket #2190.  In the current tip or release 0.7.2, the as_scalar() is called automatically and the above query can be:"},
{"body": "For simplicity this is a stripped down version of what I want to do:I know how to do this in PHP:Any way to do this?If it's a global variable, then you can do:A note about the various \"eval\" solutions: you should be careful with eval, especially if the string you're evaluating comes from a potentially untrusted source -- otherwise, you might end up deleting the entire contents of your disk or something like that if you're given a malicious string.(If it's not global, then you'll need access to whatever namespace it's defined in.  If you don't have that, there's no way you'll be able to access it.)Edward Loper's answer only works if the variable is in the current module. To get a value in another module, you can use :tada!"},
{"body": "In Matplotlib, I would like to draw a  (or a cross), but the one provided in the  is too .Even as I increase its size, it doesn't get any thicker.For :\n\nThe  drawing the red plus sign are:If I add an extra parameter , the marker will only stretch. It will be as thin as before. Can I make it thick?You can use  (or ). You'll want to combine it with , otherwise you get thick but tiny markers. For example:Use  in connection with ."},
{"body": "I have Pythong2.6, psycopg2 and pgAdmin3 installed using Macports. My settings.py is:The error I get when I run python manage.py syncdb is:Please note, I am a complete beginner in this stuff. I am originally a PHP-guy and trying out Python for a small personal project. Do I need to \"turn on\" Postgres?Also, when I  sudo python manage.py runserver 8080\nI get this error:Please guide me. Any reply will be appreciated.Thanks,Wenbert!There seems to be a problem with your  installation \u2013 Python does not find it. This is a Python installation problem, not a Django issue.You can try to load it manually using the Python interpreter and see if it works:If you get an  exception, your installation is erroneous. To get a list of all directories Python looks for modules, use :You can also add custom directories to Python's module search path by modifying the  variable. Do this somewhere before the respective  statement(s):If you have  installed, simply install the missing extension by running:For the record I got the same error for a different reason:I had putinstead ofin Although you installed it, Python can apparently not find the module psycopg2. This is usually due to the module not being in Python's path. See if you can find a folder named psycopg2 in . If it's not there, did MacPorts tell you where it put psycopg2? If you can locate it, just move it to the  directory and you should be fine.I realized I didn't have psycopg2 installedWorked like a charmHere's the solution: \nYou've to install python-dev and libpq-dev libraries before executing \"easy_install psycopg2\"Yes Tim's answer works for me too.It works without prefix 'django.db.backends.' also. But remember to create database or schema you mentioned in settings.py:manually using your database client so when you run 'python manage.py syncdb' you don't get the same problem. I was stuck because i didn't create it manually. I got the same problem may be because I used buildout. I got the same error, but it was because I was using  when my virtualenv only had  and  executables (thus the system  was being used, which didn't have  installedTim's answer worked for me too. \nBy default, settings.py shows options like 'postgresql_psycopg2', 'mysql', etc, without a package name. Prefixing with 'django.db.backends.' worked for me (for postgresql_psycopg2, at least).For me, psycopg2 was indeed installed, but not into the virtualenv in which Django was running. These two steps fixed it:I had this issue recently after updating homebrew on OSX.\npsycopg2 was already listed in my virtualenv\nI just reinstalled psycopg2 and it worked againI'm on Windows and had installed psycopg2, but the 64 bit version. So my fix was to download the 32 bit one from  then in PowerShell with my virtual environment activated, my fix was: (The Windows Python 3.4 installer automatically installs easy_install and pip, and pip is the easiest way to remove the package, even if originally installed using easy_install.)THIS HAS HELPED ME:I just added PostgreSQL bin path to ENV and it was able to fine the needed dll:\nC:\\Program Files (x86)\\PostgreSQL\\9.4\\bin"},
{"body": "I am using py.test for unit testing my python program. I wish to debug my test code with the python debugger the normal way (by which i mean pdb.set_trace() in the code) but I can't make it work. Putting pdb.set_trace() in the code doesn't work (raises IOError: reading from stdin while output is captured). I have also tried running py.test with the option --pdb but that doesn't seem to do the trick if I want to explore what happens before my assertion. It breaks when an assertion fails, and moving on from that line means terminating the program.Does anyone know a way to get debugging, or is debugging and py.test just not meant to be together?it's real simple: put an  where you want to start debugging in your code and run your tests with:done :) Alternatively, if you are using pytest-2.0.1 or above, there also is the  helper which you can put anywhere in your test code.  Here are the .  It will take care to internally disable capturing before sending you to the pdb debugger command-line.I found that I can run py.test with capture disabled, then use pdb.set_trace() as usual.The easiest way is using the py.test mechanism to create breakpointOr if you want 's debugger as a one-liner, change your  into  I'm not familiar with py.test, put for unittest, you do the following. Maybe py.test is similar:In your test module (mytestmodule.py):Then run the test withYou will get an interactive pdb shell.Looking at the docs, it looks like py.test has a  command line option:"},
{"body": "How can I clear a queue. For example I have datas in a queue, but for some reason I don't need the existing data, and  just want to clear the queue.Is there any way? Will this work:\nI omitted the issue of thread safety for clarity and brevity, but @Dan D is quite correct, the following is better.You just can not clear the queue, because every put also add the unfinished_tasks member.\nThe join method depends on this value.\nAnd all_tasks_done needs to be notified also.or in decent way, use get and task_done pair to safely clear the tasks.or just create a new Queue and delete old one.This seems to do it pretty well for me. I welcome comments/additions in case I missed anything important."},
{"body": "How do you convert a grayscale OpenCV image to black and white? I see a  has already been asked, but I'm using OpenCV 2.3, and the proposed solution no longer seems to work.I'm trying to convert a greyscale image to black and white, so that anything not absolutely black is white, and use this as a mask for , in order to ignore keypoints found on the edge of the black mask area.The following Python gets me almost there, but the threshold value sent to Threshold() doesn't appear to have any effect. If I set it to 0 or 16 or 128 or 255, the result is the same, with all pixels with a value > 128 becoming white, and everything else becoming black.What am I doing wrong?Step-by-step answer similar to the one you refer to, using the new cv2 Python bindings:which determines the threshold automatically from the image using Otsu's method, or if you already know the threshold you can use:Specifying  causes the threshold value to be ignored. From : This code reads frames from the camera and performs the binary threshold at the value 20.While converting a gray scale image to a binary image, we usually use  and set a threshold value manually. Sometimes to get a decent result we opt for .I have a small hack I came across while reading some blog posts.This is because  works for most of the images/data-set.You can also work out the same approach  by replacing  with the .Another approach would be to take an  number of standard deviations () from the mean, either on the positive or negative side; and set a threshold. So it could be one of the following: Before applying threshold it is advisable to enhance the contrast of the gray scale image  (See ).Simply you can write the following code snippet to convert an OpenCV image into a grey scale imageObserve that the image.jpg and the code must be saved in same folder. Note that:Pay attention, if you use  means every pixel greater than threshold becomes the maxValue (in your case 255), otherwise the value is 0. Obviously if your threshold is 0 everything becomes white (maxValue = 255) and if the value is 255 everything becomes black (i.e. 0).If you don't want to work out a threshold, you can use the Otsu's method. But this algorithm only works with 8bit images in the implementation of OpenCV. If your image is 8bit use the algorithm like this:No matter the value of threshold if you have a 8bit image."},
{"body": "I work in Python and would like to read user input (from command line) in Unicode format, ie a Unicode equivalent of ?Also, I would like to test Unicode strings for equality and it looks like a standard  does not work.Thank you for your help ! returns strings as encoded by the OS or UI facilities. The difficulty is knowing which is that decoding. You might attempt the following:which should work correctly in most of the cases.We need more data about not working Unicode comparisons in order to help you. However, it might be a matter of normalization. Consider the following: and  are equivalent but not equal:So you might want to use the  method:If you give us more information, we might be able to help you more, though.It should work.  returns a byte string which you must decode using the correct encoding to get your  object. For example, the following works for me under Python 2.5 / Terminal.app / OSX:As for comparing unicode strings: can you post an example where the comparison doesn't work?I'm not really sure, which format you mean by \"Unicode format\", there are several. UTF-8? UTF-16? In any case you should be able to read a normal string with  and then decode it using the strings  method:If you have a different input encoding just use \"utf-16\" or whatever instead of \"utf-8\". Also see  for different kinds of encodings.Comparing then should work just fine with . If you have string literals containing special characters you should prefix them with \"u\" to mark them as unicode:And if you want to output these strings again, you probably want to encode them again in the desired encoding:In the general case, it's probably not possible to compare unicode strings.  The problem is that there are several ways to compose the same characters.  A simple example is accented roman characters.  Although there are codepoints for basically all of the commonly used accented characters, it is also correct to compose them from unaccented base letters and a non-spacing accent.  This issue is more significant in many non-roman alphabets.  "},
{"body": "I am looking for a Python3.0 version of \"py2exe\". I tried running 2to3 on the source for py2exe but the code remained broken.Any ideas?Did you check out ? It seems to create standalone executables from your Python scripts, including support for Python 3.0 and 3.1py2exe for Python 3.x is now released! .Have a look at the py2exe SourceForge project SVN repository at:The last I looked at it, it said the last update was August 2009. But keep an eye on that to see if there's any Python 3 work in-progress.I've also submitted two feature requests on the py2exe tracker. So far, no feedback on them:Here is the original bug report:\nHere is the comment mentioning the release:\nHere is the package on pypi:\nThe  and  programs serve completely different purposes, so I'm not sure what your ultimate goal is.If you want to build an executable from a working Python program, use the version of  that is suitable for whichever Python you are using (version 2 or version 3).If you want to convert an existing Python 2 program to Python 3, use  plus any additional editing as necessary. The Python 3 documentation describes .: I now understand that you might have been trying to run  against  itself to try to make a Python 3 compatible version. Unfortunately, this is definitely beyond the capabilities of . You will probably have to wait for the  to release a Python 3 compatible version.If you have easy setup installed, type  in a shell to install."},
{"body": "I have a list which I shuffle with the Python built in shuffle function ()However, the Python reference states:Now, I wonder what this \"rather small len(x)\" means. 100, 1000, 10000,...TL;DR: It \"breaks\" on lists with over 2080 elements, but don't worry too much :)Complete answer:First of all, notice that \"shuffling\" a list can be understood (conceptually) as generating all possible permutations of the elements of the lists, and picking one of these permutations at random. Then, you must remember that all self-contained computerised random number generators are actually \"pseudo\" random. That is, they are not actually random, but rely on a series of factors to try and generate a number that is hard to be guessed in advanced, or purposefully reproduced. Among these factors is usually the previous generated number. So, in practice, if you use a random generator continuously a certain number of times,  you'll eventually start getting the same sequence all over again (this is the \"period\" that the documentation refers to).Finally, the docstring on Lib/random.py (the random module) says that \"The period [of the random number generator] is .\"So, given all that, if your list is such that there are  or more permutations, some of these will never be obtained by shuffling the list. You'd (again, conceptually) generate all permutations of the list, then generate a random number x, and pick the xth permutation. Next time, you generate another random number y, and pick the yth permutation. And so on. But, since there are more permutations than you'll get random numbers (because, at most after  generated numbers, you'll start getting the same ones again), you'll start picking the same permutations again.So, you see, it's not exactly a matter of how long your list is (though that does enter into the equation). Also,  is quite a long number. But, still, depending on your shuffling needs, you should bear all that in mind. On a simplistic case (and with a quick calculation), for a list without repeated elements, 2081 elements would yield  permutations, which is more than .I wrote that comment in the Python source originally, so maybe I can clarify ;-)When the comment was introduced, Python's Wichmann-Hill generator had a much shorter period, and we couldn't even generate all the permutations of a deck of cards.The period is astronomically larger now, and 2080 is correct for the current upper bound.  The docs could be beefed up to say more about that - but they'd get awfully tedious.There's a very simple explanation:  A PRNG of period P has P possible starting states.  The starting state wholly determines the permutation produced.  Therefore a PRNG of period P cannot generate more than P distinct permutations (and that's an absolute upper bound - it may not be achieved).  That's why comparing N! to P is the correct computation here.  And, indeed:What they mean is that permutations on n objects (noted n!) grows absurdly high very fast.Basically n! = n x n-1 x ... x 1; for example, 5! = 5 x 4 x 3 x 2 x 1 = 120 which means there are 120 possible ways of shuffling a 5-items list.On the same Python page documentation they give 2^19937-1 as the period, which is 4.something \u00d7 10^6001 or something. Based on the Wikipedia page on factorials, I guess 2000! should be around that. (Sorry, I didn't find the exact figure.)So basically there are so many possible permutations the shuffle will take from that there's probably no real reason to worry about those it won't.But if it really is an issue (pesky customer asking for a guarantee of randomness perhaps?), you could also offload the task to some third-party; see  for example."},
{"body": "The new version of Pandas uses  to load Excel files:but what if I don't know the sheets that are available? For example, I am working with excel files that the following sheetsbut I don't know  a priori.Is there any way to get the list of sheets from an excel document in Pandas?You can still use the  class (and the  attribute):"},
{"body": "I'm working with boolean index in Pandas.\nThe question is why the statement:works fine whereasexists with error?Example:When you sayYou are implicitly asking Python to convert  and  to boolean values. NumPy arrays (of length greater than 1) and Pandas objects such as Series do not have a boolean value -- in other words, they raise when used as a boolean value. That's because its . Some users might assume they are True if they have non-zero length, like a Python list. Others might desire for it to be True only if  its elements are True. Others might want it to be True if  of its elements are True. Because there are so many conflicting expectations, the designers of NumPy and Pandas refuse to guess, and instead raise a ValueError.Instead, you must be explicit, by calling the ,  or  method to indicate which behavior you desire.In this case, however, it looks like you do not want boolean evaluation, you want  logical-and. That is what the  binary operator performs:returns a boolean array."},
{"body": "Is it possible to import a Python module into a Jinja template so I can use its functions? For example, I have a  file that contains methods for formatting dates and times. In a Jinja macro, can I do something  the following? Because  is not a template, the code above gives me this error: ...but I was wondering if there was another way to achieve this. Within the template, no, you cannot import python code.The way to do this is to register the function as a jinja2 , like this:In your python file:In your template:Just pass the function into the template, like soand in the template, just call it like any other function,Functions are first-class citizens in python, so you can pass them around just like anything else. You could even pass in a whole module if you wanted.You can export all of the symbols available in a module by providing the modules __dict__ as a parameter to the jinja template render method. The following will make available functions and types of __builtin__, inspect and types module to the template.Within template, to use a function of the exported modules similar to the following:"},
{"body": "pretty sure there is a common idiom but couldn't find it with google..\nHere is what I want to do (in java):How is this done \"pythonic\" in python?Also would be great if I can get answer for this as well:Do you mean something like:Here is an example that checks if a list contains all zeros:Alternative you can also do:You can use 'all' and 'any' builtin functions in pythonhere somePredicate is a function\nand 'all' will check if bool() of that element is True"},
{"body": "I'm new to Scrapy and I'm looking for a way to run it from a Python script. I found 2 sources that explain this:I can't figure out where I should put my spider code and how to call it from the main function. Please help. This is the example code:Thank you.All other answers reference Scrapy v0.x. According to , Scrapy 1.0 demands:Though I haven't tried it I think the answer can be found within the . To quote directly from it:From what I gather this is a new development in the library which renders some of the earlier approaches online (such as that in the question) obsolete.In scrapy 0.19.x you should do this:Note these lines     Without it your spider won't use your settings and will not save the items.\nTook me a while to figure out why the example in documentation wasn't saving my items. I sent a pull request to fix the doc example.One more to do so is just call command directly from you scriptCopied this answer from my first answer in here:\nWhen there are multiple crawlers need to be run inside one python script, the reactor stop needs to be handled with caution as the reactor can only be stopped once and cannot be restarted. However, I found while doing my project that using is the easiest. This will save me from handling all sorts of signals especially when I have multiple spiders.If Performance is a concern, you can use multiprocessing to run your spiders in parallel, something like:Put this code to the path you can run  from command line. (Tested with Scrapy==0.24.6)"},
{"body": "I am trying to pip install mysql-python connector but it keeps erroring on me. Works fine on my mac and another windows machine but not this one. I have downloaded visual studio c++ and tried it as 32 bit and 64. Does anyone have an idea on how to get around this. for 64-bit windows well this worked for me:this is for python 3.x in window 7 i am not sure about other windows os versions"},
{"body": "Given a string \"VAR=value\" I want to split it (only) at the  '=' sign (< value > may contain more '=' signs), something like this:Is there a way to NOT declare a variable 'sep'? Like this (just made up the syntax):Just for completeness, I'm targetting Python v 2.6 is indeed a very popular choice for \"a name which doesn't matter\" -- it's a legal name, visually unobtrusive, etc.  However sometimes these very qualities can hinder you.  For example, the GNU  module for I18N and L10N, which is part of Python's standard library, idiomatically uses  very differently, with idioms such as...:to mark and translate all the literal-string messages in the code (also exploiting the relative visual unobtrusiveness of .  Obviously any code using this module and idiom shouldn't also be using  to mean something completely different (\"a don't care name\").So a second useful alternative can be to devote the name  to indicate such \"don't care\" situations in a visually more explicit way.  Google's  recommends using either  or a  of  -- the latter can be a bit verbose but tends to be very clear, e.g.:makes crystal-clear that  is a three-item sequence (probably a tuple) and the item you're skipping (and not using at all) is the surname (because you want to print a friendly message like \"Hello, Mr Alex!\" or \"Hello, Miss Piggy!\" ;-).  ( and similar tools can warn you if you have unused variables named otherwise than  or , and of course also warn you if you ever  use a variable named !-).Almost there: is conventionally considered a don't-care variable.There isn't anything official in the language for that; you can just use any throw-away variable.  As far as standards go, I've seen underscores used occasionally in Python and other languages.  The only issue there is that underscore is used as an alias for  when localizing.  But if you aren't doing localization, or aren't using the global-binding for it, then underscore should work fine.Really strange question, because you can do just:and don't care about  variable, but  is just a name as , as  or . By the way use Why don't you use  instead? That disregards the delimiter. (to accommodate Cristi's example in the comment):From :The _ is commonly used as a name for you  * For example, in a tuple containing the name, surname and nickname in a moment in which we are interested only in name and surname, could use _ to indicate that the name is not important at this point:As others have said, underscore () is the standard. But if underscore is being used for translations, I think double underscore is the best alternative.Better than these:There is also the option of using indexes:"},
{"body": "I don't understand what this single underscore means. Is it a magic variable? I can't see it in locals() and globals().In the standard Python REPL,  represents the last returned value -- at the point where you called ,  was the value .For example:Note that there is no such functionality within Python .  In a script,  has no special meaning and will not be automatically set to the value produced by the previous statement.Also, beware of reassigning  in the REPL if you want to use it like above!To undo the assignment (and remove the  from globals), you'll have to:then the functionality will be back to normal (the  will be visible again).Why you can't see it? It is in So it's neither global nor local. And where does this assignment happen? : 2012 Edit : I'd call it  since 's members are available everywhere, in any module.Usually, we are using _ in Python to bind a ugettext function."},
{"body": "I'm building some predictive models in Python and have been using scikits learn's SVM implementation. It's been really great, easy to use, and relatively fast.Unfortunately, I'm beginning to become constrained by my runtime. I run a rbf SVM on a full dataset of about 4 - 5000 with 650 features. Each run takes about a minute. But with a 5 fold cross validation + grid search (using a coarse to fine search), it's getting a bit unfeasible for my task at hand. So generally, do people have any recommendations in terms of the fastest SVM implementation that can be used in Python? That, or any ways to speed up my modeling?I've heard of LIBSVM's GPU implementation, which seems like it could work. I don't know of any other GPU SVM implementations usable in Python, but it would definitely be open to others. Also, does using the GPU significantly increase runtime?I've also heard that there are ways of approximating the rbf SVM by using a linear SVM + feature map in scikits. Not sure what people think about this approach. Again, anyone using this approach, is it a significant increase in runtime?All ideas for increasing the speed of program is most welcome.The most scalable kernel SVM implementation I know of is . It's written in C hence wrap-able in Python if you know ,  or . Alternatively you can use it from the command line. You can use the utilities in  to load convert data from a  or CSR format into svmlight formatted files that LaSVM can use as training / test set.Alternatively you can run the grid search on 1000 random samples instead of the full dataset:It's very likely that the optimal parameters for 5000 samples will be very close to the optimal parameters for 1000 samples. So that's a good way to start your coarse grid search. makes it possible to use all your CPUs to run the individual CV fits in parallel. It's using mulitprocessing so the python GIL is not an issue.Firstly, according to scikit-learn's benchmark (), scikit-learn is already one of the fastest if not fastest SVM package around. Hence, you might want to consider other ways of speeding up the training. As suggested by bavaza, you can try to multi-thread the training process. If you are using Scikit-learn's GridSearchCV class, you can easily set n_jobs argument to be larger than the default value of 1 to perform the training in parallel at the expense of using more memory.\nYou can find its the documentation  An example of how to use the class can be found Alternatively, you can take a look at Shogun Machine Learning Library\nShogun is designed for large scale machine learning with wrappers to many common svm packages and it is implemented in C/C++ with bindings for python. According to Scikit-learn's benchmark above, it's speed is comparable to scikit-learn. On other tasks (other than the one they demonstrated), it might be faster so it is worth giving a try.Lastly, you can try to perform dimension reduction e.g. using PCA or randomized PCA to reduce the dimension of your feature vectors. That would speed up the training process. The documentation for the respective classes can be found in these 2 links: ,  . You can find examples on how to use them in Scikit-learn's examples section.If you are interested in only using the RBF kernel (or any other quadratic kernel for that matter), then I suggest using LIBSVM on  or . I train a model of 7000 observations and 500 features in about 6 seconds.The trick is to use precomputed kernels that LIBSVM provides, and use some matrix algebra to compute the kernel in one step instead of lopping over the data twice. The kernel takes about two seconds to build as opposed to a lot more using LIBSVM own RBF kernel. I presume you would be able to do so in Python using , but I am not sure as I have not tried it.Without going to much into comparing SVM libraries, I think the task you are describing (cross-validation) can benefit from real multi-threading (i.e. running several CPUs in parallel). If you are using , it does not take advantage of your (probably)-multi-core machine, due to .You can try other implementations of Python which don't have this limitation. See  or  if you are willing to go to .NET.Try !It is a wicked-fast C implementation from the , with good Python bindings, and you can install it with .If your problem is in two classes, this wrapping of CUDA-based SVM with scikit-learn is useful:I suggest looking at Scikit-Learn's  implementation. The default hinge loss is a linear SVM. I've found it to be blazingly fast.I'd consider using a  to reduce the number of features you input.There is an option with the ExtraTreesRegressor and ExtraTreesClassifier to generate feature importances. You can then use this information to input a subset of features into your SVM."},
{"body": "The following works when I paste it on the browser: But when I try reading the URL with Python nothing happens:Do I need to encode the URL, or is there something I'm not seeing?To answer your question:You need to , not Or, just get this library here:  and seriously use it :)A solution with works with Python 2.X and Python 3.X makes use of the Python 2 and 3 compatibility library :The URL should be a string:"},
{"body": "I created Django project in home directory so it is in home directory.Permissions of Permissions of Permissions of I am getting error on browser  Please help me out with the configuration .For now I am moving forward by changing toSo,this has definitely something to do with  file configuration,but my worry is that I only added 5 lines in that file and not able to figure out what's wrong .Apparently this is an issue that is related to Apache 2.4 and older versions.\nYou need to replace in your apache configuration:within the  sectionYou can use the following:This has been reported in Django ticket 19319:Your Apache config now needs the following for your file .There is one other gotcha:Check your httpd.conf file for the following configuration:This will cause the error..py MUST NOT be configured as a CGI script"},
{"body": "when i try to send mail using gmail and python error occurred this type of question are already in this site but doesn't help to me error:Your code looks correct. Try logging in through your browser and if you are able to access your account come back and try your code again.\nJust make sure that you have typed your username and password correct:\nGoogle blocks sign-in attempts from apps which do not use modern security standards (mentioned on their support ). You can however, turn on/off this safety feature by going to the link below:Go to this link and select \nI have just sent an email with gmail through Python.\nTry to use smtplib.SMTP_SSL to make the connection. Also, you may try to change the gmail domain and port.So, you may get a chance with:As a plus, you could check the email builtin module. In this way, you can improve the readability of you your code and handle emails headers easily.Your code looks correct but sometimes google block an ip when you try to send a email since a unusual  location, so, you can unblock in the next link and clicked in accounts.google.com/DisplayUnlockCaptcha ."},
{"body": "I have the following code:When I run it in the Python Shell, it returns:I've searched and it seems this is called a , but how does it work? About your question, the list comprehension does the same thing as the following \"plain\" Python code:How do you write it in one line? Hmm...we can...probably...use  with :But isn't it clearer and simpler to just use a list comprehension?Basically, we can do anything with . Not only . For example, run a method of :Or use  as another function's argument:We can also, for example, use  as the key of a  object. Let's see:How about a combination?  And so on.You can also use  or  in a list comprehension. For example, you only want odd numbers in . You can do:Ah that's too complex. What about the following version?To use an  ternary expression, you need put the  after ,  after :Have you heard about ? You can put . For example:Let's talk about the first part,  which gives  and . Then,  gives , ,  and , , .  You always need put   :We also have , , and . and list comprehensions are basically the same, but the former returns a  instead of a :It's the same as:A   a set comprehension, but it uses  or  instead of  .For example:And it equals:Does  give a ? No!, it's a . Which returns a :It's the same as:And you can use it as a generator: If you use a list comprehension , you don't need the  if that function could loop over a generator. For example, : (about generators): .There are list, dictionary, and set comprehensions, but no tuple comprehensions (though do explore \"generator expressions\").They address the problem that traditional loops in Python are statements (don't return anything) not expressions which return a value.They are not the solution to every problem and can be rewritten as traditional loops. They become awkward when state needs to be maintained & updated between iterations.They typically consist of:but can be twisted in lots of interesting and bizarre ways.They can be analogous to the traditional  and  operations which still exist in Python and continue to be used.When done well, they have a high satisfaction quotient."},
{"body": "I have installed tensorflow in my ubuntu 16.04 using the second answer  with ubuntu's builtin apt cuda installation.Now my question is how can I test if tensorflow is really using gpu? I have a gtx 960m gpu. When I  this is the outpIs this output enough to check if tensorflow is using gpu ? No, I don't think \"open CUDA library\" is enough to tell, because different nodes of the graph may be on different devices.To find out which device is used, you can enable log device placement like this:This will confirm that tensorflow using GPU while training also ?Apart from using  which is outlined in other answers as well as in TF official , you can try to assign a computation to the gpu and see whether you have an error.Here If you have a gpu and can use it, you will see the result. Otherwise you will see an error with a long stacktrace. In the end you will have something like this: "},
{"body": "I want a program to do one thing if executed like this:and do another thing if run like thisBut if I read from stdin, then it will wait for user input, so I want to see if there is anything to read before trying to read from stdin.If you want to detect if someone is piping data into your program, or running it interactively you can use isatty to see if stdin is a terminal:You want the select module ( on unix) It will allow you to test if there is anything readable on stdin. Note that select won't work on Window with file objects. But from your pipe-laden question I'm assuming you're on a unix based os :)Bad news.  From a Unix command-line perspective those two invocations of your program  identical.Unix can't easily distinguish them.  What you're asking for isn't really sensible, and you need to think of another way of using your program.In the case where it's not in a pipeline, what's it supposed to read if it doesn't read stdin?Is it supposed to launch a GUI?  If so, you might want to have a \"-i\" (--interactive) option to indicate you want a GUI, not reading of stdin.You can, sometimes, distinguish pipes from the console because the console device is \"/dev/tty\", but this is not portable.I do not know the Python commands off the top of my head, but you should be able to do something with poll or select to look for data ready to read on standard input.That might be Unix OS specific and different on Windows Python."},
{"body": "I'm checking to see if a directory exists, but I noticed I'm using  path.exists instead of path.isdir.  Both work just fine, but I'm curious as to what the advantages are for using isdir instead of exists.  will also return  if there's a regular file with that name.  will only return  if that path exists and is a directory.Just like it sounds like: if the path exists, but is a file and not a directory,  will return . Meanwhile,  will return  in both cases.Most of the time, it is the same.But, path can exist physically whereas  returns False. This is the case if os.stat() returns False for this file.If path exists physically, then  will always return True. This does not depend on platform."},
{"body": "In numpy, I have two \"arrays\", X is (m,n) and y is a vector (n,1)using I am getting the errorWhen  (97,2)x(2,1) is clearly a legal matrix operation and should give me a (97,1) vectorEDIT:I have corrected this using  but the original question still remains.We have two arrays:With Numpy arrays the operationis done element-wise, but one or both of the values can be expanded in one or more dimensions to make them compatible. This operation are called broadcasting. Dimensions where size is 1 or which are missing can be used in broadcasting.In the example above the dimensions are incompatible, because:Here there are conflicting numbers in the first dimension (97 and 2). That is what the ValueError above is complaining about. The second dimension would be ok, as number 1 does not conflict with anything.For more information on broadcasting rules: (Please note that if  and  are of type , then asterisk can be used as matrix multiplication. My recommendation is to keep away from , it tends to complicate more than simplify things.)However, your arrays should be fine with , so there something else must have happened. If  throws an exception, it complains:If you still get this error, please post a minimal example of the problem. It works:returns a (97,1) array.Per Wes McKinney's In other words, if you are trying to multiply two matrices (in the linear algebra sense) then you want  but if you are trying to broadcast scalars from matrix  onto  then you need to perform .\nIt's possible that the error didn't occur in the dot but after.\nFor example try thisnp.dot(a,b) will be fine; however np.dot(a,b) * c is clearly wrong (12x5 X 5x5 = 12x5 which cannot element-wise multiply 5x12) but numpy will give youThe error is misleading; however there is an issue on that line."},
{"body": "What are the main differences between Python metaclasses and class decorators? Is there something I can do with one but not with the other?Decorators are much, much simpler and more limited -- and therefore should be preferred whenever the desired effect can be achieved with either a metaclass or a class decorator.Anything you can do with a class decorator, you can of course do with a custom metaclass (just apply the functionality of the \"decorator function\", i.e., the one that takes a class object and modifies it, in the course of the metaclass's  or  that make the class object!-).There are many things you can do in a custom metaclass but not in a decorator (unless the decorator internally generates and applies a custom metaclass, of course -- but that's cheating;-)... and even then, in Python 3, there are things you can only do with a custom metaclass, not after the fact... but that's a pretty advanced sub-niche of your question, so let me give simpler examples).For example, suppose you want to make a class object  such that  (or in Python 3  of course;-) displays .  You cannot possibly do that without a custom metaclass, because the metaclass's override of  is the crucial actor here, i.e., you need a  in the custom metaclass of class .The same applies to all magic methods, i.e., to all kinds of operations as applied to the class object itself (as opposed to, ones applied to its , which use magic methods as defined in the class -- operations on the class object itself use magic methods as defined in the metaclass)."},
{"body": "Say I have my object layout defined as:...and my type definition:How do I create a new instance of  somewhere within my C extension? The best way is to  the class object, just like in Python itself:"},
{"body": "I want to override my Python class's  and  methods. My use case is the usual one: I have a few special names that I want to handle, and I want the default behavior for anything else. For , it seems that I can request the default behavior simply by raising . However, how can I achieve the same in ? Here is a trivial example, implementing a class with immutable fields \"A\", \"B\", and \"C\".What goes in place of the question marks? With old-style classes, the answer was apparently , but documentation indicates that this is wrong for new-style classes.It'sin Python 2, orin Python 3.Also, raising  is  how you fall back to the default behavior for . You fall back to the default withon Python 2 oron Python 3.Raising  skips the default handling and goes to , or just produces an  in the calling code if there's no .See the documentation on . ?"},
{"body": "  How do I implement a static constructor in Python?Here is my code...  The  doesn't fire when I call App like this. The  is not a static constructor or static initializer.I have to call it like this, which instantiates the App class every time:Here is my class:The problem with calling  every time is that the App object gets recreated.  My \"real\" App class is quite long.  Hint: anything that references  is going to require an instantiation of the class. You could do it like this:But come on, that seems like a lot of fluff. I'm with SLaks, just initialize it outside of the class. Alternatively, you could look into the .There's a fundamental difference between static and dynamic languages that isn't always apparent at first.  In a static language the class is defined at compile time and everything is all nice and set in concrete before the program ever runs.  In a dynamic language the class is actually defined at runtime.  As soon as the interpreter parses and starts executing all of those class and def statements the equivalent of a static constructor is being run.  The class definitions are being executed at that point.  You can put any number of statements under the class body that you want and they are in effect a static constructor.  If you want you can place them all in a function that doesn't take self and at the end of the class call that function.You need to instantiate your App, then use that instead:"},
{"body": "I am trying to predict weekly sales using  ARIMA models. I could not find a function for tuning the order(p,d,q) in . Currently R has a function  which will tune the (p,d,q) parameters. How do I go about choosing the right order for my model? Are there any libraries available in python for this purpose?You can implement a number of approaches:actuallyI wrote these utility functions to directly calculate pdq values \n require three inputs data which is series with timestamp(datetime) as index. n_jobs will provide number of parallel processor. output will be dataframe with aic and bic value with order=(P,D,Q) in index\np and q range is [0,12] while d is [0,1]possible solutionfrom also see "},
{"body": " question got me thinking: should we apply the principle that \"flat is better than nested\" to data as well as to code? Even when there is a \"logical tree structure\" to the data?In this case, I suppose it would mean representing children as a list of IDs, rather than an actual list of children, with all the nodes in a single list:(I used tuples because I prefer not to give myself the flexibility to mutate an object until that flexibility proves itself to be useful and usable in a clear manner. In any case I would never use  here instead of an empty sequence, because it complicates the logic, and \"special cases aren't special enough\" - here, it isn't special at all.)Certainly this is shorter, but the tree structure is obscured. Does that contradict \"explicit is better than implicit\"?Personally I find that \"flat is better than nested\" is of limited applicability, and nowhere near the most important aspect of the Zen. (Certainly I could not do a lot of the nice functional-programming things that I do if I didn't allow myself significant nesting.) I suspect that the problem with \"nested\" is that it requires context switching when you comprehend the information. I really think this is more of a problem when following imperative logic than for parsing data or functional-style code - where it's easier to just mentally name the nested block, and consider its workings separately from the outer context.What say you?I wouldn't inherently prefer either, but rather use whatever seems best suited to the task.If the structure is important, nesting makes life simple. If you're regularly operating over each node, the flat structure makes it easy to use . Of course, if you define your own class, its easy enough to abstract it so both options are simple; but it may be harder to use with external systems, such as converting to JSON.This is a completely subjective question.  The answer is, \"it depends.\"It depends on the primary use of your data.  If you continually have to reference the nested structure, then it makes sense to represent it that way.  And if you never reference the flat representation except when building the nested structure, then why have the flat structure at all?The \"flat\" representation is one of the basics of the relational database model:  each type of data exists in a table just for that type, and any relationships among the items are contained in separate tables.  It's a useful abstraction, but at times difficult to work with in code.  On the other hand, processing all the data of a particular type is trivial.Consider, for example, if I wanted to find all the descendants of the record with id 2 in your example data.  If the data is already in the hierarchy (i.e. native representation is the \"nested\" structure), then it's trivial to locate record id 2 and then traverse its children, children's children, etc.But if the native representation is sequential as you've shown it then I have to pass through the entire data set to create the hierarchical structure and  find record 2 and its children.So, as I said, \"it depends.\"No.That's what \"flat is better than nested\" doesn't apply to data.  Only to code.It's not even comparable.  The \"flat tree\" is a common SQL implementation because SQL can't handle indefinite recursion of a proper tree.  Comparing the Zen of Python (the language) with data structure design is nonsensical.  The two are no more comparable than than the number \"2\" and splitting a brick of cheese into \"2\" pieces.  One's a thing, the other's an action.  Data structures are things.Python describes actions.  Yes, particularly when being explicit prevents you from implicitly shooting yourself in the foot. The \"tree\" in your example can have multiple parents claiming to own the same children. It can also have multiple root nodes (and it does: 2 is a root node; 4 is a root as well as a leaf node.)This question can't be answered \"in general\" - there is no right answer.    For this particular example, I actually like the tree structure better.  Without knowing anything about the original application, and just looking at the structure, the relationship between the items is obvious.  With the flat structure, I have to read some documentation, or application code to \"know\" that your tuple of children refer to id's.  The tree structure is self-documenting - the flat structure isn't.\"Everything should be made as simple as possible, but not simpler.\" Flat is simpler than nested. If you're dealing with data with  nesting, then flattening it probably violates the \"but not simpler\" part. I took the Zen of Python instead to be encouraging you not to complicate your life with nesting you don't really need, like an XML config file where a simpler flat format might suffice."},
{"body": "I'm trying to understand the basics of threading and concurrency. I want a simple case where two threads repeatedly try to access one shared resource. The code:  So, I have two threads, both trying to increment the counter. I thought that if thread 'A' called , the  would be established, preventing 'B' from accessing until 'A' has released. Running the makes it clear that this is not the case. You get all of the random data race-ish increments.How exactly is the lock object used?  Edit, Additionally, I've tried putting the locks inside of the thread functions, but still no luck. You can see that your locks are pretty much working as you are using them, if you slow down the process and make them block a bit more. You had the right idea, where you surround critical pieces of code with the lock. Here is a small adjustment to your example to show you how each waits on the other to release the lock.Sample output:"},
{"body": "I am trying to get the mercurial revision number/id (it's a hash not a number) programmatically in python.The reason is that I want to add it to the css/js files on our website like so:So that whenever a change is made to the stylesheet, it will get a new url and no longer use the old cached version. if you know where to find good documentation for the mercurial , that would also be helpful. I can't seem to find it anywhere.I ended up using subprocess to just run a command that gets the hg node. I chose this solution because the api is not guaranteed to stay the same, but the bash interface probably will:example use:It's true there's no official API, but you can get an idea about best practices by reading other extensions, particularly those bundled with hg. For this particular problem, I would do something like this: At some point the parameter order changed, now it's like this:Do you mean ?\nNote that, as stated in that page, there is no  API, because they still reserve the right to change it at any time. But you can see the list of changes in the last few versions, it is not very extensive.An updated, cleaner subprocess version (uses , added in Python 2.7/3.1) that I use in my Django settings file for a crude end-to-end deployment check (I dump it into an HTML comment):You could wrap it in a  if you don't want some strange hiccup to prevent startup:give a try to \n I don't know what to use if you're using Python 3, sorry. Probably .Mercurial has two official APIs.Installation:Usage:More usage information on the .Because it is maintained by the Mercurial team, and it is what the Mercurial team recommend for interfacing with Mercurial. , the following statement on interfacing with Mercurial:From the command server page:The Python interface to the Mercurial command-server, as said, is .The per-command overhead of the command line interface is no joke, by the way. I once built a very small test suite (only about 5 tests) that used  via  to create, commit by commit, a handful of repos with e.g. merge situations. Throughout the project, the runtime of suite stayed between 5 to 30 seconds, with nearly all time spent in the  calls.The signature of a Python hook function is like so: and  are part of the aforementioned discouraged unofficial . The fact that they are right there in your function args makes them terribly convenient to use, such as in this example of a  hook that disallows merges between certain branches.If your hook code is not so important, and you're not publishing it, you might choose to use the discouraged unofficial internal API. If your hook is part of an extension that you're publishing, better use .FWIW to avoid fetching that value on every page/view render, I just have my deploy put it into the  file.  Then I can reference  without all the overhead of accessing mercurial and/or another process.  Do you ever have this value change w/o reloading your server?I wanted to do the same thing the OP wanted to do, get  from a script (get tip revision of the whole REPOSITORY, not of a single FILE in that repo) but I did not want to use popen, and the code from  got me started, but wasn't what I wanted. So I wrote this... Comments/criticism welcome.  This gets the tip rev in hex as a string."},
{"body": "Is one correct in stating the following: --> can I safely use this in every situation? (haven't run into one where it caused me problems)With 'C type as an attribute' I mean bar and baz:First, read this more carefully, specifically the last paragraph, Easy way to think about it is thinking about the reference counts.The caveats to these answers are outlined in the link above.  You really shouldn't even need my poor explanation after reading that.  I hope I clarified and didn't confuse more."},
{"body": "Like many a foolhardy pioneer before me, I'm endeavoring to cross the trackless wasteland that is Understanding Monads.I'm still staggering through, but I can't help noticing a certain monad-like quality about Python's  statement.  Consider this fragment:Consider the open() call as the \"unit\" and the block itself as the \"bind\".  The actual monad isn't exposed (uh, unless  is the monad), but the pattern is there.  Isn't it?  Or am I just mistaking all of FP for monadry?  Or is it just 3 in the morning and anything seems plausible?A related question: if we have monads, do we need exceptions?In the above fragment, any failure in the I/O can be hidden from the code.  Disk corruption, the absence of the named file, and an empty file can all be treated the same.  So no need for a visible IO Exception.Certainly, Scala's  typeclass has eliminated the dreaded Null Pointer Exception.  If you rethought numbers as Monads (with NaN and DivideByZero as the special cases)...Like I said, 3 in the morning.Right below the definition, :This sounds to me exactly like the context manager protocol, the implementation of the context manager protocol by the object, and the  statement.From @Owen in a comment on this post:The full Wikipedia definition:This sounds like the  to me.The actual implementation of the context manager protocol by the object.This corresponds to  and its suite.So yes, I'd say  is a monad. I searched  and all the related rejected and withdrawn PEPs, and none of them mentioned the word \"monad\". It certainly applies, but it seems the  of the  statement was resource management, and a monad is just a useful way to get it.It's almost too trivial to mention, but the first problem is that  isn't a function and doesn't take a function as an argument. You can easily get around this by writing a function wrapper for :Since this is so trivial, you could not bother to distinguish  and .The second problem with  being a monad is that, as a statement rather than an expression, it doesn't have a value. If you could give it a type, it would be  (this is actually the type of  above). Speaking practically, you can use Python's  to get a value for the  statement. In Python 3.1:Since  uses a function rather than a code block, an alternative to  is to return the value of the function:There is another thing preventing  (and ) from being a monadic bind. The value of the block would have to be a monadic type with the same type constructor as the  item. As it is,  is more generic. Considering agf's note that every interface is a type constructor, I peg the type of  as , where M is the context manager interface (the  and  methods). In between the types of  and  is the type . To be a monad,  would have to fail at runtime when  wasn't . Moreover, while you could use  monadically as a bind operation, it would rarely make sense to do so.The reason you need to make these subtle distinctions is that if you mistakenly consider  to be monadic, you'll wind up misusing it and writing programs that will fail due to type errors. In other words, you'll write garbage. What you need to do is distinguish a construct that is a particular thing (e.g. a monad) from one that can be used in the manner of that thing (e.g. again, a monad). The latter requires discipline on the part of a programmer, or the definition of additional constructs to enforce the discipline. Here's a nearly monadic version of  (the type is ):In the final analysis, you could consider  to be like a combinator, but a more general one than the combinator required by monads (which is bind). There can be more functions using monads than the two required (the list monad also has cons, append and length, for example), so if you defined the appropriate bind operator for context managers (such as ) then  could be monadic in the sense of involving monads.Haskell has an equivalent of  for files, it's called . This:is equivalent to:Now,  might look like something monadic. Its type is:right side looks like .Another similarity: In Python you can skip , and in Haskell you can use  instead of  (or, a  block without  arrow).So I'll answer this question: is  monadic?You could think that it can be written like this:But this doesn't type check. And it cannot.It's because in Haskell : if you writeafter  is executed,  is executed and then . There is no \"backtrack\"\nto clean  at the end or something like that. , on the other hand,\nhas to close the handle after the block is executed.There is another monad, called continuation monad, which allows to do such\nthings. However, you have now two monads, IO and continuations, and using effects of two monads at once requires using monad transformers.That's ugly. So the answer is: somewhat, but that requires dealing with a lot of  subtleties that make the issue rather opaque.Python's  can simulate only a limited bit of what monads can do - add entering and finalization code. I don't think you can simulate e.g.using  (it might be possible with some dirty hacks). Instead, use for:And there's a Haskell function for this - :I recommed reading about  which bears more resemblance to monads than :\nBasically no, instead of a function that throws A or returns B you can make a function that returns . The monad for  will then behave just like exceptions - if one line of code will return an error, the whole block will.However, that would mean that division would have type  and so on, to catch division by zero. You would have to detect errors (explicitly pattern match or use bind) in any code that uses division or has even slightest possibility of going wrong. Haskell uses exceptions to avoid doing this.I have thought unnecesarily long about this and I believe the answer is \"yes, when it's used a certain way\" (thanks outis :),\nbut not for the reason I thought before.I mentioned in a comment to agf that  is just continuation passing style --\ngive it a producer and a callback and it \"runs\" the producer and feeds it to the\ncallback. But that's not quite true. Also important is that  has to run\nsome  between the producer and the result of the callback.In the case of the List monad, this would be concatenating lists. This\ninteraction is what makes monads special.But I believe that Python's   do this interaction, just not in the\nway you might expect.Here's an example python program employing two with statements:When run the output isNow, Python is an imperative language, so instead of merely producing data, it\nproduces side-effects. But you can think of those side-effects as being data\n(like ) -- you can't combine them in all the cool ways you could combine\n, but they're getting at the same goal.So what you should focus on is the  of those operations -- that is,\nthe order of the print statements.Now compare the same program in Haskell:Which produces:And the ordering is the same.The analogy between the two programs is very direct: a  has some\n\"entering\" bits, a \"body\", and some \"exiting\" bits. I used Strings instead of\nIO actions because it's easier -- I think it should be similar with IO actions\n(correct me if it's not).And  for  does  what  in Python does: it runs the\nentering statements, feeds the value to the , and runs the exiting\nstatements.(There's another huge difference which is that the body should be depend on the\nentering statements. Again I  that should be fixible)."},
{"body": "Sorry basic question I'm sure but I can't seem to figure this out.Say I have this program , the file is called :How can I call it in another program?\nI tried:Instead of 'hello world', I get  ...I have done this in the past by making the first file a class, but I was wondering how to import the function correctly?  If it helps, in my real file, I am printing a dictionaryYou need to print the result of calling the function, rather than the function itself:Additionally, rather than , you can omit the  clause:If it's more convenient, you can also use :"},
{"body": "For various reasons, I am trying to get an collections of objects out of a database and pass it to another process that is not connected to the database.  My code looks like the one below but I keep getting When I try to look at the elements of my list outside of the  method.However, if I use this I am able to use the elements but worry about the state of the session since it was not closed.  What am I missing here?If you want a bunch of objects produced by querying a session to be usable outside the scope of the session, you need to  them for the session.In your first function example, you will need to add a line:beforeMore generally, let's say the session is not closed right away, like in the first example. Perhaps this is a session that is kept active during entire duration of a web request or something like that. In such cases, you don't want to do . You will want to be more surgical:In my case, I was saving a related entity as well, and this recipe helped me to  all instances within a session, leveraging the fact that Session is iterable:This is extremely ineffective, but works. Should be fine for unit-tests.Final note: in Python3  is a generator and won't do anything. Use real loops of list comprehensions"},
{"body": "I've been searching around, and there appear to be scattered discussions about s in different programming languages, including some specific cases, but nothing exhaustive or clear.What are the most common operations that would cause a , in Python, which originate while working with NumPy or SciPy? If you do any of the following without horsing around with the floating-point environment, you should get a NaN where you didn't have one before:The canonical reference for these aspects of machine arithmetic is the .  Section 7.1 describes the invalid operation exception, which is the one that is raised when you're about to get a NaN.  \"Exception\" in IEEE 754 means something different than it does in a programming language context.Lots of special function implementations document their behaviour at singularities of the function they're trying to implement.  See the man page for  and , for instance.You're asking specifically about NumPy and SciPy.  I'm not sure whether this is simply to say \"I'm asking about the machine arithmetic that happens under the hood in NumPy\" or \"I'm asking about  and stuff.\"  I'm assuming the former, but the rest of this answer tries to make a vague connection to the higher-level functions in NumPy.  The basic rule is:  For , for instance, you're liable to get s if your input values are around  or larger and a silent loss of precision if your input values are around  or smaller.  Apart from truly ridiculously scaled inputs, though, you're quite safe with .For things involving matrix math, NaNs can crop up (usually through the  route) if your numbers are huge  your matrix is extremely ill-conditioned.  A complete discussion of how you can get screwed by numerical linear algebra is too long to belong in an answer.  I'd suggest going over a numerical linear algebra book (Trefethen and Bau is popular) over the course of a few months instead.One thing I've found useful when writing and debugging code that \"shouldn't\" generate NaNs is to tell the machine to trap if a NaN occurs.  In GNU C, I do this:"},
{"body": "I'm having some weird issues with pytz's .localize() function. Sometimes it wouldn't make adjustments to the localized datetime:.localize behaviour:As you can see, time has not been changed as a result of localize/normalize operations.\nHowever, if .replace is used:Which seems to make adjustments into datetime. Question is - which is correct and why other's wrong? just assumes that the naive datetime you pass it is \"right\" (except for not knowing about the timezone!) and so just sets the timezone, no other adjustments.You can (and it's advisable...) internally work in UTC (rather than with naive datetimes) and use  when you need to perform I/O of datetimes in a localized way ( will handle DST and the like).I realize I'm a little late on this... \nbut here is what I found to work well.\nWork in UTC as Alex stated: Then to localize:And this method does handle DST perfectlyThis DstTzInfo class is used for timezones where the offset from UTC changes at certain points in time.  For example (as you are probably aware), many locations transition to \"daylight savings time\" at the beginning of Summer, and then back to \"standard time\" at the end of Summer.  Each DstTzInfo instance only represents one of these timezones, but the \"localize\" and \"normalize\" methods help you get the right instance.For Abidjan, there has only ever been one transition (according to pytz), and that was in 1912:The tz object we get out of pytz represents the pre-1912 timezone:Now looking up at your two examples, see that when you call tz.localize(d) you do  get this pre-1912 timezone added to your naive datetime object.  It assumes that the datetime object you give it represents local time , which is the post-1912 timezone.However in your second example using d.replace(tzinfo=tz), it takes your datetime object to represent the time in the pre-1912 timezone.  This is probably not what you meant.  Then when you call dt.normalize it converts this to the timezone that is correct for that datetime value, ie the post-1912 timezone. is the correct function to use for creating datetime aware objects with an initial fixed datetime value. The resulting datetime aware object will have the original datetime value. A very common usage pattern in my view, and one that perhaps pytz can better document. is unfortunately named. It is a function that is random in its behaviour. I would advise avoiding the use of this function to set timezones unless you enjoy self-inflicted pain. I have already suffered enough from using this function."},
{"body": "When I upgrade my Ubuntu into 14.04 from 12.04, this time I get this error:This happened to me when I created a virtualenv and then upgraded from 12.04 to 14.04.I had to delete my virtualenv and recreate it, and after doing that, everything worked again.Just run this command. It worked like a charm!This just happened to me after the 14.10 update, and it seems to be because my virtual environments have old copies of  that \u2014 unlike the new binary \u2014 do not include  built-in, and so get an error when they cannot find it on disk anywhere. The new interpreter seems to import it without any file I/O (try running it under strace to check).I tried to re-install it by these steps.And it works perfectly. Thanks guys :)just reinitialize the virtualenv by:If you use , updating it might solve this issue If by chance you come across this error while trying to renew your LetsEncrypt certificate (like I did) I found the solution here:Remove this folder and rerun LetsEncrypt and it will recreate all the relevant files and avoid the error from this thread.Try...if lib-dynload not included in sys.path, You could not check it!I upgraded from Ubuntu 12.04 to 14.04 and  helped me solve it:Installing the dependencies:Making a symbolic link as suggested by mrudult:Install Pillow as usual:Same happened to me on upgrading Ubuntu from 14.04 to 15.10.I solved it by upgrading pip and then removing and recreating the virtual env:(I use virtualenvwrapper)If you face datetime import issue using IntelliJ PyCharm or Idea and from Console/Terminal it works fine, you should just duplicate/recreate running configurations."},
{"body": "In python 2.6, the following code:Gives the following output:Which means that even though there is only one value for test, it is still being parsed into a list. Is there a way to ensure that if there's only one value, it is not parsed into a list, so that the result would look like this?You could fix it afterwards...However, I don't think  would want this.  If a parameter that is normally a list happens to arrive with only one item set, then I would have a string instead of the list of strings I normally receive.A sidenote for someone just wanting a simple dictionary and never needing multiple values with the same key, try:This will give you a nice . Please note that if there  multiple values for the same key, you'll only get the last one."},
{"body": "Given this harmless little list:My goal is to pythonically concatenate the little devils using one of the following ways:A. plain ol' string function to get the job done, short, no importsB. lambda, lambda, lambda C. globalization (do nothing, import everything)Please suggest other pythonic ways to achieve this magnanimous task.Please rank (pythonic level) and rate solutions giving concise explanations.In this case, is the most pythonic solution the best coding solution?Have a look at Guido's  on python optimization, it covers converting lists of numbers to strings. Unless you have a  reason to do otherwise, use the  example.the only pythonic way:Of course it's . How do I know?  Let's do it in a really stupid way:\nIf the problem was only adding 2 strings, you'd most likely use . What does it take to get that to the next level? Instinctively, for most (I think), will be to use . Let's see how that goes:Wow! Python simply told me what to use! :)Here's the least Pythonic way:I myself use the \"join\" way, but from python 2.6 there is a base type that is little used: .Bytearrays can be incredible useful -- for string containing texts, since the best thing is to have then in unicode, the \"join\" way is the way to go -- but if you are dealing with binary data instead, bytearrays can be both more pythonic and more efficient:it is a built in data type: no imports needed - just use then -- and you can use a bytearray isntead of a list to start with - so they should be more efficinet than the \"join\", since there is no data copying to get the string representation for a bytearray.Great answer from SilenGhost BUT, just a few words about the presented  \"alternative\"Unless you've got a very very  good reason to concatenate strings using  or  (the most frequent one, that you've got few, fixed number of strings), you should use always .Just because each  generates a new string which is the concatenation of two strings, unless join that only generates one final string. So, imagine you've got 3 strings:Ok, doesn't seems not much, but you've got to reserve memory for D. Also, due python use of strings, generating a new, intermediate, string, it's somehow expensive...Now, with 5 stringsAssuming the best scenario (if we do (A+B) + (C+D) + E, we'll end having three intermediate strings at the same time on memory), that's generating 3 intermediate strings... You've got to generate a new python object, reserve memory space, release the memory a few times... Also the overhead of calling a Python function (that is not small)Now think of it with 200 strings. We'll end up with a ridiculous big number of intermediate strings, each of one consuming combining quite a lot time on being a complete list over python, and calling a lot of  functions, each with its overhead... Even if you use  functions, it won't help. It's a problem that has to be managed with a different approach: , which only generates  complete python string, the final one and calls ONE python function.(Of course, , or other similar, specialized function for arrays)"},
{"body": "I am looking for ideas on how to translate one range values to another in Python. I am working on hardware project and am reading data from a sensor that can return a range of values, I am then using that data to drive an actuator that requires a different range of values.For example lets say that the sensor returns values in the range 1 to 512, and the actuator is driven by values in the range 5 to 10. I would like a function that I can pass a value and the two ranges and get back the value mapped to the second range. If such a function was named  it could be used like this:In this example I would expect the output  to be  since the  is in the middle of the possible input range.One solution would be:You could possibly use algebra to make it more efficient, at the expense of readability.You can also use  package to do such conversions (if you don't mind dependency on SciPy):or to convert it back to normal float from 0-rank scipy array:You can do also multiple conversions in one command easily:As a bonus, you can do non-uniform mappings from one range to another, for intance if you want to map [1,128] to [1,10], [128,256] to [10,90] and [256,512] to [90,100] you can do it like this: creates piecewise linear interpolation objects (which are callable just like functions).As noted by ,  is also an option (with less dependencies):This would actually be a good case for creating a closure, that is write a function that returns a function.  Since you probably have many of these values, there is little value in calculating and recalculating these value spans and factors for every value, nor for that matter, in passing those min/max limits around all the time.Instead, try this:Now you can write your processor as:"},
{"body": "I want to build a general tree whose root node contains 'n' children, and those children may contain other children.....A tree in Python is quite simple. Make a class that has data and a list of children. Each child is an instance of the same class. This is a general n-nary tree.Then interact:This is a very basic skeleton, not abstracted or anything. The actual code will depend on your specific needs - I'm just trying to show that this is very simple in Python.I've published a Python [3] tree implementation on my site: .Hope it is of use,Ok, here's the code:I recommend  has also a powerful API with:just to show another thought on implementation if you stick to the \"OOP\""},
{"body": "I have written the following code, which should check if the entered number is a prime number or not, but there is an issue i couldn't get through:If the entered number is not a prime number, it displays \"not prime\", as it is supposed to, but if the number is a prime number, it doesn't display anything. Could you please help me with it?There are many efficient ways to test primality (and this isn't one of them). But the loop you wrote can be concisely represented in Python:That is, a is prime if all numbers between 2 and a (not inclusive) give non-zero remainder when divided into a.I don't actually think the best solution has been found in these answers, so I'm gonna post mine, and explain why this is a better one:: the  check is needed since that , and so is zero and any negative number.I'm gonna give you some insides about that almost esoteric single line of code that will check for prime numbers:I'm including an \"unpacked\" version of the  function, to make it easier to understand and read it:This is the most efficient way to see if a number is prime, if you only have a few query. If you ask a lot of numbers if they are prime try .If  is a prime then the  in your code will run forever, since  will remain .So why is that  there?I think you wanted to end the for loop when you found a factor, but didn't know how, so you added that while since it has a condition. So here is how you do it:it worked with me just fine :D"},
{"body": "Given a listhow can I getThat is, produce a new list in which each successive element is alternately taken from the two sides of the original list?\nThis code picks numbers from the beginning () and from the end () of , alternatingly (). A total of  numbers are picked, so this produces no ill effects even if  is odd.\n yields ,\n yields ,\nand  alternates between  and ,\nso the indices we extract from  are: .\nThe nice thing about this one-liner is that it's short and shows symmetry ( and ).\n\nOne might think that  were the same as  with the sign flipped.  of the result instead of truncating towards zero. So .\nAlso, I find accessing list elements by index less pythonic than iteration. between getting items from the forward  and the  one. Just make sure you stop at  with .This can easily be put into a single line but then it becomes much more difficult to read:Putting it in one line would also prevent you from using the other half of the iterators if you wanted to:A very nice one-liner in Python 2.7:First you zip the list with its reverse, take  that list, sum the tuples to form one tuple, and  convert to list. In Python 3,  returns a generator, so you have have to use  from :: It appears this only works perfectly for even-list lengths - odd-list lengths will omit the middle element :( A small correction for  to  will give you a duplicate middle value, so be warned. You can just  back and forth:Note: This destroys the original list, .Not terribly different from some of the other answers, but it avoids a conditional expression for determining the sign of the index. alternates between 0 and 1. This causes the exponent to alternate between 1 and -1.  causes the index divisor to alternate between 2 and -2, which causes the index to alternate from end to end as  increases. The sequence is , , , , , , etc.(I iterate  over  since in this case each value of  is equal to its index. In general, iterate over .)The basic principle behind your question is a so-called roundrobin algorithm. The  contains a possible implementation of it:so all you have to do is split your list into two sublists one starting from the left end and one from the right end:alternatively you could create a longer list (containing alternating items from sequence going from left to right and the items of the complete sequence going right to left) and only take the relevant elements:or using it as explicit generator with :or the  suggested by @Tadhg McDonald-Jensen (thank you!):Not sure, whether this can be written more compactly, but it is efficient as it only uses iterators / generatorsFor fun, here is an itertools variant:This works where  is even.  It would need a special code for odd-lengthened input.Enjoy!Not at all elegant, but it is a clumsy one-liner: If that breaks, then this breaks (it drops the middle term). Note that I got some of the idea from .Use the right .First, I tried something similar to Raymond Hettinger's solution with itertools (Python 3).One way to do this for even-sized lists (inspired by ):I would do something like thisTwo versions not seen yet:andYou can partition the list into two parts about the middle, reverse the second half and zip the two partitions, like so:Output:"},
{"body": "I am newbie in Python facing a problem: How to insert some fields in already existing string?For example, suppose I have read one line from any file which contains:Now I have to insert 3rd Field(Group) 3 times more in the same line before Class field. It means the output line should be: I can retrieve 3rd field easily (using  method), but please let me know the easiest way of inserting into the string?An important point that often bites new Python programmers but the other posters haven't made explicit is that strings in Python are immutable -- you can't  modify them in place. You need to retrain yourself when working with strings in Python so that instead of thinking, \"How can I modify this string?\" instead you're thinking \"how can I create a new string that has some pieces from this one I've already gotten?\"For the sake of future 'newbies' tackling this problem, I think a quick answer would be fitting to this thread.Like  said: Python strings are immutable, and so, in order to modify a string you have to make use of the pieces you already have.In the following example I insert  in to , to create In the example above, I used the index value to 'slice' the string in to 2 substrings:\n1 containing the substring before the insertion index, and the other containing the rest.\nThen I simply add the desired string between the two and voil\u00e0, we have inserted a string inside another. has a great answer explaining the subject of string slicing.I know it's malapropos, but IMHO easy way is:There are several ways to do this:One way is to use slicing:Another would be to use regular expressions:I had a similar problem for my DNA assignment and I used bgporter's advice to answer it. Here is my function which creates a new string..."},
{"body": "If only timedelta had a month argument in it's constructor.  So what's the simplest way to do this? I wasn't thinking too hard about this as was pointed out below.  Really what I wanted was any day in the last month because eventually I'm going to grab the year and month only.  So given a datetime object, what's the simplest way to return ?Try this: Corrected to handle the day as well. See also the answer from puzzlement which points out a simpler calculation for :You can use the third party  module (PyPI entry ).output:After the original question's edit to \"any datetime object in the previous month\", you can do it pretty easily by subtracting 1 day from the first of the month.What do you want the result to be when you subtract a month from, say, a date that is March 30?  That is the problem with adding or subtracting months: months have different lengths!  In some application an exception is appropriate in such cases, in others \"the last day of the previous month\" is OK to use (but that's truly crazy arithmetic, when subtracting a month then adding a month is  overall a no-operation!), in others yet you'll want to keep in addition to the date some indication about the fact, e.g., \"I'm saying Feb 28 but I really would want Feb 30 if it existed\", so that adding or subtracting another month to that can set things right again (and the latter obviously requires a custom class holding a data plus s/thing else).There can be no real solution that is tolerable for all applications, and you have not told us what your specific app's needs are for the semantics of this wretched operation, so there's not much more help that we can provide here.A variation on  (I don't have sufficient reputation to comment), which uses calendar.monthrange to dramatically simplify the computation of the last day of the month:Info on monthrange from If all you want is any day in the last month, the simplest thing you can do is subtract the number of days from the current date, which will give you the last day of the previous month.For instance, starting with any date:Subtracting the days of the current date we get:This is enough for your simplified need of any day in the last month.But now that you have it, you can also get any day in the month, including the same day you started with (i.e. more or less the same as subtracting a month):Of course, you need to be careful with 31st on a 30 day month or the days missing from February (and take care of leap years), but that's also easy to do:Here is some  to do just that. Haven't tried it out myself...Given a (year,month) tuple, where month goes from 1-12, try this:Assumes there are 12 months in every year.I Used the following code to go back n Months from a specific Date:For eg:\ninput date = 28/12/2015\nCalculate 6 months previous date.I) CALCULATE MONTH: \nThis step will give you the start_date as 30/06/2015.\n that after the calculate month step you will get the last day of the required month.II)CALCULATE DAY:\nCondition   checks whether the day from input_date is present in the required month. This handles condition where input date is 31(or 30) and the required month has less than 31(or 30 in case of feb) days. It handles leap year case as well(For Feb). After this step you will get result as 28/06/2015If this condition is not satisfied, the start_date remains the last date of the previous month. So if you give 31/12/2015 as input date and want 6 months previous date, it will give you 30/06/2015"},
{"body": "I'm not asking for the  command.I want to create an application that works similarly to heidisql, where you can specify an SQL query and when executed, returns a result set with rows and columns representing your query result. The column names in the result set should match your selected columns as defined in your SQL query.In my Python program (using ) my query returns only the row and column results, but not the column names.  In the following example the column names would be , , and .  The SQL would eventually be external from the program. The only way I can figure to make this work, is to write my own SQL parser logic to extract the selected column names.Is there an easy way to get the column names for the provided SQL?\nNext I'll need to know how many columns does the query return?cursor.description will give you a tuple of tuples where [0] for each is the column header.This is the same as thefreeman but more in pythonic way using list and dictionary comprehension Similar to @James answer, a more pythonic way can be:You can get a single column with map over the result:or filter results:or accumulate values for filtered columns:I think this should do what you need (builds on the answer above) .  I am sure theres a more pythony way to write it, but you should get the general idea.Looks like MySQLdb doesn't actually provide a translation for that API call.  The relevant C API call is , and You could also use . This turns your result set into a python list of python dictionaries, although it uses a special cursor, thus technically less portable than the accepted answer. Not sure about speed. Here's the edited original code that uses this.Standard dictionary functions apply, for example,  to count the number of columns for the first row,  for a list of column names (for the first row), etc. Hope this helps!"},
{"body": "If I use MySQLdb to connect to MySQL-Server through python. I create a connection and a cursor like thisWhen the MySQL-processing is done one should close the connection. Now I was wondering: Is it sufficient to close the connection bydoingor do I have to close the cursor first and then the connection? Like this:Think less.  Use tools more.Closing the cursor as soon as you are done with it is probably the best bet, since you have no use for it anymore. However, I haven't seen anything where it's harmful to close it after the db connection. But since you can set it as:I recommend closing it before, in case you accidentally assign it again and the DB connection is closed as this would throw an error. So you may want to close it  in order to prevent an accidental reassignment with a closed connection.(Some don't even close it at all though as it gets collected by the garbage collector (see:))References:\nClosing a connection should be good enough here in this particular context.\nIf you are working with multiple cursors etc. you need to care about proper resource management."},
{"body": "I came across these 2 papers which combined collaborative filtering (Matrix factorization) and Topic modelling (LDA) to recommend users similar articles/posts based on topic terms of post/articles that users are interested in. The papers (in PDF) are:\n\"\" and\n\"\"The new algorithm is called . I was hoping to find some python code that implemented this but to no avail. This might be a long shot but can someone show a simple python example?A very simple LDA implementation using gensin. \nYou can find more informations here: I hope it can help you[(0, u'0.066*animal + 0.065*, + 0.047*product + 0.028*philosophy'), (1, u'0.085*. + 0.047*product + 0.028*dietary + 0.028*veg')]This should get you started (although not sure why this hasn't been posted yet): More specifically: Looks nice and straightforward. I still suggest at least looking at . Radim has done a fantastic job of optimizing that software very well.As you have tagged  and , did you take a look at python  &  modules, because with both of them you Also there is a code example relative to  (with Non-negative Matrix Factorization and Latent Dirichlet Allocation)  which may fit  your exact needs and also help you to discover sklearn moduleRegards"},
{"body": "Given this initial graph:Can anyone recreate this error?The problem has something to do with the call to graphviz's agedge function, it seems to not like the format of the  parameter; when I change (line 480 of ):toit no longer fails (but loses the key labels). (so that  parameter values are retained) - Nothing I try seems to work.From , it appears that the  agedge function (which I can't see as it's in a .pyd binary) has the following format:where the  is the key.I cannot work out why it won't accept a  dtype as in the initial error.Note versions:networkx - 1.11,\npygraphviz - 1.3.1\n (installed from )\nPython 2.7 (32bit - isntalled via python(x,y)) on Windows 7 (64-bit), GraphViz - 2.38 I have also seen this issue crop up in these questions:I have tried adjusting the  input to the agedge function to a number of variants of char arrays (e.g.  (ct is ctypes module) based on ). This changes the error to:I can get it to run (but not return a multigraph) if I do this:In  replacing the linewithI don't know why just casting to a string  does not work.Found the function here - The source of the error is within  pygraphviz file, graphviz_wrap.c, line 3921:Or, it's within , graphviz.i, line 68. Either way, it seems like the error string \"agedge: no key\" is returned if  fails for any reason... Perhaps it's something to do with SWIG.Try changing the variable name from \"key\" to something else like \"temp_key\".  I mean that it is possible that you (or any previously imported module) has declared a non-string type \"key\" variable before...? Apparently if running :fails but running:give you no issue, it could only be relative to the \"key\" variable type.. did you try this: or\nIntegrate str()/unicode() in your orginal code give:both (str & unicode version) works fine on Linux.Best regardsMy environment is python2.7 32 bit on Windows 7 64-bit and I am doing a project that uses the pygraphviz/graphviz to draw a FSM. During use I also came across the same errorThis project has a closed issue that states that the error only shows up when you have transitions where the source state is the same as the destination state. Or in simpler terms when a state points to itself.I can confirm that I get error when there are states pointing to themselves, while if I create a graph that doesn't have any such transitions it works and produces an image file (.png) correctly in my case.P.S. I don't think this warrants a full answer because it is not a complete solution. But unfortunately I don't have enough reputation points to comment."},
{"body": " I learned that the behavior on  was dependent on the  in use by matplotlib.  At that time I was looking for a way to keep  from deleting the drawing elements of a figure, concluding that by switching from Qt4Agg to TkAgg, the drawing elements would be preserved even if the figures were closed.  I would like to increase my superpowers by learning, if possible, how to configure the Qt4agg backend to behave as desired.Instead of trying to exercise extra control over the backend, just create the figures and keep references to them yourself, as shown in the highest-rated answer to your first question. The problem with trying to manipulate the backend, as you are asking to do here, is that the backend is not  to hand your figures back to you intact once you have looked at them; that's not normally the backend's job. See:\n"},
{"body": "I know how to compile CPython file to exe using cx_freeze but is it possible to compile a simple program using PyPy to Exe ?There is no ready-made way or tutorial on how to do create an EXE from a program using the PyPy interpreter, as far as i know.  And it's not exactly trivial to get things going, i am afraid.In principle, there are two ways to consider for using PyPy's translations to get a EXE file, either using the PyPy interpreter or writing your own RPython program (The PyPy interpreter is itself an RPython program, i.e. using a restricted subset of Python).If you program uses a restricted subset of RPython and no dependencies, you could look into using the translate script in  where you'll also find a lot of target*.py files.  Take one and modify it for your purposes. You might first want to play with translating python functions starting from here:If you program is an application and depends on external packages, you should first try to make sure that your program works on pypy at all - not all external libraries are supported.  You might then look into modifying the targetpypystandalone script to load your application modules.  If in doubt, try to get some help on the pypy-dev mailing list or the #pypy channel on irc.freenode.net.This is a py2exe solution that might work for you: "},
{"body": "I need to open a file for reading and writing. If the file is not found, it should be created. It should also be treated as a binary for Windows. Can you tell me the file mode sequence I need to use for this?I tried 'r+ab' but that doesn't create the files if they are not found.ThanksThe mode is  the  is implied and 'a'ppend and ('w'rite '+' 'r'ead) are redundant. Since the CPython (i.e. regular python)  is based on the C stdio  type, here are the relevant lines from the fopen(3) man page:With the \"b\" tacked on to make DOS happy. Presumably you want to do something like this:should work. It opens a binary file in append/update mode."},
{"body": "I have two classes A and B and A is base class of B.I read that all methods in Python are virtual. So how do I call a method of the base because when I try to call it, the method of the derived class is called as expected?Using :Two ways:"},
{"body": "What happened is that I (by mistake) saved a dictionary with the command  (no error messages shown) and now I need to recover the data in the dictionary. When I load it with  it has type () and is 0-d, so it is not a dictionary any more and I can't access the data in it, 0-d arrays are not index-able so doing something like doesn't work. I also triedbut that didn't work either.Why numpy didn't warn me when I saved the dictionary with ?Is there a way to recover the data?Thanks in advance!Use  to obtain the array element as a Python scalar.0-d arrays can be indexed using the empty tuple:"},
{"body": "I need to search an ObjectId with python using pymongo but I always get this error. Any ideas how to search? I use pymongo 2.4.1."},
{"body": "In development, it's a bit of a hassle to run the  as well as the Django development server. Is it possible to, for example, ask  to run tasks synchronously during development? Or something similar?Yes you can do this by setting  in your settings.\nThere's also a custom Django test runner in django-celery that helps with CELERY_ALWAYS_EAGER.\nRead more about using Celery with Django on .In version  of Celery  setting was replaced by  in Django  or  natively in .Since the numerous changes in the celery configuration from version 3.x to 4.x are spread over many lines, I suggest to use the built-in settings migration tool.source:"},
{"body": "I am attempting to put together a simple c++ test project that uses python 3.2. The project builds fine but Py_Initialize raises a fatal error:The OS is 32bit Vista.The python version used is a python 3.2 debug build, built from sources using VC++ 10.The python_d.exe file from the same build runs without any problems.Could someone explain the problem and how to fix it? My own google-fu fails me.[EDIT]After going through the python source code I've found that, as the error says, no codec search functions have been registered. Both  and  are as they should be. It's just that nowhere in the code are any of these functions called.I don't really know what this means as I still have no idea when and from where these functions should have been called. The code that raises the error is entirely missing from the source of my other python build (3.1.3).[EDIT]Answered my own question below.Check PYTHONPATH and PYTHONHOME system variable and make sure it doesn't points to Python 2.xParts of this have been mentioned before, but in a nutshell this is what worked for my environment where I have multiple Python installs and my global OS environment set-up to point to a  install than the one I attempt to work with when encountering the problem.Make sure your (local or global) environment is  set-up to point to the install you aim to work with, e.g. you have two (or more) installs of, let's say a python27 and python33 (sorry these are windows paths but the following should be valid for equivalent UNIX-style paths just as well, please let me know about anything I'm missing here (probably the DLLs path might differ)):Now, if you intend to work with your python33 install but your global environment is pointing to python27, make sure you update your environment as such (while  and   be optional (e.g. if you temporarily work in a local shell)):Note, that you might need/want to append any other library paths to your  if required by your development environment, but having your ,  and  properly set-up is of prime importance.Hope this helps.So, for some reason the python dll fails to locate the encodings module. The python.exe executable apparently finds it because it has the expected relative path. Modifying the search path works.The reason for all of this? Don't know but at least it works. I highly suspect a typo on my part somewhere, that's usually the reason for odd bugs it seems.I just ran into the exact same problem (same Python version, OS, code, etc).You just have to copy Python's Lib/ directory in your program's working directory ( on VC it's the directory where the .vcproj is )There appears to be something going wrong with the release build either failing to include the appropriate codecs or else misidentifying the codec to use for system APIs. Since the  executable is working, what does that return for ? (Use the C API to call that between your Initialize/Finalize calls)From python3k, the startup need the encodings module, which can be found in PYTHONHOME\\Lib directory. \nIn fact, the  do the init and import the encodings module.\nMake sure PYTHONHOME\\Lib is in sys.path and check the encodings module is there.I had this issue with python 3.5, anaconda 3, windows 7 32 bit. I solved it by moving my pythonX.lib and pythonX.dll files into my working directory and calling before initialize so that it could find the headers that it needed, where my path was to \"...\\Anaconda3\\\". The extra step of calling Py_SetPythonHome was required for me or else I'd end up getting other strange errors where python import files."},
{"body": "Comparing boolean values with  works in Python. But when I apply the boolean  operator, the result is a syntax error:Why is this a syntax error? I would expect  to be an expression that returns a boolean value, and  to be valid syntax wherever  is an expression with valid syntax.It has to do with  (the interpreter thinks you're comparing True to not, since  has a higher precedence than ). You need some parentheses to clarify the order of operations:In general, you can't use  on the right side of a comparison without parentheses. However, I can't think of a situation in which you'd ever need to use a  on the right side of a comparison.It's just a matter of operator precedence.Try:Have a look in , you'll find that  binds tigher than , and thus  is parsed as  which is clearly an error.I think what you are looking for is \"and not\".  This gives you the results you are looking towards.  If your comparing booleans what you have is a compound boolean expression, here is an example website .  "},
{"body": "I'd like to be able to do something like this:Other module(s):But I don't know how to reference a modules variable from a function defined in the module.Is this possible? Or am I going to need to put a global declaration in every place I wan't to use this.\nOr am I going at this completely wrong?Just change Global variables are read-only from sibling methods. More accurately unless a variable is specified as global, Python consider it as local, but a read access to a local variable name will reach module-level scope if the name is not present in local scope.See also  and the  for more details about the  statementYou seem to mostly have it. You are missing only the fact that \"module-level\" variables are called global in Python. (They are not truly global, but only global to the module they are declared in, in other words.)In any function where you  a global variable (you want to make the name refer to a different object), it must be declared global. So your  function needs a  at the beginning. If you are only using the value of a global variable, or if it is a mutable type such as a list and you are modifying it, but not changing the object that the name points to, you needn't declare it global.The  statement is, as you have discovered, how you can import a module-level variable from one module into another."},
{"body": "I receive a dictionary as input, and would like to to return a dictionary whose keys will be the input's values and whose value will be the corresponding input keys. Values are unique.For example, say my input is:I would like my output to be:To clarify I would like my result to be the equivalent of the following:Any neat Pythonian way to achieve this?ThanksFrom Python 2.7 on, including 3.0+, there's an arguably shorter, more readable version:You could try:Beware that you cannot 'reverse' a dictionary ifSee  on the python mailing list for a discussion on the subject.You can make use of :or even better, but only works in Python 3:Suggestion for an improvement for Javier answer :Instead of  you can write just , because if you go through dictionary with an iterator, it will return the keys of the relevant dictionary.Ex. for this behavior : Another way to expand on 's response is to actually use the  function.In essence, your dictionary is iterated through (using ) where each item is a key/value pair, and those items are swapped with the  function. When this is passed to the  constructor, it turns them into value/key pairs which is what you want.If you're using Python3, it's slightly different:Using :- returns a list of tuples of .  goes through elements of the list and applies  to each its element (tuple) to reverse it, so each tuple becomes  in the new list spitted out of map. Finally,  makes a dict from the new list. Adding an in-place solution:In Python3, it is critical that you use  because  returns a  of the keys. If you are using Python2,  is enough."},
{"body": "So I learned that I can use DataFrame.groupby without having a MultiIndex to do subsampling/cross-sections.On the other hand, when I have a MultiIndex on a DataFrame, I still need to use DataFrame.groupby to do sub-sampling/cross-sections.So what is a MultiIndex good for apart from the quite helpful and pretty display of the hierarchies when printing?Hierarchical indexing (also referred to as \u201cmulti-level\u201d indexing) was introduced in the pandas 0.4 release. This opens the door to some quite sophisticated data analysis and manipulation, especially for working with higher dimensional data. In essence, it enables you to effectively store and manipulate arbitrarily high dimension data in a 2-dimensional tabular structure (DataFrame), for example. Imagine constructing a dataframe using  like this:-This  is simply a data structure of two dimensions But we can imagine it, looking at the output, as a 3 dimensional data structure.A.k.a.: \"effectively store and manipulate arbitrarily high dimension data in a 2-dimensional tabular structure\"This is not just a \"pretty display\". It has the benefit of easy retrieval of data since we now have a hierarchal index.For example.will give us a new data frame only for the group of data belonging to \"one\".And we can narrow down our data selection further by doing this:-And of course, if we want a specific value, here's an example:-So if we have even more indexes (besides the 2 indexes shown in the example above), we can essentially drill down and select the data set we are really interested in without a need for .We can even grab a cross-section (either rows or columns) from our dataframe...By rows:-By columns:-"},
{"body": "I'm wondering what's the best way -- or if there's a simple way with the standard library -- to convert a URL with Unicode chars in the domain name and path to the equivalent ASCII URL, encoded with domain as IDNA and the path %-encoded, as per RFC 3986.I get from the user a URL in UTF-8. So if they've typed in  I get  in Python. And what I want out is the ASCII version: .What I do at the moment is split the URL up into parts via a regex, and then manually IDNA-encode the domain, and separately encode the path and query string with different  calls.Is this correct? Any better suggestions? Is there a simple standard-library function to do this?the code given by MizardX isnt 100% correct. This example wont work:example.com/folder/?page=2check out django.utils.encoding.iri_to_uri() to convert unicode URL to ASCII urls.there's some RFC-3896  work underway (e.g. as part of the Summer Of Code) but nothing in the standard library yet AFAIK -- and nothing much on the  side of things either, again AFAIK.  So you might as well go with MizardX's elegant approach.Okay, with these comments and some bug-fixing in my own code (it didn't handle fragments at all), I've come up with the following  function -- returns a canonical, ASCII form of the URL:You might use  instead, but otherwise you seem to have a very straightforward solution, there.(You can access the domain and port separately by accessing the returned value's named properties, but as port syntax is always in ASCII it is unaffected by the IDNA encoding process.)"},
{"body": "If I import a module defining a class of the same name belonging to a package, it is imported as a Class, not a Module because of the __init__.py of the parent package. See  for details. In Python shell or ipython shell, if I doMyModule is always imported as Class thus I can not reload it (reload() works only for modules). Run again does not seem to update the Class definition. Could anyone suggest a way to update the class in python shell? ps. without restarting the python interpreter. . Just in case you have the code in hand and want to test it: I am actually talking about BioPython, and I am working on Bio.PDB.PDBParser. I have an ipython shell (v0.10) and edit PDBParser.py. Just got no way to reload it in ipython.so here is what I did:I could not see the printed text. The changes were not applied somehow. On Python 3 only, import the  function:On both Python 2.x, and 3.x, you can then simply call  on the module:However, instances of the old class will not be updated (there's simply no code that describes the update mechanism).I finally found the answer:after editing  file, to reload the class  in the file , one needs to:I have one myfile.py file which contains one class MyClassTo import just do:To reload:This is very useful while building modules.\none can put these inside a function and just call the functionIt works for me using python 3.5.2:There are thee ways to solve this:Then you can write:and it works.You can use a magic function:"},
{"body": "I am trying to figure out how many times a string occurs in a string. For example:Say the string I want to find is 123. Obviously it occurs twice in nStr but I am having trouble implementing this logic into Python. What I have got at the moment:The answer it should return is 2. I'm stuck in an infinite loop at the moment.I was just made aware that count is a much better way to do it but out of curiosity, does anyone see a way to do it similar to what I have already got?Use :A working version of your code:The problem with  and these methods shown here is the case of overlapping substrings.For example:  returns 2If you want it to return 4 [] you might try something like this:Don't know if there's a better way of doing it, but posting just to clarify the way  works.We can say that the substring 'pattern' appears len(n) times in 'string'.string.count(substring) is not useful in case of overlapping.My approach:"},
{"body": "I've got a string, a signature, and a public key, and I want to verify the signature on the string. The key looks like this:I've been reading the pycrypto docs for a while, but I can't figure out how to make an RSAobj with this kind of key. If you know PHP, I'm trying to do the following:Also, if I'm confused about any terminology, please let me know.The data between the markers is the base64 encoding of the ASN.1 DER-encoding of a PKCS#8 PublicKeyInfo containing an PKCS#1 RSAPublicKey.That is a lot of standards, and you will be best served with using a crypto-library to decode it (such as M2Crypto as ). Treat the following as some fun info about the format:If you want to, you can decode it like this:Base64-decode the string:This is the DER-encoding of:For a 1024 bit RSA key, you can treat  as a constant header, followed by a 00-byte, followed by the 128 bytes of the RSA modulus. After that 95% of the time you will get , which signifies a RSA public exponent of 0x10001 = 65537.You can use those two values as  and  in a tuple to construct a RSAobj.Use . Here's how to verify for RSA and any other algorithm supported by OpenSSL:A public key contains both a modulus(very long number, can be 1024bit, 2058bit, 4096bit) and a public key exponent(much smaller number, usually equals one more than a two to some power). You need to find out how to split up that public key into the two components before you can do anything with it. I don't know much about pycrypto but to verify a signature, take the hash of the string. Now we must decrypt the signature. Read up on ; the formula to decrypt a signature is . The last step is to check if the hash you made and the decrypted signature you got are the same.Maybe this isn't the answer you're looking for, but if all you need is to turn the key into bits, it looks like it's Base64 encoded. Look at the  module (I think) in the standard Python library.  points out in the comments that  is hard-coded to use MD5, in which case ezPyCryto can't help Andrew unless he wades into its code. I defer to : consider .More on the DER decoding. DER encoding always follows a TLV triplet format: (Tag, Length, Value) Tag basically tells how to interpret the bytes data in the Value field. ANS.1 does have a type system, e.g. 0x02 means integer, 0x30 means sequence (an ordered collection of one or more other type instances) Length presentation has a special logic: For example, say I want to encode a number of 256 bytes long, then it would be like this Now looking at your example It interprets as just what Rasmus Faber put in his replyUsing M2Crypto, the above answers does not work. Here is a tested example.And that's about itI try the code given by joeforker but it does not work.\nHere is my example code and it works fine."},
{"body": "I am trying to create a download progress bar in python using the urllib2 http client. I've looked through the API (and on google) and it seems that urllib2 does not allow you to register progress hooks. However the older deprecated urllib does have this functionality.Does anyone know how to create a progress bar or reporting hook using urllib2? Or are there some other hacks to get similar functionality?Here's a fully working example that builds on Anurag's approach of chunking in a response. My version allows you to set the the chunk size, and attach an arbitrary reporting function:Why not just read data in chunks and do whatever you want to do in between, e.g. run in a thread, hook into a UI, etc etc: has built-in support for progress notification."},
{"body": "How do you store a password entered by the user in memory and erase it securely after it is no longer need?To elaborate, currently we have the following code:After calling the  method, what do we need to do to fill the area of memory that  contains password with garbled characters so that someone cannot recover the password by doing a core dump?There is a similar question, however it is in Java and the solution uses character arrays:\nCan this be done in Python?Python doesn't have that low of a level of control over memory.  Accept it, and move on.  The  you can do is to  after calling  so that no references to the password string object remain.  Any solution that purports to be able to do more than that is only giving you a false sense of security. Python string objects are immutable; there's no direct way to change the contents of a string after it is created.  you were able to somehow overwrite the contents of the string referred to by  (which is technically possible with stupid ctypes tricks), there would still be other copies of the password that have been created in various string operations:You would somehow have to get references to all of those strings and overwrite their memory as well.There actually -is- a way to securely erase strings in Python; use the memset C function, as per If you don't need the mail object to persist once you are done with it, I think your best bet is to perform the mailing work in a subprocess (see the  module.) That way, when the subprocess dies, so goes your password.This could be done using numpy chararray:You would have to determine the maximum size of password, but this should remove the data when it is overwritten.Store the password in a list, and if you just set the list to null, the memory of the array stored in the list is automatically freed.EDIT: removed the bad advice...You can also use arrays like the java example if you like, but just overwriting it should be enough."},
{"body": "I created a module named  that provides classes and functions I often use in Python.\nSome of them need imported features. What are the pros and the cons of importing needed things inside class/function definition? Is it better than  at the beginning of a module file? Is it a good idea?It's the most common style to put  import at the top of the file. PEP 8 recommends it, which is a good reason to do it to start with. But that's not a whim, it has advantages (although not critical enough to make everything else a crime). It allows finding all imports at a glance, as opposed to looking through the whole file. It also ensures everything is imported before any other code (which may depend on some imports) is executed. s are usually easy to resolve, but they can be annoying.There's no (significant) namespace pollution to be avoided by keeping the module in a smaller scope, since all you add is the actual module (). Inside functions, you'd import again on every call (not really harmful since everything is imported once, but uncalled for)., the Python style guide, states that:Of course this is no hard and fast rule, and imports can go anywhere you want them to. But putting them at the top is the best way to go about it. You can of course import within functions or a class.But note you cannot do this:Because:I believe that it's best practice (according to some PEP's) that you keep import statements at the beginning of a module. You can add import statements to an  file, which will import those module to all modules inside the package.So...it's certainly something you can do the way you're doing it, but it's discouraged and actually unnecessary.While the other answers are mostly right, there is a reason why python allows this.It is not smart to import redundant stuff which isn\u2019t needed. So, if you want to e.g. parse XML into an element tree, but don\u2019t want to use the slow builtin XML parser if lxml is available, you would need to check this the moment you need to invoke the parser.And instead of memorizing the availability of lxml at the beginning, I would prefer to  importing and using ,  it\u2019s not there, in which case I\u2019d fallback to the builtin  module.Like flying sheep's answer, I agree that the others are right, but I put imports in other places like in  routines and function calls when I am DEVELOPING code. After my class or function has been tested and proven to work with the import inside of it, I normally give it its own module with the import following PEP8 guidelines. I do this because sometimes I forget to delete imports after refactoring code or removing old code with bad ideas. By keeping the imports inside the class or function under development, I am specifying its dependencies should I want to copy it elsewhere or promote it to its own module...Only move imports into a local scope, such as inside a function definition, if it\u2019s necessary to solve a problem such as avoiding a circular import or are trying to reduce the initialization time of a module. This technique is especially helpful if many of the imports are unnecessary depending on how the program executes. You may also want to move imports into a function if the modules are only ever used in that function. Note that loading a module the first time may be expensive because of the one time initialization of the module, but loading a module multiple times is virtually free, costing only a couple of dictionary lookups. Even if the module name has gone out of scope, the module is probably available in sys.modules."},
{"body": "I have a Django model with a start and end date range. I want to enforce validation so that no two records have overlapping date ranges. What's the simplest way to implement this so that I don't have to repeat myself writing this logic?e.g. I don't want to re-implement this logic in a Form  a   an admin form  the model's overridden .As far as I know, Django doesn't make it easy to globally enforce these types of criteria. Googling hasn't been very helpful, since \"model validation\" typically refers to validating specific model fields, and not the entire model contents, or relations between fields.The basic pattern I've found useful is to put all my custom validation in  and then simply call  (which calls  and a few other methods) from inside , e.g.:This isn't done by default, as explained , because it interferes with certain features, but those aren't a problem for my application.I would override the  method on the model. To make sure you ignore the current object when validating, you can use the following: will automatically call this for you through a , which you can use manually too.PPR has a nice discussion of a simple, correct .I think you should use this:\nJust define clean() method in your model like this: (example from the docs link)"},
{"body": "I have this simple model of Author - Books and can't find a way to make firstName and lastName a composite key and use it in relation. Any ideas?The problem is that you have defined each of the dependent columns as foreign keys separately, when that's not really what you intend, you of course want a composite foreign key.  Sqlalchemy is responding to this by saying (in a not very clear way), that it cannot guess which foreign key to use ( or ).  The solution, declaring a composite foreign key, is a tad clunky in declarative, but still fairly obvious:The important thing here is that the  definitions are gone from the individual columns, and a  is added to a  class variable.  With this, the  defined on  works just right."},
{"body": "What are the differences in performance and behavior between using Python's native  function and NumPy's ?  works on NumPy's arrays and  works on Python lists and they both return the same effective result (haven't tested edge cases such as overflow) but different types. I think my practical question here is would using  on a list of Python integers be any faster than using Python's own ?Additionally, what are the implications (including performance) of using a Python integer versus a scalar ? For example, for , is there a behavior or performance difference if the type of  is a Python integer or a ? I am curious if it is faster to use a NumPy scalar datatype such as  for a value that is added or subtracted a lot in Python code.For clarification, I am working on a bioinformatics simulation which partly consists of collapsing multidimensional s into single scalar sums which are then additionally processed. I am using Python 3.2 and NumPy 1.6.Thanks in advance!I got curious and timed it.  seems much faster for numpy arrays, but much slower on lists.Result when :Result when :I am using Python 2.7.2 and Numpy 1.6.1Numpy should be much faster, especially when your data is already a numpy array.Numpy arrays are a thin layer over a standard C array. When numpy sum iterates over this, it isn't doing type checking and it is very fast. The speed should be comparable to doing the operation using standard C. In comparison, using python's sum it has to first convert the numpy array to a python array, and then iterate over that array. It has to do some type checking and is generally going to be slower.The exact amount that python sum is slower than numpy sum is not well defined as the python sum is going to be a somewhat optimized function as compared to writing your own sum function in python.Note that Python sum on multidimensional numpy arrays will only perform a sum along the first axis:This is an extension to the the . From that answer you can see that  performs faster for  objects, whereas  performs faster for  objects. To expand upon that:On running  for an  object   for a  object, it seems that they perform neck to neck. Above,  is a  bit faster than , although, at times I've seen   timings to be , too. But mostly, it's . "},
{"body": "I am trying to convert time-stamps of the format \"2012-07-24T23:14:29-07:00\" \nto datetime objects in python using strptime method. The problem is with the time offset at the end(-07:00). Without the offset i can successfully doBut with the offset i triedBut it gives a Value error saying \"z\" is a bad directive.Any ideas for a work around?The Python 2  function indeed does not support the  format for timezones (because the underlying  doesn't support it). You have two options:Quick demo on the command prompt:You could also upgrade to Python 3.2 or newer, where timezone support has been improved to the point that  would work, provided you remove the last  from the input, and the  from before the :In Python 3.2+:Note: it doesn't support  in the  part (both formats are allowed by ). See .On older Python versions such as Python 2.7, you could parse the utc offset manually:where .(note: I have to stick to python 2.7 in my case)I have had a similar problem parsing commit dates from the output of  which actually isn't the ISO8601 format (hence the addition of  in a later version).Since I am using  I can leverage the utilities there.Instead of  you could use your own regular expression."},
{"body": "I would like to zoom a portion of data/image and plot it inside the same figure. It looks something like this figure.Is it possible to insert a portion of zoomed image inside the same plot. I think it is possible to draw another figure with subplot but it draws two different figures. I also read to add patch to insert rectangle/circle but not sure if it is useful to insert a portion of image into the figure. I basically load data from the text file and plot it using a simple plot commands shown below. I found one related example from matplotlib image gallery  but not sure how it works. Your help is much appreciated.Playing with runnable code is one of the\nfastest ways to learn Python.So let's start with the .Given the comments in the code, it appears the code is broken up into 4 main stanzas.\nThe first stanza generates some data, the second stanza generates the main plot,\nthe third and fourth stanzas create the inset axes.We know how to generate data and plot the main plot, so let's focus on the third stanza:Copy the example code into a new file, called, say, .What happens if we change the  to ?Run the script:You'll find the \"Probability\" inset moved to the left.\nSo the  function controls the placement of the inset.\nIf you play some more with the numbers you'll figure out that (.35, .6) is the\nlocation of the lower left corner of the inset, and (.2, .2) is the width and\nheight of the inset. The numbers go from 0 to 1 and (0,0) is the located at the\nlower left corner of the figure.Okay, now we're cooking. On to the next line we have:You might recognize this as the , but\nif not, changing the number 400 to, say, 10, will produce an image with a much\nchunkier histogram, so again by playing with the numbers you'll soon figure out\nthat this line has something to do with the image inside the inset.You'll want to call  here.The line \nobviously generates the text above the inset.Finally we come to . There are no numbers to play with,\nso what happens if we just comment out the whole line by placing a  at the beginning of the line:Rerun the script. Oh! now there are lots of tick marks and tick labels on the inset axes.\nFine. So now we know that  removes the tick marks and labels from the axes .Now, in theory you have enough information to apply this code to your problem.\nBut there is one more potential stumbling block: The matplotlib example uses\n\nwhereas you use . says \nis the recommended way to use matplotlib when writing scripts, while \n is for use in interactive sessions. So you are doing it the right way, (though I would recommend using  instead of  too).So how do we convert the matplotlib example to run with ?Doing the conversion takes some experience with matplotlib. Generally, you just\nadd  in front of bare names like  and , but sometimes the\nfunction come from numpy, and sometimes the call should come from an axes\nobject, not from the module . It takes experience to know where all these\nfunctions come from. Googling the names of functions along with \"matplotlib\" can help.\nReading example code can builds experience, but there is no easy shortcut. So, the converted code becomesAnd you could use it in your code like this:The simplest way is to combine \"zoomed_inset_axes\" and \"mark_inset\", whose description and \nrelated examples could be found here:\nThe nicest way I know of to do this is to use mpl_toolkits.axes_grid1.inset_locator (part of matplotlib).There is a great example with source code here: ."},
{"body": "In python classes, the @property is a nice decorator that avoids using explicit setter and getter functions. However, it comes at a cost of an overhead 2-5 times that of a \"classical\" class function. In my case, this is quite OK in the case of setting a property, where the overhead is insignificant compared to the processing that needs to be done when setting. However, I need no processing when getting the property. It is always just \"return self.property\". Is there an elegant way to use the setter but not using the getter, without needing to use a different internal variable?Just to illustrate, the class below has the property \"var\" which refers to the internal variable \"_var\". It takes longer to call \"var\" than \"_var\" but it would be nice if developers and users alike could just use \"var\" without having to keep track of \"_var\" too.For those interested, on my pc calling \"var\" takes 204 ns on average while calling \"_var\" takes 44 ns on average.Don't use a  in this case. A  object is a data descriptor, which means that any access to  will invoke that descriptor and Python will never look for an attribute on the instance itself.You have two options: use the  hook or build a descriptor that only implements .Now normal attribute lookups are used when   but when assigning to  the  method is invoked instead, letting you intercept  and adjust it as needed.Demo:A setter descriptor would only intercept variable assignment:Note how we need to assign to the instance  attribute to prevent invoking the setter again.Demo: python docs: output:"},
{"body": "There are several ways to iterate over a result set. What are the tradeoff of each?The canonical way is to use the built-in cursor iterator.You can use  to get all rows at once.It can be convenient to use this to create a Python list containing the values returned:This can be useful for smaller result sets, but can have bad side effects if the result set is large.If you know there's a single row being returned in the result set you can call  to get the single row.Finally, you can loop over the result set fetching one row at a time.  In general, there's no particular advantage in doing this over using the iterator.My preferred way is the cursor iterator, but setting first the arraysize property of the cursor. In this example, cx_Oracle will fetch rows from Oracle 256 rows at a time, reducing the number of network round trips that need to be performedThere's also the way  seems to do it... From what I gather, it seems to create dictionary-like row-proxies to map key lookup into the memory block returned by the query. In that case, fetching the whole answer and working with a similar proxy-factory over the rows seems like useful idea. Come to think of it though, it feels more like Lua than Python.Also, this should be applicable to all  interfaces, not just Oracle, or did you mean just  using ?"},
{"body": "I have a problem which requires a reversable 1:1 mapping of keys to values. That means sometimes I want to find the value given a key, but at other times I want to find the key given the value. Both keys and values are guaranteed unique. The obvious solution is to simply invert the dictionary every time I want a reverse-lookup: Inverting a dictionary is very easy, .The other alternative is to make a new class which unites two dictionaries, one for each kind of lookup. That would most likely be fast but would use up twice as much memory as a single dict. So is there a better structure I can use?Not really. Have you measured that? Since both dictionaries would use references to the  as keys and values, then the memory spent would be just the dictionary structure. That's a lot less than  and is a fixed ammount regardless of your data size.What I mean is that the actual data wouldn't be copied. So you'd spend little extra memory.Example:Only a single copy of the \"really big\" string exists, so you end up spending just a little more memory. That's generally affordable.I can't imagine a solution where you'd have the key lookup speed when looking by value, if you don't spend  enough memory to store a reverse lookup hash table (which is exactly what's being done in your \"unite two s\" solution).Not really, since they would just be holding two references to the same data. In my mind, this is not a bad solution. Have you considered an in-memory database lookup? I am not sure how it will compare in speed, but lookups in relational databases can be  fast.Here is my own solution to this problem: The goal is to make it as transparent to the user as possible. The only introduced significant attribute is . subclasses from  - I know that , but I think I have the common use cases covered. The backend is pretty simple, it () keeps a weakref to a 'partner'  () which is its inverse. When  is modified  is updated accordingly as well and vice versa.From the docstring:When I get some free time, I intend to make a version that doesn't store things twice. No clue when that'll be though :)Assuming that you have a key with which you look up a more complex mutable object, just make the key a property of that object. It does seem you might be better off thinking about the data model a bit.\"We can guarantee that either the key or the value (or both) will be an integer\"That's weirdly written -- \"key or the value (or both)\" doesn't feel right.  Either they're all integers, or they're not all integers.    It sounds like they're all integers.Or, it sounds like you're thinking of replacing the target object with an integer value so you only have one copy referenced by an integer.  This is a false economy.  Just keep the target object.  All Python objects are -- in effect -- references.  Very little actual copying gets done.Let's pretend that you simply have two integers and can do a lookup on either one of the pair.  One way to do this is to use heap queues or the bisect module to maintain ordered lists of integer key-value tuples.See See You have one heapq  tuples.  Or, if your underlying object is more complex, the ) tuples.You have another heapq  tuples.  Or, if your underlying object is more complex,  tuples.An \"insert\" becomes two inserts, one to each heapq-structured list.A key lookup is in one queue; a value lookup is in the other queue.  Do the lookups using .  How about using sqlite? Just create a :memory: database with a two-column table. You can even add indexes, then query by either one. Wrap it in a class if it's something you're going to use a lot. It so happens that I find myself asking this question all the time (yesterday in particular).  I agree with the approach of making two dictionaries.  Do some benchmarking to see how much memory it's taking.  I've never needed to make it mutable, but here's how I abstract it, if it's of any use:"},
{"body": "In my application I encountered the following and was surprised by the results: (both integers).what does this means?For the actual values, i.e. , the result is roughly .Your result using integer division is being rounded down toward the more negative value of . (This is also known as \"Floor division\")This is why you will get the somewhat perplexing answers of: This is \"fixed\" in Python 3, where the result of  would be . So if you have no reason to be using Python 2, you should upgrade. ;)In Python 3, if you still want integer division, you can use the  operator. This will give you the same answer as  would in Python 2.Here's a Python Enhancement Proposal on the subject: to have python automatically convert integer division to float, you can use:now:this feature is not in the standard python 2 not to break existing code that relied on integer division.\nHowever, this is the default behavior for python 3.Python always does the \"floor division\" for both negative numbers division and positive numbers division.That isBut sometime we need 1/-10 to be 0I figure out it can be done by using the float division first then cast result to int, e.g.That works fine for me, no need to import the future division or upgrade to Python 3Hope it can help you~When both values are integers when dividing Python uses Floor division.In python,  operator is for integer division. You can look at it as float division followed by a  operation.For example,  "},
{"body": "Is try-catch the only method to do that?If  is your deque, useThis will implicitly convert  to a , which yields  if the deque contains any items and  if it is empty."},
{"body": "I am using PyDev for development and unit-testing of my Python application.\nAs for unit-testing, everything works great behalf the fact that content logged to any logging. Logger is not captured by the \"Captured output\" of PyDev.I already forward everything logged to the standard output like this:Nevertheless the \"Captured output\" does not display stuff logged to loggers.Here an example unittest-script: \nThe console output is:But the  for the test is:Does anybody knows how to capture everything is logged to a  during the execution of this test?The issue is that the  runner replaces / before the testing starts, and the  is still writing to the original . If you assign the 'current'  to the handler, it should work (see the code below).Although, a better approach would be adding/removing the handler during the test:I grew tired of having to manually add  to all s, so I subclassed  with some ing:Now your test case can simply inherit from , i.e.  instead of  and you're done. Alternatively, you can add the  line and define the  either in the test or a slightly modified .I'd suggest using a LogCapture and testing that you really are logging what you expect to be logging:I came across this problem also. I ended up subclassing StreamHandler, and overriding the stream attribute with a property that gets sys.stdout. That way, the handler will use the stream that the unittest.TestCase has swapped into sys.stdout:You can then setup the logging handler before running tests like so (this will add the custom handler to the root logger):If, like me, you have your tests in separate modules, you can just put a line after the imports of each unit test module that will make sure the logging is setup before tests are run:This might not be the cleanest approach, but it's pretty simple and worked well for me."},
{"body": "I'm currently using hbase with my Python apps and wanted to try out Amazon . Is there a way to use Python to read, write and query data?You can use boto: docs: api reference: Another alternative is . PynamoDB provides an ORM like interface to DynamoDB and supports  Python 2 and Python 3. The entire DynamoDB API is supported by PynamoDB - including global and local secondary indexes, batch operations, binary attributes, queries, scans, etc. Disclaimer: I wrote PynamoDB.Disclaimer: I'm the current maintainerYou can use  Python library. It's a simple/tiny abstraction layer that allows you to . It also features a transaction engine.For advanced tasks such as table management it is still better to directly use  (which we rely on, anyway)."},
{"body": "I have a root-ish directory containing multiple subdirectories, all of which contain a file name data.txt. What I would like to do is write a script that takes in the \"root\" directory, and then reads through all of the subdirectories and reads every \"data.txt\" in the subdirectories, and then writes stuff from every data.txt file to an output file.Here's a snippet of my code:My dosomething() part -- I've tested and confirmed for it to work if I am running that part just for one file. I've also confirmed that if I tell it to print the file instead (the commented out line) the script prints out 'data.txt'.Right now if I run it Python gives me this error: I'm not sure why it can't find it -- after all, it prints out data.txt if I uncomment the 'print file' line. What am I doing incorrectly?You need to use absolute paths, your  variable is just a local filename without a directory path. The  variable is that path:"},
{"body": "Is it possible to embed rendered HTML output into iPython output?One way is to useor (IPython multiline cell alias)Which return a formatted link, butHow can I overcome thes shortcomings and make iPython output a bit more interactive?This seems to work for me:The trick is to wrap it in \"display\" as well.Source: "},
{"body": "I'm trying to get gettext to work in Django on my OSX LeopardIn Terminal I get the same error, unless I put this in my bash profile:But then I get this error:After installing, try linking gettext. This solved the problem for me.I think you need to install gettext. Poedit includes only some of the programs provided by the gettext package.Probably the easiest way to install (not only) gettext is via . Once you have homebrew installed, run . After that, make sure that the programs in  are on your . Note that you need to have Xcode installed for homebrew to work as it usually installs packages from source (you can get Xcode for Lion for free from the Mac App Store). Edit: I overlooked that you don't use Lion. For Snow Leopard, you can get XCode from the App Store for $5. XCode For Leopard is I think on the installation disk.. It's better to modify virtual environment's PATH instead of force-linking. So,"},
{"body": "I am self teaching myself python 2.7. I  have some experience in using BATCH, which has a GOTO statement.  How do I do that in python? For example, suppose I want to jump from line 5 to line 18.I realize there have been previous questions regarding this topic, but I have not found them sufficiently informative or, are too high level in python for my current understanding.s are universally reviled in computer science and programming as they lead to very unstructured code.Python (like almost every programming language today) supports  which controls flow using if/then/else, loop and subroutines.The key to thinking in a structured way is to understand how and why you are branching on code.For example, lets pretend Python had a  and corresponding  statement . Look at the following code. In it if a number is greater than or equal to 0 we print if it If we want to know when a piece of code is executed, we need to carefully traceback in the program, and examine how a label was arrived at - which is something that can't really be done.For example, we can rewrite the above as:Here, there are two possible ways to arrive at the \"end\", and we can't know which one was chosen. As programs get large this kind of problem gets worse and results in In comparison, below is how you  write this program in Python:I can look at a particular line of code, and know under what conditions it is met by tracing back the tree of  blocks it is in. For example, I know that the line  will be run when a .Forgive me - I couldn't resist ;-)I entirely agree that  is poor poor coding, but no one has actually answered the question. There  in fact a  (though it was released as an April fool joke and is not recommended to be used, it  work).There's no  instruction in the Python programming language. You'll have to write your code in a  way... But really, why do you want to use a ? that's been  for decades, and any program you can think of can be written without using .Of course, there are some  where an unconditional jump might be , but it's never , there will always exist a semantically equivalent, structured solution that doesn't need .The modern equivalent of  (arguable, only my opinion, etc) is explicit exception handling:Pretend pseudocode in a fake python-like language with :Compared to python:Explicit named exceptions are a  better way to deal with non-linear conditional branching.Would give you what you want with no goto statement.  "},
{"body": "I have a temperature file with many years temperature records, in a format as below:Every year has different numbers, time of the records, so the pandas datetimeindices are all different.I want to plot the different year's data in the same figure for comparing . The X-axis is Jan to Dec, the Y-axis is temperature. How should I go about doing this? Although Chang's answer explains how to plot multiple times on the same figure, in this case you might be better off in this case using a  and ing:Now it's easy to plot (each year as a separate line):Try:"},
{"body": "So I'm trying to make this program that will ask the user for input and store the values in an array / list.\nThen when a blank line is entered it will tell the user how many of those values are unique.\nI'm building this for real life reasons and not as a problem set.My code is as follows:..and that's about all I've gotten so far.\nI'm not sure how to count the unique number of words in a list?\nIf someone can post the solution so I can learn from it, or at least show me how it would be great, thanks!You can use a  to remove duplicates, and then the  function to count the elements in the set:In addition, use  to refactor your code:Use a :Armed with this, your solution could be as simple as:The following should work. The lambda function filter out the duplicated words.I'd use a set myself, but here's yet another way:Although a set is the easiest way, you could also use a dict and use  to populate a dictionary with only unique keys and values.Assuming you have already populated  with input from the user, create a dict mapping the unique words in the list to a number:"},
{"body": "So in Ruby there is a trick to specify infinity:I believe in Python you can do something like thisThese are just examples though, I'm sure most languages have infinity in some capacity. When would you actually use this construct in the real world? Why would using it in a range be better than just using a boolean expression? For instanceTo summarize, what I'm looking for is a real world reason to use Infinity.: I'm looking for real world code. It's all well and good to say this is when you \"could\" use it, when have people  used it.Dijkstra's Algorithm typically assigns infinity as the initial edge weights in a graph.  This doesn't  to be \"infinity\", just some arbitrarily constant but in java I typically use Double.Infinity.  I assume ruby could be used similarly.Off the top of the head, it can be useful as an initial value when searching for a minimum value.For example:Which I prefer to setting  initially to the first value of Of course, in Python, you should just use the min() built-in function in most cases.  There seems to be an implied \"Why does this functionality even exist?\" in your question.  And the reason is that Ruby and Python are just giving access to the full range of values that one can specify in floating point form as specified by IEEE.  This page seems to describe it well: \nAs a result, you can also have NaN (Not-a-number) values and -0.0, while you may not immediately have real-world uses for those either.  In some physics calculations you can normalize irregularities (ie, infinite numbers) of the same order with each other, canceling them both and allowing a approximate result to come through.When you deal with limits, calculations like (infinity / infinity) -> approaching a finite a number could be achieved. It's useful for the language to have the ability to overwrite the regular divide-by-zero error.Use  and  when implementing a mathematical algorithm calls for it.In Ruby,  and  have nice comparative properties so that  <  <  for any real number .  For example,  returns , extending to  the property that  implies that .  Also,  is  if x > 0,  if x < 0, and 'NaN' (not a number; that is, undefined) if x is 0.For example, I use the following bit of code in part of the calculation of some . I explicitly reference  to define a value even if  is  or  AND  is  or .  I use it to specify the mass and inertia of a static object in physics simulations.  Static objects are essentially unaffected by gravity and other simulation forces.In Ruby infinity can be used to implement lazy lists. Say i want N numbers starting at 200 which get successively larger by 100 units each time:More info here: I've used it for cases where you want to define ranges of preferences / allowed.For example in 37signals apps you have like a limit to project numberthen you can do checks like I used it for representing camera focus distance and to my surprise in Python:I wonder why is that.I've used it in the . When I'm generating new moves, if the min player wins on that node then the value of the node is -\u221e. Conversely, if the max player wins then the value of that node is +\u221e.Also, if you're generating nodes/game states and then trying out several heuristics you can set all the node values to -\u221e/+\u221e which ever makes sense and then when you're running a heuristic its easy to set the node value:I've used it in a DSL similar to Rails'  and :This makes it easy to express concepts like Kleene star and plus in your DSL.I use it when I have a Range object where one or both ends need to be openI've used symbolic values for positive and negative infinity in dealing with range comparisons to eliminate corner cases that would otherwise require special handling:Given two ranges A=[a,b) and C=[c,d) do they intersect, is one greater than the other, or does one contain the other?If you have values for positive and negative infinity that respectively compare greater than and less than all other values, you don't need to do any special handling for open-ended ranges.  Since floats and doubles already implement these values, you might as well use them instead of trying to find the largest/smallest values on your platform.  With integers, it's more difficult to use \"infinity\" since it's not supported by hardware.I ran across this because I'm looking for an \"infinite\" value to set for a maximum, if a given value doesn't exist, in an attempt to create a binary tree.  (Because I'm selecting based on a range of values, and not just a single value, I quickly realized that even a hash won't work in my situation.)Since I expect all numbers involved to be positive, the minimum is easy:  0.  Since I don't know what to expect for a maximum, though, I would like the upper bound to be Infinity of some sort.  This way, I won't have to figure out what \"maximum\" I should compare things to.Since this is a project I'm working on at work, it's technically a \"Real world problem\".  It may be kindof rare, but like a lot of abstractions, it's convenient when you need it!Also, to those who say that this (and other examples) are contrived, I would point out that  abstractions are somewhat contrived; that doesn't mean they are useful when you contrive them.When working in a problem domain where trig is used (especially tangent) infinity is an answer that can come up.  Trig ends up being used heavily in graphics applications, games, and geospatial applications, plus the obvious math applications.  I'm sure there are other ways to do this, but you could use Infinity to check for reasonable inputs in a String-to-Float conversion. In Java, at least, the Float.isNaN() static method will return false for numbers with infinite magnitude, indicating they are valid numbers, even though your program might want to classify them as invalid. Checking against the Float.POSITIVE_INFINITY and Float.NEGATIVE_INFINITY constants solves that problem. For example:String representation: -999999999999999999999999999999999999999999999\nResult of isNaN: false\nThat number is too small.String representation: 12345\nResult of isNaN: false\nThat number is jussssst right.String representation: 999999999999999999999999999999999999999999999\nResult of isNaN: false\nThat number is too big.It is used quite extensively in graphics.  For example, any pixel in a 3D image that is not part of an actual object is marked as infinitely far away.  So that it can later be replaced with a background image.I'm using a network library where you can specify the maximum number of reconnection attempts. Since I want mine to reconnect forever:In my opinion, it's more clear than the typical \"set to -1 to retry forever\" style, since it's literally saying \"retry until the number of connection attempts is greater than infinity\".Some programmers use Infinity or s to show a variable has never been initialized or assigned in the program.If you want the largest number from an input but they might use very large negatives. If I enter -13543124321.431 it still works out as the largest number since it's bigger than -inf.I've seen it used as a sort value, to say \"always sort these items to the bottom\".If you're dealing with numbers,  represents an unknown quantity, and should be preferred to  for that case. Similarly,  represents an unbounded quantity, and should be preferred to  in that case.I think it can make the code cleaner. For example, I'm using   for exactly that: the user can specify a maximum string length for a message, or they can specify . In that case, I represent the maximum length as , so that later when I check \"is this message longer than the maximum length?\" the answer will always be false, without needing a special case.You can to use:or:"},
{"body": "Which is more pythonic?or is faster, because it doesn't create a temporary list object.From the :So: These are two different operations, what you are doing with += is the extend operation. Here is what Python documents have to say about this:So in += you provide a list, in append you just add a new element.Since there's alsowhich appends all elements of the given list, I would usefor symmetry and readability's sake.While most people here are preferring the append option, I personally prefer the other one because it looks nicer even though it may be slower (or maybe its optimized).When you write lots of Python code, I don't usually see something like this:It's more like this:So to me at least, it is nicer to see:Because its easy to recognize at a glance that you are adding to a list. Having said that, I sometimes find myself using both forms.list.append(1)\nmore readable and to consistent with the contextIf you've got a single element, , that you want to add to your list , then putting  into its own list and extending  with it seems like adding unnecessary complexity. I would thus prefer overIf  is already a list, then choosingor is a matter of preference, IMO. On the other hand, if you're going to be doing a lot of extends, you can get a performance boost by \"hoisting\" the method lookup:Finally, I think that the append operation isn't used too often in Pythonic code, because append is used very often in \"accumulator\" idioms, where I'd expect a more experienced Python programmer to use a list comprehension/etc.So instead of:You'd probably see:orHave you noticed the term \"more pythonic\" in your question?\nI think it implies that you are asking a not-so-helpful question.If one is merely  pythonic than the other (rather than being \npythonic while the other is not) it could be said that you have\nviolated the Zen of Python on a meta level:\n should also hold for the\nprocess of searching for the form in which you express\nyour logic -- once you have found something pythonic,\nthat is good enough. Keep the search simple.So my answer would be:\nNeither of them is more pythonic.\n"},
{"body": "I have used  to implement a client. It works fine. Now I want to be able to pass command line arguments to it, so I need to implement a   plugin. I have performed many searches to find resources that would show how to convert the program to a plugin. However I couldn't find exactly what I as looking for.Here is a related part of my  code:Here is what I wrote for my :I have used the same folder hierarchy mentioned in the documentation. As I haven't been able to find enough examples of plugins on the web, I'm really stuck at this point. I would appreciate any help. Could anybody tell me how to change the code? Thank you in advance.Should you perhaps be using the  files to run it from command line?Then using your  class should be easier in another program...You have to return a main service object from your  method. Try adding  then in  add this at the beginning: \nCreate the TCPServer service: \nAdd it to the top service: \nThen return the top service: You may also need to take a look at this excellent series of tutorials from   (article n\u00b016 is the one which is useful for your problem)"},
{"body": "For a project I am making some 3D scatter plots with the three corresponding projections under it. I use different colors to indicate a fourth parameter. First I plot data with a certain color and then I overplot that with other data with a different color, so that in the end the order is such that I can see everything as I want:In the beginning this worked fine, but when I try to do the same thing with slightly different data, the colors get messed up. The colors shown in the projections are the right ones, but some of them are missing in the 3D plot so they don't match anymore:When I rotate the 3D plot in a funny way, the colors are recovered and I can see them as they were supposed to be: However, I don't want a 3D plot that is rotated in a funny way, because the axes get messed up and it's impossible to read it properly like that. I found one solution to the problem here: \n.\nIt basically says that I should replace my ax.scatter(X,Y) with ax.plot(X,Y,'o'). When I do this the colors are shown the way they were supposed to be, but the plot is much messier and uglier this way.\nBasically I just want to be able to do this with a scatter plot.Does anyone know how to solve this?Here's a minimum example of my code, for only two colors:Rather than using  try  or . The  is specifying the  to use, and the 'o' marker is a large filled circle, which is why your plot looks ugly."},
{"body": "I have shifted from OpenCV 2.4.9 to 3.0 to make use of  and  function. I came to know that it does not come along with non-free algorithms like SIFT , SURF. So I installed  from  by following stepsI also cross checked in modules of ,  was there. Then when I tried to do It gives me following errorWhat am I doing wrong here. Just FYI that I am using OpenCV 3.0 beta version . has OpenCV deactivated python wrappers for  or I have not installed it correct way?I am writing down an answer to my own question, as it might help somebody fixing the same issue.The opencv package I was trying for was downloaded from opencv . I just downloaded latest code from  for opencv and installed it. It is working fine now with opencv-contrib."},
{"body": "Is it possible to increase the antialiasing in matplotlib?\nI can still see some aliasing in my data, I tried several backends and it is still there. The antialiasing flag of the lines is set.Here you can see what I mean It's a sample taken from a Screenshot. It's probably not the best example but I guess one can see the stairs in the line. It was taken with the wxagg backend.I'm using matplotlib version 1.01 with Windows 7.Update:\nI don't have the code which produced the previous picture anymore, but I still have the problem. Below is a simple code example which shows the aliasing.It print's \nAnd the resulting plot looks like the following.\nEspecially the lower red curve shows clear aliasing. The picture you added to your question is already perfectly anti-aliased. It doesn't get any better than this. Have a look at a zoomed version of the image:If you save the picture as a .svg file, you will have infinite precision. You can then edit this .svg with something like InkScape, and get how much precision/antialiasing as you like."},
{"body": "Unfortunately, due to my oversight, I had an older version of MKL (11.1) linked against numpy. Newer version of MKL (11.3.1) gives same performance in C and when called from python. What was obscuring things, was even if linking the compiled shared libraries explicitly with the newer MKL, and pointing through LD_* variables to them, and then in python doing import numpy, was somehow making python call old MKL libraries. Only by replacing in python lib folder all libmkl_*.so with newer MKL I was able to match performance in python and C calls.Matrix multiplication was done via sgemm (single-precision) and dgemm (double-precision) Intel's MKL library calls, via numpy.dot function. The actual call of the library functions can be verified with e.g. oprof. Using here 2x18 core CPU E5-2699 v3, hence a total of 36 physical cores.\nKMP_AFFINITY=scatter. Running on linux.1) Why is numpy.dot, even though it is calling the same MKL library functions, twice slower at best compared to C compiled code?2) Why via numpy.dot you get performance decreasing with increasing number of cores, whereas the same effect is not observed in C code (calling the same library functions).I've observed that doing matrix multiplication of single/double precision floats in numpy.dot, as well as calling cblas_sgemm/dgemm directly from a compiled C  give noticeably worse performance compared to calling same MKL cblas_sgemm/dgemm functions from inside pure C code.Doing exactly the same as above, but with double precision A, B and C, you get:\n3 cores: 20s, 6 cores: 10s, 12 cores: 5s, 18 cores: 4.3s, 24 cores: 3s, 30 cores: 2.8s, 36 cores: 2.8s.The topping up of speed for single precision floating points seem to be associated with cache misses.\nFor 28 core run, here is the output of perf.\nFor single precision:And double precision:C shared library, compiled withPython wrapper function, calling the above compiled library:However, explicit calls from a C-compiled binary calling MKL's cblas_sgemm / cblas_dgemm, with arrays allocated through malloc in C, gives almost 2x better performance compared to the python code, i.e. the numpy.dot call. Also, the effect of performance degradation with increasing number of cores is NOT observed.  and was achieved when using all 36 physical cores via mkl_set_num_cores and running the C code with numactl --interleave=all.Perhaps any fancy tools or advice for profiling/inspecting/understanding  this situation further? Any reading material is much appreciated as well.\nFollowing @Hristo Iliev advice, running numactl --interleave=all ./ipython did not change the timings (within noise), but improves the pure C binary runtimes.I suspect this is due to unfortunate thread scheduling. I was able to reproduce an effect similar to yours. Python was running at ~2.2 s, while the C version was showing huge variations from 1.4-2.2 s.Applying:\n\nThis ensures that the 28 threads are always running on the same processor thread.Reduces both runtimes to more stable ~1.24 s for C and ~1.26 s for python.This is on a 28 core dual socket Xeon E5-2680 v3 system.Interestingly, on a very similar 24 core dual socket Haswell system, both python and C perform almost identical even without thread affinity / pinning.Why does python affect the scheduling? Well I assume there is more runtime environment around it. Bottom line is, without pinning your performance results will be non-deterministic.Also you need to consider, that the Intel OpenMP runtime spawns an extra management thread that can confuse the scheduler. There are more choices for pinning, for instance  - but for some reason that is totally messed up on my system. You can add  to the variable to see how the runtime is pinning your threads. is a useful alternative providing more convenient control.In general single precision should be at least as fast as double precision. Double precision can be slower because:I would think that once you get rid of the performance anomaly, this will be reflected in your numbers.When you scale up the number of threads for MKL/*gemm, considerI don't think there is a really simple way to measure how your application is affected by bad scheduling. You can expose this with  and there is  to visualize this, but this will come with a high learning curve. And then again - for parallel performance analysis you should have the threads pinned anyway."},
{"body": "I used easy_install to install pip, pip to install django, virtualenv, and virtualenvwrapper.I have just returned to it a few weeks later and django does not seem to work anymore, but more concerning is I can't start the process over again as easy_install is returning the following error:After a good evening of hunting I am stumped as to how to resolve this.You seem to have a version conflict; note the  path, but the  script wants to load  instead.The latter is a development version; it has the revision number of a subversion repository embedded in the version ().I suspect you have  python installations; one is the system version (in  and the other is installed with the python installer into , and the stub script in  may be installed with the system python.If so, there'll be another copy of the stub at , which should work correctly.I had the same issue, eventually ended up running the 2.7.3 easy_install by\n/opt/python2.7.3/bin/easy_install which worked fine"},
{"body": "I'm trying to figure out if there is a defacto pattern for file access using twisted. Lots of examples I've looked at (twisted.python.log, twisted.persisted.dirdbm, twisted.web.static) actually don't seem to worry about blocking for file access. It seems like there should be some obvious interface, probably inheriting from abstract.FileDescriptor, that all file access should be going through it as a producer/consumer.Have I missed something or is it just that the primary use for twisted in asynchronous programming is for networking and it hasn't really been worked out for other file descriptor operations, not worrying about the purity of non-blocking IO ?I think you are looking for the . For more information on non-blocking I/O  in Python, you can also watch this .There is an open ticket for this in Twisted - .After plenty of searching, trial, and error, I finally figured how to use .[Edit: I also just figured out a simpler way that doesn't require the use of a protocol]Run either like so:The fdesc module might be useful for asynchronously talking to a socket or pipe, but when given an fd that refers to an ordinary filesystem file, it does blocking io (and via a rather odd interface at that). For disk io, fdesc is effectively snake oil; don't use it.As of May 2017, the only reasonable way to get async disk io in twisted is by wrapping synchronous io calls in a .I'm not sure what you want to achieve. When you do logging, then Python will make sure (by the global interpreter log) that log messages from several threads go into the file one after the other.If you're concerned about blocking IO, then the OS adds default buffers for your files (usually 4KB), and you can pass a buffer size in the  call.If you're concerned about something else, then please clarify your question."},
{"body": "I have the following directory:mymodule.py:main.py:But when I compile it, then I get this error:This works:But I get no auto completion when I type this in and there is a message: \"unresolved reference: mymodule\" and \"unresolved reference: myclass\"\nAnd in my other project, which I am working on, I get the error: \"ImportError: No module named 'mymodule'What can I do?I had the same problem and I solved it by using an absolute import instead of a relative one.for example in your case, you will write something like this:You can see in the .if you just run the main.py under the app ,just import like if you want to call main.py on other folder, use:for example:so i think the main question of you is how to call app.mainIf you go one level up in running the script in the command line of your bash shell, the issue will be resolved. To do this, use  command to change the working directory in which your script will be running. The result should look like this:rather than this:Once you are there, instead of running the script in the following format:Change it to this:This process can be repeated once again one level up depending on the structure of your Tree diagram. Please also include the compilation command line that is giving you that mentioned error message. I usually use this workaround:Which means your IDE should pick up the right code location and the python interpreter will manage to run your code."},
{"body": "I'm struggling to figure out how to profile a simple multiprocess python scriptI'm starting 5 processes and therefore cProfile generates 5 different files. Inside of each I want to see that my method 'worker' takes approximately 3 seconds to run but instead I'm seeing only what's going on inside the 'start'method.I would greatly appreciate if somebody could explain this to me.You're profiling the process startup, which is why you're only seeing what happens in  as you say\u2014and  returns once the subprocess is kicked off.  You need to profile inside the  method, which will get called in the subprocesses."},
{"body": "In my Django website, I'm creating a class that interact dynamically with other applications installed in the website. I have to do a manipulation on each field of each application.So I want to save the name of all installed applications in a list and get the attributes of each one. There is a way to do that using an iterator or something else ?Under Django 1.6 and below.If you want all models, try:Under Django 1.7 and above (thanks Colin Anderson):I believe the older function still works.[edit]So the blessed way is:Older version of this answer:All applications are registered in the  file.You can import each application and list their attributes:In order to use the new print function instead of the print statement in older Python you may have to issue a  (or just change the line containing the  call).The list of installed applications is defined in . It contains a tuple of strings, so you can iterate on it to access each application's name.However, I'm not sure what you mean by each application's  and .You can retrieve installed apps like that (in interpreter) :To get the actual  themselves (), this is what I came up with:Though you may want to do some error handling, or filtering."},
{"body": "I want to create \"heart rate monitor\" effect from a 2D array in numpy and want the tone to reflect the values in the array.You can use the  from  to create a wav file which you can then play however you wish. Note that the array must be integers, so if you have floats, you might want to scale them appropriately:If you want Python to actually play audio, then  provides an overview of some of the packages/modules.For the people coming here in 2016 scikits.audiolab doesn't really seem to work anymore. I was able to get a solution using sounddevice.In addition, you could try . It features file IO and the ability to 'play' arrays. Arrays don't have to be integers. To mimick dbaupp's example:I had some problems using , so I looked for another options for this task. I came up with , which seems a lot more up-to-date. I have not checked if it works with Python 3.A simple way to perform what you want is this:PyGame has the module  which can play numpy data as audio. The other answers are probably better, as PyGame can be difficult to get up and running. Then again, scipy and numpy come with their own difficulties, so maybe it isn't a large step to add PyGame into the mix.If you are using Jupyter, the best option is:Not sure of the particulars of how you would produce the audio from the array, but I have found  to be a great command-line audio player, and could potentially work for you. I use it as my player of choice for , which is written in python and has libraries that could be a great starting place for interfacing your code/arrays with audio.Check out:"},
{"body": "In the following, male_trips is a big pandas data frame and stations is a small pandas data frame. For each station id I'd like to know how many male trips took place. The following does the job, but takes a long time:how should I go about this instead?Update! So there were two main approaches:  followed by , and the simpler . I did a quick , and the  approach wins by quite a large margin! Here is the code:and here is the result:Note that, at this speed, for exploring data  value_counts is marginally quicker and less remembering!I'd do like Vishal but instead of using sum() using size() to get a count of the number of rows allocated to each group of 'start_station_id'. So:My answer below works in Pandas 0.7.3. Not sure about the new releases.This is what the  method is for:It should be straight-forward to then inspect  based on the values in . However, if you insist on  considering those values, you could do the following:and this will only give counts for station IDs actually found in .doesnt work?\nedit: after seeing in the answer above that  and  exist (and  even comes with its own entry in  and also  isn't simply ) I updated the three methods belowYou could also do an inner join on stations.id:\n followed by .\nOr:If you have the time I'd be interested how this performs differently with a huge DataFrame.how long would this take:"},
{"body": "I'm asking this because I know that the pythonic way to check whether a list is empty or not is the following: will print , etc. So this leads me to identify  with  truth-values; however, if I try to compare [] and False \"directly\", I obtain the following: etc... What's going on here? I feel like I'm missing something really obvious.The  statement evaluates everything in a Boolean context, it is like there is an implicit call to the  built-in function.Here is how you would actually check how things will be evaluated by an  statement:See the documentation on , empty lists are , but this doesn't mean they are equivalent to . also has some excellent information on why it was implemented this way, see the very last bullet in the  section for the part that deals with  and  specifically.The most convincing aspect to me is that  is generally transitive, so  and  implies .  So if it were the way you expected and  were true and  were true, one might assume that  should be true (even though it obviously should not be in a language without implicit type conversion).Empty containers are \"falsy,\" that is, they evaluate to  . That doesn't mean they are literally equal to the constant .In other words, the following is :The truth value of an object is determined by its  or its  method. (In Python 3,  has been renamed to .) Containers have a  method, so they are truthy when they have anything in them and falsy when they are empty.If empty containers were literally equal to , by the way, then any empty container would be equal to any other empty container: for example,  would be . And that just wouldn't make any sense at all!However, just to blow your mind, the following is :This is because Booleans are a subclass of integers in Python, and  is basically just a zero that gets printed a little differently.Built-in types in Python have a truth value which allows you to test them for truthfulness. See .This is different than saying  which is doing an actual value test (equality test). It is using the objects  method to determine if their values are equal.In your example, the not operator is causing your list to be converted to a boolean.  Try this:\"is\" and \"==\" don't do that conversion."},
{"body": "I am trying to create a list of tuples where the tuple contents are the number  and the number before it in the list.  Cleaner Pythonic approach:What is the code above doing:Part of your issue is that  will always return an empty list. The end of a slice is exclusive, so when you do  you're trying to take the elements of  that exist  index 0 and index 0.You're on the right track, but you want to zip the list with itself.You were pretty close, I'll show you an alternative way that might be more intuitive if you're just starting out:Get the index in the range of the list lenght, and if the value at the position  is equal to , grab the adjacent elements.The result is:This is  but I decided to un-delete it to show you a different way of doing it. You can make it go a bit faster by using  instead: that  the behavior of the comprehension without  and the behavior of the comprehension with  is .Specifically, if  then:while:It is up to you to decide which of these fits your criteria, I'm just pointing out that they don't behave the same in all cases.You can also do it without slicing by creating iterators:Or use the  to create your pairsIf using python3, just import  and use the regular .It is really surprising that no one has added a functional approach. Another alternative answer is using a . This builtin function returns an iterator (list in Python2) consisting of all the elements present in the list that return  for a particular function It is to be noted that the . The difference between the functional approach and list comprehensions is discussed in . My solution is similar to one of Jim's advanced with zero-index check"},
{"body": "When reading lines from a text file using python, the end-line character often needs to be truncated before processing the text, as in the following example:Is there an elegant way or idiom for retrieving text lines without the end-line character?The  way to do this in Python is to use :Each of the other alternatives has a gotcha:Simple. Use What's wrong with your code?  I find it to be quite elegant and simple.  The only problem is that if the file doesn't end in a newline, the last line returned won't have a  as the last character, and therefore doing  would incorrectly strip off the last character of the line.The most elegant way to solve this problem would be to define a generator which took the lines of the file and removed the last character from each line only if that character is a newline:Long time ago, there was Dear, clean, old, BASIC code that could run on 16 kb core machines:\nlike that: Now, to read a file line by line, with far better hardware and software (Python), we must reinvent the wheel:I am induced to think that something has gone the wrong way  somewhere...What do you thing about this approach?Generator expression avoids loading whole file into memory and  ensures closing the fileYou may also consider using line.rstrip() to remove the whitespaces at the end of your line."},
{"body": "I have a string that can be a hex number prefixed with \"0x\" or a decimal number without a special prefix except for possibly a minus sign. \"0x123\" is in base 16 and \"-298\" is in base 10.How do I convert this to an int or long in Python?I don't want to use eval() since it's unsafe and overkill.(why doesn't  do that?)Base 16 to 10 (return an integer):Base 10 to 16 (return a string):Something like this might be what you're looking for.If you are doing a lot of go between with different number systems, the bitstring library make a lot of binary/Hex operations easier.  It also supports string interpretation: This is a simpler version which works like a charm.\nReturns string since hex() returns the same type:"},
{"body": "I have a list with ~10^6 tuples in it like this:I want to find the maximum value of the Ys in this list, but also want to know the X that it is bound to.How do I do this?Use :\u00a0\nUsing :using : comparison:In addition to max, you can also sort:You could loop through the list and keep the tuple in a variable and then you can see both values from the same variable..."},
{"body": "I have some problem for a while now, I'm experiencing CSRF Cookie not set. Please look at the codes belowIn the HTML here is the codeIm stuck, I already cleared the cookie, used other browser but still csrf cookie not set.This can also occur if  is set and you are accessing the site non-securely.From \nYou can solve it by adding the  to your viewif this method doesn't work. you will try to comment csrf in middleware. and test again.This problem arose again recently due to a bug in Python itself.Among the versions affected were 2.7.8 and 2.7.9. \nThe cookie was not read correctly if one of the values contained a  character.Updating Python (2.7.10) fixes the problem.I was using Django 1.10 before.So I was facing this problem.\nNow I downgraded it to Django 1.9 and it is working fine.try to check if your have installed in the settings.pyIn the template the data are formatted with the csrf_token:Problem seems that you are not handling  requests appropriately or directly posting the data without first getting the form. When you first access the page, client will send  request, in that case you should send html with appropriate form.Later, user fills up the form and sends  request with form data.Your view should be:Check that chrome's cookies are set with default option for websites. Allow local data to be set (recommended).Method 1:Method 2 :Because render_to_response method may case some problem of response cookies.                       Make sure your django session backend is configured properly in settings.py. Then try this,Add this middleware in  under MIDDLEWARE_CLASSES or MIDDLEWARE depending on the django versionget_token - Returns the CSRF token required for a POST form. The token is an alphanumeric value. A new token is created if one is not already set.If you're using the  to make POST requests as a logged in user and getting , it could be because by default  does not include session cookies, resulting in Django thinking you're a different user than the one who loaded the page.You can include the session token by passing the option  to fetch:in your view are you using the csfr decorator ??I have just met once, the solution is to empty the cookies.\nAnd may be changed while debugging SECRET_KEY related.Clearing my browser's cache fixed this issue for me. I had been switching between local development environments to do the django-blog-zinnia tutorial after working on another project when it happened. At first, I thought changing the order of INSTALLED_APPS to match the tutorial had caused it, but I set these back and was unable to correct it until clearing the cache."},
{"body": "I'm trying to parse timestamp strings like  in Python, but I'm having trouble finding a solution that will handle the abbreviated timezone.I'm using 's  function, but it doesn't parse the timezone. Is there an easy way to do this?That probably won't work because those abbreviations aren't unique. See  for details. You might wind up just having to manually handle it yourself if you're working with a known set of inputs.'s  accepts as keyword argument  a dictionary of the kind  (that is, matching the zone name to GMT offset in seconds). So assuming we have that, we can do:Regarding the content of , here is how i populated mine:ps. per @Hank Gay time zone naming is not clearly defined. To form my table i used  and  . I looked at each conflict and resolved conflicts between obscure and popular names towards the popular (more used ones). There was one -  - that was not as clear cut (it can mean , ,  or ), so i left it out of the table - you may need to chose what to add for it based on your location. Oh - and I left out the Republic of Kiribati with their absurd \"look at me i am first to celebrate New Year\" GMT+13 and GMT+14 time zones.You might try pytz module: The parse() function in dateutil can't handle time zones.  The thing I've been using is the %Z formatter and the time.strptime() function.  I have no idea how it deals with the ambiguity in time zones, but it seems to tell the difference between CDT and CST, which is all I needed.Background: I store backup images in directories whose names are timestamps using local time, since I don't have GMT clocks handy at home.  So I use time.strptime(d, r\"%Y-%m-%dT%H:%M:%S_%Z\") to parse the directory names back into an actual time for age analysis.I used  to generate a  mapping: Usage UsageThe UTC conversion is needed since there are many timezones available for each abbreviation. Since  is a , it only has the last timezone per abbreviation. And you may not get the one you were expecting pre conversion."},
{"body": "According to , a bytes object is used to store a mutable sequence of bytes (0-255), raising if this is not the case.However, my python 2.7 says otherwiseDoes anyone have a clue on the reason why the PEP is declared Final, but the implementation does not conform ?The  type was introduced in Python 3, but what's being discussed in the PEP is a mutable sequence ( is immutable) which was introduced in Python 2.6 under the name .The PEP clearly wasn't implemented as stated (and it does say that it was partially superseded by ) but I think it's only a question of things being renamed, not features missing. In Python 2  is just an alias for  to aid forward compatibility and so is a red-herring here.Example bytearray usage:The new   is . The 2.x  built-in is just an alias to the  type. There is no new type called  in 2.x; Just a new alias and literal syntax for .Here's the  everybody loves: objects only really exist in Python 3.x.   is an alias for str in Python 2.7.  It exists to help writing portable code between Python 2 and 3."},
{"body": "Is there a regexp which would find longest common prefix of two strings? And if this is not solvable by one regexp, what would be the most elegant piece of code or oneliner using regexp (perl, ruby, python, anything).PS: I can do this easily programatically, I am asking rather for curiosity, because it seems to me that this could be solveable by regexp.PPS: Extra bonus for O(n) solution using regexps. Come on, it should exist!If there's some character that neither string contains \u2014, say,  \u2014 you could writeand the longest common prefix would be saved as . This is obviously very inefficient. I think that if efficiency is a concern, then this simply isn't the approach we should be using; but we can at least improve it by changing  to  to prevent useless greediness that will just have to be backtracked again, and wrapping the second  in  to prevent backtracking that can't help. This:This will yield the same result, but much more efficiently. (But still not nearly  efficiently as a straightforward non\u2013regex-based approach. If the strings both have length , I'd expect its worst case to take at least O() time, whereas the straightforward non\u2013regex-based approach would take O() time in  worst case.)Here's a Python one-liner:Here's one fairly efficient way which uses a regexp.  The code is in Perl, but the principle should be adaptable to other languages:(A subtlety worth noting  is that Perl's string XOR operator () in effect pads the shorter string with nulls to match the length of the longer one.  Thus, if the strings might contain null characters,  if the shorter string happens to be a prefix of the longer one, the common prefix length calculated with this code might exceed the length of the shorter string.)simple and efficientThe problem you're going to have is that a regular expression matches against one string at a time so isn't intended for comparing two strings.If there's a character that you can be sure isn't in either string you can use it separate them in a single string and then search using back references to groups.So in the example below I'm using whitespace as the separatorHere's an O(N) solution with Foma-like pseudocode regular expressions over triples (for lcp, you have two inputs and an output). To keep it simple, I assume a binary alphabet {a,b}:Now you just need a language that implements multi-tape transducers.Another attempt for O(n) solution:it depends whether .{n} is considered O(1) or O(n), I do not know how efficiently this is implemented.Inspired by ruakh's answer, here is the O(n) regexp solution: well it is not correct as rukach metions, but the idea is correct, but we should push regexp machine not to check the beginning letters repeatedly. The basic idea can be also rewritten in this perl oneliner.I wonder if it can be incorporated back into regexp solution.Could be useful in some  cases so here it goes:RegEx only solution in 3 steps ():String A: \nString B: And here's the   in PHP:  Code live at: Non regexp, non duplicating string at each iteration solution:I second ruakh's answer for the regexp (with my suggested optimization in the comments). Simple to write, but not simple and efficient to run if the first string is long.Here is an efficient, non-regexp, readable, one-line answer:Using extended regular expressions as in Foma or Xfst.The hardest part here is to define \"longest\". Generally speaking, to\noptimize, you construct the set of non\u2013optimal strings (worsening) and\nthen remove these (filtering).This is really a purist approach, which avoids non-regular operations\nsuch a capturing. I have the idea this is most inefficient. No err checking, etc."},
{"body": "Consider this Python code for printing a list of comma separated valuesWhat is the preferred method for printing such that a comma does not appear if  is the final element in the list.exA  as suggested in other answers is the typical Python solution; the normal approach, which peculiarly I don't see in any of the answers so far, isknown as a generator expression or genexp.If you prefer a loop (or need one for other purposes, if you're doing more than just printing on each item, for example), there are of course also excellent alternatives:this is a first-time switch (works for any iterable a, whether a list or otherwise) so it places the comma  each item but the first.  A last-time switch is slightly less elegant and it work only for iterables which have a  (not for completely general ones):this example also takes advantage of the last-time switch to terminate the line when it's printing the very last item.The  built-in function is very often useful, and well worth keeping in mind!That's what  is for.print (stringTokenizer(u\"\u043f\u0440\u0438\u0432\u0435\u0442 \u0441\u043a\u043e\u0442\u044b!  \u044f \u0432\u0430\u043c \u0441\u0442\u0438\u0445\u0430\u043c\u0438, \u043c\u043e\u0436\u0435\u0442 \u0431\u044b\u0442\u044c, \u0435\u0449\u0435  \u043e\u0442\u0432\u0435\u0447\u0443\",\", !\"))"},
{"body": "I'm trying to find out a way in python to redirect the script execution log to a file as well as stdout in pythonic way. Is there any easy way of acheiving this?I came up with this [untested] in python will expect a  function in . You could use your own custom object which has this. Or else, you could also have sys.stdout refer to your object, in which case it will be tee-ed even without .Use logging module ():I just want to build upon Serpens answer and add the line:This will allow you to chose what level of message gets logged.For example in Serpens example,Will not get recorded as it defaults to only recording Warnings and above.More about levels used can be read about   You should use the  library, which has this capability built in. You simply add handlers to a logger to determine where to send the output.The easiest solution is to redirect the standard output. In your python program file use the following:Any std output (e.g. the output of ) will be redirected to  or if you uncomment the second line, any output will just be suppressed."},
{"body": "I'd like to take an HTML table and parse through it to get a list of dictionaries. Each list element would be a dictionary corresponding to a row in the table.If, for example, I had an HTML table with three columns (marked by header tags), \"Event\", \"Start Date\", and \"End Date\" and that table had 5 entries, I would like to parse through that table to get back a list of length 5 where each element is a dictionary with keys \"Event\", \"Start Date\", and \"End Date\".Thanks for the help!You should use some HTML parsing library like :printsSven Marnach  is directly translatable into  which is part of recent Python distributions:same output as Sven Marnach's answer...If the HTML is  XML you can't do it with . But even then, you don't have to use an external library for parsing a HTML table. In python 3 you can reach your goal with  from . I've the code of the simple derived HTMLParser class .You can use that class (here named ) the following way:The output of this is a list of 2D-lists representing tables. It looks maybe like this:"},
{"body": "I wrote this simple function:Examples:Whilst this is clear enough for me and fits my use case (a simple test tool for a simple printer) I can't help thinking there's a lot of room for improvement and this could be squashed down to something very concise.What other approaches are there to this problem?Use the new  string method:How about this:Use  to pass width and  for uppercaseAs suggested by  and "},
{"body": "How can I make unique URL in Python a la  or \nWhen using uuid from python I get a very large one. I want something shorter for URLs.: Here, I wrote a module for you.  Use it. Counting up from 1 will guarantee short, unique URLS. /1, /2, /3 ... etc.Adding uppercase and lowercase letters to your alphabet will give URLs like those in your question.  And you're just counting in base-62 instead of base-10.Now the only problem is that the URLs come consecutively.  To fix that, read my answer to this question here:Basically the approach is to simply swap bits around in the incrementing value to give the appearance of randomness while maintaining determinism and guaranteeing that you don't have any collisions.I'm not sure most URL shorteners use a random string.  My impression is they write the URL to a database, then use the integer ID of the new record as the short URL, encoded base 36 or 62 (letters+digits).Python code to convert an int to a string in arbitrary bases is .This module will do what you want, guaranteeing that the string is globally unique (it is a UUID):If you need something shorter, you should be able to truncate it to the desired length and still get something that will reasonably probably avoid clashes.The reason UUIDs are long is because they contain lots of information so that they can be guaranteed to be globally unique.If you want something shorter, then you'll need to do something like generate a random string, checking whether it is in the universe of already generated strings, and repeating until you get an unused string. You'll also need to watch out for concurrency here (what if the same string gets generated by a separate process before you inserted into the set of strings?).If you need some help generating random strings in Python, this  might help.It doesn't really matter that this is Python, but you just need a hash function that maps to the length you want.  For example, maybe use MD5 and then take just the first  characters.  You'll have to watch out for collisions in that case, though, so you might want to pick something a little more robust in terms of collision detection (like using primes to cycle through the space of hash strings).I don't know if you can use this, but we generate content objects in Zope that get unique numeric ids based on current time strings, in millis (eg, 1254298969501)Maybe you can guess the rest. Using the recipe described here:\n, we encode and decode the real id on the fly, with no need for storage. A 13-digit integer is reduced to 7 alphanumeric chars in base 62, for example.To complete the implementation, we registered a short (xxx.yy) domain name, that decodes and does a 301 redirect for \"not found\" URLs, If I was starting over, I would subtract the \"starting-over\" time (in millis) from the numeric id prior to encoding, then re-add it when decoding. Or else when generating the objects. Whatever. That would be way shorter.. is an awesome tool for this.Edit:Here's how to use Hashids to generate a unique short URL with Python:Try this  ... It's still under development, but very useful!! Generate a unique identifier of a specified fixed length consisting of the characters  and . For example:Here's my solution. "},
{"body": "I am translating a django app and I would like to translate also the homepage of the django admin site. On this page are listed the application names and the model class names. I would like to translate the model class name but I don't find how to give a user-friendly name for a model class.Does anybody know how to do that?Look at the   and , both of which are translatable."},
{"body": "So AFAIK in CPython, function definitions are compiled into function objects when executed at parse time. But what about inner functions? Do they get compiled into function objects at parse time or do they get compiled (or interpreted) every single time the function is called? Do inner functions incur any performance penalty at all?To give a general explaination - assuming you have the following code in a module:When the file is parsed by python via , the above text is turned into bytecode for how to execute the . In the module bytecode, there are two \"code objects\", one for the bytecode of  and one for the bytecode . Note that I said code objects, not functions - the code objects contain little more than the bytecode used by the function, and any information that could be known at compile time - such as the bytecode for  containing a ref to the bytecode for . When the module actually loads, by evaluating the code object associated with the module, one thing which happens is an actual \"function object\" is created for , and stored in the module's  attribute. The function object acts as a collection of the bytecode and all context-relavant things that are needed to call the function (eg which globals dict it should pull from, etc) that can't be known at compile time. In a way, a code object is a template for a function, which is a template for execution of the actual bytecode with all variables filled in. None of this involved -as-a-function yet - Each time you actually get around to calling , that's when a new  function object is created , which binds the already-created inner bytecode object to a list of globals, including the value of  as passed into that call to outer. As you can imagine, this is pretty fast, since no parsing is needed, just filling in a quick struct with some pointers to other already-existing objects.I suspect this is heavily implementation dependent, but that was CPython 2.6.6, and the inner function looks like it was compiled. Here's another example:So we can conclude that they are compiled. As for their performance characteristics, use them. If you start having performance issues, profile. I know it's not really an answer, but it almost never matters and when it does, general answers don't cut it. Function calls incur some overhead and it looks like inner functions are just like functions.Easy test: the default arguments to a function are called once, at define time.So yes: this is a minor (very very! minor) performance hit.To extend nmichaels answer inner function are compiled in  time as he guessed and there byte code is saved in the  and they are accessed using the opcode  as you can see in the disassembly of the function.Example:I'm late on this, but as a little experimental complement to these thorough answers: you may use the  to verify whether a new object is created or not:The actual numbers may differ, but their difference indicates that two distinct instances of  are indeed created.This time, as expected, the two s are the same.Now for some  measurements. Inner first, outer second:So, on my machine, it seems that the inner version is 50% slower (Python 2.7, IPython Notebook)."},
{"body": "What I'm trying to do here is use a keyboard interrupt to exit all ongoing threads in the program. This is a pared down version of my code where the thread is created:The program itself is far more complicated, accounting for tons of different variables that affect the threads and even having the option to launch in a sequential mode, where tasks are not threaded, but instead launched one-by-one so there might be some problems with this small variation I just conjured up.\nI have done this in ways that produced 50/50 results. The interrupts would work but the threads would never cleanly exit. Sometimes they would keep going but halt the execution of future threads, some times they would exit with a massive error regarding the interrupt, other times the interrupts would do nothing at all. Last time I ran this the program stopped any future threads from executing but did not stop the current thread.\nIs there any way to exit the threads without going into the module the thread is actually executing in?A similar question is \"How do you kill a thread?\"You create an exit handler in your thread that is controlled by a lock or event object from the threading module. You then simply remove the lock or signal the event object. This informs the thread it should stop processing and exit gracefully. After signaling the thread in your main program, the only thing left to do is to use the  method in  which will wait for the thread to shut down.A short example:If you REALLY need the functionality of  a thread, use multiprocessing. It allows you to send SIGTERMs to individual \"processes\" (it's also very similar to the threading module). Generally speaking, threading is for when you are IO-bound, and multiprocessing is for when you are truly processor-bound."},
{"body": "I am using sublime text 2 for python development along with virtualenv!The standard sublime text 2 build system uses the standard python install rather than my virtualenv where my packages are installed.How can I get sublime text 2 to build using my virtualenv? I currently use the terminal to activate my environment and run my scripts.UPDATE: Never got it working, but seeing as i am using flask and it builds when you make a change, it's not a big issueYou have to edit your  and add the following:You can also set the path for the build system to the  directory of your virtualenv, like so:This also allows other tools, like  in the example, to find the correct python binary from the virtualenv.In windows this works for me:Sublime's Build System supports variables which can be used with Sublime project files to make this a bit more portable across projects. If your virtual environments are in a standard spot, create a new project file (Project -> Save Project As) into the root directory of your project just above your virtual environment directory. Then create a new build file with something like this:It seems to then pick up the rest automatically - the same as if you typed ./venv/bin/python from the command line - no need to mess with paths, environment variables, etc. I'm using Flask, but I think it's apply to nearly every case.\nMy actual build is like this, where \"benicio\" is the directory of my project:Sorry to add yet another answer to this - but this caused me a large amount of grief figuring this out. Not only do you need to make a build system like:but you HAVE to change a setting in Sublime Text - go to Tools --> Build System --> \"Maths\". In my case I need to choose \"Maths\" because that's what I named my build system. If you don't do this - Sublime Text does not use your build system!!I have just got sublime text 3 to working in a virtualenv. Although the OP specified ST2, there all likely more like myself who are using ST3. Thanks to user1248490 and Russell Beattie I arrived at the following:Note that \"cmd\" is now \"shell_cmd\" in ST3. See Under MAC OSX, this works for meWhat i did was keep it simple:Went to root drive and created python folder:then went in there and created the virtualenvthen created a newbuild in ST2 with the above command and it worksThis is what I have as a build system (assuming my virtualenv is created as a folder called 'env' in my current project directory). This at least means I don't need to constantly change the build system between projects:I saved this as a New Build System (Tools -> Build System -> New Build System).I use this to build my Flask project. I have added the following code to my Project Settings: Project -> Edit Projectand then I just switch to my  file and hit this combination worked great:2 steps1) add the 2 appropriate keys to the 'env' key.\nA) DJANGO_SETTINGS_MODULE \nB) PYTHONPATH2) update cmd to reflect the version of python you want to use.I have an answer for anyone who uses Heroku and uses their foreman tool, and it works great. Simply create a new build system like so:This pulls in all of the environment variables available to Foreman,  your virtualenv's $PATH variable, which adds the virtualenv Python to your python path.source did not work for me inside the build on lubuntu.\nuse '.' or dot instead of 'source'.this did work:{}this worked for me:saved build in:as Selectdone!using windows 8.1, st2"},
{"body": "I've loaded uWSGI v 1.9.20, built from source.  I'm getting this error, but how do I tell which plugin is needed?Which plugin should be loaded?I had this problem and was stuck for hours.my issue is different than the answer listed, make sure you have  in your uwsgi ini file and you install the  plugin:After I did the above my application worked. Obviously this is for  projects, but a similar approach is required for other projects.It might be easiest to install uwsgi through pip instead of the package manager from the OS you're using, the package in pip is usually more up to date than the package managers from the OS you might be using:This solved it for me anyway.For using multiple Python versions on the same server, I would advice to take a look at virtualenv:\nOn my side, this is because instead of having  as the header of my configuration inside , I put something else (the name of the app). "},
{"body": "I have a string representing a path.  Because this application is used on Windows, OSX and Linux, we've defined environment variables to properly map volumes from the different file systems.  The result is:What I want to do is evaluate the environment variables in the string so that they're replaced by their respective volume names.  Is there a specific command I'm missing, or do I have to take os.environ.keys() and manually replace the strings?Use  to expand the environment variables in the string, for example:"},
{"body": "I'm trying to get hold of Django. I use Pydev on Eclipse. I have written a simple signup page that I can't get to work. Eclipse complains that User.DoesNotExist is undefined. Most likely, I am missing something trivial. Here's the relevant portion of the code:The problem is really with PyDev, not your code. What you have done is absolutely correct, but IDEs will always have difficulty resolving attributes in a dynamic language like Python. In the case of the DoesNotExist exception, it is added via a  rather than through normal object inheritance, so PyDev is unlikely to be able to find it. However, it should definitely work.I just discovered Pydev actually has a nice workaround for this.Go to  > , then  >  > .Click the  tab and add \"DoesNotExist\" to the text box titled .Pydev has a workaround for such cases (when the members are defined at runtime).\nJust add #@UndefinedVariable at the end of the string which cause the warning (or ctrl+1 on keyboard when the cursor is at \"DoesNotExist\"), and it won't complain.Can Eclipse resolve attributes created runtime via es?Notice that you never define a  on any of your models and it is not defined on  either.You can also solve it in a different way: just go to the User class, and add @DynamicAttrs in the docstring.\nThis will tell PyDev that attributes of the class are added dynamically, and will make it not complain any longer for \"issues\" like DoesNotExist.I have same problem on Ubuntu in a VirtualEnv to solve problem I have used this snippets.In parituclar he make custom User Fields with code: What do you mean by that? Do you have python error and stack trace? This code have to work (as in ). Looks like an eclipse issue. Just run dev server and see if it works or not:"},
{"body": "Is it possible with Python to set the timezone just like this in PHP:I can't really install any other modules etc as I'm using shared web hosting.Any ideas?To get the specific values you've listed:See  for a complete list of directives. Keep in mind that the strftime() function will always return a string, not an integer or other type.It's not an answer, but...To get  components individually, better use :It's now easy to get the parts:"},
{"body": "I was wondering if it is possible to perform a certain number of operations without storing the loop iteration number anywhere.For instance, let's say I want to print two \"hello\" messages on the console. Right now I know I can do:but then the \"i\" variable is going to take the values 0 and 1 (which I don't really need). Is there a way to achieve the same thing without storing that anywhere?I know it's not big deal... I'm just curious :-)Thank you in advanceThe idiom (shared by quite a few other languages) for an unused variable is a single underscore . Code analysers typically won't complain about  being unused, and programmers will instantly know it's a shortcut for . There is no way to iterate without having an item variable - as the Zen of Python puts it, \"special cases aren't special enough to break the rules\".should work, but I'm kind of ashamed that I thought of it.  Just thought of another one:Well I think the forloop you've provided in the question is about as good as it gets, but I want to point out that unused variables that have to be assigned can be assigned to the variable named , a convention for \"discarding\" the value assigned. Though the  reference will hold the value you gave it, code linters and other developers will understand you aren't using that reference. So here's an example:Although I agree completely with delnan's answer, it's not impossible:Note, however, that this will not work for an arbitrary list: If the first value in the list (the last one popped) does not evaluate to , you will get another iteration and an exception on the next pass: . Also, your list () will be empty after the loop.Just for curiosity's sake. ;)Sorry, but in order to iterate over anything in any language, Python and English included, an index must be stored. Be it in a variable or not. Finding a way to obscure the fact that python is internally tracking the for loop won't change the fact that it is. I'd recommend just leaving it as is.Others have addressed the inability to completely avoid an iteration variable in a  loop, but there are options to reduce the work a tiny amount.  has to generate a whole bunch of numbers after all, which involves a tiny amount of work; if you want to avoid even that, you can use  to just get the same (ignored) value back over and over, which involves no creation/retrieval of different objects:This will run faster in microbenchmarks than , but if the loop body does meaningful work, it's a drop in the bucket. And unlike multiplying some anonymous sequence for your loop iterable,  has only a trivial setup cost, with no memory overhead dependent on length.It's not idiomatic Python, but neither is what you're trying to do.It turns out that using dummy* (starting word is dummy) as a variable do the same trick as _. But you don't mess with already defined and sometimes used variable or a function. "},
{"body": "To deal with the  in django-admin, I've put special cases into two of the templates to create links between the admin change pages and inline admins of two models.I would like a general solution that I can apply to the admin change page or inline admin of any model.I have one model,  (not its real name) that is both an inline on the  admin page, and also has its own admin page. The reason it can't just be inline is that it has models with foreign keys to it that only make sense when edited with it, and it only makes sense when edited with .For the  admin page, I changed part of \"fieldset.html\" from:toto create a link to the  admin page, where  is a method on the model:I couldn't find the  of the  instance anywhere outside .On the  admin page, where  is inline, I modified part of \"stacked.html\" from:toto create a link to the  admin page since here I was able to find the  stored in the foreign key field.New in Django 1.8 : .Set  to  (False by default) in your inline model, so that inline objects have a link to their change form (where they can have their own inlines).Use :This is my current solution, based on what was suggested by Pannu (in his edit) and Mikhail.I have a couple of top-level admin change view I need to link to a top-level admin change view of a related object, and a couple of inline admin change views I need to link to the top-level admin change view of the same object. Because of that, I want to factor out the link method rather than repeating variations of it for every admin change view.I use a class decorator to create the  callable, and add it to .You can also pass a custom callable if you need to get your link text in some way than just calling  on the object you're linking to.I use it like this:Of course none of this would be necessary if I could nest the admin change views for  and  inside the inline admin of  on the  admin change page without patching Django.I think that agf's solution is pretty awesome -- lots of kudos to him. But I needed a couple more features:Solution:Usage:I am sorry that the example is so illogical, but I didn't want to use my data.I agree that its hard to do template editing so, I create a custom widget to show an  on the admin change view page(can be used on both forms and inline forms).So, I used the anchor widget, along with form overriding to get the link on the page.now in your  override the  attribute and you should get the desired result. I assumed you have a  relationship between these tables, If you have one to many then the  side will not work.  \n    I've made some changes to dynamically add links and that also solves the  issue with the  to  hope this solves the issue. :)\n   In Your  I noticed , that means your trying to show the  link on  which lists all the posts. If I'm correct then you should add a method to show the link on the page.As far as the showing  links on   you can do the same as above. My earlier solution will show you the link one level lower at the  page where you can edit each instance.If you want the  page to show the links to the  in the  page then you will have to include each in the  dynamically by overriding the  method for  and adding the link's dynamically, in  set the , but first don't use tuples to for  instead use a list.  Based on agfs and SummerBreeze's suggestions, I've improved the decorator to handle unicode better and to be able to link to backwards-foreignkey fields (ManyRelatedManager with one result). Also you can now add a short_description as a list header:Edit: updated due to link being gone.Looking through the source of the admin classes is enlightening: it shows that there is an object in context available to an admin view called \"original\".Here is a similar situation, where I needed some info added to a change list view:  (on my blog)."},
{"body": "In the below example I would expect all the elements to be tuples, why is a tuple converted to a string when it only contains a single string?Because those first two elements aren't tuples; they're just strings. The parenthesis don't automatically make them tuples. You have to add a comma after the string to indicate to python that it should be a tuple.To fix your example code, add a comma here:From the :Your first two examples are not tuples, they are strings. Single-item tuples require a trailing comma, as in: is not a tuple, but just a string.You need to add an extra comma at the end to make  take them as : -"},
{"body": "I am trying to solve a big numerical problem which involves lots of subproblems, and I'm using Python's multiprocessing module (specifically Pool.map) to split up different independent subproblems onto different cores. Each subproblem involves computing lots of sub-subproblems, and I'm trying to effectively memoize these results by storing them to a file if they have not been computed by any process yet, otherwise skip the computation and just read the results from the file. I'm having concurrency issues with the files: different processes sometimes check to see if a sub-subproblem has been computed yet (by looking for the file where the results would be stored), see that it hasn't, run the computation, then try to write the results to the same file at the same time. How do I avoid writing collisions like this?@GP89 mentioned a good solution. Use a queue to send the writing tasks to a dedicated process that has sole write access to the file. All the other workers have read only access. This will eliminate collisions.  Here is an example that uses apply_async, but it will work with map too:good luck,MikeIt looks to me that you need to use Manager to temporarily save your results to a list and then write the results from the list to a file.  Also, use starmap to pass the object you want to process and the managed list.  The first step is to build the parameter to be passed to starmap, which includes the managed list.from multiprocessing import Manager, Pool\n   import pandas as pdFrom this point you need to decide how you are going to handle the list.  If you have tons of RAM and a huge data set feel free to concatenate using pandas. Then you can save of the file very easily as a csv or a pickle."},
{"body": "I have a yaml file that looks likeI am able to read this correctly in Perl using YAML but not in python using YAML. It fails with the error: Program:Error:The yaml documents are separated by , and if any stream (e.g. a file) contains more than one document then you should use the  function rather than .  The code:results in for the input file as provided in the question:"},
{"body": "I'm using Pandas 0.10.1Considering this Dataframe:How can i group subtotals per state?I tried with a pivot table but i only can have subtotals in columnsI can achieve this on excel, with a pivot table.If you put State and City not both in the rows, you'll get separate margins. Reshape and you get the table you're after:I admit this isn't totally obvious. You can get the summarized values by using groupby() on the State column.Lets make some sample data first:Then apply the groupby function and add a column City:We can append the original data to the summed df by using append:I added the set_index and sort_index to make it look more like your example output, its not strictly necessary to get the results.How about this one ?I Think this subtotal example code is what you want(similar to excel subtotal)I assume that you want group by columns A, B, C, D, than count column value of Eoutput:"},
{"body": "I want to handle s both to hide unnecessary parts of the stack trace from the user and to print a message as to why the error occurred and what the user should do about it.I don't want to have to add this to every assert statement:because it repeats information.Use the  module:"},
{"body": "I'm getting the following error when performing recursive feature selection with cross-validation:The code that generates the error is the following:I know I should also perform GridSearch for the parameters of the LogisticRegression classifier, but I don't think that's the source of the error (or is it?). I should mention that I'm testing with around 50 features, and almost all of them are categoric (that's why I use the DictVectorizer to transform them appropriately). Any help or guidance you could give me is more than welcome. Thanks!Here's some training data examples:I finally got to solve the problem. Two things had to be done:Thanks to everyone who tried to help!If anyone is still interested,I used the  on something very similar and it gave me the same error. I realized that the vectorizer gives me a COO sparse matrix which is basically a coordinate list. Elements in COO matrices can't be accessed through row indexes. It is best to convert it to a CSR matrix (Compressed Sparse Row) which indexes row-wise. The conversion can be done easily . No other change is required, this worked for me."},
{"body": "Output: I roughly understand about decorators and how it works with one of it in most examples.In this example, there are 2 of it. From the output, it seems that  executes first, then .Does this mean that for decorated functions, it will first run the function first then moving towards to the top for other decorators? Like  first then , instead of the opposite.So this means that it is different from the norm of top-down approach in most programming lang? Just for this case of decorator? Or am I wrong? Decorators  the function they are decorating. So  decorated the result of the  decorator, which decorated the  function.The  syntax is really just syntactic sugar; the following:is really executed as:replacing the original  object with whatever  returned.Stacking decorators repeats that process .So your sample:can be expanded to:When you call  now, you are calling the object returned by , really.  returned a  that calls the function  wrapped, which is the return value of , which is also a lambda that calls the original . Expanding all these calls you get:so the output becomes:"},
{"body": "As a toy example I'm trying to fit a function  from 100 no-noise data points. The matlab default implementation is phenomenally successful with mean square difference ~10^-10, and interpolates perfectly.I implement a neural network with one hidden layer of 10 sigmoid neurons. I'm a beginner at neural networks so be on your guard against dumb code.Mean square difference ends at ~2*10^-3, so about 7 orders of magnitude worse than matlab. Visualising withwe can see the fit is systematically imperfect:\n\nwhile the matlab one looks perfect to the naked eye with the differences uniformly < 10^-5:\n\nI have tried to replicate with TensorFlow the diagram of the Matlab network:Incidentally, the diagram seems to imply a tanh rather than sigmoid activation function. I cannot find it anywhere in documentation to be sure. However, when I try to use a tanh neuron in TensorFlow the fitting quickly fails with  for variables. I do not know why.Matlab uses Levenberg\u2013Marquardt training algorithm. Bayesian regularization is even more successful with mean squares at 10^-12 (we are probably in the area of vapours of float arithmetic).Why is TensorFlow implementation so much worse, and what can I do to make it better?I tried training for 50000 iterations it got to 0.00012 error. It takes about 180 seconds on Tesla K40.It seems that for this kind of problem, first order gradient descent is not a good fit (pun intended), and you need Levenberg\u2013Marquardt or l-BFGS. I don't think anyone implemented them in TensorFlow yet.\nUse  for this problem. It gets to  after 4000 iterations. Also, GPU with default strategy also seems like a bad idea for this problem. There are many small operations and the overhead causes GPU version to run 3x slower than CPU on my machine.btw, here's a slightly cleaned up version of the above that cleans up some of the shape issues and unnecessary bouncing between tf and np.  It achieves 3e-08 after 40k steps, or about 1.5e-5 after 4000:All that said, it's probably not too surprising that LMA is doing better than a more general DNN-style optimizer for fitting a 2D curve.  Adam and the rest are targeting very high dimensionality problems, and  (see 12-15)."},
{"body": "I want my Python script to be able to read Unicode command line arguments in Windows. But it appears that sys.argv is a string encoded in some local encoding, rather than Unicode. How can I read the command line in full Unicode?Example code: On my PC set up for Japanese code page, I get:That's Shift-JIS encoded I believe, and it \"works\" for that filename. But it breaks for filenames with characters that aren't in the Shift-JIS character set\u2014the final \"open\" call fails:Note\u2014I'm talking about Python 2.x, not Python 3.0. I've found that Python 3.0 gives  as proper Unicode. But it's a bit early yet to transition to Python 3.0 (due to lack of 3rd party library support).A few answers have said I should decode according to whatever the  is encoded in. The problem with that is that it's not full Unicode, so some characters are not representable.Here's the use case that gives me grief: I have . I have file names with all sorts of characters, including some not in the system default code page. My Python script doesn't get the right Unicode filenames passed to it via sys.argv in all cases, when the characters aren't representable in the current code page encoding.There is certainly some Windows API to read the command line with full Unicode (and Python 3.0 does it). I assume the Python 2.x interpreter is not using it.Here is a solution that is just what I'm looking for, making a call to the Windows  function:\n (from ActiveState)But I've made several changes, to simplify its usage and better handle certain uses. Here is what I use:Now, the way I use it is simply to do:and from then on,  is a list of Unicode strings. The Python  module seems happy to parse it, which is great.Dealing with encodings is very confusing.I  if your inputing data via the commandline it will encode the data as whatever your system encoding is and is not unicode. (Even copy/paste should do this)So it should be correct to decode into unicode using the system encoding:running the following Will output:\n    Prompt> python myargv.py \"PC\u30fb\u30bd\u30d5\u30c8\u7533\u8acb\u66f808.09.24.txt\"Where the \"PC\u30fb\u30bd\u30d5\u30c8\u7533\u8acb\u66f808.09.24.txt\" contained the text, \"\u65e5\u672c\u8a9e\".\n(I encoded the file as utf8 using windows notepad, I'm a little stumped as to why there's a '?' in the begining when printing.  Something to do with how notepad saves utf8?)The strings 'decode' method or the unicode() builtin can be used to convert an encoding into unicode.Also, if your dealing with encoded files you may want to use the codecs.open() function in place of the built-in open().  It allows you to define the encoding of the file, and will then use the given encoding to transparently decode the content to unicode.So when you call   will be in unicode.codecs.open:\nIf I'm miss-understanding something please let me know.If you haven't already I recommend reading Joel's article on unicode and encoding:\nTry this:Maybe you have to substitute  or  for . You should be able to infer the proper encoding name from the registry key The command line might be in Windows encoding. Try decoding the arguments into  objects:"},
{"body": "With pytest, one can mark tests using a decoratorThen, from the command line, one can tell pytest to skip the tests marked \"slow\"If I have an additional tag:I would like to be able to skip both long AND slow tests.  I've tried this:and this:And neither seems to work.At the command line, how do I tell pytest to skip both the slow AND the long tests?Additionally, with the recent addition of the \"-m\" command line option you should be able to write:IOW, the \"-m\" option accepts an expression which can make use of markers as boolean values (if a marker does not exist on a test function it's value is False, if it exists, it is True).  Looking through the  code () and further experimentation shows the following seems to work:(Using the  option speeds up experimentation)It's also possible to stack the mark decorators.I then called  and only the tests with both decorators were called. resulted in the other tests running"},
{"body": "I have a dataframe  in pandas that was built using  from a csv file. The dataframe has several columns and it is indexed by one of the columns (which is unique, in that each row has a unique value for that column used for indexing.) How can I select rows of my dataframe based on a \"complex\" filter applied to multiple columns? I can easily select out the slice of the dataframe where column  is greater than 10 for example:But what if I wanted a filter like: select the slice of  where  of the columns are greater than 10? Or where the value for  is greater than 10 but the value for  is less than 5?How are these implemented in pandas?\nThanks.I encourage you to pose these questions on the , but in any case, it's still a very much low level affair working with the underlying NumPy arrays. For example, to select rows where the value in any column exceed, say, 1.5 in this example:Multiple conditions have to be combined using  or  (and parentheses!):I'd be very interested to have some kind of query API to make these kinds of things easierThere are at least a few approaches to shortening the syntax for this in Pandas, until it gets a full query API down the road (perhaps I'll try to join the github project and do this is time permits and if no one else already has started).One method to shorten the syntax a little is given below:To fully solve this, one would need to build something like the SQL select and where clauses into Pandas. This is not trivial at all, but one stab that I think might work for this is to use the Python  built-in module. This allows you to treat things like greater-than as functions instead of symbols. So you could do the following:Then a test example like yours would be to do the following:You can shorten the syntax even further by either building in more arguments to  to handle the different common logical operators automatically, or by importing them into the namespace with shorter names.Note that the  function above only works with logical-and chains of constraints. You'd have to modify it to get different logical behavior. Or use  and DeMorgan's Laws."},
{"body": "Let we have this code:The Python documentation says about  statement:So, the question is:\nDoes  execute every time when  is invoked?\nQuestion is about  statement exactly, not the  body.You can check the bytecode with the  module:As you can see the code for the inner function is  only . Every time you call  it is loaded and a new function object is created(in this sense the   executed every time  is called), but this doesn't add much overhead.The code in the inner function is compiled only once, so there shouldn't be a significant runtime penalty.Only inner's function  gets updated each time the outer function is called.  See , for example, for more details about closures.Here's a quick demonstration, examining the closure:"},
{"body": "I understand that Django, PyPy, and Psycopg2 all work correctly together, and  claims great performance improvements over CPython. Are there any downsides?The PyPy  lists Django as compatible, but it doesn't go into great detail about how much of Django was tested. I am not aware of any major Django deployment that runs PyPy instead of CPython. A better question is why you'd want to switch to PyPy for a Django app, especially as Django has been extensively tested and deployed with CPython.PyPy is good for tasks that are computationally intensive. Web apps are usually not. The Django benchmark they base their performance numbers off is essentially a template rendering benchmark which is a CPU intensive task. This is not representative of most web apps where the bottle neck tends to be I/O. As such, PyPy may not speed up your site as much as those graphs lead you to believe."},
{"body": "when I read the file I seeAm i doing anything wrong?This is the full text of the blog post linked below:If you've tried installing a package with pip recently, you may have encountered this error:This seems to be an issue with an old version of OpenSSL being incompatible with pip 1.3.1. If you're using a non-stock Python distribution (notably EPD 7.3), you're very likely to have a setup that isn't going to work with pip 1.3.1 without a shitload of work.The easy workaround for now, is to install pip 1.2.1, which does not require SSL:If you are using EPD, and you're not using it for a class where things might break, you may want to consider installing the new incarnation: Enthought Canopy. I know they were aware of the issues caused by the previous version of OpenSSL, and would imagine they are using a new version now that should play nicely with pip 1.3.1.I used to use the  workaround but I randomly  that if you're having this bug, you probably installed a 32bit version of python on a  64bit machine.In short : If you install a 64bit version of it by installing it from the source and then build your virtualenv upon it, you wont have that pip bug anymore.It's because in the pip configuration the index-url parameter should point to the new pypi url.Probably you have this:You shoudl change it for this:And then all should work as before.\nI hope it helps you.I had a similar problem with pip and easy_install:As suggested in the referenced blog post, there must be an issue with some older versions of OpenSSL being .Installing pip-1.2.1 is a working workaround..This definitely happens in RHEL/CentOS 4 distrosI faced the same issue and this error is because of 'Proxy Setting'. The syntax below helped me in resolving it successfully:The explanation is in your logs:Notice the . It seems the site was down when you were trying to do this. It's good to know that HTTP 5xx errors are server side errors, so you can know the problem was not in your local network but in the remote network.It means try again later ;-) (and cross fingers...) (It works for me now btw.)This has happened to my because of proxy-authntication, so I did this to resolve itOn Virtualenv try , like so:look at the first line and check if it corresponds to the project folder, if not just change it.pip has As of version 1.5, :Your system clock is likely set in the past.Check the time using the date command and set it rightI had the same issue with pip 1.5.6.I just deleted the ~/.pip folder and it worked like a charm.I had this error message occur as I had set a Windows Environment Variable to an invalid certificate file.Check if you have a CURL_CA_BUNDLE variable by typing  at the command prompt. You can override it for the current session with The pip.log contained the following:For me it worked a simple .As  states,  ignores installed packages, forcing reinstall instead.In my case the https port (443) wasn't open so my firewall blocked all traffic and pip couldn't download the packages. I had that problem too, after I tried to reset my network settings. it solves problem.This problem is most-likely caused by DNS setup: server cannot resolve the Domain Name, so cannot download the package. Solution: add a line: dns-nameservers 8.8.8.8save file and exitThen pip install should be working now. "},
{"body": "I can get Python to work with Postgresql but I cannot get it to work with MySQL. The main problem is that on the shared hosting account I have I do not have the ability to install things such as Django or PySQL, I generally fail when installing them on my computer so maybe it's good I can't install on the host.I found  really good because it does not require an install, it's a single file that I can look at, read and then call the functions of. Does anybody know of something like this for MySQL?MySQLdb is what I have used before.If you host is using Python version 2.5 or higher, support for sqlite3 databases is built in (sqlite allows you to have a relational database that is simply a file in your filesystem).  But buyer beware, sqlite is not suited for production, so it may depend what you are trying to do with it.Another option may be to call your host and complain, or change hosts.  Honestly these days, any self respecting web host that supports python and mysql ought to have MySQLdb pre installed.I don't have any experience with  as a web host personally.  This is just a guess, but it's common for a shared host to support Python and MySQL with the MySQLdb module (e.g., GoDaddy does this).  Try the following CGI script to see if MySQLdb is installed.I uploaded it and got an internal errorAfter much playing around, I found that if I hadIt would give me a much more useful answer and say that it was not installed, you can see it yourself -> Oddly enough, this would produce an errorI looked at some of the other files I had up there and it seems that library was one of the ones I had already tried.You could try setting up your own python installation using .  Check out how to setup Django using it .  That was written a long time ago, but it shows how I got MySQLdb setup without having root access or anything like it.  Once you've got the basics going, you can install any python library you want.You really want MySQLdb for any MySQL + Python code.  However, you shouldn't need root access or anything to use it.  You can build/install it in a user directory (~/lib/python2.x/site-packages), and just add that to your PYTHON_PATH env variable.  This should work for just about any python library.Give it a shot, there really isn't a good alternative.Take a pick atMySQLdb is mostly used driver, but if you are using python3 and django 1.8.x that will not work, then you should use mysqlclient that is a folk of MySQLdb on the following link"},
{"body": "I see quit a few implementations of unique string generation for things like uploaded image names, session IDs, et al, and many of them employ the usage of hashes like SHA1, or others.I'm not questioning the legitimacy of using custom methods like this, but rather just the reason. If I want a unique string, I just say this:And I'm done with it. I wasn't very trusting before I read up on uuid, so I did this:Not one repeater (I wouldn't expect one now considering the odds are like 1.108e+50, but it's comforting to see it in action). You could even half the odds by just making your string by combining 2 s.So, with that said, why do people spend time on random() and other stuff for unique strings, etc? Is there an important security issue or other regarding uuid?Using a hash to uniquely identify a resource allows you to generate a 'unique' reference from the object. For instance, Git uses SHA hashing to make a unique hash that represents the exact changeset of a single a commit. Since hashing is deterministic, you'll get the same hash for the same file every time. Two people across the world could make the same change to the same repo independently, and Git would know they made the same change. UUID v1, v2, and v4 can't support that since they have no relation to the file or the file's contents.Well, sometimes you want collisions.  If someone uploads the same exact image twice, maybe you'd rather tell them it's a duplicate rather than just make another copy with a new name.One possible reason is that you want the unique string to be human-readable.  UUIDs just aren't easy to read.uuids are long, and meaningless (for instance, if you order by uuid, you get a meaningless result).And, because it's too long, I wouldn't want to put it in a URL or expose it to the user in any shape or form.In addition to the other answers, hashes are really good for things that should be immutable. The name is unique and can be used to check the integrity of whatever it is attached to at any time.Also note other kinds of UUID could even be appropriate.  For example, if you want your identifier to be orderable, UUID1 is based in part on a timestamp.  It's all really about your application requirements..."},
{"body": "No idea why this error is popping up. Here are the models I created -Here is the view from which I access these models - At this point I get an error - As you see this column is not even present in my models? Why this error?maybe your tables schema has been changed? Also, running  does not update already created tables.You might need to drop all the tables & then run  again. Also remember to take backup of your data!!As @inception said my tables schema had changed & running  did not update already created tables.Apparently any changes to the models when updated through  does not change (as in update/modify) the actual tables. So I dropped the relevant DB & ran  on empty DB. Now it works fine. :) For others,  data migration tool for Django seems to be favorite option. It seems to provide options which django models &  falls short on. Must check out...Normally I get this when when I'm trying to access field which doesn't exist in Database.Check if the field exist in the database. If you change model and perform syncdb it won't update the database, I'm not sure if that's the case. On other note Django offers shortcut to replace try/except block in your code using get_object_or_404. (available in django.shortcuts )can be changed to:I have met the same problems:First, run and you can find:and I add the column manual:and then it worked.I think django will add the column called id itself.you can click  for details.Good luck.I created a model file for my app and then did several  as I refined my tables. One of the changes I made was I set  to one of my fields. At the end called for . Added a dummy value and and tried to access it with ,  being my model class. Although this worked fine, this error came up while printing the list returned by the  call. It read Surprisingly, I tried and could get it resolved with another call to . I remember not having seen the id column in the table at any time throughout the exercise when I checked it through the mysql command line client.In the appropriate field explicitly setI received this error while trying to use Django Rest Framework serializers. Make sure if you have a serializer that subclasses , that you migrate any changes to the Models before writing your serializer classes (or just comment anything related to the serializer, migrate, then uncomment). "},
{"body": "I have just started learning Python & have come across  concept in Python. While I got the jist of what it is, but am unable to appreciate the gravity of this concept. Some browsing on the net revealed that one of the reasons going against PHP is that it has no native support for namespaces. (not just in Python, as I assume namespaces in not a concept limited to a particular language). I am predominantly coming from Java and C programming backgrounds.Namespace is a way to implement scope.In Java (or C) the compiler determines where a variable is visible through static scope analysis.  In Python, each package, module, class, function and method function owns a \"namespace\" in which variable names are resolved.  Plus there's a global namespace that's used if the name isn't in the local namespace.Each variable name is checked in the local namespace (the body of the function, the module, etc.), and then checked in the global namespace.Variables are generally created only in a local namespace.  The  and  statements can create variables in other than the local namespace.When a function, method function, module or package is evaluated (that is, starts execution) a namespace is created.  Think of it as an \"evaluation context\".  When a function or method function, etc., finishes execution, the namespace is dropped.  The variables are dropped.  The objects may be dropped, also.Namespaces prevent conflicts between classes, methods and objects with the same name that might have been written by different people.Coming from a Java background you are probably familiar with how this is achieved using packages e.g. you might create a  class and I can create a  class and the package allows code using the classes to distinguish between them. (Python has .)Namespaces were added to PHP in 5.3.0 but in earlier versions (and in other languages that don't provide namespaces) you would have to prefix your class and method names with something to reduce the risk of a name clash. e.g. a  function.To understand namespaces, you also have to have some understanding of modules in Python. A module is simply a file containing Python code. This code can be in the form of Python classes, functions, or just a list of names. Each module gets it\u2019s own global namespaces. So you can\u2019t have two classes or two functions in the same module with the same name as they share the namespace of the module.reference: If you make a big program with someone else, you could write your own part of the program as you want. All variables in the file will be private, there will be no collisions.\nWhen you write PHP programs, it is easy to rewrite global variables by mistake. In python you can import other modules variables if you want, and they will be \"global\" on your module.You could think one file one object in Python. When you write PHP programs you can achieve the same by writing classes with instance variables.I complete the answer of S.Lott.I would say, namespace is a way to implement name management inside a scope, because a scope does more than name management.In C the scopes are of 4 types: global, function, block and function-parameters(prototype).  Each of these kinds can create one or more namespaces, depending on the needs. There are 4 ns in C\n -- tags for s/u/e\n -- ids for typenames, function names and var names\n -- parameters inside function prototype\n -- members and bitfields inside s/u.Like that, tag identifiers and function names do not collide, but typenames defined by typedef can collide with variable names.In  python there is a builtin namespace that encloses the global ns, and the global ns is provided by the loaded module. The builtin ns contain variables.  A symbol for a variable can define an object or a function -- for example,  is defined there.  The module's global ns lasts up to the termination.See also  and of course ."},
{"body": "I've seen this argument in a few places, and now, recently i saw it again on a reddit post.\nThis is by no means a flame against any of these two languages. I am just puzzled why there is this bad reputation about python not being scalable.\nI'm a python guy and now I'm getting started with Java and i just want to understand what makes Java so scalable and if the python setup that I have in mind is a good way to scale large python apps.Now back to my idea of scaling a Python app. Let's say you code it using Django. Django runs its apps in fastcgi mode. So what if you have a front Nginx server and behind it as many other servers as needed that will each run your Django app in fastcgi mode. The front Nginx server will then load balance between your backend Django fastcgi running servers. Django also supports multiple databases so you could write to one master DB and then read from many slaves, again for load balancing. Throw a memcached server in to this mix and there you go you have scalability. Don't you? Is this a viable setup? What does Java makes better? How do you scale a Java app?Scalability is a very overloaded term these days. The comments probably refer to in-process vertical scalability.Python has a global interpreter lock (GIL) that severely limits its ability to scale up to many threads. It releases it when calling native code (reacquiring it when the native returns), but this still requires careful design when trying to write scalable software in Python.While I don't agree with the statement, I suppose they think Java is more scalable because it runs a  faster. The JVM is very efficient (except perhaps in memory usage). Also Python's GIL (Global Interpreter Lock) doesn't allow \"real\" threading, while Java doesn't have a GIL and has true multithreading.Without getting into a flamewar, consider how Python handles multi-threaded apps as compared to Java? For example, what global locks are in place in both languages that hurt concurrency (hint, Python's GIL - Global Interpreter Lock)?I think this article sums up many of the arguments about scaling and dynamic languages:It's worth noting its two definition for scaling...One often used argument about any dynamic language scaling is that as the code-base grows it becomes harder to refactor it without IDE support. Due to the lack of type information at compile time this support is often impossible to implement in dynamic languages.Hmmm - scalable could mean many things - scalable by distributed architecture, scalable by speed?On the scalable by speed front, Java generally can process instructions faster than python - for the right kind of problem, much faster (I guess the main reason for that is that Java is compiled whereas Python is interpreted). From that point of view, Java can generally do more with less, and so is more scalable.I'm referring my source experimental information back to two sources;  and I get mad when I see arguments like this.  Not because I get all butthurt about haters harshing my Python mellow, but because to my mind, saying \"X doesn't scale\" is meaningless.  It is necessary to specify a dimension, at the very least.People are reluctant to do this, as it often reveals the fact that they don't have a good handle on the problem that they're speaking with confidence about.  The global interpreter lock is a good touchstone here:  threads are not the only way to perform concurrent operations."},
{"body": "When I install ipython on my osx and run it, I get the following warning: I have have installed readline, and do not use the system python that was originally installed in . The  points to version 2.7 as shown belowI have read the question in  - I added  to the   so that it now looks like this: but I cannot figure out why I am getting the following error:I do not think the above path is an issue, and my goal is to get ipython to working without complaining about readline even though it is installed and imports correctly.When pip installs readline, it will never be imported, because readline.so goes in site-packages, which ends up behind the libedit System one, located in  (OSX Python path order is very odd).   will actually install usable readline.  So you can either use easy_install, or use pip and muck about with your PYTHONPATH/sys.path (which essentially means: DO NOT USE PIP).A bit more detail on the IPython list (though there really isn't anything IPython-specific about this issue): EDIT: extra note about virtualenv.There is a bug in virtualenv < 1.8.3, where readline would not be properly staged when you create an env.If you don't mind mucking around with your PYTHONPATH, here's how you can get rid of that pesky warning:If you're using Django, you can put this in the  method of your site-packages/django/core/management/commands/shell.py so that it runs when you run .Additional note to future readers of this answer.In my case -- running a MacPorts installation of IPython -- there were several versions of  in /opt/local/bin/, but no non-versioned symlink pointing to the most current. Performing  worked."},
{"body": "Suppose I have an arrayHow can I (efficiently, Pythonically) find which elements of  are duplicates (i.e., non-unique values)?  In this case the result would be  or possibly  if efficient.I've come up with a few methods that appear to work:This one is cute but probably illegal (as  isn't actually unique):Is there anything I've missed?  I'm not necessarily looking for a numpy-only solution, but it has to work with numpy data types and be efficient on medium-sized data sets (up to 10 million in size).Testing with a 10 million size data set (on a 2.8GHz Xeon):The fastest is sorting, at 1.1s.  The dubious  is second at 2.6s, followed by masking and Pandas  at 3.1s,  at 5.6s, and  and senderle's  both at 7.3s.  Steven's  is only a little slower, at 10.5s; trailing behind are Burhan's  at 110s and DSM's  subtraction at 360s.I'm going to use sorting for performance, but I'm accepting Steven's answer because the performance is acceptable and it  clearer and more Pythonic.Edit: discovered the Pandas solution.  If Pandas is available it's clear and performs well.I think this is most clear done outside of .  You'll have to time it against your  solutions if you are concerned with speed.  This is similar to Burhan Khalid's answer, but the use of  without subscripting in the condition should be faster.People have already suggested  variants, but here's one which doesn't use a listcomp:[Posted not because it's efficient -- it's not -- but because I think it's cute that you can subtract  instances.]For Python 2.7+Here's another approach using set operations that I think is a bit more straightforward than the ones you offer:I suppose you're asking for -only solutions, since if that's not the case, it's very difficult to argue with just using a  instead. I think you should make that requirement explicit though.If  is made up of small integers you can use numpy.bincount directly:This is very similar your \"histogram\" method, which is the one I would use if  was not made up of small integers.If the array is a sorted numpy array, then just do:I'm adding my solution to the pile for this 3 year old question because none of the solutions fit what I wanted or used libs besides numpy. This method finds both the indices of duplicates and values for  sets of duplicates."},
{"body": "What's the proper way to tell a looping thread to stop looping? I have a fairly simple program that pings a specified host in a separate  class. In this class it sleeps 60 seconds, the runs again until the application quits.I'd like to implement a 'Stop' button in my  to ask the looping thread to stop. It doesn't need to end the thread right away, it can just stop looping once it wakes up. Here is my  class (note: I haven't implemented looping yet, but it would likely fall under the run method in PingAssets)And in my wxPyhton Frame I have this function called from a Start button:This has been asked before on Stack. See the following links:Basically you just need to set up the thread with a stop function that sets a sentinel value that the thread will check. In your case, you'll have the something in your loop check the sentinel value to see if it's changed and if it has, the loop can break and the thread can die.Instead of subclassing , one can modify the function to allow\nstopping by a flag.We need an object, accessible to running function, to which we set the flag to stop running.We can use  object.The trick is, that the running thread can have attached additional properties. The solution builds\non assumptions:Running the code, we get following output:Other alternative is to use  as function argument. It is by\ndefault , but external process can \"set it\" (to ) and function can\nlearn about it using  function.We can  with zero timeout, but we can also use it as the sleeping timer (used below).Advantage of pill to kill is better seen, if we have to stop multiple threads\nat once, as one pill will work for all.The  will not change at all, only the  handles the threads a bit differently.I read the other questions on Stack but I was still a little confused on communicating across classes. Here is how I approached it:I use a list to hold all my threads in the  method of my wxFrame class: As recommended in  I use a signal in my thread class which is set to  when initializing the threading class.and I can stop these threads by iterating over my threads:"},
{"body": "I have searched for other posts, as I felt this is a rather common problem, but all other Python exception questions I have found didn't reflect my problem.I will try to be as specific here as I can, so I will give a direct example. And pleeeeease do not post any workarounds for this specific problem. I am not specifically interested how you can send an email much nicer with xyz.\nI want to know how you generally deal with dependent, error prone statements.My question is, how to handle exceptions nicely, ones that depend on one another, meaning:\nOnly if the first step was successful, try the next, and so on. One more criterion is: All exceptions have to be caught, this code has to be robust.For your consideration, an example:This looks extremely unpythonic to me, and the error handling code is triple the real business code, but on the other hand how can I handle several statements that are dependent on one another, meaning statement1 is prerequisite for statement2 and so on?I am also interested in proper resource cleanup, even Python can manage that for itself. Thanks, TomInstead of using the try/except's else block, you could simply return when it errors:That's perfectly readable and Pythonic..Another way of doing this is, rather than worry about the specific implementation, decide how you want your code to look, for example..Then write the code for the  method, catching any errors you expect, and raising your own custom one, and handle that where it's relevant. Your class may look something like..In general, you want to use as few try blocks as possible, distinguishing failure conditions by the kinds of exceptions they throw.  For instance, here's my refactoring of the code you posted:Here, we use the fact that smtplib.SMTP(), server.login(), and server.sendmail() all throw different exceptions to flatten the tree of try-catch blocks.  In the finally block we test server explicitly to avoid invoking quit() on the nil object.We could also use three  try-catch blocks, returning False in the exception conditions, if there are overlapping exception cases that need to be handled separately:This isn't quite as nice, as you have to kill the server in more than one place, but now we can handle specific exception types different ways in different places without maintaining any extra state.If it was me I would probably do something like the following:All errors are caught and debugged, the return value == True on success, and the server connection is properly cleaned up if the initial connection is made.Just using one try-block is the way to go. This is exactly what they\nare designed for: only execute the next statement if the previous\nstatement did not throw an exception. As for the resource clean-ups,\nmaybe you can check the resource if it needs to be cleaned up\n(e.g. myfile.is_open(), ...) This does add some extra conditions, but\nthey will only be executed in the exceptional case. To handle the case\nthat the same Exception can be raised for different reasons, you\nshould be able to retrieve the reason from the Exception.I suggest code like this:It is no uncommon, that error handling code exceeds business code. Correct error handling can be complex.\nBut to increase maintainability it helps to separate the business code from the error handling code.I would try something like this:All of the methods (including , really) follow the same protocol:  they return True if they succeeded, and unless they actually  an exception, they don't trap it.  This protocol also makes it possible to handle the case where a method needs to indicate that it failed without raising an exception.  (If the only way your methods fail is by raising an exception, that simplifies the protocol.  If you're having to deal with a lot of non-exception failure states outside of the method that failed, you probably have a design problem that you haven't worked out yet.)The downside of this approach is that all of the methods have to use the same arguments.  I've opted for none, with the expectation that the methods I've stubbed out will end up manipulating class members.The upside of this approach is considerable, though.  First, you can add dozens of methods to the process without  getting any more complex.  You can also go crazy and do something like this:...though at that point, I might say to myself, \"self, you're working the Command pattern pretty hard without creating a Command class.  Maybe now is the time.\"Why not one big try: block? This way, if any exception is caught, you'll go all the way to the except. And as long as all the exceptions for the different steps are different, you can always tell which part it was that fired the exception.I like David's answer but if you are stuck on the server exceptions you can also check for server if is None or states.  I flattened out the method a bit bit it is still a but unpythonic looking but more readable in the logic at the bottom."},
{"body": "I'm writing a mixin which will allow my Models to be easily translated into a deep dict of values (kind of like .values(), but traversing relationships). The cleanest place to do the definitions of these seems to be in the models themselves, a la:However, Django complains about my including this in  with:(entire stack trace )Now, I suppose I could elaborately override this in my mixin, but is there a more elegant way of storing this information?I don't know about elegant, but one pragmatic way is:Obviously, this would break if Django ever added a 'schema' attribute of its own. But hey, it's a thought...you could always pick an attribute name which is less likely to clash."},
{"body": "When I try to open a file in  mode with the following code:Gives me the following error:The \"w\" mode should create the file if it doesn't exist, right? So how can this error ever occur?You'll see this error if the  containing the file you're trying to open does not exist, even when trying to open the file in \"w\" mode.Since you're opening the file with a relative path, it's possible that you're confused about exactly what that directory is. Try putting a quick print to check:Since you don't have a 'starting' slash, your python script is looking for this file relative to the current working directory (and not to the root of the filesystem). Also note that the directories leading up to the file must exist!And: use  to combine elements of a path.e.g.: Check that that the script has write permissions on that directory. Try this:Note that this will give write permissions to everyone on that directory.I had the same error, but in my case the cause was, under Windows, a path longer than ~250 characters."},
{"body": "I'm trying to compare a image to a list of other images and return a selection of images (like Google search images) of this list with up to 70% of similarity.I get this code in  and change for my contextMy question is, how can I compare the image with the list of images and get only the similar images? Is there any method to do this?I suggest you to take a look to the earth mover's distance (EMD) between the images.\nThis metric gives a feeling on how hard it is to tranform a normalized grayscale image into another, but can be generalized for color images. A very good analysis of this method can be found in the following paper:It can be done both on the whole image and on the histogram (which is really faster than the whole image method). I'm not sure of which method allow a full image comparision, but for histogram comparision you can use the  function.The only problem is that this method does not define a percentage of similarity, but a distance that you can filter on.I know that this is not a full working algorithm, but is still a base for it, so I hope it helps.Here is a spoof of how the EMD works in principle. The main idea is having two normalized matrices (two grayscale images divided by their sum), and defining a flux matrix that describe how you move the gray from one pixel to the other from the first image to obtain the second (it can be defined even for non normalized one, but is more difficult).In mathematical terms the flow matrix is actually a quadridimensional tensor that gives the flow from the point (i,j) of the old image to the point (k,l) of the new one, but if you flatten your images you can transform it to a normal matrix, just a little more hard to read.This Flow matrix has three constraints: each terms should be positive, the sum of each row should return the same value of the desitnation pixel and the sum of each column should return the value of the starting pixel.Given this you have to minimize the cost of the transformation, given by the sum of the products of each flow from (i,j) to (k,l) for the distance between (i,j) and (k,l).It looks a little complicated in words, so here is the test code. The logic is correct, I'm not sure why the scipy solver complains about it (you should look maybe to openOpt or something similar):the variable res contains the result of the minimization...but as I said I'm not sure why it complains about a singular matrix.The only problem with this algorithm is that is not very fast, so it's not possible to do it on demand, but you have to perform it with patience on the creation of the dataset and store somewhere the resultsI wrote a program to do something very similar maybe 2 years ago using Python/Cython. Later I rewrote it to Go to get better performance. The base idea comes from  IIRC.It basically computes a \"fingerprint\" for each image, and then compares these fingerprints to match similar images.The fingerprint is generated by resizing the image to 160x160, converting it to grayscale, adding some blur, normalizing it, then resizing it to 16x16 monochrome. At the end you have 256 bits of output: that's your fingerprint. This is very easy to do using :(The  in  is used to only extract the first frame of animated GIFs; if you're not interested in such images you can just remove it.)After applying this to 2 images, you will have 2 (256-bit) fingerprints,  and .The similarity score of these 2 images is then computed by XORing these 2 values and counting the bits set to 1. To do this bit counting, you can use the  function from : will be a number between 0 and 256 indicating how similar your images are. In my application I divide it by 2.56 (normalize to 0-100) and I've found that images with a normalized score of 20 or less are often identical.If you want to implement this method and use it to compare lots of images, I  suggest you use Cython (or just plain C) as much as possible: XORing and bit counting is very slow with pure Python integers.I'm really sorry but I can't find my Python code anymore. Right now I only have a Go version, but I'm afraid I can't post it here (tightly integrated in some other code, and probably a little ugly as it was my first serious program in Go...).There's also a very good \"find by similarity\" function in GQView/Geeqie; its source is .You are embarking on a massive problem, referred to as \"content based image retrieval\", or CBIR.  It's a massive and active field.  There are no finished algorithms or standard approaches yet, although there are a lot of techniques all with varying levels of success.Even Google image search doesn't do this (yet) - they do text-based image search - e.g., search for text in a page that's like the text you searched for.  (And I'm sure they're working on using CBIR; it's the holy grail for a lot of image processing researchers)If you have a tight deadline or need to get this done and working soon... yikes.Here's a ton of papers on the topic:Generally you will need to do a few things:This can involve , , . etc."},
{"body": "Given a sparse matrix listing, what's the best way to calculate the cosine similarity between each of the columns (or rows) in the matrix? I would rather not iterate n-choose-two times.Say the input matrix is:The sparse representation is:In Python, it's straightforward to work with the matrix-input format:Gives:That's fine for a full-matrix input, but I really want to start with the sparse representation (due to the size and sparsity of my matrix). Any ideas about how this could best be accomplished? Thanks in advance.You can compute pairwise cosine similarity on the rows of a sparse matrix directly using sklearn.  As of version 0.17 it also supports sparse output:Results:If you want column-wise cosine similarities simply transpose your input matrix beforehand: The following method is about 30 times faster than . It works pretty quickly on large matrices (assuming you have enough RAM)See below for a discussion of how to optimize for sparsity.If your problem is typical for large scale binary preference problems, you have a lot more entries in one dimension than the other. Also, the short dimension is the one whose entries you want to calculate similarities between. Let's call this dimension the 'item' dimension.If this is the case, list your 'items' in rows and create  using .  Then replace the first line as indicated.If your problem is atypical you'll need more modifications. Those should be pretty straightforward replacements of basic  operations with their  equivalents.You should check out  (). You can apply operations on those sparse matrices just like how you use a normal matrix.Hi you can do it this wayI took all these answers and wrote a script to 1. validate each of the results (see assertion below) and 2. see which is the fastest.\nCode and results are below:Results:"},
{"body": "I remember when I was developing in C++ or Java, the compiler usually complains for unused methods, functions or imports.  In my Django project, I have a bunch of Python files which have gone through a number of iterations.  Some of those files have a few lines of import statement at the top of the page and some of those imports are not used anymore.  Is there a way to locate those unused imports besides eyeballing each one of them in each file?All my imports are explicit, I don't usually write  (similar to lint) will give you this information.Use a tool like  which will signal these code defects (among a lot of others).Doing these kinds of 'pre-runtime' checks is hard in a language with dynamic typing, but pylint does a terrific job at catching these typos / leftovers from refactoring etc ...Have a look at . It is a debugging tool and able to find unused variables and modules.If you use the  IDE with  and , it provides automatic checking and highlighting for unused imports, among other things.  It integrates with pylint as well.I have been using  successfully and wished to auto-remove the unused imports.  I recently found :"},
{"body": "How does one convert a  from the  to a ? I have yet to find the magical function in  to do this, but one must be there.I like the survival function (upper tail probability) of the normal distribution a bit better, because the function name is more informative:normal distribution \"norm\"  is one of around 90 distributions in scipy.statsnorm.sf also calls the corresponding function in scipy.special as in gotgenes examplesmall advantage of survival function, sf: numerical precision should better for quantiles close to 1 than using the cdfAha! I found it: ! This also appears to be under  as well (which is just a pointer to ).Specifically, given a one-dimensional  instance , one can obtain the p-values asor alternativelyI think the cumulative distribution function (cdf) is preferred to the survivor function. The survivor function is defined as 1-cdf, and may communicate improperly the assumptions the language model uses for directional percentiles. Also, the percentage point function (ppf) is the inverse of the cdf, which is very convenient. from formula"},
{"body": "I'm trying to get the links from a page with xpath. The problem is that I only want the links inside a table, but if I apply the xpath expression on the whole page I'll capture links which I don't want.For example:The problem is that applies the expression to the whole document. I located the element I want, for example:But that seems to be performing the query in the whole document as well, as I still am capturing the links outside of the table.  says that \"When xpath() is used on an Element, the XPath expression is evaluated against the element (if relative) or against the root tree (if absolute):\". So, what I using is an absolute expression and I need to make it relative? Is that it?Basically, how can I go about filtering only elements that exist inside of this table?Your xpath starts with a slash () and is therefore absolute. Add a dot () in front to make it relative to the current element i.e."},
{"body": "I understand Django can only be run on Python 2.7 or below.  When will it run on Python 3.1 or above?  I see that Django 1.3 is coming out, does that support Python 3.x?It might run on Python 3 by the end of summer.  I don't know if that means \"officially\" or not, but it doesn't sounds like it, so official support is probably slightly later.The question is why you want to know? You can't run it on Python 3 today. That's all that matters. The day you can run it on python 3, then it matters. Before that it doesn't. [Except to those doing the porting, of course ;-) ].Predicting the future is easy. It's making the predictions come true that is the hard part. :-)Currently Django has passed py3k test.\nThe last port has been made available at \nCheck also news at And, finally, news from PyCon 2012:.Maybe targeting up to python 3.3 as both will be released in the same period (python 3.3 is scheduled for august 2012)    loewis has been working on a port of django to python 3 for a while.On Sept 9th, 2011 jacob (on of the core devs) started the py3k feature branch and pulled these changes in.AFAIK, no official release number or date has been set for when django will support python 3. More info on this thread:, but it is still experimental: The official FAQ mentions it might take , but I don't know when was that last revised. I've been trying hard to find exactly where I've read their exact policies on Python support, but based on Django's , you can understand that it will take some considerable time as it will likely happen through a major release only. Even if they'd decide to provide Py3k support , it could take anything from 1 to 4 major releases to deprecate support from Python 2.4 up to Python 2.7.Django 1.3 will not run on Python3k.Alex Gaynor thinks about making the port a google summer of code project:New version has been come out with Python 3 support (but still experimental). Check it out: "},
{"body": "Is there a way of telling pyplot.text() a location like you can with pyplot.legend()?Something like the legend argument would be excellent:I am trying to label subplots with different axes using letters (e.g. \"A\",\"B\"). I figure there's got to be a better way than manually estimating the position.ThanksJust use  and specify axis coordinates. For example, \"upper left\" would be: You could also get fancier and specify a constant offset in points:For more explanation see the examples  and the more detailed examples .I'm not sure if this was available when I originally posted the question but using the loc parameter can now actually be used. Below is an example:"},
{"body": "Just trying to test out very simple Python json commands, but having some trouble.should outputbut I get that same string, with a 'b' in front:Subsequently, when I try to runit gives me the error message, \"TypeError: can't use a string pattern on a bytes-like object\", which I'm assuming has something to do with the 'b'?I imported urlopen from urllib.request, and am running Python 3.Any ideas?The content from read() is of type  so you need to convert it to a string before trying to decode it into a json object.To convert  to a string, change your code to:\nIt worked well :You need to examine the charset specified in the  header and decode by that before passing it to .urllib is returning a byte array, which I assume is the default in py3, and json is expecting a string. Try wrapping the return value in a str() call before invoking the json callLooks like a .  Investigate how you get the data with http,  or how the API returns the data in the headers."},
{"body": "I currently try to use the mock library to write some basic nose unittests in python.After finishing some basic example I now tried to use  and now I have the mock package and the package I tried to 'mock away' are shown in the coverage report. Is there a possibility to exclude these?Here is the class I want to test:And the testcase:\n    from mock import patchI now get the following output for Is there any way to exclude the mock package and the imaplib package  having to manually whitelisting all but those packages by Thanks to Ned Batchelder I now know about the .coveragerc file, thanks for that!I created a .coveragerc file with the following content:Now my output for mock in the coverage report is:It does not cover the mock package any longer but still shows it in the report.I use Coverage.py, version 3.5.2 if this is any help.Create a .coveragerc file that excludes what you don't want in the report: In your .coveragerc move your  entry from the  section to the  section.I had a similar situation testing a series of sub-packages within my main package directory.  I was running  from within the top directory of my module and  and other libraries were included in the coverage report.  I tried using  in nosetests, but then the subpackages were not included.Running the following solved my problem:So,  all the code that you want to test is in the same directory,  you can get coverage for it alone by specifying the module path to .  This avoids the need to whitelist each of the submodules individually.(Python 2.7.6, coverage 4.0.3, nose 1.3.7)"},
{"body": "Is there any method to replace values with  in Pandas in Python?You can use  and can replace a value with another, but this can't be done if you want to replace with  value, which if you try, you get a strange result.So here's an example:which returns a successful result.But,which returns a following result:Why does such a strange result be returned?Since I want to pour this data frame into MySQL database, I can't put  values into any element in my data frame and instead want to put . Surely, you can first change  to  and then convert  to , but I want to know why the dataframe acts in such a terrible way.Actually in later versions of pandas this will give a TypeError:You can do it by passing either a list or a dictionary:But I recommend using NaNs rather than None: is probably what you're looking for. SoFrom the : "},
{"body": "I have two dataframes with the following column names:I would like to get a dataframe with the following columns by joining (left) on :I cannot figure out how to do it if the columns on which I want to join are not the index. What's the easiest way? Thanks!you can use the left_on and right_on options as follows:I was not sure from the question if you only wanted to merge if the key was in the left hand dataframe. If that is the case then the following will do that (the above will in effect do a many to many merge)you need to make  as index for the right frame:for your information, in pandas left join breaks when the right frame has non unique values on the joining column. see this .so you need to verify integrity before joining by "},
{"body": "I have a DataFrame like this one:In here, I want to ask how to get column name which has maximum value for each row, the desired output is like this:You can use  to find the column with the greatest value on each row:To create the new column use .You could  on dataframe and get  of each row via Here's a benchmark to compare how slow  method is to  for And if you want to produce a column containing the name of the column with the maximum value but considering only a subset of columns then you use a variation of @ajcr's answer:"},
{"body": "On the , an elbow method is described for determining the number of clusters in k-means.  provides an implementation but I am not sure I understand how the distortion as they call it, is calculated. Assuming that I have the following points with their associated centroids, what is a good way of calculating this measure? I am specifically looking at computing the 0.94.. measure given just the points and the centroids. I am not sure if any of the inbuilt methods of scipy can be used or I have to write my own. Any suggestions on how to do this efficiently for large number of points? In short, my questions (all related) are the following:The output for the first set of points is accurate. However, when I try a different set:I guess the last value does not match because  seems to be diving the value by the total number of points in the dataset.My code so far (should be added to Denis's K-means implementation):And following is the output for k=2:On my real dataset (does not look right to me!):The distortion, as far as  is concerned, is used as a stopping criterion (if the change between two iterations is less than some threshold, we assume convergence)If you want to calculate it from a set of points and the centroids, you can do the following (the code is in MATLAB using  function, but it should be straightforward to rewrite in Python/Numpy/Scipy):the result:I had some time to play around with this.. Here is an example of KMeans clustering applied on the  (4 features, 150 instances). We iterate over , plot the elbow curve, pick  as number of clusters, and show a scatter plot of the result.Note that I included a number of ways to compute the within-cluster variances (distortions), given the points and the centroids. The  function returns this measure by default (computed with Euclidean as a distance measure). You can also use the  function to calculate the distances with the function of your choice (provided you obtained the cluster centroids using the same distance measure:  have a solution for that), then compute the distortion from that.\nIn response to the comments, I give below another complete example using the : it has 1797 images of digits from 0 to 9, each of size 8-by-8 pixels. I repeat the experiment above slightly modified:  is applied to reduce the dimensionality from 64 down to 2:\n\nYou can see how some clusters actually correspond to distinguishable digits, while others don't match a single number.Note: An implementation of  is included in  (as well as many other clustering algorithms and various ).  is another similar example.A simple cluster measure:\n1) draw \"sunburst\" rays from each point to its nearest cluster centre,\n2) look at the lengths \u2014 distance( point, centre, metric=... ) \u2014 of all the rays.  For  and 1 cluster,\nthe average length-squared is the total variance ; for 2 clusters, it's less ... down to N clusters, lengths all 0.\n\"Percent of variance explained\" is 100 % - this average.Code for this, under :Like any long list of numbers, these distances can be looked at in various ways: np.mean(), np.histogram() ... Plotting, visualization, is not easy.\nSee also , in particular\n"},
{"body": "I know to never use built-in function names as variable identifiers. But are there any reasons not to use them as attribute or method identifiers?For example, is it safe to write , or define an instance method  in my own class?It won't confuse the interpreter but it may confuse people reading your code.  Unnecessary use of builtin names for attributes and methods should be avoided.Another ill-effect is that shadowing builtins confuses syntax highlighters in most python-aware editors (vi, emacs, pydev, idle, etc.)  Also, some of the lint tools will warn about this practice. No, that's fine. Since an object reference is required there is no way to have them shadow the built-in.Yes it's bad practice. It might not immediately break anything for you, but it still hurts readability of the code.To selectively quote from PEP20:Seeing a call to  it would be natural to assume that it's going to return , or that  returns the same thing as It's possible for them to find out that they're wrong; but that will take time and effort and probably lead to some mistakes while they figure it out. Calling your attribute  is much longer, but makes it clearer that it's different to "},
{"body": "Is that possible to add more static paths for my local dev Flask instance?\nI want to have default  folder for storing js/css/images files for the site and another folder, e.g.  to keep my specific assets. I don't want to place  folder inside  if there is a better solution exists.I have been using following approach:The  variable is defined in my configuration.And in templates: - I'm not really sure whether it's secure ;)You can use a Blueprint with its own static dir\nBlueprintTemplate"},
{"body": "Ruby's regular expressions have a feature called atomic grouping , described , is there any equivalent in Python's  module?Python does not directly support this feature, but you can emulate it by using a zero-width lookahead assert (), which matches from the current point with the same semantics you want, putting a named group () inside the lookahead, and then using a named backreference () to match exactly whatever the zero-width assertion matched. Combined together, this gives you the same semantics, at the cost of creating an additional matching group, and a lot of syntax.For example, the link you provided gives the Ruby example ofWe can emulate that in Python as such:We can show that I'm doing something useful and not just spewing line noise, because if we change it so that the inner group doesn't eat the final , it still matches:You can also use anonymous groups and numeric backreferences, but this gets awfully full of line-noise:(Full disclosure: I learned this trick from perl's  documentation, which mentions it under the documentation for .)In addition to having the right semantics, this also has the appropriate performance properties. If we port an example out of :We see a dramatic improvement:Which only gets more dramatic as we extend the length of the search string.According to , the answer is no. A  was created to add it to Python 3, but was declined in favor of the new  module, which supports it:Note:  is also available for Python 2.It would seem not.You can emulate the non-capturing-ness of them by using non-capturing groups, , but if I'm reading it right, that still won't give you the optimization benefits.from (?P<name>...)Similar to regular parentheses, but the substring matched by the group is accessible within the rest of the regular expression via the symbolic group name name. Group names must be valid Python identifiers, and each group name must be defined only once within a regular expression. A symbolic group is also a numbered group, just as if the group were not named. So the group named id in the example below can also be referenced as the numbered group 1.For example, if the pattern is (?P[a-zA-Z_]\\w*), the group can be referenced by its name in arguments to methods of match objects, such as m.group('id') or m.end('id'), and also by name in the regular expression itself (using (?P=id)) and replacement text given to .sub() (using \\g).(?P=name)"},
{"body": "I connected to a mysql database using python \nThe program that I wrote takes a lot of time in full execution i.e. around 10 hours. Actually, I am trying to read distinct words from a corpus. \nAfter reading was finished there was a timeout error.I checked Mysql default timeouts which were:How can I change the default timeout ?Do:You change default value in MySQL configuration file (option  in  section) -If this file is not accessible for you, then you can set this value using this statement -I know this is an old question but just for the record this can also be done by passing appropriate connection options as arguments to the  call. For example,Notice the use of keyword parameters (host, passwd, etc.). They improve the readability of your code.For detail about different arguments that you can pass to , see open phpmyadmin>setting>Feature then incress \"Login cookie validity\" by default its 1440 second"},
{"body": "I am doing some scripts in python. I create a string that I save in a file. This string got lot of data, coming from the arborescence and filenames of a directory.\nAccording to convmv, all my arborescence is in UTF-8.I want to keep everything in UTF-8 because I will save it in MySQL after.\nFor now, in MySQL, which is in UTF-8, I got some problem with some characters (like \u00e9 or \u00e8 - I'am French).I want that python always use string as UTF-8. I read some informations on the internet and i did like this.My script begin with this :And when I execute, here is the answer : Edit:\nI see, in my file, the accent are nicely written. After creating this file, I read it and I write it into MySQL.\nBut I dont understand why, but I got problem with encoding.\nMy MySQL database is in utf8, or seems to be SQL query  returns me only utf8 or binary.My function looks like this : And artiste who are nicely displayed in the file writes bad into the BDD.\nWhat is the problem ?You don't need to encode data that is  encoded. When you try to do that, Python will first try to  it to  before it can encode it back to UTF-8. That is what is failing here:Just write your data directly to the file, there is  need to encode already-encoded data.If you instead build up  values instead, you would indeed have to encode those to be writable to a file. You'd want to use  instead, which returns a file object that will encode unicode values to UTF-8 for you.You also  don't want to write out the UTF-8 BOM,  you  to support Microsoft tools that cannot read UTF-8 otherwise (such as MS Notepad).For your MySQL insert problem, you need to do two things:It may actually work better if you used  to decode the contents automatically instead:You may want to brush up on Unicode and UTF-8 and encodings. I can recommend the following articles:Unfortunately, the string.encode() method is not always reliable. Check out this thread for more information: "},
{"body": "Is it possible to mock a return value of a function called within another function I am trying to test? I would like the mocked method (which will be called in many methods I'm testing) to returned my specified variables each time it is called. For example: and in the unit test, I would like to use mock to change the return value of  so that any time is is called in , it will return what I defined in There are two ways you can do this; with patch and with patch.objectPatch assumes that you are not directly importing the object but that it is being used by the object you are testing as in the followingIf you are directly importing the module to be tested, you can use patch.object as follows:In both cases some_fn will be 'un-mocked' after the test function is complete.Edit:\nIn order to mock multiple functions, just add more decorators to the function and add arguments to take in the extra parametersNote that the closer the decorator is to the function definition, the earlier it is in the parameter list.It can be done with something like this:Here's a source that you can read: Let me clarify what you're talking about: you want to test  in a testcase, which calls external method . Instead of calling the actual method, you want to mock the return value.Okay, suppose the above code is in ,  is defined in module . Here is the unittest:If you want to change the return value every time you passed in different arguements,  provides ."},
{"body": "Difference between setUpClass vs. setUp in python Unittest framework, why not do set up part in setUp instead of setUpClass?I want to understand what part of setup is done in setUp and setUpClass functions as well with tearDown and tearDownClass.The difference manifests itself when you have more than one test method in your class.  and  are run once for the whole class;  and  are run before and after each test method.For example:When you run this test, it prints:(The dots () are 's default output when a test passes.) Observe that  and  appear before and after   , whereas  and  appear only once, at the beginning and end of the whole test case."},
{"body": "How do I install a .whl file?  I have the Wheel library but I don't know how to use it to install those files.  I have the .whl file but I don't know how to run it.  Please help.You normally use a tool like  to install wheels. Leave it to the tool to discover and download the file if this is for a project hosted on PyPI.For this to work, you do need to install the  package:You can then tell  to install the project (and it'll download the wheel if available), or the wheel file directly:The  module, once installed, also is runnable from the command line, you can use this to install already-downloaded wheels:Also see the ."},
{"body": "I'm speaking of this module:\nFrom the article:I'm not sure I understand the benefit or purpose of this module.Possibly the most popular usage is operator.itemgetter.  Given a list  of tuples, you can sort by the ith element by: Certainly, you could do the same thing without operator by defining your own key function, but the operator module makes it slightly neater.As to the rest, python allows a functional style of programming, and so it can come up -- for instance, Greg's reduce example.You might argue: \"Why do I need  when I can just do: ?\" The answers are:One example is in the use of the  function:"},
{"body": "So I knocked up some test code to see how the multiprocessing module would scale on cpu bound work compared to threading. On linux I get the performance increase that I'd expect:My dual core macbook pro shows the same behavior:I then went and tried it on a windows machine and got some very different results.Why oh why, is the multiprocessing approach so much slower on windows?Here's the test code:Processes are much more  lightweight under UNIX variants. Windows processes are heavy and take much more time to start up. Threads are the recommended way of doing multiprocessing on windows.The  blames the lack of os.fork() for the problems in Windows. It may be applicable here.See what happens when you import psyco. First, easy_install it:Add this to the top of your python script:I get these results without:I get these results with:Parallel is still slow, but the others burn rubber.Edit: also, try it with the multiprocessing pool. (This is my first time trying this and it is so fast, I figure I must be missing something.)Results:It's been said that creating processes on Windows is more expensive than on linux.  If you search around the site you will find some information. Here's  I found easily.Currently, your counter() function is not modifying much state.  Try changing counter() so that it modifies many pages of memory.  Then run a cpu bound loop.  See if there is still a large disparity between linux and windows.I'm not running python 2.6 right now, so I can't try it myself.Just starting the pool takes a long time. I have found in 'real world' programs if I can keep a pool open and reuse it for many different processes,passing the reference down through method calls (usually using map.async) then on Linux I can save a few percent but on Windows I can often halve the time taken. Linux is always quicker for my particular problems but even on Windows I get net benefits from multiprocessing."},
{"body": "I need to create a diff file using standard UNIX  command with python  module. The problem is that I must compare file and stream without creating tempopary file. I thought about using named pipes via  method, but didn't reach any good result. Please, can you write a simple example on how to solve this stuff? I tried like so:but it seems like  doesn't see the second argument.You can use \"-\" as an argument to  to mean .You could perhaps consider using the  python module (I've linked to an example here) and create something that generates and prints the diff directly rather than relying on . The various function methods inside difflib can receive character buffers which can be processed into diffs of various types. Alternatively, you can construct a shell pipeline and use process substitution like soFor details, check out "},
{"body": "I'm interested in subclassing the built-in  type in Python (I'm using v. 2.5), but having some trouble getting the initialization working.Here's some example code, which should be fairly obvious.However, when I try to use this I get:where I'd expect the result to be .What am I doing wrong? Google, so far, hasn't been very helpful, but I'm not really sure what I should be searching for is immutable so you can't modify it after they are created, use  instead"},
{"body": "when I use the Django shell, it shows an error; this is the error:What can I do?The model definition must come in an application - the error you're seeing there is that it tries to take the   - which should be something like  for  - and get the app name, . In the interactive console, the module's  is  - so it fails.To get around this, you'll need to specify the  yourself in the  class;For explanation of why you can do that, look at that file mentioned in the traceback, :(Where  is the  class, see just above in that file.)That other answer definitely works for the interactive prompt, however, I don't think that the intention of the first block of code was intended to actually be run. Immediately following that code in , you are expected to put the next codes into your models.py file created during the previous tutorial... I guess that's why they subtly labeled that section \"Quick Example.\" What a headache for me too!I ran into this problem using Eclipse, Django and PyDev. I needed to have the application (instead of some .py file for example) selected in the PyDev Package Explorer (left panel) before clicking Run for everything to work properly."},
{"body": "I'm working on a classification problem with unbalanced classes (5% 1's). I want to predict the class, not the probability.In a binary classification problem, is scikit's  using  by default?\nIf it doesn't, what's the default method? If it does, how do I change it?In scikit some classifiers have the  option, but not all do. With , would  use the actual population proportion as a threshold?What would be the way to do this in a classifier like  that doesn't support ? Other than using  and then calculation the classes myself.In probabilistic classifiers, yes. It's the only sensible threshold from a mathematical viewpoint, as others have explained.You can set the , which is the prior probability P() per class . That effectively shifts the decision boundary. E.g.The threshold in scikit learn is 0.5 for binary classification and whichever class has the greatest probability for multiclass classification. In many problems a much better result may be obtained by adjusting the threshold. However, this must be done with care and NOT on the holdout test data but by cross validation on the training data. If you do any adjustment of the threshold on your test data you are just overfitting the test data.Most methods of adjusting the threshold is based on the  and  but it can also be done by other methods such as a search with a genetic algorithm.Here is a peer review journal article describing doing this in medicine: So far as I know there is no package for doing it in Python but it is relatively simple (but inefficient) to find it with a brute force search in Python.This is some R code that does it. You seem to be confusing concepts here. Threshold is not a concept for a \"generic classifier\" - the most basic approaches are based on some tunable threshold, but most of the existing methods create complex rules for classification which cannot (or at least shouldn't) be seen as a thresholding.So first - one cannot answer your question for scikit's classifier default threshold because there is no such thing.Second - class weighting is not about threshold, is about classifier ability to deal with imbalanced classes, and it is something dependent on a particular classifier. For example - in SVM case it is the way of weighting the slack variables in the optimization problem, or if you prefer - the upper bounds for the lagrange multipliers values connected with particular classes. Setting this to 'auto' means using some default heuristic, but once again - it cannot be simply translated into some thresholding.Naive Bayes on the other hand  estimates the classes probability from the training set. It is called \"class prior\" and you can set it in the constructor with \"class_prior\" variable.From the :0.5 is not related to the population proportion in any way. Its a probability output. There is no \"threshold\", if one class has a probability of 0.51, then it appears to be the most likely class. 0.5 if always the what should be used*, and no package uses a different \"threshold\". If your probability scores are a*ccurate and truly representative*, then you . To do otherwise can only reduce your accuracy. Since we are using various algorithms that make assumptions, we don't know that the probability is true - but you would be going against the assumptions made by your model. You  confused on what class_weight does. Changing the class weight increase the weights for data points in the less represented classes (/ decreasing for the over represented class) so that the \"weight\" of each class is equal - as if they had the same number of positive and negative examples. This is a common trick for trying to avoid a classifier that always votes for the most common class. Because this way, both classes are equally common from the learning algorithm's view. "},
{"body": "Let's say I have a dictionary:It has a method :... which has a  attribute:... which is callable:So why can't I hash it? I know that  objects are unhashable \u2013 I'm curious as to why this restriction extends to their methods, even though, as noted above, they appear to claim otherwise?It is a bound method, and bound methods have a reference to , e.g. the dictionary. This makes the method un-hashable.You  hash the unbound  method:Methods on instances that  hashable will themselves be hashable, so the object type for built-in bound methods implements a  method but raises  when the  attribute is not hashable. This is consistent with the  method documentation; if you can set it to  or not implement it at all then that is preferable but for these cases where the hashability is only known at runtime raising a  is the only option available.Martijn is right as he very often is. If you have a  subclass that does implement the  method, even the bound methods become hashableOutput:"},
{"body": "I have a fresh install (started with a wiped drive) of Snow Leopard with the developer tools installed during the Snow Leopard installation.I then installed Python 2.6.2, replacing the Snow Leopard default python 2.6.1. I've tried to install PIL by:All yield the same error (link to  log: ). I've seen others have had success installing PIL using the Snow Leopard default python 2.6.1, so I'm not sure why I'm having so much trouble getting it to work with 2.6.2.The python.org Python was built with an earlier gcc. Try using gcc-4.0 instead of SL's default of 4.2:See similar problem .That gets past the stdarg problem.  You may then run into later build problems with various dependent libraries.BTW, gcc-4.0 and gcc-4.2 are both included with Snow Leopard's Xcode  so no additional installs are needed.UPDATED 2011-05:  Note, that the newer Xcode , released for experimental use with 10.6 and expected to be standard with 10.7, no longer includes PPC support so, if you install Xcode 4, this suggestion will not work.  Options include using the newer 64-bit/32-bin Python 2.7.x installers from python.org or installing a newer Python 2.6 and PIL and the various 3rd-party libs using MacPorts, Homebrew, or Fink.The problem I ran into was that PIL was being compiled against PowerPC architecture (-arch ppc).Do this before setup/build/compile:(Assuming you're on i386)Here are the steps that I took to successfully install PIL on Mac OS X 10.6 (without using MacPorts or Fink).: I was not able to install PIL using , so I installed from the URL as shown above.From what I can see in your  file it appears that you installed Python 2.6.2 using the  from Python.org released on April 16, 2009. Can you confirm this?From the pip log, gcc failed with exit status 1. The offending  command from your pip log is as follows:This appears to be a problem related to Snow Leopard changing the default value for the -arch flag from  to  according to Ronald Oussoren in  of . There is a patch available Python 2.6.2, but it has not been integrated into the Mac Installer Disk Image.Your best solution that doesn't involve MacPorts or Fink would probably be to compile and install Python from the 2.6 release branch from either the  or the . According to  of , Ronald Oussoren fixed this in . I've been seeing similar errors using Python 2.6.2 installed from the Mac Disk Image while trying to then install Fabric in a virtualenv, so I plan to compile and install from the 2.6 release maintenance branch. If you want, I'll update when successful.10.6 Snow Leopard install PIL without the hassle and without keeping MacPorts :)\n\nStep 1: Install MacPorts\nStep 2: sudo port install py26-pil\nStep 3: mv /opt/local/Library/Frameworks/Python.framework/Versions/2.6/lib/python2.6/site-packages/* /Library/Python/2.6/site-packages/\nStep 4: Uninstall MacPorts\n\nBest of both worlds?Following steps worked for me:IT seems to me that the \"No such file\" is conjunction with stdarg.h is the most interesting error. There seems to be a header file missing. I don't know how to make sure it's installed on OS X, so this only half an answer, sorry about that, but maybe it pushes you in the right direction.May be you should try pre-build universal binaries from pythonmac siteThese are for python2.5 , with python2.5 included(so may or may not be usable for you), I have been using it since I had problem using self build PIL with py2app.I found a simpler method.\nsudo port install python26\nsudo port install python_selectThen use python_select set python26 as default.Then just install PIL as normal.I was able to get PIP installed with SL's Python using these instructions:Do you have  (comes on the Snow Leopard disc) installed? There are some key components (most notably ) that need to be installed which XCode handles for you. Uninstalled and Installed Xcode, suggested here:\nto remove Xcode properlly follow this answer:\nuse the install Xcode.app after you restart your mac after xcode was reinstalled, the installation failed to resolve that i followed this post:\nGood luck!On OS X Lion with current XCode and no gcc-4.0 I'm able to get around the missing stdard.h error by setting the follow environment variables:I can't say I understand why this works.By the way this works for the Pillow fork of PIL too."},
{"body": "I need to create a fake helper class to be used in unit tests (injected into tested classes). Is there some way to use TestCase assertions in such class?I would like to use the assertions for some common checks performed by the Fake class. Something like:You can create a instance of  and call the methods on that, provided you pass in the name of an  method on the class.  will do in this case:However, you are probably looking for a good  instead.  would be a good choice."},
{"body": "Is there a standard way (without installing third party libraries) to do cross platform filesystem mocking in python? If I have to go with a third party library, which library is the standard?The standard mocking framework in Python 3.3+ is ; you can use this for the filesystem or anything else.You could also simply hand roll it by mocking via monkey patching:A trivial example:A bit more full (untested):In this example, the actual mocks are trivial, but you could back them with something that has state so that can represent filesystem actions, such as save and delete. Yes, this is all a bit ugly since it entails replicating/simulating basic filesystem in code.Note that you can't monkey patch python builtins. That being said...For earlier versions, if at all possible use a third party library, I'd go with Michael Foord's awesome , which is now  in the standard library since 3.3+ thanks to , and you can get it on  for Python 2.5+. And, it can mock builtins! () does what you want \u2013 a  filesystem; it\u2019s third-party, though that party is Google. See  for discussion of use.For ,  is the standard library for Python 3.3+ (); for earlier version see  (for Python 2.5+) ().Terminology in testing and mocking is inconsistent; using the  terminology of Gerard Meszaros, you\u2019re asking for a \u201cfake\u201d: something that behaves like a filesystem (you can create, open, and delete files), but isn\u2019t the actual file system (in this case it\u2019s in-memory), so you don\u2019t need to have test files or a temporary directory.In classic mocking, you would instead  out the system calls (in Python, mock out functions in the  module, like   and ), but that\u2019s much more fiddly. is gaining a lot of traction, and it can do all of this using  and  (mocking).You can use the  function argument which will provide a temporary directory unique to the test invocation, created in the base temporary directory (which are by default created as sub-directories of the system temporary directory).The  function argument helps you to safely set/delete an attribute, dictionary item or environment variable or to modify  for importing. You can also pass it a function instead of using lambda.If your application has a lot of interaction with the file system, then it might be easier to use something like , as mocking would become tedious and repetitive. allows you to create a fake file system, write and read files, set permissions and more without ever touching your real disk.It also contains a practical example and tutorial showing you how to apply it to unittest and doctest.Personally, I find that there are a lot of edge cases in filesystem things (like opening the file with the right permissions, string-vs-binary, read/write mode, etc), and using an accurate fake filesystem can find a lot of bugs that you might not find by mocking. In this case, I would check out the  module of  (it has various concrete implementations of the same interface, so you can swap them out in your code).That said, if you really want to mock, you can do that easily with Python's  library:The above example only demonstrates creating and writing to files via mocking the  method, but you could just as easily mock any method."},
{"body": "Does anyone know of a good platform agnostic example or library that does Facebook authentication and Graph API access via Python?The official  is tied to Google App Engine and  is deeply woven with Django. I just want to be able to mess around in terminal and go through the process of authenticating a user and then doing simple requests from the Facebook API.Thanks.I ran across same problem some time ago and later found out that PyFacebook isn't deeply tied up with Django. It just uses a few utils from django.My recommendation is that you setup PyFacebook alongwith django and then play around with it using command line. To use PyFacebook you won't have to go through or even know anything about django at all.Here's an example:You might need to get an infinite session key which you can get from here: Use this code to get convert the code from above URL into infinite session key:A new library that is available is:\nIt currently support authentication and the dialog API.  Planned in the near future(currently being worked on) is a wrapper around the graph API.The project goal is to be platform agnostic, single file, and use only standard Python libraries.How about taking the Facebook Python SDk itself and stripping off the GAE part from it and using the other API calls only?"},
{"body": "I would like to compute the following using numpy or scipy:where  is a  matrix,  is the transpose of  and  is an  diagonal matrix.Since  is a diagonal matrix I store only its diagonal elements as a vector.Currently I can think of two ways of how to calculate :Clearly option 2 is better than option 1 since no real matrix has to be created with  (if this is what numpy really does...)\nHowever, both methods suffer from the defect of having to allocate more memory than there really is necessary since  and  have to be stored along with  in order to calculate .Is there a method in numpy/scipy that would eliminate the unnecessary allocation of extra memory where you would only pass two matrices  and  (in my case  is ) and a weighting vector  along with it?(w/r/t the last sentence of the OP: i am  aware of such a numpy/scipy method but w/r/t the Question in the OP Title (i.e., improving NumPy dot performance) what's below should be of some help. In other words, my answer is directed to improving performance of most of the  your function for Y).\nFirst, this should give you a noticeable boost over the vanilla NumPy  method:Note that the two arrays, v1, v2 are  in C_FORTRAN orderYou can access the byte order of a NumPy array through an array's  attribute like so:to change the order of one of the arrays so both are aligned, just call the NumPy array constructor, pass in the array and set the appropriate  flag to TrueYou can further optimize by exploiting array-order alignment to .But why are the arrays copied before being passed to ?The dot product relies on BLAS operations. These operations require arrays stored in C-contiguous order--it's this constraint that causes the arrays to be copied.On the other hand, the  does  effect a copy, though unfortunately returns the result in :Therefore, to remove the performance bottleneck, you need to ; to do that just requires passing both arrays to  in C-contiguous order*.So to calculate   making an extra copy:In sum, the expression just above (along with the predicate import statement) can substitute for dot, to supply the same functionality but better performanceyou can bind that expression to a function like so:I just wanted to put that up on SO, but this pull request should be helpful and remove the need for a separate function for numpy.dot\n\nThis should be available in numpy 1.7In the meantime, I used the example above to write a function that can replace numpy dot, whatever the order of your arrays are, and make the right call to fblas.dgemm.\nHope this helps, is what you're looking for:This shall not need any additional memory (though usually einsum works slowlier than BLAS operations)"},
{"body": "This question is not a duplicate. It pertains not just to  a virtual environment, but to actually  it to a different directory, including, potentially, a different user's directory. This is not the same as merely renaming a virtual environment, especially to people unfamiliar with virtualenvs.If I create a virtualenv, and I move it to a different folder, will it still work? ...later that day, the virtual environment MOVED...  Question: Will this work? I mean this as less of a question about the wisdom of trying this (unless that wisdom is humorous, of course), and more about whether it's possible. I really want to know whether it's possible to do in Python 3, or whether I just have to  and clone it. Can I just  a  like that without sadness? I do want to avoid sadness.Yes. It is possible to move it on the same platform. You can use  on an existing environment.From :HOWEVER, this does NOT seem to change the  script, and rather only changes the  and  scripts.  In the  script, the   environment variable hardcoded as the original . The  variable is used to set the  of your active environment too, so it must be changed based on the new location in order to call  and  etc. without absolute path.To fix this issue, you can change the  environment variable in the  script (for example using ), and everything should be good to go.An example of usage:Hooray, now you can install things and load them into your newly located virtual environment.The  argument to  appears to allow you to do this.Yes, this should be possible if you haven't done anything that depends on the current directory of the virtualenv.However, if you have the choice, the best thing to do is to create new virtualenv and start using the new virtualenv instead. This is the safest choice and least likely to cause issues later.The documentation :For example, if you have run  then it won't be able to switch to the right directory after you run  so in that case you'd need to fix that manually.In general a virtualenv is little more than a directory with the necessary Python interpreter files plus packages that you need.// , BUT ALAS: ... presses  a lot in frustration, and the following worksExcept it is not from , ergo sadness. Want to  your  and use it, otherwise unmodified?  Well, ya ."},
{"body": "I'm trying to run a very simple code that outputs a .png file in a cluster. Here's the code:If I run this code with the command  in my system which has matplotlib 1.2.1 installed I get the warning:The .png image is still produced so I have no problems here. But if I use the same command and code in a cluster which has matplotlib 1.3.0 installed it fails with the error:What is happening here?Add, this is the script I use to login into the cluster:Your problem is in ssh command. What you need to do is to write it this way:"},
{"body": "Is there a function for drawing a caption box underneath a figure/graph using matplotlib? I have searched google and haven't found any such function.something like what is shown in the image would be great.Use .  Here is some sample code:It produces this:\nIf you want automatic word-wrapping, have a look at .I'm not sure I can help you get it full-justified.You can use the  function (). If you want it outside the axes, you can pass to it a specific location ( keyword argument). The legend function takes a  argument, which might help getting what you want. However, for a caption without any legend, Paul's answer is more suited."},
{"body": "My code is:but the scriptlib is in some other directory, so I will have to include that directory in environment variable \"PYTHONPATH\".Is there anyway in which I can first add the scriptlib directory in environment variable \"PYTHONPATH\" before import statement getting executed like :If so, is the value only for that command prompt or is it global ?Thanks in advanceThis will add a path to your Python process / instance (i.e. the running executable). The path will not be modified for any other Python processes. Another running Python program will not have its path modified, and if you exit your program and run again the path will not include what you added before. What are you are doing is generally correct.set.py:loop.pyrun: This will run loop.py, connected to your STDOUT, and it will continue to run in the background. You can then run . Each has a different set of environment variables. Observe that the output from  does not change because  does not change 's environment.Python imports are dynamic, like the rest of the language. There is no static linking going on. The import is an executable line, just like .As also noted in the docs .\nGo to  and add these lines to the  there,This changes your  so that on every load, it will have that value in it..  As stated  about ,  For other possible methods of adding some path to  see "},
{"body": "Python provides a convenient method long() to convert string to long:; converts '234' into a longIf user keys in 234.89 then python will raise an error message:How should we a python programmer handles scenarios where a string with a decimal value ? Thank you =)can only take string convertibles which can end in a base 10 numeral. So, the decimal is causing the harm. What you can do is,  the value before calling the . If your program is on Python 2.x where int and long difference matters, and you are sure you are not using large integers, you could have just been fine with using  to provide the key as well.So, the answer is  or it could just be  if you are not using large integers. Also note that this difference does not arise in Python 3, because int is upgraded to long by default. All integers are long in python3 and call to covert is just Well, longs can't hold anything but integers. One option is to use a float:  The other option is to truncate or round.  Converting from a float to a long will truncate for you:  "},
{"body": "I work in an psudo-operational environment where we make new imagery on receipt of data.  Sometimes when new data comes in, we need to re-open an image and update that image in order to create composites, add overlays, etc.  In addition to adding to the image, this requires modification of titles, legends, etc.  Is there something built into matplotlib that would let me store and reload my matplotlib.pyplot object for later use?  It would need to maintain access to all associated objects including figures, lines, legends, etc.  Maybe pickle is what I'm looking for, but I doubt it.  As of 1.2 matplotlib ships with experimental pickling support. If you come across any issues with it, please let us know on the mpl mailing list or by opening an issue on github.com/matplotlib/matplotlibHTH: Added a simple exampleThen in a separate session:A small modification to Pelson's answer for people working on a JupyterhubUse  before loading the pickle.  Using  did not work for me in either jupyterhub or jupyter notebook. and gives a traceback ending in \nAttributeError: 'module' object has no attribute 'new_figure_manager_given_figure'.Then in a separate session:I produced figures for a number of papers using matplotlib. Rather than thinking of saving the figure (as in MATLAB), I would write a script that plotted the data then formatted and saved the figure. In cases where I wanted to keep a local copy of the data (especially if I wanted to be able to play with it again) I found  and  to be very useful.At first I missed the shrink-wrapped feel of saving a figure in MATLAB, but after a while I have come to prefer this approach because it includes the data in a format that is available for further analysis.Did you try the pickle module?  It serialises an object, dumps it to a file, and can reload it from the file later."},
{"body": "I have a sequence of new objects. They all look like similar to this:Foo(pk_col1=x, pk_col2=y, val='bar')Some of those are Foo that exist (i.e. only val differs from the row in the db)\nand should generate update queries. The others should generate inserts.I can think of a few ways of doing this, the best being:Is there an easier way?I think you are after . This will merge an object in a detached state into the session if the primary keys match and will make a new one otherwise. So  will work for both insert and update."},
{"body": "Is there a good wxpython GUI builder that does not require much coding, like in the case of the form builder in MS visual studio ?There is . Here is a screenshot:and Also, have a look here for more alternatives: I've tried a few, and the only one I seem to have any luck with is In addition to those, some people really like the XRCed application that's included with wxPython. Basically you create your GUI in XML. There's also the defunct Boa Constructor that I see people still using on the wxPython user's list.In a long ago day, I tried them all and found  to be the most professional of the bunch.  Along with that goes the fact that it is commercial software.It's by Julian Smart, the creator of wxWidgets, so I suppose it's not too surprising that it's a quality product.  At $90, it's a bit expensive but not outrageous."},
{"body": "Assuming I have two Python modules and path_b is in the import path:Is it possible to see where the module is imported from? I want an output like \"I was imported from path_a/app.py\", if I start app.py (because I need the file name).\nFor better understanding; I could write:So the output would be:There may be an easier way to do this, but this works:Note that the path will be printed relative to the current working directory if it's a parent directory of the script location.Try this: In that case write into the  file of your module:Try  to find out where it is from. If you get an , it is probably not a Python source (.py) file.Also, if you have a function/class  from a module  you can get the path of the module using the module I've written a simple script so I have the command \nthat allows me to find where a Python module comes from.\nIt doesn't work for some builtins like sys which doesn't have a  attribute.This can be run from a Linux command line to find where a python script run in the current environment will get a module from, for example:Other answers are OK, but if you want to tell it from  the imported module then do"},
{"body": "Is there a way in matplotlib to partially specify the color of a string?Example:How can I show \"today\" as red, \"is\" as green and \"cloudy.\" as blue?Thanks.here's the interactive version (same one I posted to )I only know how to do this non-interactively, and even then only with the 'PS' backend.To do this, I would use Latex to format the text. Then I would include the 'color' package, and set your colors as you wish. Here is an example of doing this:This results in (converted from ps to png using ImageMagick, so I could post it here):\nExtending Yann's answer, LaTeX coloring now :Note that this python script sometimes fails with  errors in the first attempt. Running it again is then successful."},
{"body": "I'm writing a program in which I would like to have arguments like this:Is there a way to get argparse to do this for me?I'm using Python 3.2.Since there seems to be no way to do this, I'm opting for this idea instead...IMHO, not quite as elegant, but it works.I figured out a reasonably nice solution involving deriving my own class derived from  that I've detailed in an .Well, none of the answers so far are quite satisfactory for a variety of reasons. So here is my own answer:And an example of use:Unfortunately, the  member function isn't documented, so this isn't 'official' in terms of being supported by the API. Also,  is mainly a holder class. It has very little behavior on its own. It would be nice if it were possible to use it to customize the help message a bit more. For example saying  at the beginning. But that part is auto-generated by stuff outside the  class.Does the  of  help?Here's how it looks when run:This is different from the following in the mutually exclusive group allows  option in your program (and I'm assuming that you want  because of the  syntax).  This implies one or the other:If these are required (non-optional), maybe using  is what you're looking for.Logically different, but maybe cleaner:And running it:Write your own subclass.For fun, here's a full implementation of :Here's the output:I modified the solution of @Omnifarious to make it more like the standard actions:You can add the Yes/No argument as you would add any standard option. You just need to pass  class in the  argument:Now when you call it:Extending  's answerThis yields help output like so:\n    usage: myscript.py [-h] [--flaga] [--flabb]Gist version here, pull requests welcome :)\nBefore seeing this question and the answers I wrote my own function to deal with this:It can be used like this:Help output looks like this:I prefer the approach of subclassing  that the other answers are suggesting over my plain function because it makes the code using it cleaner, and easier to read.This code has the advantage of having a standard default help, but also a  and  to reconfigure the rather stupid defaults.Maybe someone can integrate."},
{"body": "I can't seem to be able to get the python ldap module installed on my OS X Mavericks 10.9.1 machine.Kernel details:\nuname -a\nDarwin  13.0.0 Darwin Kernel Version 13.0.0: Thu Sep 19 22:22:27 PDT 2013; root:xnu-2422.1.72~6/RELEASE_X86_64 x86_64I tried what was suggested here:\nBut when I try to use pip I get a different error Modules/LDAPObject.c:18:10: fatal error: 'sasl.h' file not found*#include sasl.hI also tried what was suggested here:\nBut with the same error.I am hoping someone could help me out here. \n/usr/include appears to have movedNow run pip install!using pieces from both @hharnisc and @mick-t answers.In my particular case, I couldn't simply use the  arguments noted in other answers because I'm using it with  to install dependencies from a requirements.txt file, and I need my tox.ini to remain compatible with non-Mac environments.I was able to resolve this in much simpler fashion: exporting  such that it adds an include path to the sasl headers already installed by Xcode:Depending on whether or not you use any userspace-friendly Python tools (I use ), you may have to prefix your pip commands with .I had the same problem. I'm using Macports on my Mac and I have cyrus-sasl2 installed which provides sasl.h in  /opt/local/include/sasl/. You can pass options to build_ext using pip's global-option argument. To pass the include PATH to /opt/local/include/sasl/sasl.h run pip like this:Alternatively you could point it to whatever the output from  provides. On my box that's:\n Then you need to determine the PATH to the sasl header files. For me that's:Let me know if that helps or you need a hand.I used a combination of posts I found about this problem (including this one) to eventually come up with this (copied from a larger script):You can test it with The main reason I didn't follow the advice of @hharnisc was that on my local machine /usr/local had not moved, so I just temporarily put $XC_SDK before it on the path, and that seems to work. some sources:\n"},
{"body": "I've read the manual for pseudo-randomness in Python, and to my knowledge, you can only generate numbers up to a given maximum value, i.e. 0-1, 0-30, 0-1000, etc.  I want to:I've looked around, and I can't find anywhere that explains this.Create an integer random between e.g.  and multiply it by . Simple math.produces e.g.should work in any Python >= 2.If you don't want to do it all by yourself, you can use the  function.For example  prints a number that is between 10 and 25 (10 included, 25 excluded) and is a multiple of 5. So it would print 10, 15, or 20.The simplest way is to generate a random nuber between 0-1 then strech it by multiplying,  and shifting it.\nSo yo would multiply by (x-y)  so the result is in the range of 0 to x-y,\nThen add x and you get the random number between x and y.   To get a five multiplier use rounding. If this is unclear let me know and I'll add code snippets. "},
{"body": "Could someone please tell me what I'm doing wrong with this code? It is just printing 'count' anyway.  I just want a very simple prime generator (nothing fancy). There are some problems:Here's your code with a few fixes, it prints out only primes:For much more efficient prime generation, see the Sieve of Erastothenes, as others have suggested. Here's a nice, optimized implementation with many comments:Note that it returns a generator.We will get all the prime numbers upto 20 in a list.\nI could have used Sieve of Eratosthenes but you said\nyou want something very simple. ;)To test if a number is prime:Here's a  (Python 2.6.2) solution... which is in-line with the OP's original request (now six-months old); and should be a perfectly acceptable solution in any \"programming 101\" course... Hence this post.This simple \"brute force\" method is \"fast enough\" for numbers upto about about 16,000 on modern PC's (took about 8 seconds on my 2GHz box).Obviously, this could be done much more efficiently, by not recalculating the primeness of every even number, or every multiple of 3, 5, 7, etc for every single number... See the  (see eliben's implementation above), or even the  if you're feeling particularly brave and/or crazy.Caveat Emptor: I'm a python noob. Please don't take anything I say as gospel.To my opinion it is always best to take the functional approach,So I create a function first to find out if the number is prime or not then use it in loop or other place as necessary.Then run a simple list comprehension or generator expression to get your list of prime,This seems homework-y, so I'll give a hint rather than a detailed explanation. Correct me if I've assumed wrong.You're doing fine as far as bailing out when you see an even divisor. But you're printing 'count' as soon as you see even  number that doesn't divide into it. 2, for instance, does not divide evenly into 9.  But that doesn't make 9 a prime. You might want to keep going until you're sure  number in the range matches.(as others have replied, a Sieve is a much more efficient way to go... just trying to help you understand why this specific code isn't doing what you want)How about this if you want to compute the prime directly:Another simple example, with a simple optimization of only considering odd numbers. Everything done with lazy streams (python generators).Usage: primes = list(create_prime_iterator(1, 30))re is powerful:You need to make sure that all possible divisors don't evenly divide the number you're checking.  In this case you'll print the number you're checking any time just one of the  possible divisors doesn't evenly divide the number.Also you don't want to use a continue statement because a continue will just cause it to check the next possible divisor when you've already found out that the number is not a prime.Here is what I have:It's pretty fast for large numbers, as it only checks against already prime numbers for divisors of a number.Now if you want to generate a list of primes, you can do:using generators here might be desired for efficiency.And just for reference, instead of saying:you can simply say:You can create a list of primes using list comprehensions in a fairly elegant manner. Taken from Similar to user107745, but using 'all' instead of double negation (a little bit more readable, but I think same performance):Basically it iterates over the x in range of (2, 100) and picking only those that do not have mod == 0 for all t in range(2,x)Another way is probably just populating the prime numbers as we go: is a Python library for symbolic mathematics. It provides several functions to generate prime numbers.Here are some examples.For me, the below solution looks simple and easy to follow. "},
{"body": "I'm trying to make a script that gets data out from an sqlite3 database, but I have run in to a problem.The field in the database is of type text and the contains a html formated text. see the text belowand the python code that try to extract the data is as follows.Does anybody have any idea of how to print/write this to a file. Yes I know that this is printed to stdout, but I get the same UnicodeEncodeError when I try to write to a file. I tried both write method of a file object and .When you open the file you want to write to, open it with a specific encoding that can handle all the characters.Maybe a little late to reply. I happen to run into the same problem today. I find that on Windows you can change the console encoder to  or other encoder that can represent your data. Then you can print it to .First, run following code in the console:Then, start  do anything you want.While Python 3 deals in Unicode, the Windows console or POSIX tty that you're running inside does not. So, whenever you , or otherwise send Unicode strings to , and it's attached to a console/tty, Python has to encode it.The error message indirectly tells you what character set Python was trying to use:This means the charset is .You can test or yourself that this charset doesn't have the appropriate character just by doing . Or you can look up cp850 online (e.g., at ).It's possible that Python is guessing wrong, and your console is really set for, say UTF-8. (In that case, just manually set .) It's also possible that you intended your console to be set for UTF-8 but did something wrong. (In that case, you probably want to follow up at superuser.com.)But if nothing is wrong, you just can't print that character. You will have to manually encode it with one of the non-strict error-handlers. For example:So, how do you print a string that won't print on your console?You  replace every  function with something like this:\u2026 but that's going to get pretty tedious pretty fast.The simple thing to do is to just set the error handler on :For printing to a file, things are pretty much the same, except that you don't have to set  after the fact, you can set it at construction time. Instead of this:Do this:\u2026 Or, of course, if you can use UTF-8 files, just do that, as Mark Ransom's answer shows:"},
{"body": "I've always used dictionaries. I write in Python.A dictionary is a general concept that maps keys to values.  There are many ways to implement such a mapping.A hashtable is a specific way to implement a dictionary.Besides hashtables, another common way to implement dictionaries is .Each method has it's own pros and cons.  A red-black tree can always perform a lookup in O(log N).  A hashtable can perform a lookup in O(1) time although that can degrade to O(N) depending on the input.A dictionary is a data structure that maps keys to values.A hash table is a data structure that maps keys to values by taking the hash value of the key (by applying some hash function to it) and mapping that to a bucket where one or more values are stored.IMO this is analogous to asking the difference between a list and a linked list.For clarity it may be important to note that it MAY be the case that Python currently implements their dictionaries using hash tables, and it MAY be the case in the future that Python changes that fact without causing their dictionaries to stop being dictionaries.\"A dictionary\" has a few different meanings in programming, as  will tell you -- \"associative array\", the sense in which Python uses the term (also known as \"a mapping\"), is one of those meanings (but \"data dictionary\", and \"dictionary attacks\" in password guess attempts, are also important).Hash tables are important data structures; Python uses them to implement two important built-in data types,  and .So, even in Python, you can't consider \"hash table\" to be a synonym for \"dictionary\"... since a similar data structure is also used to implement \"sets\"!-)A Python dictionary is internally implemented with a hashtable.A hash table always uses some function operating on a value to determine where a value will be stored. A Dictionary (as I believe you intend it) is a more general term, and simply indicates a lookup mechanism, which might be a hash table or might be implemented by a simpler structure which does not consider the value itself in determining its storage location.Dictionary is implemented using hash tables. In my opinion the difference between the 2 can be thought of as the difference between Stacks and Arrays where we would be using arrays to implement Stacks."},
{"body": "I've been using Boost::Python for a while, and everything always turned out ok. However yesterday I was trying to find out why a particular type I thought I had registered (a tuple) was giving me errors when I was trying to access it from Python.Turns out that while the tuple was actually registered, when trying to access it through an  wrapped via the  this is not enough anymore.I was wondering, why is it not working? Is there any way to make this work? Should I try to wrap the vector by hand?Below is my MVE:Accessing the resulting  via Python results in:EDIT:\nI've realized that the error does not happen if the  is used with the  parameter set to true. However, I'd prefer if this wasn't necessary, as it makes the exported classes unintuitive in Python. registers C++-to-Python converters and Python-to-C++ converters. This is fine.On the other hand, you want your vector elements to be returned by reference. But there's nothing on the Python side that can serve as a reference to your tuple. A converted-to-Python tuple may hold the same values, but it is completely detached from the original C++ tuple.It looks like in order to export a tuple by reference, one would need to create an indexing suite for it, rather than to/from-Python converters. I have never done that and cannot guarantee it will work.Here's how one could expose a tuple as a minimal tuple-like Python object (with only len() and indexing). First define some helper functions:Then expose specific tuples:Note these quasi-tuples, unlike real Python tuples, are mutable (via C++). Because of tuple immutability, exporting via converters and  looks like a viable alternative to this."},
{"body": "To read data from a socket in python, you call , which has this signature:The  vaguely state:: What does \"\" mean?  What is the  impact of setting bufsize to a non-power-of-two?I've seen    to make this read a power of 2.  I'm also well aware of reasons when it is often useful to have array lengths as powers of two (bitshift/masking operations on the length, optimal FFT array size, etc), but these are application dependent.  I just am not seeing the general reason for it with .  Certainly not to the point of the  in the python documentation.  I also don't see any power-of-two optimizations in the  to make it a python-specific recommendationFor example... if you have a protocol where the incoming packet length is exactly known, it is obviously preferrable to only read \"at most\" what is needed for the packet you are dealing with, otherwise you could potentially eat into the next packet and that would be irritating.  If the packet I'm currently processing only has 42 bytes pending, I'm only going to set bufsize to 42.What am I missing?  When I have to choose an arbitrary buffer/array size I usually (always?) make the length a power of two, just in case.  This is just a habit developed over many years.  Are the python docs also just a victim of habit?This isn't exclusive to python, but since I'm specifically referencing the python docs I'll tag it as such.: I just checked the size of the buffer at the kernel level on my system (or at least I think I did... I did ) and it was 124928.  Not a power of two.   was 131071, also clearly not a power of two.In looking into this more I really cannot see any benefit in the power of two recommendation(s) yet.  I'm about ready to call it as a bogus recommendation...I also added  and  tags since they are also relevant.In regards to: \"if you have a protocol where the incoming packet length is exactly known, it is obviously preferrable to only read \"at most\" what is needed for the packet you are dealing with, otherwise you could potentially eat into the next packet and that would be irritating.\"This may be preferable for the application developer, but is probably inefficient for the underlying network stack.  First, it ties up socket buffer space that can be used for additional network I/Os.  Second, each recv() you make means dipping into a system call/kernel space and there is a performance penalty for the transition.  It is always preferable to get as much data as you can out of kernel space and into user space with as few system calls as possible and do your message parsing there.  This adds more complexity to the application code and message handling but is probably the most efficient.That said, given the speed of today's processors and amount of available memory, this may not be an issue for most applications, but this was a common recommendation for network applications back in the \"old days\".I am not sure about the power of 2 recommendation from a user-space application.  I have seen these types requirements for drivers due to alignment and page size issues, etc.  but its not clear what effect this has from user space unless it somehow aids in copying data out of kernel buffers into user buffers.  Maybe somebody with more OS development knowledge could comment.Have a look at the answers on . It deals with  too and the answers may be helpfull for a more complete understanding. It won't answer the question but will provide a bit more insight."},
{"body": "What is the the time complexity of each of python's set operations in  notation?I am using Python's  for an operation on a large number of items. I want to know how each operation's performance will be affected by the size of the set. For example, , and the test for membership:Googling around hasn't turned up any resources, but it seems reasonable that the time complexity for Python's set implementation would have been carefully considered.If it exists, a link to something like  would be great. If nothing like this is out there, then perhaps we can work it out?Extra marks for finding the time complexity of  set operations.The operation  should be independent from he size of the container, ie.  -- given an optimal hash function. This should be  true for Python strings. Hashing strings is always critical, Python should be clever there and thus you can expect near-optimal results.According to ,  is implemented as a . So you can expect to lookup/insert/delete in  average. Unless your hash table's load factor is too high, then you face collisions and O(n).P.S. for some reason they claim O(n) for delete operation which looks like a mistype.P.P.S. This is true for CPython, pypy is a ."},
{"body": "Is there an established approach to embed gettext  in a ? Specifically to have Gtks automatic widget translation pick them up  the ZIP archive.For other embedded resources  or / work well enough. But system  Python  depend on  being supplied a plain old ; no resources or strings etc.So I couldn't contrive a workable or even remotely practical workaround:Is there a more dependable approach perhaps? This my example Glade/GtkBuilder/Gtk application. I've defined a function  which transparently translates glade xml files and passes to  instance as a string.  I've archived my locale directories into  which is included in the  bundle.\nThis is contents of   To make the locale.zip as a filesystem I use ZipFS from .  Fortunately Python  is not GNU gettext.  is pure Python it doesn't use GNU gettext but mimics it.  has two core functions  and . I've redefined these two in a seperate module named  to make them use files from the .   uses  , and  to find files and open them which I replace with the equivalent ones form  module.  This is contents of my application.  Because  files have text, usually a shebang, prepended to it, I skip this line after opening the  file in binary mode. Other modules in the application that want to use the  function, should import  instead from  and make it an alias to .  Here goes .  The following two shouldn't be called because  doesn't use Python .  "},
{"body": "We have a distributed architecture based on  and .\nWe can launch in parallel multiple tasks without any issue. The scalability is good. Now we need to control the task remotely: PAUSE, RESUME, CANCEL.\nThe only solution we found is to make in the Celery task a RPC call to another task that replies the command after a DB request. The Celery task and RPC task are not on the same machine and only the RPC task has access to the DB.Do you have any advice how to improve it and easily communicate with an ongoing task?\nThank you\nIn fact we would like to do something like in the picture below. It's easy to do the  configuration or the , but we don't know how to do both simultaneously.\n\nWorkers are subscribing to a common  and each worker has its own  declared on an exchange. \nIF this is not possible with , I'am open to a solution with other frameworks like .It look like the .For a better scalability and in order to reduce the RPC call, I recommend to reverse the logic. The  command are push to the Celery tasks through a control bus when the state change occurs. The Celery app will store the current state of the Celery app in a store (could be in memory, on the filesystem..). If task states must be kept even after a stop/start of the app, It will involve more work in order to keep both app synchronized (eg. synchronization at startup)."},
{"body": "I am running socket.io on an Apache server through Python Flask. We're integrating it into an iOS app (using the  library) and we're having a weird issue.From the client side code in the app (written in Swift), I can view the actual connection log (client-side in XCode) and see the connection established from the client's IP and the requests being made. The client never receives the information back (or any information back; even when using a global event response handler) from the socket server.I wrote a very simple test script in Javascript on an HTML page and sent requests that way and received the proper responses back. With that said, it seems to likely be an issue with iOS. I've found these articles (but none of them helped fix the problem):\nMy next thought is to extend the logging of socket.io to find out exact what data is being POSTed to the socket namespace. Is there a way to log exactly what data is coming into the server (bear in mind that the 'on' hook on the server side that I've set up is not getting any data; I've tried to log it from there but it doesn't appear to even get that far).I found mod_dumpio for Linux to log all POST requests but I'm not sure how well it will play with multi-threading and a socket server.Any ideas on how to get the exact data being posted so we can at least troubleshoot the syntax and make sure the data isn't being malformed when it's sent to the server?Thanks!When testing locally, we got it working (it was a setting in the Swift code where the namespace wasn't being declared properly). This works fine now on localhost but we are having the exact same issues when emitting to the Apache server.We are not using mod_wsgi (as far as I know; I'm relatively new to mod_wsgi, apologies for any ignorance). We used to have a .wsgi file that called the main app script to run but we had to change that because mod_wsgi is not compatible with Flask SocketIO (as stated in the uWSGI Web Server section ). The way I am running the script now is by using  to run the .py file as a daemon (using that specifically so it will autostart in the event of a server crash).Locally, it worked great once we installed the eventlet module through pip. When I ran  on my virtual environment on the server, eventlet was installed. I uninstalled and reinstalled it just to see if that cleared anything up and that did nothing. No other Python modules that are on my local copy seem to be something that would affect this.One other thing to keep in mind is that in the function that initializes the app, we change the port to port 80:because we have other API functions that run through a domain that is pointing to the server in this app. I'm not sure if that would affect anything but it doesn't seem to matter on the local version.I'm at a dead end again and am trying to find anything that could help. Thanks for your assistance!I'm not exactly sure what was happening yet but we went ahead and rewrote some of the code, making sure to pay extra special attention to the namespace declarations within each socket event  function. It's working fine now. As I get more details, I will post them here as I figure this will be something useful for other who have the same problem. This thread also has some really valuable information on how to go about debugging/logging these types of issues although we never actually fully figured out the answer to the original question.I assume you have verified that Apache does get the POST requests. That should be your first test, if Apache does not log the POST requests coming from iOS, then you have a different kind of problem.If you do get the POST requests, then you can add some custom code in the middleware used by Flask-SocketIO and print the request data forwarded by Apache's mod_wsgi. The this is in file . The relevant portion is this:You can find out what's in  in the . In particular, the body of the request is available in , which is a file-like object you read from.Keep in mind that once you read the payload, this file will be consumed, so the WSGI server will not be able to read from it again. Seeking the file back to the position it was before the read may work on some WSGI implementations. A safer hack I've seen people do to avoid this problem is to read the whole payload into a buffer, then replace  with a brand new  or  object.Are you using flask-socketio on the server side? If you are, there is a lot of debugging available in the constructor. socketio = SocketIO(app, async_mode=async_mode, logger=True, engineio_logger=True)"},
{"body": "I've recently switched entirely to Vim for all my Python/Django development. It took me a lot of time to customize it to the point it is today, and God knows how hard it was for me to find help regarding the best vim plugins out there suited for Python/Django development.I decided to ask this question so people like me could benefit directly from your experience:\nYou've built the perfect Python/Djangoish Vim editor? Describe it for us (plugins, scripts, customized .vimrc, colorschemes ..etc).ThanksOk, this is my own configuration. actually I've chosen to create a simple Vim configuration so I can master the little number of plugins I've chosen to install instead of make a big stack of plugins that I'll never master nor use. This is the list of the plugins I use the most:Also I've created a python.vim file in $HOME/.vim/ftplugin/ containing this script so I can run python code from Vim just by running Shift+e:Also I've collected some useful .vimrc customizations:I don't really have much Django specific mods, although I have given the jinja2 syntax a higher priority than the django template syntax.I am not going to post my whole .vimrc file here, but I have a similiar setup as you. This is less Python/Django specific though, except for some custom snippets for snipMate and python-mode. Here the vim plugins I am using:Some custom python snippets I use quite frequently:"},
{"body": "I would like to roll my own login system for my python Google App Engine application (rather than using Google's ).I am using webapp2, and I noticed that there is a  module and an .Does anyone know how I can use this API to create:Once I have the email and password, where do I store it? In the AuthStore?\nAnd how do I authenticate against the AuthStore?How and where you store user credentials and information is entirely up to you; the webapp2 module you reference merely provides an interface you must conform to if you want to use its features. An obvious (perhaps the only sensible) choice would be the datastore.I'd strongly, strongly recommend using the built in  instead of rolling your own, though. By doing so, you're forcing users to create yet another username and password, and you're taking on a whole set of password storage and security hassles for yourself.New answer to an old question: Anyone looking to add own authentication and login to webapp2 on Google App Engine should consider .Signup, login, logout, password reset, federated login (Google, Twitter, Facebook, etc), user profiles, etc are implemented.Technologies leveraged include, Python 2.7, NDB, Jinja2, WTForms, unittest, webtest, pyquery, OpenID (Google App Engine), and OAuth2 (for federated login providers that do not support OpenID).Online demo is ."},
{"body": "Python 3 has  and  but no . So, why a number representing the infinite set of integers is missing in the language? Is  unreasonable?Taken from here: That is, the representation of  and  can store these special values. However, there is nothing within the basic type  that can store the same. As you exceed the limit of 2^32 in an unsigned 32-bit int, you simply roll over to 0 again.If you want, you could create a class containing an integer which could feature the possibility of infinite values.For python 2.\nIt is sometimes the case that you need a very large integer. For example, I may want to produce a subarray with x[:n] and, I may wish to sometimes set n to a value such that the whole array will be produced. Since you can't use a float for n (python wants an integer), you need a \"large integer\". A good way of doing this is to use the largest integer available: sys.maxint.\nHere is an example:So, while testing, I could use a smaller sources array but use the full array in production.You are right that an integer infinity is possible, and none has been added to Python standard.  This is probably because  supplants it in almost all cases."},
{"body": "I need to walk through folders with long file names in Windows.I tried using , but it crashes with long pathnames, which is bad.I tried using , but it ignores the pathnames longer than ~256, which is worse.I tried the magic word workaround described , but it only works with mapped drives, not with .Here is an example with short pathnames, that shows that UNC pathnames don't work with the magic word trick.Any idea on how to deal with long pathnames or with unicode UNC pathnames?Edit:Following the suggestion of the comments below, I created some test functions to compare Python 2.7 and 3.3, and I added the test of  and  after . The  didn't help as expected (see this ).The  is the only one that in Python 3.3 works better, but only in one condition: using the magic word and with the drive name.Here is the code I used (it works on both 2.7 and 3.3). I am learning Python now, and I hope these tests make sense:And here is the result:-Use the 8.3 fallback to avoid the long pathname,  browsing in Win7 explorer this seems to be what windows itself does, ie every long paths has a shorter 'true name':but you can use win32api (pywin32) to 'build' up a shorter version, ieclearly you can just fold the win32api.GetShortPathName call into you dir exploration rather than nesting as in my example.\nI've done it like this with 3 calls because if you've already got a 'too long' path then win32api.GetShortPathName wont cope with it either,  but you can do it per dir and stay below the limit.To locate files on UNC paths, the the magic prefix is  rather than just .Reference: So to access , you'd need toResulting unicode string: I've only experimented a little (much less than @stenci did) but with Python 2.7 it seems to work OK with , and to fail with .In my previous comment I said that the nested recursive call of  is not required. I found it is not required most of the times, but once in a while it crashes. I wasn't able to figure out when, so I made this little function that has been working smoothly for some time:This is the function that I use now:"},
{"body": "I've been following the saga of Python 3.x and have watched the 3.x features gradually getting back-ported to the 2.x line.  Most of the libraries I use haven't been ported and some (e.g. Twisted) seem covertly or overtly hostile to 3.x to varying degrees.  At any rate, there has been very little movement towards compatible versions of many of them.  Expecially the larger ones.So, my question is, with all the features that have been backported, what is still available in 3.x that's NOT been back-ported?  It's pretty easy to find what  been backported, but not .Right now, porting to 3.x just seems like all pain, and I can't see the gain; maybe an \"Only in 3.x\" list would let me see the light...Thanks,Stu The most important thing is probably unicode throughout. So there is no need anymore to fiddle around with str/unicode. This sounds small but has huge (positive) implications when you think of OS interaction - for example everyone has to try hard to give you 'usable' strings instead of 'a binary thing that might be an error message'.There's also a lot of stuff in the interpreter itself that was improved. One example is the global interpreter lock (GIL) which did not vanish but it way better in py3k: Speed might be a feature worth mentioning.The speed imporovements of project \"Unladen Swallow\" have been approved to be merged into Python. But as far as I know only to Python 3. See  for details."},
{"body": "This is tricking me out.\nI understand elem is the lists inside of the list from \nI don't quite understand the usage of  and  in the beginning and the end.How does python interpret this?\nWhat's the order it looks at?Lets break it down.A simple list-comprehension:This is easy to understand if we break it into parts: In this way, one could write:In order to convert all words in a list to lowercase.It is when we complicate this with another list like so:Here, something special happens. We want our final list to include  items, and  items are found inside  items, so we have to tell the list-comprehension that.This logic is similar to the normal for loop:To expand on this, and give a great example + explanation, imagine that there is a train.The train engine (the front) is always going to be there (the result of the list-comprehension) Then, there are any number of train cars, each train car is in the form: A list comprehension could look like this:Which would be like having this regular for-loop:In other words, instead of going down a line and indenting, in a list-comprehension you just add the next loop on to the end. To go back to the train analogy: -  -  -  ... What is the tail? The tail is a special thing in list-comprehensions. You don't  one, but if you have a tail, the tail is a condition, look at this example:This would give you every line in a file as long as the line didn't start with a hashtag (), others are just skipped.The trick to using the \"tail\" of the train is that it is checked for True/False at the same time as you have your final 'Engine' or 'result' from all the loops, the above example in a regular for-loop would look like this: Though in my analogy of a train there is only a 'tail' at the end of the train, the condition or 'tail' can be after  'car' or loop...for example:In regular for-loop:Your code equals:From the :In other words, pretend that the  loops are nested. Reading from left to right your list comprehension can be nested as:where the list comprehension will use that last, innermost block as the values of the resulting list."},
{"body": "I would like to know how to pass a variable to all my templates, without repeating the same code on every method in my views.py file? In the example below I would like to make categories (an array of category objects) available to all templates in the web app.What you want It's a  and it's very easy to create one for example assuming you have got an app named  you have to follow the next steps to create one.And now  you can use  in all the templates :DTo add a , in the settings you must add the next code:"},
{"body": "How can I test whether two JSON objects are equal in python, disregarding the order of lists?For example ...JSON document :JSON document : and  should compare equal, even though the order of the  lists are different.If you want two objects with the same elements but in a different order to compare equal, then the obvious thing to do is compare sorted copies of them - for instance, for the dictionaries represented by your JSON strings  and :... but that doesn't work, because in each case, the  item of the top-level dict is a list with the same elements in a different order, and  doesn't try to sort anything except the \"top\" level of an iterable.To fix that, we can define an  function which will recursively sort any lists it finds (and convert dictionaries to lists of  pairs so that they're orderable):If we apply this function to  and , the results compare equal:Another way could be to use  option:This works for nested dictionaries and lists.Decode them and compare them as mgilson comment.Order does not matter for dictionary as long as the keys, and values matches. (Dictionary has no order in Python)But order is important in list; sorting will solve the problem for the lists.Above example will work for the JSON in the question. For general solution, see Zero Piraeus's answer."},
{"body": "I want to connect too and execute a process on a remote server using python.  I want to be able to get the return code and stderr(if any) of the process.  Has anyone ever done anything like this before.  I have done it with ssh, but I want to do it from python script.Cheers.Well, you can call ssh from python...Use the  which was created for this purpose instead of using . Here's an example below:UPDATE: The example used to use the  module, but that is now deprecated and  is the up-to-date module that provides ssh functionality in python.Maybe if you want to wrap the nuts and bolts of the ssh calls you could use \nThis library is geared towards deployment and server management, but it could also be useful for these kind of problems.Also have a look at . This implements a task queue for Python/Django on various brokers. Maybe an overkill for your problem, but if you are going to call more functions on multiple machines it will save you a lot of headache managing your connections. "},
{"body": "I'm trying to dump a list of all active threads including the current stack of each. I can get a list of all threads using threading.enumerate(), but i can't figure out a way to get to the stack from there. Background: A Zope/Plone app freaks out from time to time, consuming 100% of cpu and needs to be restarted. I have a feeling it's a loop which doesn't terminate properly, but i cannot reproduce it in the test-environemt for verification. I managed to register a signal handler which can be triggered from the outside, so i can trigger some code as soon as the situation occurs again. If I could dump the stacktrace for all active threads, that would give me a clue what goes wrong. The hole thing runs on python 2.4...Any ideas on how to trace down situations like these are appreciated :)Cheers,\n   ChrissWhen using Zope, you want to install  or ; these were designed for just this purpose!Send a USR1 signal to your Zope server and it'll immediately dump stack traces for all threads to the console. It'll do this even if all Zope threads are locked up.Under the hood these packages indirectly use ; for Python versions 2.5 and up, when  using Zope, you can build the same functionality using the  function to access per-thread stack frames.As of  this functionality is integrated into Zope itself, and there is no need to install additional packages anymore.As jitter points out in an earlier answer  gives you what you need for v2.5+. For the lazy the following code snippet worked for me and may help you:For Python 3.3 and later, there is .The code below produces similar output, but includes the thread name and could be enhanced to print more information.2.4. Too bad. From Python 2.5 on there is .But you could try . And if the makefile gives you trouble you could try this Just for completeness sake,  is super helpful to identify bottlenecks, and to do so it dumps stacktraces at specific intervals.There is an applicable recipe on . You can use  to get all the tids, then just call _async_raise() with some suitable exception to force a stack trace."},
{"body": "Given an RFC822 message in Python 2.6, how can I get the  text/plain content part?  Basically, the algorithm I want is this:Of these things, I have  and  down pat, but I'm not quite sure how to get the decoded text from the MIME part.  I can get the  text using , but if I try to use the  parameter of the  method (see ) I get an error when I call it on the text/plain part:In addition, I don't know how to take HTML and render it to text as closely as possible.In a multipart e-mail,  returns a list with one item for each part. The easiest way is to walk the message and get the payload on each part:For a non-multipart message, no need to do all the walking. You can go straight to get_payload(), regardless of content_type.If the content is encoded, you need to pass  as the first parameter to , followed by True (the decode flag is the second parameter). For example, suppose that my e-mail contains an MS Word document attachment:As for getting a reasonable plain-text approximation of an HTML part, I've found that  works pretty darn well.Flat is better than nested ;)"},
{"body": "Which is preferred (\".\" indicating whitespace)?A)B)My intuition would be B (that's also what vim does for me), but I see people using A) all the time. Is it just because most of the editors out there are broken?The  does not seem to be clear on this issue, although the statements about \"blank lines\" could be interpreted in favor of B. The PEP 8 style-checker (pep8.py) prefers B and warns if you use A; however, both variations are legal. My own view is that since Python will successfully interpret the code in either case that this doesn't really matter, and trying to enforce it would be a lot of work for very little gain. I suppose if you are very adamantly in favor of one or the  other you could automatically convert the one to the other. Trying to fix all such lines manually, though, would be a huge undertaking and really not worth the effort, IMHO.If you use , you could copy paste your block in python shell,  will get unexpected indentation error.That empty line belongs to , so I would consider  to be the most natural. But I guess it's just a matter of opinion.Adding proper indentation to blank lines (style  in the question) vastly improves code readability with display whitespace enabled because it makes it easier to see whether code after a blank line is part of the same indentation block or not.For a language like Python, where there is no end statement or close bracket, I'm surprised this is not part of PEP.  Editing Python with display whitespace on is strongly recommended, to avoid both trailing whitespace and mixed indentation.Compare reading the following:A)B)In , it is far clearer that the last two lines are part of .  This is even more useful at higher indentation levels.TextMate breaks block collapsing if you use B, and I prefer A anyway since it's more \"logical\".I wouldn't necessarily call the first example \"broken\", because I know some people hate it when the cursor \"jumps back\" when moving the cursor up or down in code. E.g. Visual Studio (at least 2008) automatically prevents this from happening without using any whitespace characters on those lines.Emacs does B) for me, but I really don't think it matters. A) means that you can add in a line at the correct indentation without any tabbing.My experience in open-source development is that one should never leave whitespace inside blank lines.  Also one should never leave trailing white-space.  It's a matter of coding etiquette.I prefer B since it makes hopping around in vim easier via the { and } commands.It's also a waste of space to write A, albeit minimal. A presumes that the subsequent code will be indented, which is not always the case."},
{"body": "I'm trying to copy  and all its contents (and their contents, etc.) to  in python. Furthermore, I want the copy to overwrite everything in .It  like  might be the right tool for the job, but not sure if there's anything easier/more obvious to use for such a simple task.If it is the right tool, how do I use it? According to the  there are 8 parameters that it takes. Do I have to pass all 8 are just ,  and , and if so, how (I'm brand new to Python).If there's something out there that's better, can someone give me an example and point me in the right direction? Thanks in advance!You can use . It works just fine and you don't have to pass every argument, only  and  are mandatory.However in your case you can't use a similar tool like because it behaves differently: as the destination directory must not exist this function can't be used for overwriting its contents.If you want to use the  tool as suggested in the question comments beware that using the  module is currently the recommended way for spawning new processes as you can see in the .Have a look at the  package, especially  and . You can check if a file / path exists with .Vincent was right about  not working, if dirs already exist. So  is the nicer version. Below is a fixed version of . It's basically copied 1-1, except the first  put behind an if-else-construct:Here's a simple solution to recursively overwrite a destination with a source, creating any necessary directories as it goes.  This does not handle symlinks, but it would be a simple extension (see answer by @Michael above)."},
{"body": "I have the following error when I run django allauth example and it tries to send an email:Under OS X 10.9.1From allauth documentation:"},
{"body": "Sometimes I download python source code from github and don't know how to install all the dependencies. If there is no requirements.txt file I have to create it by hands. \nThe question is:\nGiven python source code directory is it possible to create requirements.txt automatically from import section?If you use virtual environment,  just fine. If not,  will be a good choice for you.By the way, I do not ensure it will work with 2.6.You can use the following code to generate a requirements.txt file:more info related to pipreqs can be found .Sometimes you come across , but this saves all packages in the environment including those that you don't use in your current project. "},
{"body": "When working with the default global graph, is it possible to remove nodes after they've been added, or alternatively to reset the default graph to empty? When working with TF interactively in IPython, I find myself having to restart the kernel repeatedly. I would like to be able to experiment with graphs more easily if possible.There's , but not part of public API (I think it should be, does someone wants to  on GitHub?)My work-around to reset things is this:By default, a session is constructed around the default graph.\nTo avoid leaving dead nodes in the session, you need to either control the default graph or use an explicit graph."},
{"body": "I have a small multithreaded script running in django and over time its starts using more and more memory. Leaving it for a full day eats about 6GB of RAM and I start to swap.Following  I see this as the most common types (with only 800M of memory used):which doesn't show anything weird. What should I do now to help debug the memory problems? Trying some things people are recommending. I ran the program overnight, and when I work up, 50% * 8G == 4G of RAM used. That doesn't sum to 4G, nor really give me any big data structured to go fix. The unicode is from a set() of \"done\" nodes, and the list's look like just random s. I didn't use guppy since it required a C extension and I didn't have root so it was going to be a pain to build.None of the objectI was using have a  method, and looking through the libraries, it doesn't look like django nor the python-mysqldb do either. Any other ideas?See  . Short answer: if you're running django but not in a web-request-based format, you need to manually run  (and of course have DEBUG=False, as others have mentioned). Django automatically does  after a web request, but in your format, that never happens.Is DEBUG=False in settings.py?If not Django will happily store all the SQL queries you make which adds up.Have you tried  ?You need to ask yourself simple questions:See, the main issue would be a cycle of objects containing  methods:I would really encourage you to flag objects / concepts that are cycling in your application and focus on their lifetime: when you don't need them anymore, do we have anything referencing it?Even for cycles without  methods, we can have an issue:If you have to use \"seen\"-like dictionaries, or history, be careful that you keep only the actual data you need, and no external references to it.I'm a bit disappointed now by , I wish it could be configured to output data somewhere else than to stderr, but hopefully .See  on how they traced down real memory leak in HP's Tabblo. A classic and worth reading.I think you should use different tools. Apparently, the statistics you got is only about GC objects (i.e. objects which may participate in cycles); most notably, it lacks strings.I recommend to use ; this should provide you with more detailed statistics.Do you use any extension? They are a wonderful place for memory leaks, and will not be tracked by python tools.Try .Basicly, you need more information or be able to extract some. Guppy even provides graphical representation of data."},
{"body": "How Do I add a single item to a serialized panda series. I know it's not the most efficient way memory wise, but i still need to do that.Something along:also, how can i add a single row to a pandas DataFrame? How to add single item. This is not very effective but follows what you are asking for:produces x:Obviously there are better ways to generate this series in only one shot.  For your second question check answer and references of SO question . I think the question in its current form is a bit tricky. And the accepted answer does answer the question. But the more I use pandas, the more I understand that it's a bad idea to append items to a Series one by one. I'll try to explain why for pandas beginners.You might think that appending data to a given Series might allow you to reuse some resources, but in reality a Series is just a container that stores a relation between an index and a values array. Each is a numpy.array under the hood, and the index is immutable. When you add to Series an item with a label that is missing in the index, a new index with size n+1 is created, and a new values values array of the same size. That means that when you append items one by one, you create two more arrays of the n+1 size on each step.By the way, you can not append a new item by position (you will get an IndexError) and the label in an index does not have to be unique, that is when you assign a value with a label, you assign the value to all existing items with the the label, and a new row is not appended in this case. This might lead to subtle bugs.The moral of the story is that you should not append data one by one, you should better extend with an ordered collection. The problem is that you can not extend a Series inplace. That is why it is better to organize your code so that you don't need to update a specific instance of a Series by reference.If you create labels yourself and they are increasing, the easiest way is to add new items to a dictionary, then create a new Series from the dictionary (it sorts the keys) and append the Series to an old one. If the keys are not increasing, then you will need to create two separate lists for the new labels and the new values.Below are some code samples:When we update an existing item, the index and the values array stay the same (if you do not change the type of the value)But when you add a new item, a new index and a new values array is generated:That is if you are going to append several items, collect them in a dictionary, create a Series, append it to the old one and save the result:You can use the append function to add another element to it. Only, make a series of the new element, before you append it:If you have an index and value. Then you can add to Series as:this will add a new value to Series (at the end of Series).Adding to joquin's answer the following form might be a bit cleaner (at least nicer to read):which would produce the same outputalso, a bit less orthodox but if you wanted to simply add a single element to the end:"},
{"body": "Have a list of python dictionaries in the following format. How would you do a search to find a specific name exists?The following did not work:You'd have to search through all dictionaries in your list; use  with a generator expression:This will short circuit; return  when the  match is found, or return  if none of the dictionaries match.You might also be after:Or possibly more clearly:"},
{"body": "Using virtualenv and just trying to install pyodbc. All resources I can find claim this should be extremely straightforward. After all the basic installs of MySQL, etc., just do:However, I am seeing a very strange error. It has nothing (as far as I can tell) to do with missing libraries, and after Googling for this sort of error for a long time, I can't find anything constructive on it at all.So I tried with the \"allow-external\" option and it does not help:But the help documentation makes it appear that I am using this option correctly, e.g. from the output of running :Here's the result in the PIP log file:It makes it seem that if I can just get the  option to work, it will work because it clearly sees the common location for getting pyodbc.But I am puzzled why this is needed at all, when virtually all references to installing pyodbc do not need to go out of the way to enable external links. I've also been installing many packages all day today, with few issues and certainly not encountering this  need in any other case.I am using pip version 1.5 with Python 2.7.3 on Ubuntu 12.04.Running  will work if the PyPI directory structure is correct. Based on the pip.log output when I originally posted this answer, I think that package's website directory structure was broken. You can always work around this type of problem by specifying the URL of the package like , links for different versions are described .I just tried installing pyodbc as well and hit the same wall. I think the problem you and I  both hit is that --allow-external consumes the next argument as well. So you actually need to write:I then hit another error about unverified sources so that command that actually worked for me was:Note that once that was done the permissions on the installed files prevented normal users from access pyodbc. So I went in manually adjusted the permissions on all installed packages (seemed easier than finding just the files needed by pyodbc).As per the suggestion of EMS I have added an issue to the Pip bug tracker. It can be found at:FWIW, on ms windows anything other than installing the binary (windows install) for me was cumbersome in a windows/cygwin hybrid  environment.I opted to use   as it's pure python and didn't require any changes for my windows/linux development. It can be seen as an  with no compilation needed, and of course can be installed easily with .There is an \"easier\" way to install pyodbc on windows in a virualenv using :as used for pywin32 see: I had success when installed withHowever, I also needed the  package in order for pyodbc to actually compile.Both  and  take package name as an argument.So your usage should beor the little confusing: use  when you just want to allow one and not . It kills the whole purpose of putting in the security check."},
{"body": "I'm using the Pandas package and it creates a DataFrame object, which is basically a labeled matrix. Often I have columns that have long string fields, or dataframes with many columns, so the simple print command doesn't work well. I've written some text output functions, but they aren't great.What I'd really love is a simple GUI that lets me interact with a dataframe / matrix / table. Just like you would find in a SQL tool. Basically a window that has a read-only spreadsheet like view into the data. I can expand columns, page up and down through long tables, etc.I would suspect something like this exists, but I must be Googling with the wrong terms. It would be great if it is pandas specific, but I would guess I could use any matrix-accepting tool. (BTW - I'm on Windows.)Any pointers?Or, conversely, if someone knows this space well and knows this probably doesn't exist, any suggestions on if there is a simple GUI framework / widget I could use to roll my own? (But since my needs are limited, I'm reluctant to have to learn a big GUI framework and do a bunch of coding for this one piece.)I use  from PyQt to display a . I create a  and then populate with  created with  values.\nFollowing is the snippet of code that reads a CSV file ,create a , then display in a GUI:I wasn't fully satisfied with some other GUIs, so I created my own, which I'm now maintaining . Example:Apart from the basic table + plot functionality, I wanted to have a specific way to filter data:You could use the to_html() dataframe method to convert the dataframe to html and display it in your browser. Here is an example assuming you have a dataframe called df. You should check the documentation to see what other options are available in the to_html() method.If you want to get the table to be nicely formatted and scrollable then you can use the datatables plug-in for jQuery . Here is the javascript I use to display a table the scrolls in both x and y directiions.Pandas 0.13 provides as an experimental feature:PySide support for the qtpandas  and see you can add this feature usingIn addition to all the valuable answers, I would like to mention that the Spyder IDE () has this feature as you can see in my printscreen below:This is just an objective fact and not advertisement for any IDE :) I don't want to trigger any debate on this question. The question was post in 2012 and other answers may be too old to apply.The answer in 2016 is, we should use Pycharm and it's shipped with  viewer.I've been working on a PyQt GUI for pandas DataFrame you might find useful.  It includes copying, filtering, and sorting.The nicest solution I've found is using  (see , and also mentioned in the ). You can install byand then you need to do a further install (just once) in your  notebookAfterwards, it's as easy as taking your   and runningThe other nice thing is that it renders in  too. See it in action It seems there is no easy solution. So, below is a little function to open a dataframe in Excel. It's probably not production quality code, but it works for me!I use ipython notebooks to drive pandas -- notebooks provide a nice clean way of incrementally building and interacting with pandas data structures, including HTML-ized display of dataframes:  I'm not a Pandas user myself, but a quick search for \"pandas gui\" turns up the Pandas project's :So, there's no GUI, but if you'd write one using Qt or Tk, the project might be interested in your code.There's  for python2.7 and  for python3. You can use GitHub Atom with Hydrogen plugin. In Mac you can use Cmd+Shift keys to execute line by line. Even you can select only the variable and see inside. DataFrames are nicely display and you can even copy. I wrote a blog to show the way to configure these.\n"},
{"body": "How can I make the following functionality compatible with versions of Python earlier than Python 2.7?Use:That's the  function with a generator expression producing  pairs.Or, to put it generically, a dict comprehension of the form:can always be made compatible with Python < 2.7 by using:"},
{"body": "How can I auto fill the username and password over the link below:After that I really do not know:Notes to your code:Use  method to simulate key typing. in the code (, ) does not match actual  of the elements (, ).Note:"},
{"body": "In a iPython notebook, I have a while loop that listens to a Serial port and  the received data in real time.What I want to achieve to only show the latest received data (i.e only one line showing the most recent data. no scrolling in the cell output area)What I need(i think) is to clear the old cell output when I receives new data, and then prints the new data. I am wondering how can I clear old data programmatically ?You can use  to clear the output of a cell.At the end of this loop you will only see one .Without a code example it's not easy to give you working code. Probably buffering the latest n events is a good strategy. Whenever the buffer changes you can clear the cell's output and print the buffer again."},
{"body": "Demo:Why does  call ?  It doesn't seem to use the result for anything obvious.  A  loop doesn't do it.  This isn't mentioned anywhere in the , which just talks about  and .  Is this Python reserving space for the list in advance, or something clever like that?  (CPython 3.6.0 on Linux)See the  that introduced  and offers insight on the motivation:Also see the documentation :So it seems that it's simply the case that  can result in some nice optimizations., , then tries to see if  is available, and, if neither is there, returns a default value of  for lists. , which is called from  as Eli stated in his answer, was modified according to this PEP to offer this optimization for anything that defines either a  or a .  isn't the only one that benefits from this, of course, :so :and  objects which create  populate themselves:If anybody is wandering why exactly  is printed   in class  and not after as happens with class : This is because if the object in hand defines an  , thereby running the  too. The same doesn't happen if it falls back to using . is a list object constructor that will allocate an initial slice of memory for its contents.  The list constructor attempts to figure out a good size for that initial slice of memory by checking the length hint or the length of any object passed into the constructor .  See the call to  in the Python . This place is called from the list constructor -- If your object has no  or , that's OK -- a  is used; it just may be less efficient due to reallocations."},
{"body": "Let's say I have the following classes set up:How can I (if I can at all) use super() in this context to eliminate the duplicate code?In Python >=3.0, like this:Read more here: EDIT: As said in another answer, sometimes just using  can be the better solution. (For instance, when working with certain forms of multiple inheritance.)Assuming you want class Bar to set the value 34 within its constructor, this would work:However,  introduces its own complications. See e.g. . For completeness, here's the equivalent version without ."},
{"body": "I'm confused about what the different between  and  is in matplotlib. Could someone please explain in an easy-to-understand way?Axis is the axis of the plot, the thing that gets ticks and tick labels. The axes is the area your plot appears in.This figure from the documentation will answer your question:\"axes\" is the plural of \"axis\"."},
{"body": "I have two lists:and I need to create a list of tuples from these lists, as follows:I tried doing it like this:but resulted in:i.e. a list of tuples of every element in x with every element in y... what is the right approach to do what I wanted to do? thank you... The other two duplicates mentioned before the edit is my fault, indented it in another for-loop by mistake...Use the builtin function :You're looking for the  function.\nFrom the docs:You're after the zip function.Taken directly from the question: "},
{"body": "I'm writing a script which will have to work on directories which are modified by hand by Windows and Linux users alike. The Windows users tend to not care at all about case in assigning filenames.Is there a way to handle this on the Linux side in Python, i.e. can I get a case-insensitive, glob-like behaviour? Use case-insensitive regexes instead of glob patterns.  generates a regex from a glob pattern, sogives you a case-insensitive version of a glob pattern as a compiled RE.Keep in mind that, if the filesystem is hosted by a Linux box on a Unix-like filesystem, users will be able to create files ,  and  in the same directory.You can replace each alphabetic character c with [cC], viaIn order to retrieve the files (and files only) of a directory \"path\", with \"globexpression\":with walk:Better also compile the regular expression, so instead of do (before the loop):and then replace in the loop:Depending on your case, you might use  on both file pattern and results from folder listing and only then compare the pattern with the filename"},
{"body": "I am making a chart in matplotlib and I have many subplots in it each of them with a different title, but on the top I also want to a put a title to the whole chart. How this can be done?You can use the  command to add a  in addition to sub plot titles."},
{"body": "In WeasyPrint\u2019s public API I accept filenames (among other types) for the HTML inputs. Any filename that works with the built-in  should work, but I need to convert it to an URL in the  scheme that will later be passed to .(Everything is in URL form internally. I need to have a \"base URL\" for documents in order to resolve relative URL references with .) is a start:The emphasis is mine, but I do need a complete URL. So far this seems to work:UTF-8 seems to be recommended by . But in this case (the\u00a0URL is meant for urllib, eventually) maybe I should use ?However, based on  I should prepend not just  but  ... except when I should not: On Windows the results from  already start with three slashes.So the question is: is there a better way to do this and make it cross-platform?For completeness, in Python 3.4+, you should do:I'm not sure the docs are rigorous enough to guarantee it, but I think this works in practice:Credit to comment from  above.For Python3, the following code will work:Does the following work for you?"},
{"body": "I am trying to use selenium from python to scrape some dynamics pages with javascript. However, I cannot call firefox after I followed the instruction of selenium on the pypi page(http://pypi.python.org/pypi/selenium). I installed firefox on AWS ubuntu 12.04. The error message I got is:I did search on the web and found that this problem happened with other people (https://groups.google.com/forum/?fromgroups=#!topic/selenium-users/21sJrOJULZY). But I don't understand the solution, if it is. Can anyone help me please? Thanks!The problem is Firefox requires a display. I've used  in my example to simulate a display. The solution is:This should resolve your issue.I too had faced same problem.I was on Firefox 47 and Selenium 2.53. So what I did was downgraded Firefox to 45. This worked.1) Remove Firefox 47 first :2) Check for available versions:It will show available firefox versions like:3) Tell which build to download4) Next you have to not upgrade to the newer version again.5) If you want to upgrade later\nHope this helps."},
{"body": "I am using  from  to do binary classification.  I am using its predict_proba() function to get probability estimates.  Can anyone tell me how predict_proba() internally calculates the probability?Scikit-learn uses LibSVM internally, and this in turn uses , as detailed in , to calibrate the SVM to produce probabilities in addition to class predictions.Platt scaling requires first training the SVM as usual, then optimizing parameter vectors  and  such thatwhere  is the signed distance of a sample from the hyperplane (scikit-learn's  method). You may recognize the  in this definition, the same function that logistic regression and neural nets use for turning decision functions into probability estimates.Mind you: the  parameter, the \"intercept\" or \"bias\" or whatever you like to call it, can cause predictions based on probability estimates from this model to be inconsistent with the ones you get from the SVM decision function . E.g. suppose that , then the prediction for  is positive; but if  and , then . I'm pulling these numbers out of thin air, but you've noticed that this can occur in practice.Effectively, Platt scaling trains a probability model on top of the SVM's outputs under a cross-entropy loss function. To prevent this model from overfitting, it uses an internal five-fold cross validation, meaning that training SVMs with  can be quite a lot more expensive than a vanilla, non-probabilistic SVM.Actually I found a slightly different answer that they used this code to convert decision value to probabilityHere A and B values can be found in the model file (probA and probB).\nIt offers a way to convert probability to decision value and thus to hinge loss.Use that ln(0) = -200."},
{"body": "Looking to store usernames and passwords in a database, and am wondering what the safest way to do so is.  I know I have to use a salt somewhere, but am not sure how to generate it securely or how to apply it to encrypt the password.  Some sample Python code would be greatly appreciated.  Thanks.Store the password+salt as a hash and the salt. Take a look at how Django does it:  and .\nIn the db they store   in a single char field. You can also store the three parts in separate fields.The function to set the password:The get_hexdigest is just a thin wrapper around some hashing algorithms. You can use hashlib for that. Something like And the function to check the password:I think it is best to use a package dedicated to hashing passwords for this like passlib:  for reasons as I explained here: I answered this here: , and so did @Koffie.I don't know how to emphasize enough that the accepted answer is NOT secure. It is better than plain text, and better than an unsalted hash, but it is still  to dictionary and even brute-force attacks. Instead, please  like bcrypt (or  PBKDF2 with 10,000 iterations)If you have enough control over both endpoints of the application, the absolute best way is using .Here is a simpler way (taken from effbot), provided passwords with a length greater than 8 will not be a problem*:for generate the password :*: A password with a length greater than 8 is stripped from the right down to 8 chars long"},
{"body": "I'm looking for the equivalent of  in Google App Engine and Python.Thanks!I slapped a quick and dirty example together based on the tutorial. It's been tested on my local appengine sdk. You should be able to adapt it to your needs:Try with:or with the :"},
{"body": "I have a quirk(?) with Django queryset filtering:orandso, how to use values_list()? (to produce):or is python list comprehension the 'way to go'?Try .  That returns a list of ids instead of a list of single id tuples.One thing to note is that there is a difference in the behaviour of values/values_list from a list comprehension: Choosing the wrong one will either result in unnecessary database hits, or unnecessary faffing around, depending on what you are trying to do."},
{"body": "Consider this snippet:Where  is the dict I'm after that would contain ,  and , among other things.I'd like to basically be able to reference all the variables that are currently in scope inside the string. Hence the expected output would be:I can achieve this by passing  to . Is this always correct or are there some corner cases that this expression would handle incorrectly?Best way to merge two dicts as you're doing (with locals overriding globals) is .What the approach of merging globals and locals is missing is (a) builtins (I imagine that's deliberate, i.e. you don't think of builtins as \"variables\"... but, they COULD be, if you so choose!-), and (b) if you're in a  function, any variables that are local to enclosing functions (no really good way to get a dict with all of , plus -- only those explicitly accessed in the nested function, i.e. \"free variables\" thereof, survive as cells in a closure, anyway).I imagine these issues are no big deal for your intended use, but you did mention \"corner cases\";-).  If you need to cover them, there are ways to get the built-ins (that's easy) and (not so easy) all the cells (variables from enclosing functions that you explicitly mention in the nested function --  to get the names,  to get the cells,  on each cell to get its value). (But, remember, those will only be variables from enclosing functions that are  in your nested function's code!).Does this do what you intended?If I read the documentation correctly, you create a copy of the  dict, then you overwrite any duplicates  insert new entries from the  dict (since the  should have preference within your scope, anyway).I haven't had  luck in getting a proper function to return the full dictionary of variables in scope of the   function.  Here's the code (I only used pprint to format the output nicely for SO):and the output:Note that in the second output, we have overwritten , and x is present; the first output only included the local vars within the scope of .So if you want to access the full variable scope, you cannot put locals() inside another function.I had suspected there was some sort of frame object, I just didn't (know where to) look for it.This works to your spec, I believe:-->You could make your own:or combine the first two lines:Interpolation into strings works in the simplest possible way.  Just list your variables.  Python checks locals and globals for you."},
{"body": "I'm currently trying to automate django tests using hudson, and am struggling to find an option that will automatically destroy the test database if it already exists (typically it will ask for confirmation to destroy it, which the automatic testing obviously cannot provide for).Any suggestions would be much appreciated!Cheers,\nRUse --help to see the docs of the test command:And use --noinput which defaults to destroying the test db;)"},
{"body": "I came across this sample of code from a :What does the '' do in Python? is the integer division operator.In Python 3 the ordinary  division operator returns floating point values even if both operands are integers, so a different operator is needed for integer division. This is different from Python 2 where  performed integer division if both operands were integers and floating point division if at least one of the operands was a floating point value.The  operator was first introduced for forward-compatibility in Python 2.2 when it was decided that Python 3 should have this new ability. Together with the ability to enable the Python 3 behavior via  (also introduced in Python 2.2), this enables you to write Python 3-compatible code in Python 2.You can just try it:This should be self-explanatory. (This is in Python 2.7.)"},
{"body": "I saw a python example today and it used -> for example this was what I saw:What is that code doing? I'm not quite sure I've never seen code like that I don't really get what is doing either, can someone explain this for me? I googled, \"what does -> do in python\" but no good searches came up that I found.It is function annotation for a return type.  do nothing inside the code, they are there to help a user with code completion (in my experience).Here is the  for it.Let me demonstrate, what I mean by \"annotations do nothing inside the code\". Here is an example:The above code will run without any errors. but as you can see the first parameter  be a , and the second an . But, this only is a problem in my IDE, the code runs just fine:They're . They don't really do anything by themselves, but they can be used for documentation or in combination with metaprogramming."},
{"body": "Usage of relative imports in Python has one drawback, you will not be able to run the modules as standalones anymore because you will get an exception: How should I modify the sample code in order to be able to execute all: ,  and I'm looking for a solution that works with python 2.6+ (including 3.x).First, I assume you realize what you've written would lead to a circular import issue, because foo imports bar and viceversa; try adding to test.py, and you'll see it fails. The example must be changed in order to work.So, what you're asking is really to fallback to absolute import when relative import fails; in fact, if you're executing foo.py or bar.py as the main module, the other modules will just lie at the root level, and if they share the name with another module on the system which one will be picked depends on the order in sys.path. Since the current dir is usually the first, local modules will be picked if available - i.e., if you've got an 'os.py' file in the current working dir, it'll be picked instead of the builtin one.A possibile suggestion is:foo.pybar.py:By the way calling scripts from the proper position is usually  better.Is probably the best way to go.You could just start 'to run the modules as standalones' in a bit a different way:Instead of:Use:Of course, the  file must be present.Please also note, that you have a circular dependency between  and  \u2013 this won't work. I guess it is just a mistake in your example.Update: it seems it also works perfectly well to use this as the first line of the :Then you can execute the script directly in POSIX systems.Ditch relative imports: you should think of your package namespace as a global one, anyway.The trick to making this palatable is editing  appropriately. Here is some food for thought:You need  in each folder.Relative import works only when you do:test.py imports foo.py and foo.py can relative import anything from the folder of test.py and above.You can't do:It will never work.You can try the sys.path.append or sys.path.insert solution but you gonna screw up the paths and you'll have problems with the f=open(filename).Why not just put the \"main\" in a different .py file?So far the only solution I found was not to use relative imports at all. Due to current limitation, I'm wondering when someone is supposed to use relative imports in python.On all configurations that I used the  contained the current directory as first argument so just use  instead of  because it will do the same."},
{"body": "When information about a type is needed you can use:gets:or:gets:Now, in the documentation of Python information can be found about these functions, but I would like to get info about these functions in the terminal/command-line.\nHow should this be done?In python:  for example, will give you the docstring of the function.Tryto get built-in help messages.Orif you're generally poking around."},
{"body": "I have a property in a Django Model that I'd like to expose via a TastyPie ModelResource. My Model isMy ModelResource isHowever all I'm currently getting out of the tastypie api is:I have tried playing with the fields property in the ModelResource, but that hasn't helped. Would love to understand what is going on here.You should be able to define it as a  try:You also have to include:  on your , or TastyPie will try to set its value on insertion or update.A full example with dehydrate:"},
{"body": "When considering design, in which cases does it make sense to use class versus dict? Pros & Cons would be useful too.For example,versusI would say the first and most important criteria to distinguish whether to use a class or a  dictionary is whether you just want to have some data storage or you also want to have some logic (i.e., methods).If you only need to keep together several data, then a dictionary might be the way to go. On the other hand, if you need operations performed on that data, and there is a very tight relationship between the data and the operations (forming a kind of entity), then that really calls for using a class.As @Ben is suggesting, you may start using just a dictionary when you have a bunch of related data, and if at some point your realize that you also need some logic for that data, you can turn the dictionary into a class.You may also have a look at the  and its  to get a better feeling on how to use classes. If you are just doing some scripting or programming a small application you may not need to use UML, but if you are designing a bigger and more complex system, it can really help you to decide what and how many classes do you need beforehand.A  can use any hashable value as a key, while a  instance needs to have strings that are legal identifiers as its \"keys\".  So you can bundle together arbitrary data in a .It's pretty common to use a trivial class as a more-convenient  when your key values will all be valid identifier strings anyway:As soon as you start to do proper object oriented design, and you have method functions that go along with the data, you want to have the data in a proper class so you can bundle the method functions in.  It's legal to just have a dict and a handful of global functions that operate on it, but if they are all related, it just makes sense to bundle them up into a .And finally, note that a  is a building-block that is used to implement a .  If you store values in a class, they are actually stored inside an internal  and there is just some convenient syntax to access them."},
{"body": "I'd like to reference a previously-documented function parameter elsewhere in a Python docstring.  Consider the following (admittedly completely artificial) example:Is there a simple way to embed a parameter reference using Sphinx markup, or will this happen automagically?(I'm a complete Sphinx newbie.  I've been scanning the Sphinx docs and haven't found an answer to this question, or an example demonstrating proper markup.)I've just built an extension to accomplish this task.  So far it seems to be working with standalone HTML build and additionally with readthedocs (after some more tweaks).  the extension is available at: .I'm rolling it out right now for the Alembic and SQLAlchemy projects.  ().I take disagreement with the suggestion that linking to params means the docs are too lengthy.  The Python standard library is a poor example here as stdlib functions are necessarily granular and simple.  Software that is accomplishing a more coarse-grained task, where a single function rides on top of a complex problem to be solved, will often have parameters that require a lot more explanation; this explanation is often quite valuable as the solution to a particular problem elsewhere, and therefore being able to link to it is very important.There is no simple way to get a direct reference to a parameter of a function with  and I don't know an extension for this problem.The  explains which objects can be cross referenced.A possible way to give the user a reference to parameter  of function  would beMaybe a direct reference would be possible by writing an extension.If you are looking for a way to link directly to the  definition of  then your documentation is too lengthy or you are asking your reader to ignore the forest for one tree or some combination of the two.Taking an example from :if I can't be bothered to read five sentences into  to find the meaning of  I probably don't deserve to be lead there. Note that the attribute reference syntax is the same as in the section above:but it looks like Sphinx doesn't reach out of the current section scope and so renders the later reference as styled text rather than as an anchor. It would not surprise me if this was intentional."},
{"body": "I have unittest code like the following:When I try to run this code, I get an error like this:  be a class method. From the :Your version is missing the  decorator:The error is thrown because  is called on the , not on an instance, but you didn't mark your method as a classmethod and thus it was not passed in the automatic  argument."},
{"body": "Celery doesn't seem to be handling exceptions properly.  If I have task:and then I callAnd it will hang like this indefinitely.  Going and checking the logs shows that the error IS getting thrown in the task (and if you want the message, ask), and I know that the backend and everything is set up properly because other tasks just work and return results correctly.  Is there something funky that I need to do to catch exceptions in Celery?/Celery version is 3.0.13, broker is RabbitMQ running on my local machineYou can define an  function in your  subclass to handle them correctly.  If you're just looking to find out what happened you can setup  that will send you the stack trace in your celery config.If you are running Celery with the CELERY_ALWAYS_EAGER set to True, then make sure you include this line in your settings too:Going to make @primalpython's answer more explicit.  This will fail:Input/Output:This will succeed:Input/Output:"},
{"body": "Using algorithms like leveinstein ( leveinstein or difflib) , it is easy to find approximate matches.eg.The fuzzy matches can be detected by deciding a threshold as needed.Current requirement : To find fuzzy substring based on a threshold in a bigger string.eg. One brute force solution is to generate all substrings of length N-1 to N+1 ( or other matching length),where N is length of query_string, and use levenstein on them one by one and see the threshold.Is there better solution available in python , preferably an included module in python 2.7 , or an externally available module .The new regex library that's soon supposed to replace re includes fuzzy matching.The fuzzy matching syntax looks fairly expressive, but this would give you a match with one or fewer insertions/additions/deletions.How about using ?Above code print: Recently I've written an alignment library for Python: Using it, you can perform both global and local alignments with arbitrary scoring strategies on any pair of sequences. Actually, in your case, you need semi-local alignments as you don't care for the substrings of . I've simulated semi-local algorithm using local alignment and some heuristics in the following code but it is easy to extend the library for a proper implementation.Here is the example code in the README file modified for your case.The output for  is as follows.If you remove the  argument, you will get only the best scoring matches.Note that all algorithms in the library have  time complexity,  and  being the lengths of the sequences.I use  to fuzzy match based on threshold and  to fuzzy extract words from the match.   takes a query, list of words and a cutoff score and returns a list of tuples of match and score above the cutoff score.   takes the result of  and returns the start and end indices of words. I use the indices to build the words and use the built word to find the index in the large string.  of  is 'Levenshtein distance' which has to be adjusted to suit the needs.Test;  Output;\nquery: manhattan\nstring: thelargemanhatanproject is a great project in themanhattincity\nmatch: manhatan\nindex: 8\nmatch: manhattin\nindex: 49  query: citi\nstring: thelargemanhatanproject is a great project in themanhattincity\nmatch: city\nindex: 58  query: greet\nstring: thelargemanhatanproject is a great project in themanhattincity\nmatch: great\nindex: 29  The approaches above are good, but I needed to find a small needle in lots of hay, and ended up approaching it like this:Yields:"},
{"body": "Then we are given the sample code:We create a memoryview object to expose the internal data of a buffer object without\ncopying, however, in order to do anything useful with the object (by calling the methods\nprovided by the object), we have to create a copy!Usually memoryview (or the old buffer object) would be needed when we have a large object,\nand the slices can be large too. The need for a better efficiency would be present\nif we are making large slices, or making small slices but a large number of times.With the above scheme, I don't see how it can be useful for either situation, unless\nsomeone can explain to me what I'm missing here.We have a large chunk of data, we want to process it by advancing through it from start to \nend, for example extracting tokens from the start of a string buffer until the buffer is consumed.In C term, this is advancing a pointer through the buffer, and the pointer can be passed\nto any function expecting the buffer type. How can something similar be done in python?People suggest workarounds, for example many string and regex functions take position \narguments that can be used to emulate advancing a pointer. There're two issues with this: first \nit's a work around, you are forced to change your coding style to overcome the shortcomings, and\nsecond: not all functions have position arguments, for example regex functions and  do, / don't. Others might suggest to load the data in chunks, or processing the buffer in small\nsegments larger than the max token.  Okay so we are aware of these possible\nworkarounds, but we are supposed to work in a more natural way in python without\ntrying to bend the coding style to fit the language - aren't we?A code sample would make things clearer. This is what I want to do, and what I assumed memoryview would allow me to do at first glance. Lets use pmview (proper memory view) for the functionality I'm looking for: objects are great when you need subsets of binary data that only need to support indexing. Instead of having to take slices (and create new, potentially large) objects to pass to  you can just take a  object.One such API example would be the  module. Instead of passing in a slice of the large  object to parse out packed C values, you pass in a  of just the region you need to extract values from. objects, in fact, support  unpacking natively; you can target a region of the underlying  object with a slice, then use  to 'interpret' the underlying bytes as long integers, or floating point values, or n-dimensional lists of integers. This makes for very efficient binary file format interpretations, without having to create more copies of the bytes.One reason  are useful is because they can be sliced without copying the underlying data, unlike /.For example, take the following toy example.On my computer, I getYou can clearly see quadratic complexity of the repeated string slicing. Even with only 400000 iterations, it's already unmangeable. Meanwhile, the memoryview version has linear complexity and is lightning fast.Edit: Note that this was done in CPython. "},
{"body": "I am fairly new to python, and noticed these posts:\n and \nAfter playing around with it, however, I noticed that these two classes give apparently equivalent results-(from ) and Is there any real difference between these two? Or, more generally, does  change anything inherently about the attributes of a class? In  it is mentioned that  is called when the instance is created. Does this mean that  in class  is established before instantiation? Yeah, check this out:and now try:and this:Yes, it's a class attribute (it is shared between instances). While in class A it's an instance attribute. It just happens that strings are immutable, thus there is no real difference in your scenario (except that class B uses less memory, because it defines only one string for all instances). But there is a huge one in my example.In the first exemple you have the variable of the instance of the class. This variable is only accessible through an instance (self required).In the second exemple you have a static variable. You can access to this variable thanks to the name of the class AIn fact you can have a static variable and an instance variable with the same name:The static value is shared between all the instancesYou have a good article here:\nAs others have stated, it's the difference between a variable on a class and a variable on a class . See the following example.For immutable objects like tuples, strings, etc. it's harder to notice the difference, but for mutables, it changes \u2014the changes applied are shared between ALL instances of that class.Note also that the same behavior happens for keyword argument defaults!This behavior bites a lot of new Python programmers. Good for you in discovering these distinctions early on!you use init you dont have to use it in your case you can justand if you want use init try it"},
{"body": "Does anyone know an algorithm to either calculate the moon phase or age on a given date or find the dates for new/full moons in a given year?Googling tells me the answer is in some Astronomy book, but I don't really want to buy a whole book when I only need a single page.I should have qualified my statement about googling a little better. I did find solutions that only worked over some subset of time (like the 1900's); and the trig based solutions that would be more computationally expensive than I'd like. S Lott in his Python book has several algorithms for calculating Easter on a given year, most are less than ten lines of code and some work for all days in the Gregorian calendar. Finding the full moon in March is a key piece of finding Easter so I figured there should be an algorithm that doesn't require trig and works for all dates in the Gregorian calendar.I ported some code to Python for this a while back.  I was going to just link to it, but it turns out that it fell off the web in the meantime, so I had to go dust it off and upload it again.  See  which is derived from .I can't find a reference for this for what time spans it's accurate for either, but seems like the authors were pretty rigorous.  Which means yes, it does use trig, but I can't imagine what the heck you would be using this for that would make it computationally prohibitive.  Python function call overhead is probably more than the cost of the trig operations.  Computers are pretty fast at computing.The algorithms used in the code are drawn from the following sources:A must-have; if you only buy one book, make sure it's this one. Algorithms are presented mathematically, not as computer programs, but source code implementing many of the algorithms in the book can be ordered separately from the publisher in either QuickBasic, Turbo Pascal, or C. Meeus provides many worked examples of calculations which are essential to debugging your code, and frequently presents several algorithms with different tradeoffs among accuracy, speed, complexity, and long-term (century and millennia) validity.Despite the word Calculator in the title; this is a valuable reference if you're interested in developing software which calculates planetary positions, orbits, eclipses, and the like. More background information is given than in Meeus, which helps those not already versed in astronomy learn the often-confusing terminology. The algorithms given are simpler and less accurate than those provided by Meeus, but are suitable for most practical work.I think you searched on wrong google:If you're like me, you try to be a careful programmer. So it makes you nervous when you see random code scattered across the internet that purports to solve a complex astronomical problem, but doesn't explain why the solution is correct.You believe that there must be authoritative sources such as  which contain careful, and complete, solutions. For instance:You place your trust in widely-used, well-tested, open source libraries which can have their errors corrected (unlike static web pages). Here then, is a Python solution to your question based on the  library, using the  interface.This returnsAlso,  [], which is a Python package but has the , and that  sayPyephem by default uses coordinated universal (UTC) time. I wanted a program that would generate a list of full moons that would be accurate in the pacific time zone. The code below will calculate the full moons for a given year and then adjust that using the ephem.localtime() method to calibrate to the desired time zone. It also appears to properly account for daylight savings time as well. Thank you to Richard, this code is similar to what he had written.The code above will return:I know that you're looking for Python but if you can understand C# there's an open source project out there called  which does this very well.A quick google revealed .If you don't need high accuracy, you can always (ab)use a lunar (or lunisolar) calendar class (e.g.,  or  in Microsoft .NET) to calculate the (approximate) moon phase of any date, as the calendar's \"day-of-month\" property, being a lunar (or lunisolar) calendar day,  (e.g., day 1 is the new moon, day 15 is the full moon, etc.)"},
{"body": "For the following code:Pylint reported an error: Could anyone give a hint what is happening here? From pylint source code the description is:But I do not have a clue what it means. Could anyone give an example of the problem?The name  in the body of the  will be looked up when the function is actually called, so it will see what ever value  had most recently. Since you are calling  immediately, the value of  will not change before the resulting function object is used, so you can safely ignore the warning. To silence it, you can make  the default value of a parameter to the :"},
{"body": "I am trying to use a list comprehension that compares string objects, but one of the strings is utf-8, the byproduct of json.loads.  Scenario:Part one of my question, is why does this return False? :Part two - how can I compare within a list comprehension?EDIT:  I'm using Google App Engine, which uses Python 2.7Here's a more complete example of the problem:You must be looping over the wrong data set; just loop directly over the JSON-loaded dictionary, there is no need to call  first:You may want to use  to avoid implicit conversions between Unicode and byte strings:Both versions :Note that in your first example,  is  a UTF-8 string; it is unicode data, the  library has already decoded it for you. A UTF-8 string on the other hand, is a sequence . You may want to read up on Unicode and Python to understand the difference:On Python 2, your expectation that your test returns  would be correct, you are doing something else wrong:There is  need to encode the strings to UTF-8 to make comparisons; use unicode literals instead:You are trying to compare a string of bytes () with a string of Unicode code points ().  This is an \"apples and oranges\" comparison.  Unfortunately, Python 2 pretends in some cases that this comparison is valid, instead of always returning :It's up to you as the designer/developer to decide what the correct comparison should be.  Here is one possible way:I recommend the above instead of  because all  style strings can be encoded into bytes with UTF-8, except possibly in some bizarre cases, but not all byte-strings can be decoded to Unicode that way.But if you choose to do a UTF-8 encode of the Unicode strings before comparing, that will fail for something like this on a Windows system: .  But if you  instead it would succeed.  That's why it's an apples and oranges comparison.I'm assuming you're using Python 3.  returns  because the  function is :In Python 3, strings are , so the  is superfluous."},
{"body": "Reading through the Python docs I came across .Can someone explain to me (with example) a scenario in which  would be preferred to ?With particular reference to:This is one example where I see the use:Usually I start programming with the Lock and when case 1 or 2 occur, I switch to an RLock.  the RLock should be a bit slower because of the additional code. It uses Lock:within the given thread you can acquire a  as often as you like. Other threads need to wait until this threads releases the resource again. This is different to the  which implies 'function-call ownership'(I would call it this way): Another function call has to wait until the resource is released by the last blocking function even if it is in the same thread = even if it is called by the other function. When you make a call to the outside of the resource which you can not control.The code below has two variables: a and b and the RLock shall be used to make sure a == b * 2In  the Lock would be the right choice although it does block. Or one can enhance it with errors using . Functions like  may occur when you have implemented a Observer pattern or a Publisher-Subscriber and add locking afterwards.A primitive lock (Lock) is a synchronization primitive that is not owned by a particular thread when locked.For the repeatable Lock (RLock) In the locked state, some thread owns the lock; in the unlocked state, no thread owns it.\nWhen invoked if this thread already owns the lock, increment the recursion level by one, and return immediately. if thread doesn't own the lock It waits until owner release lock.\nRelease a lock, decrementing the recursion level. If after the decrement it is zero, reset the lock to unlocked.I don't think there is some performance difference rather conceptual one. Here is another use case for RLock.  Suppose you have a web-facing user interface that supports concurrent access, but you need to manage certain kinds of access to an external resource.  For instance, you have to maintain consistency between objects in memory and objects in a database, and you have a manager class that controls access to the database, with methods that you must ensure get called in a specific order, and never concurrently.What you can do is create an RLock and a guardian thread that controls access to the RLock by constantly acquiring it, and releasing only when signaled to.  Then, you ensure all methods you need to control access to are made to obtain the lock before they run.  Something like this:This way, you're ensured of the following things:"},
{"body": "I am trying to run the following Python code to create a Firefox Webdriver window via Selenium:While this code worked fine a few weeks ago, it now produces the following foreboding message:Does anyone know what this means, or what I can do to remedy the error and get the code to run as expected? I've found related error messages through Google searches, but nothing that has allowed me to resolve the issue.For what it's worth, I can open a Chrome Webdriver without issue by changing the second line of the above to .I'm using Python 2.7, Selenium 2.35.0 (I just ran \"pip install selenium --upgrade) and Firefox 26.0 on a Windows 8 machine. Any tips or advice others can offer are most appreciated.Selenium 2.35 is not compatible with Firefox 26.  As the  say, FF 26 support was added in Selenium 2.39.  You need to update to 2.39.  Try  instead.Not sure if it works fine on Windows too, but for me the combination of Firefox 26 and selenium 2.37.0 works fine.I also have this issue in Win8.1 FF28 and python3.4/selenium 2.41. But after I degraded FF to 24, it worked! And I also tested in Win8.1/FF27/Python3.4/Selenium 2.41, it worked too.  I just ran into the same thing with FF36 and selenium 2.44.0. Re-installing FF 32.0 fixed it.selenium-2.45.0  ===works==> FF36(win7)I've experienced the same problem on my Kubuntu 14.04 desktop, I removed the Firefox 47.XX and re-installed Firefox 45.XX and the problem resolved.Download firefox debian package"},
{"body": "I've built a few Flask apps, but on my latest project I noticed something a little strange in development mode.  The second line of the usual message in the terminal which always reads:has been replaced by:I don't think I've done anything different, in fact, I started by cloning a starter-kit project that I have used many times, which itself, does not display this behavior.  I also notice that this project consumes about 15% CPU steadily, whereas my other project are barely a blip.Any ideas why this is happening?Check your version of Werkzeug.  Version 0.10 was just released and numerous changes went into the reloader.  One change is that a default polling reloader is used; the old pyinotify reloader was apparently inaccurate.  If you want more efficient polling, install the  package.  You can see the code related to this .When Werkzeug can't find watchdog, it uses the  reloader, otherwise it uses whatever reloader watchdog uses, which can vary by platform.  This message is just so you know which one is in use."},
{"body": "Using Python, how can information such as CPU usage, memory usage (free, used, etc), process count, etc be returned in a generic manner so that the same code can be run on Linux, Windows, BSD, etc?Alternatively, how could this information be returned on all the above systems with the code specific to that OS being run only if that OS is indeed the operating environment?Regarding cross-platform: your best bet is probably to write platform-specific code, and then import it conditionally.  e.g.For specific resources, as Anthony points out you can access  under linux.  For Windows, you could have a poke around at the .  I'm not sure where to get that kind of information on Macs, but I can think of a great website where you could ask :-)In a Linux environment you could read from the /proc file system.I recommend the platform module: should provide what you need:It looks like you want to get a lot more information than the standard Python library offers. If I were you, I would download the source code for 'ps' or 'top', or the Gnome/KDE version of the same, or any number of system monitoring/graphing programs which are more likely to have all the necessary Unix cross platform bits, see what they do, and then make the necessary native calls with ctypes.It's trivial to detect the platform. For example with ctypes you might try to load libc.so, if that throws an exception try to load 'msvcrt.dll' and so on. Not to mention simply checking the operating system's name with os.name. Then just delegate calls to your new cross-platform API to the appropriate platform-specific (sorry) implementation.When you're done, don't forget to upload the resulting package to pypi.take a look at the There's the  (Python System Information) project with that aim, but they don't cover Windows yet.You can probably use PSI and recpies  and create a basic library that meets your needs."},
{"body": "I'm trying to understand how Python's garbage collector detects circular references. When I look at the documentation, all I see is a statement that circular references are detected, except when the objects involved have a  method. If this happens, my understanding (possibly faulty) is that the gc module acts as a failsafe by (I assume) walking through all the allocated memory and freeing any unreachable blocks.How does Python detect & free circular memory references before making use of the gc module?It doesn't. The gc exists only  detect and free circular references. Non-circular references are handled through refcounting.Now, to see  gc determines the set of objects referenced by any given object, take a look at the  function in . The relevant bit is:The major function here is . Each C-level type defines a  function (or in the case of objects which don't hold any references, like , sets it to ).  One example of  is , the traversal function for :You are correct \u2014 Python's cycle detector can detect and collect cycles  they contain objects with a  method, as there is no way for the interpreter to safely delete these objects (to get an intuition on why this is, imagine you've got two objects with  methods that reference each other. In which order should they be freed?).When objects with a  method are involved in a cycle, the garbage collector will stick them in a separate list (accessible through ) so that the programmer can manually \"deal with\" them.Python's garbage collector (not actually the  module, which is just the Python interface to the garbage collector) does this. So, Python  detect and free circular memory references before making use of the garbage collector.Python ordinarily frees most objects as soon as their reference count reaches zero. (I say \"most\" because it never frees, for example, small integers or interned strings.) In the case of circular references, this never happens, so the garbage collector periodically walks memory and frees circularly-referenced objects.This is all CPython-specific, of course. Other Python implementations have different memory management (Jython = Java VM, IronPython = Microsoft .NET CLR).I think I found the answer I'm looking for in some links provided by @SvenMarnich in comments to the original question: Container objects are Python objects that can hold references to other Python objects. Lists, Classes, Tuples etc are container objects; Integers, Strings etc. are not. So, only container objects are at risk for being in a circular reference.Each Python object has a field - *gc_ref*, which is (I believe) set to NULL for non-container objects. For container objects it is set equal to the number of  that reference itAny container object with a *gc_ref* count greater than 1 (? I would've thought 0, but OK for now ?) has references that are not container objects. So they are reachable and are removed from consideration of being unreachable memory islands.Any container object reachable by an object known to be reachable (i.e. those we just recognized as having a *gc_ref* count greater than 1) also does not need to be freed.The remaining container objects are not reachable (except by each other) and should be freed. is a link providing a fuller explanation\n is a link to the source code, which has comments further explaining the thoughts behind the circular reference detection"},
{"body": "I came across bizarre eval behavior in Python 3 - local variables aren't picked up when eval is called in a list comprehension.It errors in Python 3:And it works fine in Python 2:Moving it outside of the list comprehension removes the problem.Is this intended behavior, or is it a bug?There is a  issue in the bug tracker for this: .The resolution for this bug is .Some comments from the Issue read:If you want:to work you'll need to capture the locals and globals as the issue is that  is the same has  but as the  appears within the generator function the results of those functions aren't the same as they were when the  didn't have a wrapping function so capture them outside the generator and use them inside:Or better as  are locals and the strings are only variable names:"},
{"body": "Following-up from  years ago, is there a canonical \"shift\" function in numpy? I don't see anything from .Here's a simple version of what I'm looking for:Using this is like:So  is much faster than . This version of the function performs a lot better:An even faster version simply pre-allocates the array:Not numpy but scipy provides exactly the shift functionality you want,where default is to bring in a constant value from outside the array with value , set here to . This gives the desired output,and the negative shift works similarly,Provides output There is no single function that does what you want. Your definition of shift is slightly different than what most people are doing. The ways to shift an array are more commonly looped:However, you can do what you want with two functions. Consider :After running cProfile on your given function and the above code you provided, I found that the code you provided makes 42 function calls while  made 14 calls when arr is positive and 16 when it is negative. For those who want to just copy and paste the fastest implementation of shift, there is a benchmark and conclusion(see the end). In addition, I introduce fill_value parameter and fix some bugs.benchmark result:shift5 is winner! It's OP's third solution. "},
{"body": "I read in a string from a GUI textbox entered by the user and process it through . The string contains latex directives for math which have backslash characters. I want to send in the string as a raw string to pandoc for processing. But something like '\\theta' becomes a tab and 'heta'.How can i convert a string literal that contains backslash characters to a raw string...?Edit: Thanks develerx, flying sheep and unutbu. But none of the solutions seem to help me. The reason is that there are other backslashed-characters which do not have any effect in python but do have a meaning in latex. For example '\\lambda'. All the methods suggested produce which does not go through in latex processing -- it should remain as \\lambda.Another edit:If i can get this work, i think i should be through. @Mark: All three methods give answers that i dont desire.Python\u2019s raw strings are just a way to tell the Python interpreter that it should interpret backslashes as literal slashes. If you read strings entered by the user, they are already past the point where they could have been raw. Also, user input is most likely read in literally, i.e. \u201craw\u201d.This means the interpreting happens somewhere else. But if you know that it happens, why not escape the backslashes for whatever is interpreting it?(Note that you can't do  as , but I could have used  as well for the second argument.)If that doesn\u2019t work, your user input is for some arcane reason interpreting the backslashes, so you\u2019ll need a way to tell it to stop that.This shows that there is a single backslash before the ,  and :There is something funky going on with your GUI. Here is a simple example of grabbing some user input through a . Notice that the text retrieved only has a single backslash before the , , and . Thus no extra processing should be necessary:If you type  into the Entry box, the console will (correctly) print:If your GUI is not returning similar results (as your post seems to suggest), then I'd recommend looking into fixing the GUI problem, rather than mucking around with  and string .When you read the string from the GUI control, it is already a \"raw\" string. If you print out the string you might see the backslashes doubled up, but that's an artifact of how Python displays strings; internally there's still only a single backslash."},
{"body": "I'm using python 2.7.2 and windows 7. I searched through internet, helps and other sources but i can't find an answer to my problem. One of my source imports , and this one imports . At this moment it say : I searched _tkinter and i found it in Python27/libs as a lib file.On many sites it says to install  or , but I don't find a separate installation for windows.  Using Windows x86 MSI Installer (2.7) from . In windows 7 64-bit. The python version is 32 bit.I had a similar problem importing Tkinter on Windows 7 64-bit - seems that the 64-bit library was still in the libs folder from a previous 64-bit python install.Uninstalling 64-bit python properly and then repairing with the 32-bit installer fixed the problem for me - you don't need to use 64-bit python.Re-install Python.If this still doesn't work, there is another simple solution:I had the same issue and was able to uninstall Python 2.7 (using the Windows 7 'uninstall' service) and then reinstall it from here:\nAlso, if you're concerned about ensuring 64bit conformance across your python libraries then you may find the following . It has numerous 64bit versions of python libraries.I have a lot of installed python packages on my machine so I did not want to unistall and reinstall the whole python, I did as follow and the problem was solved:Please pay attention to get a backup form your tcl and DLLs folder to recover it in case you face any further problem."},
{"body": "I have Windows Vista 64.I have some projects requiring Python 2.7.3 64 bit and others requiring Python 2.7.3 32 bit (because some extensions do not work in 64 bit).How do I prevent the Python 2.7.3 MSI installer (32 or 64 bot) from deleting the other  version.Side by side worked for me with Python 2.7.2 without problems.This appears to be working for me on Windows 7 64 bit. Choose one version to be your default installation, e.g. 64 bit,  and install it first. Before doing anything else install the other version.Specify a different installation directory and in the  screen select  and select .You can generally install multiple Python Version side by side. When installing in Windows, apart from file installation, registry is also updated so that any subsequent package/tools installation can enumerate all the Python Installation in the current m/c.\nSolutionAs Ruediger Jungbeck pointed out that he can see two different registry keys, I suddenly recollected an important change post Vista. On a 64 bit system in order to enable accessing the registry keys from 32 bit and 64 bit application, Windows splits the registry at important nodes. For 64 bit applications, its the native registry path but for 32 bit applications its the WoW6432Node. So when running a 32 bit Version of Python\\Tools\\extension, it will only see the Wow6432Node. 64 bit applications will only see the native node. There are APIs to access the cross bitness nodes but that is outside the scope of this question. \nSo to summarize, windows intelligently handles 32 and 64 bit applications by splitting the node which will allow 32 bit extensions to transparently access the 32 bit Python Installation.While installing 2nd python, for Register Extensions select \"Entire feature will be unavailable\" option.\nPython 64 Installation path will be in registry \"HKLM:SOFTWARE\\Python\\PythonCore\\2.7\\InstallPath\"Python 64 bit PythonPath will be in \n\"HKLM:SOFTWARE\\Python\\PythonCore\\2.7\\PythonPath\"Python 32 Installation path will be in registry\n\"HKLM:SOFTWARE\\Wow6432Node\\Python\\PythonCore\\2.7\\InstallPath\"Python 32 bit PythonPath will be in \n\"HKLM:SOFTWARE\\Wow6432Node\\Python\\PythonCore\\2.7\\PythonPath\""},
{"body": "I have an event loop that runs some co-routines as part of a command line tool. The user may interrupt the tool with the usual  + , at which point I want to clean up properly after the interrupted event loop.Here's what I tried.Running this and hitting  +  yields:Clearly, I didn't clean up correctly. I thought perhaps calling  on the tasks would be the way to do it.What's the correct way to clean up after an interrupted event loop?When you CTRL+C, the event loop gets stopped, so your calls to  don't actually take effect. For the tasks to be cancelled, you need to start the loop back up again.Here's how you can handle it:Once we catch , we  call  and then start the  up again.  will actually exit as soon as  gets cancelled (note that cancelling the  returned by  also cancels all the  inside of it), because the interrupted  call added a  to  that stops the loop. So, when we cancel , that callback fires, and the loop stops. At that point we call , just to avoid getting a warning about not fetching the exception from the .Based on the other answers and some thinking I arrived at this handy solution that should work almost all use cases and does not depend on you manually keeping track of tasks that need to be cleaned up on +:The above code will obtain all currently tasks from the event loop using  and place them in a single combined future using . All tasks in that future (which are all currently running tasks) are then canceled using the future's  method. The  then ensures that all the received  exceptions are stored instead of causing the future to become errored.The above code will override the default exception handler to prevent Unless you are on Windows, set up event-loop based signal handlers for SIGINT (and also SIGTERM so you can run it as a service). In these handlers, you may either exit the event loop immediately, or initiate some kind of cleanup sequence and exit later.Example in official Python documentation: "},
{"body": "Is it possible to parse a file line by line, and edit a line in-place while going through the lines?It can be simulated using a backup file as stdlib's  does.Here's an example script that removes lines that do not satisfy  from files given on the command line or :Example: On completion  and  files will contain only lines that satisfy  predicate.No. You cannot safely write to a file you are also reading, as any changes you make to the file could overwrite content you have not read yet. To do it safely you'd have to read the file into a buffer, updating any lines as required, and then re-write the file.If you're replacing byte-for-byte the content in the file (i.e. if the text you are replacing is the same length as the new string you are replacing it with), then you can get away with it, but it's a hornets nest, so I'd save yourself the hassle and just read the full file, replace content in memory (or via a temporary file), and write it out again.If you only intend to perform localized changes that do not change the length of the part of the file that is modified (e.g. changing all characters to lower case), then you can actually overwrite the old contents of the file dynamically.To do that, you can use random file access with the  method of a  object.Alternatively, you may be able to use an  object to treat the whole file as a mutable string. Keep in mind that  objects may impose a maximum file-size limit in the 2-4 GB range on a 32-bit CPU, depending on your operating system and its configuration.You have to back up by the size of the line in characters.  Assuming you used , then you can get the length of the line and back up using:Set whence to , set offset to .See  or look at the manpage for ."},
{"body": "I'd like to embed an IPython qt console widget in a PyQt application I am working on. The code provided below (and adapted from ) Accomplishes this for IPython v0.12. However, this crashes in IPython v0.13 at the line  with . Commenting out this line brings up the widget, but doesn't respond to user input.Does anyone know how to achieve the equivalent functionality for IPython v0.13?Traceback for v0.13Ok, this code seems to do the trick (i.e. it puts a non-blocking ipython interpreter in a Qt widget, which can be embedded into other widgets). Keywords passed to  get added to the namespace of the widgetThe accepted answer by @ChrisB is fine for IPython version 0.13, but it doesn't work with newer versions. From the  of the IPython kernel repository on github,  is the way to do it in v1.x+ (currently tested with 4.0.1), which has the feature that the console and kernel are in the same process.Here is an example, based on the official one, which gives a convenience class that can be easily plugged into an application. It's setup to work with pyqt4 and IPython 4.0.1 on Python 2.7:IPython 0.13 version with some cleanups:pberkes was kind enough to update this example for me:I have it working on the latest 1.0-dev in a separate window, now I just need to figure out how to get access to all my program variables and stick it in my main window.Possibly helping others researching this: I came across this example:Tested and works with PySide, IPython 2.1.0, Python 3.4.1. It appears I can even use matplotlib directly. Have you tried some of the most recent examples provided in the ipython source code?  I recently came across the following from fperez (I'd like to buy that man a beer) and they seem to illustrate a nice way of capturing variables embedded in your GUI.A 2016 update working in PyQt5:"},
{"body": "I'm working with Python itertools and using groupby to sort a bunch of pairs by the last element. I've gotten it to sort and I can iterate through the groups just fine, but I would really love to be able to get the length of each group without having to iterate through each one, incrementing a counter.The project is cluster some data points. I'm working with pairs of (numpy.array, int) where the numpy array is a data point and the integer is a cluster labelHere's my relevant code:On the last line, 'if len(clusterList) < minLen:', I get an error that object of type 'itertools._grouper' has no len().I've looked up the operations available for _groupers, but can't find anything that seems to provide the length of a group.Just because you call it  doesn't make it a list! It's basically a lazy iterator, returning each item as it's needed. You can convert it to a list like this, though:Or do that and get its length in one step:If you don't want to take up the memory of making it a list, you can do this instead:Be aware that the original iterator will be consumed entirely by either converting it to a list or using the  formulation. is  but it is not a .  This can be a little confusing sometimes.  You can do a  loop over  but you can't do other list things over it (slice, len, etc).Fix: assign the result of  to ."},
{"body": "When i try to follow python wiki page example related to URL encoding:An error is raised on the second line:What i am missing? has been split up in Python 3.  The  function is now , and the  function is now .You use the Python 2 docs but write your program in Python 3."},
{"body": "Why can't I perform an action like the following:I would expect it to print  since we're overwriting the instance with it, but instead it doesn't do anything at all. Doesn't even throw an error. Just ignores the assignment.I understand that there would be hardly any situations where one would want to do that, but it still seems odd that you can't. I now understand  it doesn't work, but I'd still like to know if there is any way of replacing an instance from within the instance. simple assignment to  argument of  function behaves exactly the same way in Python: binds that name to a different value, and does nothing else whatsoever. \"No special case is special enough to break the rules\", as the Zen of Python says!-)So, far from it being odd (that simply=assigning to a specific argument in a specific function has no externally visible effect whatsoever), it would be utterly  if this specific case worked in any other way, just because of the names of the function and argument in question.Should you ever want to make a class that constructs an object of a different type than itself, such behavior is of course quite possible -- but it's obtained by overriding the special method ,  :This  emit .  The  /  behavior in Python is an example of the \"two-step construction\" design pattern: the \"constructor\" proper is  (it builds and returns a (normally uninitialized) object (normally a new one of the type/class in question);  is the \"initializer\" which properly initializes the new object.This allows, for example, the construction of objects that are immutable once constructed: in this case everything must be done in , before the immutable object is constructed, since, given that the object is immutable,  cannot mutate it in order to initialize it.It doesnt \"ignore\" the assignment. The assignment works just fine, you created a local name that points to the data 5.If you  want to do what you are doing...I just ran a quick test, and you can assign to self.  Inside your () method, print out the value of self.  You'll see that it's 5.What you're missing here is that parameters are passed by value in Python.  So, changing the value of a variable in a function or method won't change it for the outside world.All that being said, I would strongly advise against ever changing self.Sometimes you want to do this, though not with immutable types like :Dr. Egon Spengler: It would be bad.\nDr. Peter Venkman: I'm fuzzy on the whole good/bad thing. What do you mean, \"bad\"?\nDr. Egon Spengler: Try to imagine all life as you know it stopping instantaneously and every molecule in your body exploding at the speed of light.is like having this PHP (only other lang i know, sorry)I don't see how it makes sense. replacing the instance with a value?"},
{"body": "I was unable to upload to an AppEngine as  was telling me :I was only a developer on the AppEngine, so as I was just testing I created a new AppEngine where I was the owner but I still get the same message on a newly created AppEngine.The fix I found was to add the parameter :This can happen when you upload AppEngine applications from different Google accounts on the same computer.You have to log in in your gae account and create an application before you upload it, and your app_id has to have the same name as you app. You can't just upload it.So go , create your app, for example \"example_app\" and set the app_id=example_app and everything should work :)The same problem occurs with the Java/Eclipse plugin version of App Engine. The 404 happens when you're logged in to the wrong Google account from within the plugin. In that case, look at the bottom-left of Eclipse to see what account you're currently using.Regarding the Python command line updater, if your cookies indicate that you're logged in to a Google account that doesn't have access to the application you're updating, then that would explain why ignoring those cookies by using \"appcfg.py update --nocookies\" fixes it.This is really old, so I wouldn't be surprised if this isn't picked up by the poster, but I wanted to ensure that I have a reference for the next time I have the issue.I had this issue. My problem ended up being that I had not invited the user that I was authenticating as to be a developer on the project. In fact I had just created the user and not even logged on yet.I logged on to the google infrastructure as the new user, then added the new user as a developer on the application. I was then able to upload the app.Tried to upload to the app this morning (first time i've tried since Friday) and it just worked (first time...!) No idea what the issue was as I haven't done any work on this over the weekend so everything should have been the same as it was on Friday.I got the same error message trying to make the first deployment of a Python Application using a Google Apps account.In my case the problem was caused by the fact that my 2-way authentication is enabled.With 2-way authentication enabled you need to generate an authentication password in your Google Account Management Application and use it to do the deployments.this way worked:I fixed this problem by editing my app.yaml file to have the correct application name. In the app.yaml file where it says 'application:[insert app name here]', I had to change the [insert app name here] part. I'm not sure why the name was wrong, but I do remember toying around with it before while attempting to fix a different bug. The way I figured out the proper name was to go to my applications in my GAE admin console and look to see what the app's name was there. Hope this helps anyone else that runs into this same problem :)If you are using  make sure that you are logged into the correct account when generating the token.I navigated to my app engine directory folder and ran the above command. This deployed it successfully from the command line for me (Mac OSX terminal)--although I still couldn't deploy from app engine launcher. I believe Launcher still thinks I'm running off port 8080. However, running doesn't fix this issue for me-- what ended up solving it was just removing the directory from the app engine launcher then just re-adding it.What worked for me is to use the project id from the google app engine console instead of the project name in app.yamlPosting Ric Moore's comment in the selected answer because that is a valid answer and it worked in my case (and hopefully it helps someone else having the same issue spot it quickly).   I had this issue and I simply needed to run 'gcloud app create' for this project within the console console. Previously I don't think this step was needed."},
{"body": "Using python2.7, I'm trying to print to screen tabular data.This is roughly what my code looks like:The problem is that, depending on the length of  or  the data won't be aligned.This is what I'm getting:What I want to get:Are there any modules that permit doing this?It's not really hard to roll your own formatting function:There is a nice module for this in pypi, PrettyTable.For more beautiful table use the tabulate module:Here reported an example:It seems like you want your columns left-justified, but I haven't seen any answers mention the  string method, so I'll demonstrate that in Python 2.7:For your reference, running  gives you this:It looks like the  method takes your specified width and subtracts the length of the string from that, and pads the right side of your string with that many characters. You can try .\nHere's an example:"},
{"body": "For instance, I've tried things like , which, doesn't work.. Is there some kind of structure with this kind of functionality? I realize that I could obviously do this just as easily with a bunch of def statements:  But the number of statements I need is getting pretty unwieldy and tough to remember. It would be nice to wrap them nicely in a dictionary that I could examine the keys of now and again. Functions are first class objects in Python and so you can dispatch using a dictionary. For example, if  and  are functions, and  is a dictionary like so. Note that the values are  and  which are the function objects, and NOT  and .To call , you can just do EDIT: If you want to run  functions stored in a list, you can possibly do something like this."},
{"body": "First thing I have to mention here, I'm new to python.Now I have a file located in:I want to copy to my home directory with a new folder created:My expected result is:Is there any existing library to do that? If no, how can I achieve that?To create all intermediate-level destination directories you could use  before copying:take a look at .  will copy a file to another file.Note that  will not create directories that do not already exist.  for that, use "},
{"body": "My Python version is 2.6.I would like execute the setUp method only once since I'm doing there things which are needed for every test.My idea was to create a boolean var which will be set to 'true' after the first execution.The output:why is this not working? \nDid I missed something?You can use  to define methods that only run once per testsuite.Daniel's answer is correct but better with an example to avoid some common mistakes I found, like not calling super in setUpClass.The documentation  don't states the need to call super and you will get an error if you don't, solved here .Don't try to dedupe the calls to setUp, just call it once.For example:This will call _set_up() when the module's first loaded. I've defined it to be a module-level function, but you could equally make it a class method of MyClass.Place all code you want set up once outside the mySelTest.Another possibility is having a Singleton class that you instantiate in , which will only run the  code once and return the object instance for the rest of the calls.\nSee: Your way works too though."},
{"body": "I'd like to make a scatter plot where each point is colored by the spatial density of nearby points.  I've come across a very similar question, which shows an example of this using R:What's the best way to accomplish something similar in python using matplotlib?In addition to  or  as @askewchan suggested, you can use the same method that the accepted answer in the question you linked to uses.If you want to do that:If you'd like the points to be plotted in order of density so that the densest points are always on top (similar to the linked example), just sort them by the z-values.  I'm also going to use a smaller marker size here as it looks a bit better:You could make a histogram:"},
{"body": "Using a python flask server, I want to be able to throw an http error response with the abort command and use a custom response string and a custom message in the bodyBut the error.message variable comes up as an empty string.  I can't seem to find documentation on how to get access to the second variable of the abort function with a custom error handlerIf you look at  you will see that  is actually imported from .  Looking at the , we can see that when called with a numeric code, the particular  subclass is looked up and called with all of the arguments provided to the  instance.  Looking at , paying particular attention to  we can see that the second argument passed to  is stored in the  property, as @dirn pointed out.You can either access the message from the  property:or just pass the description in by itself:People rely on  too much. The truth is that there are much better ways to handle errors.For example, you can write this helper function:Then from your view function you can return an error with:If the error occurs deeper in your call stack in a place where returning a response isn't possible then you can use a custom exception. For example:Then in the function that needs to issue the error you just raise the exception:I hope this helps."},
{"body": "What would be the best way in Python to parse out chunks of text contained in matching brackets?should initially return:putting that as an input should return:which should return:Pseudocode:Or this pyparsing version:I'm kind of new to Python, so go easy on me, but here is an implementation that works:Parse using  (installable via ):Output:You could also parse them all at once, though I find the  to mean  rather than  slightly weird.  If I've understood the format correctly:If you want to use a parser (lepl in this case), but still want the intermediate results rather than a final parsed list, then I think this is the kind of thing you were looking for:That might look opaque at first, but it's fairly simple really :o) is a recursive definition of a matcher for nested brackets (the \"+\" and [...] in the definition keep everything as a single string after it has been matched).  Then  says match as many as possible (\"[:]\") of something that is surrounded by \"{\" ... \"}\" (which we discard with \"Drop\") and contains either a nested expression or any letter.Finally, here's a lepl version of the \"all in one\" parser that gives a result in the same format as the pyparsing example above, but which (I believe) is more flexible about how spaces appear in the input:Cleaner solution.  This will find return the string enclosed in the outermost bracket.  If None is returned, there was no match.Using :Here is a solution I came up with for a similar use case. This was loosely based on the accepted psuedo code answer. I didn't want to add any dependencies for external libraries:"},
{"body": "Is there a generic \"form sanitizer\" that I can use to ensure all html/scripting is stripped off the submitted form? form.clean() doesn't seem to do any of that - html tags are all still in cleaned_data. Or actually doing this all manually (and override the clean() method for the form) is my only option?Django comes with a template filter called , which you can use in a template:It uses the function  which lives in . You can utilize it also to clean your form data:strip_tags actually removes the tags from the input, which may not be what you want.To convert a string to a \"safe string\" with angle brackets, ampersands and quotes converted to the corresponding HTML entities, you can use the  filter:Alternatively, there is a Python library called :Example:"},
{"body": "With Sqlite, a \"select..from\" command returns the results \"output\", which prints (in python):It seems to be a list of tuples. I would like to either convert \"output\" in a simple 1D array (=list in Python I guess):or a 2x3 matrix:to be read via \"output[i][j]\"The flatten command does not do the job for the 1st option, and I have no idea for the second one... :)Could you please give me a hint?  Some thing fast would be great as real data are much bigger (here is just a simple example).Thank you.By far the fastest (and shortest) solution posted:About 50% faster than the  solution, and about 70% faster than the  solution.use  chain:Or you can flatten the list like this:For the 2 by 3 matrix, there's nothing special to do:In Python 3 you can use the  syntax to flatten a list of iterables:you could easily move from list of tuple to single list as shown above.In case of arbitrary nested lists(just in case):"},
{"body": "I am trying to write a list comprehension statement that will only add an item if it's not currently contained in the list. Is there a way to check the current items in the list that is currently being constructed? Here is a brief example:You can use  and set comprehension:As , we don't use the  here, so we can use  instead:If you really need a list as the result, you can do this (but notice that usually you can work with sets without any problem):As  suggests: you can use a uniqueness filter:and call with:I would implement the  separately since a design rule says . Furthermore you can simply reuse this method if necessary.Another advantage is - as is written at the  - that the  of the items is preserved. For some applications, this might be necessary.sets and dictionaries are your friends here:There's another way of writing this that is a bit more descriptive of what you're actually doing, and doesn't require a nested (double ) comprehension:This becomes even nicer when you'd represent the input to be more conceptually sound, i.e. use sets for the hobbies of each person (since there shouldn't be repetitions there either):If you really really want a listcomp and only a list-comp, you can doHere,  is a result of a side effect and  is your original dictionary. The unique advantage here is that .A list comprehension is not well-suited for this problem. I think a set comprehension would be better, but since that was already shown in another answer, I'll show a way of solving this problem with a compact one-liner:Another interesting solution using bitwise or operator which serves as a union operator for sets:Or (unintentional pun, I swear), instead of using bitwise or operator, just use  and pass it the unpacked set-mapping of your values. No need to import  and ! This idea is inspired by .Use a set:How about this:Or more nicely:you can apply the  function to list_ like  to return a list rather than a set."},
{"body": "I am looking to do some tinkering with openGL and Python and haven't been able to find good reasons for using PyOpenGl versus pygletWhich would you recommend and why?As Tony said, this is really going to depend on your goals.  If you're \"tinkering\" to try to learn about OpenGL or 3D rendering in general that I would dispense with all pleasantries and start working with PyOpenGL, which is as close are you're going to get to \"raw\" 3D programming using Python.On the other hand, if you're \"tinkering\" by way of mocking up a game or multimedia application, or trying to learn about programming practices in general than Pyglet will save you lots of up-front development time by providing hooks for input events, sounds, text/billboarding, etc. Often, this up-front investment is what prevents people from completing their projects, so having it done for you is not something to be ignored.  (It is also very Pythonic to avoid reinventing the wheel.)If you are looking to do any sort of heavy-duty lifting (which normally falls outside my definition of \"tinkering\", but maybe not if you're tinkering with 3D engine design) then you might want to take a look at , which wraps the  full-featured and robust  graphics engine.Start with pyglet. It contains the best high-level API, which contains all you need to get started, from opening a window to drawing sprites and OpenGL primitives using their friendly and powerful Sprite and Batch classes.Later, you might also want to write your own lower-level code, that makes calls directly to OpenGL functions such as glDrawArrays, etc. You can do this using pyglet's OpenGL bindings, or using PyOpenGL's. The good news is that whichever you use, you can insert such calls right into the middle of your existing pyglet application, and they will 'just work'. Transitioning your code from Pyglet to PyOpenGL is fairly easy, so this is not a decision you need to worry about too much upfront. The trades-off between the two are:PyOpenGL's bindings make the OpenGL interface more friendly and pythonic. For example, you can pass vertex arrays in many different forms, ctypes arrays, numpy arrays, plain lists, etc, and PyOpenGL will convert them into something OpenGL can use. Things like this make PyOpenGL really easy and convenient.pyglet's OpenGL bindings are automatically generated, and are not as friendly to use as PyOpenGL. For example, sometimes you will have to manually create ctypes objects, in order to pass 'C pointer' kinds of args to OpenGL. This can be fiddly. The plus side though, is pyglet's bindings tends to be significantly faster.This implies that there is an optimal middle ground: Use pyglet for windowing, mouse events, sound, etc. Then use PyOpenGL's friendly API when you want to make direct OpenGL function calls. Then when optimising, replace just the small percentage of performance-critical PyOpenGL calls that lie within your inner render loop with the pyglet equivalents. For me, this gives me between 2 and 4 times faster framerates, with PyOpenGL's convenience for 90% of my code.I'd say that Pyglet is actually more evolved than PyOpenGL. It has a nice API of it's own, and it has a full wrapper around OpenGL accessed through the pyglet.gl module! PyOpenGL doesn't even wrap all the functions OpenGL has.\nPyglet also has a great library for rendering 2D with hardware acceleration through OpenGL, and it's really well made.If you want a powerful ready made 3D engine you've got  and suchpyglet has a lot of nice extras included with it (like image loading and sound).  If you're starting out, I'd try pyglet first, and then switch to PyOpenGL if you feel like you want to get closer to the metal.The real important question though is: what are you trying to accomplish?  I promote pyglet because it has the nicest API I've yet seen on stuff like this.Pyglet has opengl API as well. But it's often nicer to use the recently added vertex list support.pyglet.glHmm, i would suggest pyglet, it really provides everything needed by a game or application.  Mind you, you can do a lot of things pyglet does with PyOpenGL, for example to create a window all you need to do is:\nglutInitWindow(title)Although i think glutInitDisplayMode has to be called before that.  Simple summary: if you don't wanna code until you cry, choose pyglet, but if you want to be a master, choose PyOpenGL. Goto  to read the docs on PyOpenGL and to  to read the docs on pyglet. Hope this was helpful!I would recommend Pyglet because it is very easy to get started and have something basic running, and then you can add more advanced techniques at your own pace.pyglet's GL API is nowhere near as nice as PyOpenGL's - pyglet's is at the raw ctypes layer which means you'll need to learn ctypes as well. If you're planning on doing a bunch of OpenGL programming you'll want to use PyOpenGL. The nice thing is you can mix the two just fine. Use pyglet to provide the GL context, sounds, events, image loading and texture management, text rendering, etc, etc. Use PyOpenGL for the actual OpenGL programming you need to do.Try .In ModernGL you can create a simple vertex shader with a single call:With ModernGL you have full control over the OpenGL API."},
{"body": "I'm writing a class in python and I have an attribute that will take a relatively long time to compute, so .  Also, it will not be needed by every instance of the class, so  in .  I'm new to Python, but not to programming.  I can come up with a way to do this pretty easily, but I've found over and over again that the 'Pythonic' way of doing something is often much simpler than what I come up with using my experience in other languages.Is there a 'right' way to do this in Python?The usual way would be to make the attribute a  and store the value the first time it is calculatedI used to do this how gnibbler suggested, but I eventually got tired of the little housekeeping steps.So I built my own descriptor:Here's how you'd use it:You should use both  and  decorators: has more detailed examples and also mentions a backport for previous Python versions.The Python wiki has a  (MIT licensed) that can be used like this:Or any implementation mentioned in the others answers that fits your needs.\nOr the above mentioned backport.You could try looking into memoization. The way it works is that if you pass in a function the same arguments, it will return the cached result. You can find more information on .Also, depending on how your code is set up (you say that it is not needed by all instances) you could try to use some sort of flyweight pattern, or lazy-loading.The most simple way of doing this would probably be to just write a method (instead of using an attribute) that wraps around the attribute (getter method). On the first call, this methods calculates, saves and returns the value; later it just returns the saved value."},
{"body": "The best I can come up with for now is this monstrosity:I.e., in English, get the current time (in UTC), convert it to some other timezone, set the time to midnight, then convert back to UTC.I'm not just using now() or localtime() as that would use the server's timezone, not the user's timezone.I can't help feeling I'm missing something, any ideas?I think you can shave off a few method calls if you do it like this:BUT\u2026 there is a bigger problem than aesthetics in your code: it will give the wrong result on the day of the switch to or from Daylight Saving Time.The reason for this is that neither the datetime constructors nor  take DST changes into account.For example:However, the documentation for  states:Thus, your problem is solved like so:No guarantees for dates before 1582, though. is wrong on the day of transition from Daylight Saving Time (DST) e.g., Apr 1, 2012. To fix it  could be used:The same with comments:Setting the TZ environment variable modifies what timezone Python's date and time functions work with.Each time zone has a number, eg US/Central = -6. This is defined as the offset in hours from UTC. Since 0000 is midnight, you can simply use this offset to find the time in any time zone when it is midnight UTC. To access that, I believe you can use According to , time.timezone actually gives the negative value of this number:So you would simply use that number for the time in hours if it's positive (i.e., if it's midnight in Chicago (which has a +6 timezone value), then it's 6000 = 6am UTC).If the number is negative, subtract from 24. For example, Berlin would give -1, so 24 - 1 => 2300 = 11pm."},
{"body": "Is there any way to modify the bound value of one of the variables inside a closure? Look at the example to understand it better.I don't think there is any way to do that in Python. When the closure is defined, the current state of variables in the enclosing scope is captured and no longer has a directly referenceable name (from outside the closure). If you were to call  again, the new closure would have a different set of variables from the enclosing scope.In your simple example, you might be better off using a class:If you do use this technique I would no longer use the name  because it is no longer actually a closure. However, it works the same as one.It is quite possible in python 3 thanks to the magic of .I've found an alternate answer answer to Greg's, slightly less verbose because it uses Python 2.1's custom function attributes (which conveniently enough can be accessed from inside their own function).Thought I'd post it for completeness. Cheers anyways.We've done the following.  I think it's simpler than other solutions here.I worked around a similar limitation by using one-item lists instead of a plain variable.  It's ugly but it works because modifying a list item doesn't get treated as a binding operation by the interpreter. For example:Why not make var_a and var_b arguments of the function foo?"},
{"body": "In , the votes clearly show that the  function is preferred over the simple  string manipulation. Does anyone have a moment to explain exactly why that is? Is it faster, or more accurate, or what? I'm willing to accept that there's something better about it, but I can't immediately see what it might be. Might importing a whole module to do this be overkill, at least in simple cases?EDIT: The OS specificity is a big win that's not immediately obvious; but even I should've seen the \"what if there isn't a dot\" case! And thanks to everybody for the general comments on library usage.Well, there are separate implementations for separate operating systems. This means that if the logic to extract the extension of a file differs on Mac from that on Linux, this distinction will be handled by those things. I don't know of any such distinction so there might be none.:  comments that an example like  would of course not work with a simple  call, and you would have to know both that directories can use extensions, as well as the fact that on some operating systems, forward slash is a valid directory separator.This just emphasizes the  part of my answer.Thanks .Additionally, where a file doesn't have an extension, you would have to build in logic to handle that case. And what if the thing you try to split is a directory name ending with a backslash? No filename nor an extension.The rule should be that unless you have a specific reason not to use a library function that does what you want, use it. This will avoid you having to maintain and bugfix code others have perfectly good solutions to.os.path.splitext will correctly handle the situation where the file has no extension and return an empty string. .split will return the name of the file. does a reverse search for '.' and returns the extension portion as soon as it finds it.  will do a forward search for all '.' characters and is therefore almost always slower. In other words  is specifically written for returning the extension unlike .(see posixpath.py in the Python source if you want to examine the implementation).There exist operating systems that do not use \u2018.\u2019 as an extension separator.(Notably, RISC OS by convention uses \u2018/\u2019, since \u2018.\u2019 is used there as a path separator.)The only reason to worry about importing the module is concern for overhead - that's not likely to be a concern in the vast majority of cases, and if it is that tight then it's likely other overhead in Python will be a bigger problem before that.A clearly defined and documented method to get the file extension would always be preferred over splitting a string willy nilly because that method would be more fragile for various reasons.Edit: This is not language specific.The first and most obvious difference is that the split call has no logic in it to default when there is no extension.This can also be accomplished by a regex, in order to get it to behave as a 1 liner without extra includes, but still return, empty string if the extension isn't there.Also, the path library can handle different contexts for paths having different seperators for folders.In the comment to the answer that provided this solution:Not every file has an extension.Besides being standard and therefore guaranteed to be available, : - like that of a missing extension.\n - Besides correctly returning the extension if one exists, it guarantees that  +  will always return the full path.\n - in the Python source there are actually three different version of os.path, and they are called based on which operating system Python thinks you are on.\n - consider that your version requires users to know that arrays can be indexed with negative numbers.  btw, it should not be any faster. 1) simple split('.')[-1] won't work correctly for the path as C:\\foo.bar\\Makefile so you need to extract basename first with os.path.basename(), and even in this case it will fail to split file without extension correctly. os.path.splitext do this under the hood.2) Despite the fact os.path.splitext is cross-platform solution it's not ideal. Let's looking at the special files with leading dot, e.g. .cvsignore, .bzrignore, .hgignore (they are very popular in some VCS as special files). os.path.splitext will return entire file name as extension, although it does not seems right for me. Because in this case name without extension is empty string. Although this is intended behavior of Python standard library, it's may be not what user wants actually."},
{"body": "You know how django passwords are stored like this: and that is the \"hashtype $salt $hash\".  My question is, how do they get the $hash?  Is it the password and salt combined and then hashed or is something else entirely?As always, use the source:As we can see, the password digests are made by concatenating the salt with the password using the selected hashing algorithm.  then the algorithm name, the original salt, and password hash are concatenated, separated by \"$\"s to form the digest.  To validate passwords django just verifies that the same salt and same password result in the same digest.:According to the code of :As the documentation describes, the hash is the salt, algorithm, and password, hashed.For a long time, until version 1.3, Django indeed followed the irresponsible practice of using a plain single iteration of SHA1, with a salt that was too short, to store password information.  That approach has been .  Any passwords still stored that way are highly vulnerable to brute force attack.  For reasons why, see Since version 1.4 in 2012, Django has a default hashing algorithm based on a good, standard key derivation function, PBKDF2, with a configurable number of iterations, whose default increases with each release (20000 in version 1.7).  It also provides bcrypt support, and is backwards compatible with earlier releases, automatically upgrading password hashes when users log in.  See more at "},
{"body": "If I have a list of dictionaries, say:and I would like to remove the dictionary with  of 2 (or name john), what is the most efficient way to go about this programmatically (that is to say, I don't know the index of the entry in the list so it can't simply be popped).: as some doubts have been expressed in a comment about the performance of this code (some based on misunderstanding Python's performance characteristics, some on assuming beyond the given specs that there is exactly one dict in the list with a value of 2 for key 'id'), I wish to offer reassurance on this point.On an old Linux box, measuring this code:of which about 57 microseconds for the random.shuffle (needed to ensure that the element to remove is not ALWAYS at the same spot;-) and 0.65 microseconds for the initial copy (whoever worries about performance impact of shallow copies of Python lists is most obviously out to lunch;-), needed to avoid altering the original list in the loop (so each leg of the loop does have something to delete;-).When it is known that there is exactly one item to remove, it's possible to locate and remove it even more expeditiously:(use the  builtin rather than the  method if you're on Python 2.6 or better, of course) -- but this code breaks down if the number of dicts that satisfy the removal condition is not exactly one. Generalizing this, we have:where the shuffling can be removed because there are already three equispaced dicts to remove, as we know. And the listcomp, unchanged, fares well:totally neck and neck, with even just 3 elements of 99 to be removed. With longer lists and more repetitions, this holds even more of course:All in all, it's obviously not worth deploying the subtlety of making and reversing the list of indices to remove, vs the perfectly simple and obvious list comprehension, to possibly gain 100 nanoseconds in one small case -- and lose 113 microseconds in a larger one;-). Avoiding or criticizing simple, straightforward, and perfectly performance-adequate solutions (like list comprehensions for this general class of \"remove some items from a list\" problems) is a particularly nasty example of Knuth's and Hoare's well-known thesis that \"premature optimization is the root of all evil in programming\"!-)This is not properly an anwser (as I think you already have some quite good of them), but... have you considered of having a dictionary of  instead of a list of dictionaries?Here's a way to do it with a list comprehension (assuming you name your list 'foo'):Substitute  or whatever as appropriate. also works:And if you want a generator you can use itertools:However, as of Python 3,  will return an iterator anyway, so the list comprehension is really the best choice, as Alex suggested.list.pop() is a good choice:You can develop other ways of tracking done  dictionary you want to eliminate, so long as it resolves to an integer. Here is one of those ways:Another possibility is to use del:Will probably be faster than the list comprehension methods on average because it doesn't traverse the whole list if it finds the item in question early on.You could try something along the following lines:Unless you build something akin to an index over your data, I\ndon't think that you can do better than doing a brute-force \"table\nscan\" over the entire list. If your data is sorted by the key you\nare using, you might be able to employ the  module to\nfind the object you are looking for somewhat faster.You can try the following: If You can't pop from the beginning - pop from the end, it won't ruin the for loop."},
{"body": "Can someone please explain how you can write a url pattern and view that allows optional parameters? I've done this successfully, but I always break the url template tag.Here's what I have currently:If I use the url template tag in this example providing both arguments, it works just fine; however, if I omit the optional argument, I get a reversing error.How can I accomplish this?Thanks,\nPeteI generally make two patterns with a :Django urls are polymorphic:its obious that you have to make your views like this:so you can call it with the same name and it would work work url resolver fine. However be aware that you cant pass None as the required argument and expect that it will get you to the regexp without argument: I dont know whether this is documented anywhere - I have discovered it by accident - I forgot to rewrite the url names and it was working anyway :)Others have demonstrated the way to handle this with two separate named URL patterns. If the repetition of part of the URL pattern bothers you, it's possible to get rid of it by using include():And then add a required_urls.py file with:Normally I wouldn't consider this worth it unless there's a common prefix for quite a number of URLs (certainly more than two).Why not have two patterns:For anyone who is still having this problem.\nI use Django 1.5 (updated: using 1.8) and it's still working fine. I use:Then when I want to have the two urlsandI use:This will create the urlsIf you create a url manually you have to keep the / in mind.I hoop this helps anyone!in views.py you do simple thing.And when you dont get  param in url string it will be None in your code. Simple and elegant :)Depending on your use case, you may simply want to pass a url parameter like so:call this in your view:this will return 'foo'"},
{"body": "I have a file where the first byte contains encoded information.  In Matlab I can read the byte bit by bit with var=fread(file,8, 'ubit1') then retrieve each bit by var(1),var(2), etc.Is there any equivalent bit reader in python?Read the bits from a file, low bits first.The smallest unit you'll be able to work with is a byte.  To work at the bit level you need to use .You won't be able to read each bit one by one - you have to read it byte by byte. You can easily extract the bits out, though:With  it is easy like this:More info here:\nThere are two possible ways to return the i-th bit of a byte.  The \"first bit\" could refer to the high-order bit or it could refer to the lower order bit.Here is a function that takes a string and index as parameters and returns the value of the bit at that location.  As written, it treats the low-order bit as the first bit.  If you want the high order bit first, just uncomment the indicated line.The indexing starts at 0.  If you want the indexing to start at 1, you can adjust index in the function before calling .Example usage:Now, for how it works:A string is composed of 8-bit bytes, so first we use divmod() to break the index into to parts: We use the  function to convert the character at  into an integer type.  Then,  computes the value of the j-th bit by left-shifting 1 by .  Finally, we use bitwise-and to test if that bit is set.  If so return 1, otherwise return 0.To read a byte from a file: . Note: the file is opened in the  mode.To get bits, convert the bytestring into an integer:  (Python 3) or  (Python 2) and extract the desired bit: :Joining some of the previous answers I would use:For each byte read from the file. The results for an 0x88 byte example is:You can assign it to a variable and work as per your initial request.\nThe \"{0.08}\" is to guarantee the full byte lengthThis is pretty fast I would think:Supposing you have a file called bloom_filter.bin which contains an array of bits and you want to read the entire file and use those bits in an array.First create the array where the bits will be stored after reading,Open the file,\nusing open or with, anything is fine...I am sticking with open here,Now load all the bits into the array 'a' at one shot using,'a' is now a bitarray containing all the bits"},
{"body": "I'm writing a doctest for a function that outputs a dictionary. The doctest looks likeWhen I run it, it fails withMy best guess as to the cause of this failure is that doctest isn't checking dictionary equality, but  equality.  indicates that there's some way to trick doctest into checking dictionary equality. How can I do this?Doctest doesn't check  equality, per se, it just checks that the output is exactly the same. You have to ensure that whatever is printed will be the same for the same dictionary. You can do that with this one-liner:Although this variation on your solution might be cleaner:Another good way is to use  (in the standard library).According to its source code, it's sorting dicts for you:I ended up using this. Hacky, but it works.turn it into a list via dict.items() and then sort it ...You can create an instance of  class inside your doctests, and use it to compare dictionaries:Note: this approach is better than simply checking if dictionaries are equal, because it will show diff between the two dictionaries.Most of it has been already said here.. anyway JSYK: there is a dedicated section in doctest documentation:"},
{"body": "I have a very long and complicated json object but I only want to get the items/keys in the first level!Example:I want to get  as result!I found this code:But it prints all keys (also )Just do a simple If you need a sorted list:"},
{"body": "I can create a new conda environment, with program  with this:What if I do not want to install any program? It seems I can not do that:You can give a package name of just \"python\" to get a base, empty install. "},
{"body": "I have the following problem. How do I accomplish this in Python? With a buffer object somehow?Slicing lists does not generate copies of the objects in the list; it just copies the references to them. That is the answer to the question as asked.First, let's test the basic claim. We can show that even in the case of immutable objects like integers, only the reference is copied. Here are three different integer objects, each with the same value:They have the same value, but you can see they are three distinct objects because they have different s:When you slice them, the references remain the same. No new objects have been created:Using different objects with the same value shows that the copy process doesn't bother with  -- it just directly copies the references.Testing with mutable values gives the same result:Of course the references  are copied. Each one costs 8 bytes on a 64-bit machine. And each list has its own memory overhead of 72 bytes:As Joe Pinsonault , that overhead adds up. And integer objects themselves are not very large -- they are three times larger than references. So this saves you some memory in an absolute sense, but asymptotically, it might be nice to be able to have multiple lists that are \"views\" into the same memory.Unfortunately, Python provides no easy way to produce objects that are \"views\" into lists. Or perhaps I should say \"fortunately\"! It means you don't have to worry about where a slice comes from; changes to the original won't affect the slice. Overall, that makes reasoning about a program's behavior much easier. If you really want to save memory by working with views, consider using  arrays. When you slice a  array, the memory is shared between the slice and the original: What happens when we modify  and look again at ?But this means you have to be sure that when you modify one object, you aren't inadvertently modifying another. That's the trade-off when you use : less work for the computer, and more work for the programmer!Depending on what you're doing, you might be able to use .Since it operates via iteration, it won't make new lists, but instead will simply create iterators that  elements from the original list as requested for their ranges."},
{"body": "I am trying to parse a JSON object into a Python . I've never done this before. When I googled this particular error, (), other posts have said that the string being loaded is not actually a JSON string.  I'm pretty sure this is, though.In this case,  works fine, but I'm wondering if there is a more appropriate way? This string comes directly from Twitter, via ptt tools.That's definitely not JSON - not as printed above anyhow.  It's already been parsed into a Python object - JSON would have , not , and wouldn't show strings as  for unicode (all JSON strings are unicode).  Are you sure you're not getting your json string turned into a Python object for free somewhere in the chain already, and thus loading it into json.loads() is obviously wrong because in fact it's not a string?Sometimes you can have this error because your string values are not well recognized by python. As an example: I've spent quite a lot of time searching for the origin of this kind of error. Here is what I found. Sometimes a language recognizes a kind of quotes and not another one:\nbtw, to parse a string in to json in JavaScript all quotes have to be in the ' formatto parse a string into json in JavaScript all quotes have to be in the \" format which is not really logic.Hopefully you can use the replace function. For Python:Hope it will save you the time I've spent hunting this bug! I got this error when I had a hanging comma at the end of a list of properties. Because of the comma it was expecting another property name but there was none. "},
{"body": "how can I change the values of the diagonal of a matrix in numpy?I checked , but the function there is not implemented in numpy v 1.3.0.lets say we have a np.array X and I want to set all values of the diagonal to 0.Did you try ? See the following  and this . Or the following from the documentation (although currently broken):If you're using a version of numpy that doesn't have  (the  to set the diagonal to a constant) or , you can do this pretty easily with array slicing:This is much faster than an explicit loop in Python, because the looping happens in C and is potentially vectorized.One nice thing about this is that you can also fill a diagonal with a list of elements, rather than a constant value (like , but for modifying an existing matrix rather than making a new one). For example, this will set the diagonal of your matrix to 0, 1, 2, ...:If you need to support more array shapes, this is more complicated (which is why fill_diagonal is nice...):(The  call is only necessary in Python 3, where  returns an iterator.)Here's another good way to do this. If you want a one-dimensional view of the array's main diagonal use:For the i'th superdiagonal use:For the i'th subdiagonal use:Or in general, for the i'th diagonal where the main diagonal is 0, the subdiagonals are negative and the superdiagonals are positive, use:These are  and not copies, so they will run faster for extracting a diagonal, but any changes made to the new array object will apply to the original array.\nOn my machine these run faster than the fill_diagonal function when setting the main diagonal to a constant, but that may not always be the case. They can also be used to assign an array of values to a diagonal instead of just a constant.Notes: for small arrays it may be faster to use the  attribute of the NumPy array.\nIf speed is a major issue it could be worth it to make  a local variable.\nAlso, if the array is not contiguous,  will return a copy, so, in order to assign values to a strided slice, it will be necessary to creatively slice the original array used to generate the strided slice (if it is contiguous) or to use the  attribute.Also, it was originally planned that in NumPy 1.10 and later the 'diagonal' method of arrays will return a view instead of a copy.\nThat change hasn't yet been made though, but hopefully at some point this trick to get a view will no longer be necessary.\nSee Where size is n in an n x n matrix.Minimal. Code.You can do the following.Assuming your matrix is 4 * 4 matrix."},
{"body": "How can I declare a bit array of a very large size, say 6 million bits?You can check out more info about this module at The  module may help:This will take less than a megabyte of memory, and it's easy to set, read, slice and interpret bits. Unlike the bitarray module it's pure Python, plus it works for Python 3.See  for more details.Get the bitarray module using Then, this code will create a bit array of size 6 million,You can initialize all the bits to zero usingTo set a particular bit, say bit number 25, to 1, do this:This is one-liner converts bytes to a list of True/False bit values. Might be not performant for 6M but for small flags it should be fine and does not need additional dependencies."},
{"body": "I have a directory with a bunch of files inside: ,  ... and .I want to exclude all files that start with  with the  function.How can I do it?You can't exclude patterns with the  function, globs only allow for  patterns.  is very limited (even a  character class  match a character).You'll have to do your own filtering; a list comprehension usually works nicely here:The pattern rules for glob are not regular expressions. Instead, they follow standard Unix path expansion rules. There are only a few special characters: two different wild-cards, and character ranges are supported [from ].  So you can exclude some files with patterns.\nFor example to exclude manifests files (files starting with ) with glob, you can use:  You can deduct sets:More generally, to exclude files that don't comply with some shell regexp, you could use module :    The above will first generate a list from a given path and next pop out the files that won't satisfy the regular expression with the desired constraint.Late to the game but you could alternatively just apply a python  to the result of a :or replacing the lambda with an appropriate regex search, etc...EDIT: I just realized that if you're using full paths the  won't work, so you'd need a regex As mentioned by the accepted answer, you can't exclude patterns with glob, so the following is a method to filter your glob result.The accepted answer is probably the best pythonic way to do things but if you think list comprehensions look a bit ugly and want to make your code maximally numpythonic anyway (like I did) then you can do this (but note that this is probably less efficient than the list comprehension method): (In my case, I had some image frames, bias frames, and flat frames all in one directory and I just wanted the image frames)"},
{"body": "Environment:django debug toolbar breaking while using to get sql stats else it's working fine on the other pages, breaking only on the pages which have sql queries.sqlparse latest version was released today and it's not compatible with django-debug-toolbar version 1.4, Django version 1.9workaround is force pip to install the latest version of  is not compatible with .  Your choices are:@Rex Salisbury\nThat's not correct.You have to installorTested on Cloud9, with django 1.9.2Sorry,but for me, with Django 1.8.11, it only worked with this:"},
{"body": "How can I turn the minor ticks only on y axis on a linear vs linear plot?When I use the function to turn minor ticks on, they appear on both x and y axis.nevermind, I figured it out.Here's another way I found in the :This will place minor ticks on  the y-axis, since minor ticks are off by default.Also, if you only want minor ticks on the actual y-axis, rather than on both the left and right-hand sides of the graph, you can follow the  with , like so:"},
{"body": "I am working on python and selenium. I want to download file from clicking event using selenium. I wrote following code.  I want to download both files from links with name \"Export Data\" from given url. How can I achieve it as it works with click event only.ThanksFind the link using , then call  method.Added profile manipulation code to prevent download dialog.I'll admit this solution is a little more \"hacky\" than the Firefox Profile saveToDisk alternative, but it works across both Chrome and Firefox, and doesn't rely on a browser-specific feature which could change at any time. And if nothing else, maybe this will give someone a little different perspective on how to solve future challenges.: Ensure you have selenium and pyvirtualdisplay installed...We first load a URL on the domain we're targeting a file download from. This allows us to perform an AJAX request on that domain, without running into  issues.Next, we're injecting some javascript into the DOM which fires off an AJAX request. Once the AJAX request returns a response, we take the response and load it into a FileReader object. From there we can extract the base64 encoded content of the file by calling readAsDataUrl(). We're then taking the base64 encoded content and appending it to , a gobally accessible variable.Finally, because the AJAX request is asynchronous, we enter  a Python while loop waiting for the content to be appended to the window. Once it's appended, we decode the base64 content retrieved from the window and save it to a file.This solution should work across all modern browsers supported by Selenium, and works whether text or binary, and across all mime types.While I haven't tested this, Selenium does afford you the ability to wait until an element is present in the DOM. Rather than looping until a globally accessible variable is populated, you could create an element with a particular ID in the DOM and use the binding of that element as the trigger to retrieve the downloaded file.In chrome what I do is downloading the files by clicking on the links, then I open  page and then retrieve the downloaded files list from shadow DOM like this:This solution is restrained to chrome, the data also contains information like file path and download date. (note this code is from JS, may not be the correct python syntax)"},
{"body": "I'm making a simple login app in django 1.6 (and python 2.7) and I get an error at the beggining that is not letting me continue.This is the site's url.pyAnd this is login/urls.py:This is login/views,pyI have a form that has this as action:And that's where the problem is, when I try to load the page, I get:But if I set the url pattern toit works fine, only it sets the action as '/'.I've been looking all around for an answer and I don't understand why it doesn't work.I tried changing the login url pattern to url(r'^login/$', include('login.urls', namespace='login')), and it didn't change anything.The problem is in the way you include the auth URLs in the main one.\n Because you use both ^ and $, only the empty string matches. Drop the $."},
{"body": "As I know, %debug magic can do debug within one cell.However, I have function calls across multiple cells.For example,What I tried:     What is the right way to set a break point within ipython notebook?Use  Install it via Usage: For executing line by line use  and for step into a function use  and to exit from debugging prompt use .For complete list of available commands: You can use  inside jupyter with:: the functions above are deprecated since IPython 5.1. This is the new approach:Add  where you need a breakpoint. Type  for  commands when the input field appears.You can always add this in any cell:and the debugger will stop on that line.  For example:Your return function is in line of def function(main function), you must give one tab to it.\nAnd Use instead ofto debug the whole cell not only line. Hope, maybe this will help you."},
{"body": "I am trying to implement , which requires to 'freeze' one or the other part of the graph during alternating training minibatches. I.e. there two sub-networks: G and D.where loss function of  depends on .First I need to train parameters in D with all G parameters fixed, and then parameters in G with parameters in D fixed. Loss function in first case will be negative loss function in the second case and the update will have to apply to the parameters of whether first or second subnetwork.I saw that tensorflow has  function. For purpose of training the D (downstream) subnetwork I can use this function to block the gradient flow to The  is very succinctly annotated with no in-line example (and example  is too long and not that easy to read), but looks like it must be called during the graph creation.  Also it seems that As an alternative I saw that one can pass the list of variables to the optimizer call as , which would be an easy solution if one could get all variables in the scopes of each subnetwork. The easiest way to achieve this, as you mention in your question, is to create two optimizer operations using separate calls to . By default, the optimizer will use all of the variables in . If you want to filter the variables to a particular scope, you can use the optional  argument to  as follows:Another option you might want to consider is you can set trainable=False on a variable. Which means it will not be modified by training.@mrry's answer is completely right and perhaps more general than what I'm about to suggest. But I think a simpler way to accomplish it is to just pass the python reference directly to : I have a self-contained example here:  I don't know if my approach has down sides, but I solved this issue for myself with this construct:So if , the values and gradients will flow through just fine, but if , then the values will only flow through the stop_gradient op, which will stop the gradients flowing back.For my scenario, hooking do_gradient up to an index of a random_shuffle tensor let me randomly train different pieces of my network."},
{"body": "I've read some about .egg files and I've noticed them in my lib directory but what are the advantages/disadvantages of using then as a developer?From the :-AdamOne egg by itself is not better than a proper source release. The good part is the dependency handling. Like debian or rpm packages, you can say you depend on other eggs and they'll be installed automatically (through ).A second comment: the egg format itself is a binary packaged format. Normal python packages that consist of just python code are best distributed as \"source releases\", so \"python setup.py sdist\" which result in a .tar.gz. These are also commonly called \"eggs\" when uploaded to pypi.Where you need binary eggs: when you're bundling some C code extension. You'll need several binary eggs (a 32bit unix one, a windows one, etc.) then.Eggs are a pretty good way to distribute python apps.  Think of it as a platform independent .deb file that will install all dependencies and whatnot.  The advantage is that it's easy to use for the end user.  The disadvantage are that it can be cumbersome to package your app up as a .egg file.You should also offer an alternative means of installation in addition to .eggs.  There are some people who don't like using eggs because they don't like the idea of a software program installing whatever software it wants.  These usually tend to be sysadmin types..egg files are basically a nice way to deploy your python application. You can think of it as something like .jar files for Java. More info .Whatever you do, do not stop distributing your application, also, as a tarball, as that is the easiest packagable format for operating systems with a package sysetem.For simple Python programs, you probably don't need to use eggs. Distributing the raw .py files should suffice; it's like distributing source files for GNU/Linux. You can also use the various OS \"packagers\" (like py2exe or py2app) to create .exe, .dmg, or other files for different operating systems.More complex programs, e.g. Django, pretty much require eggs due to the various modules and dependencies required."},
{"body": "I've been using virtualenv lately while developing in python. I like the idea of a segregated development environment using the --no-site-packages option, but doing this while developing a PyGTK app can be a bit tricky. The PyGTK modules are installed on Ubuntu by default, and I would like to make a virtualenv (with --no-site-packages) aware of specific modules that are located elsewhere on the system.What's the best way to do this? Or should I just suck it up and drop the --no-site-packages option?One way is to add the paths to your code using sys.path.Another way is to use site, which processes .pth files in addition to adding to sys.path.But you probably don't want to add this to all your related code.  I've seen mention of sitecustomize.py being used to perform something like this, but after some testing I couldn't get it to work as might be expected. Here it mentions that auto-import of sitecustomize.py ended in 2.5, if your not on 2.5 try it out.  (just add one of the path add methods above to the file and drop it in the directory your program is run)\nA work around method is mentioned in the post for users of 2.5 and up. I find in this situation, symlinks, or even copying specific files (packages, modules, extensions) works really well.It allows the program to emulate being run in the target environment, rather than changing the application to suit the development environment.Same deal for something like AppEngine.Check out the postmkvirtualenv hook script here: In that case, he's using it to import PyQt and SIP after a new Virtualenv is created, but you can add the packages that you need to LIBS. And vote that script up because it's fantastic :)If you want to include the links to the relevant system's python gtk-2.0 in the virtualenv, you can just use pip to install :pip install ruamel.venvgtk\nYou don't have import anything, the links are setup during installation.This is especially handy if you are using , in that case you only need to include the dependency (for tox):and a newly setup python2.7 environment will have the relevant links included before the tests are run.More detailed information on how the links are setup can be found in "},
{"body": "How would I go about using Python to read the frequency peaks from a WAV PCM file and then be able to generate an image of it, for spectogram analysis?I'm trying to make a program that allows you to read any audio file, converting it to WAV PCM, and then finding the peaks and frequency cutoffs. will let you import the audio. After that, you can  of the audio.Then,  makes very nice charts and graphs - absolutely comparable to MATLAB.It's old as dirt, but  would probably get you started on almost exactly the problem you're describing (article in Python of course).Loading WAV files is easy using :or for reading any general audio format and converting to WAV:The spectrogram is built into PyLab:Specifically, it's part of .  is the easiest. Also quite handy in this context:But be warned: Matplotlib is very slow but it creates beautiful images. You should not use it for demanding animation, even less when you are dealing with 3DIf you need to convert from PCM format to integers, you'll want to use struct.unpack."},
{"body": " -Quote suggests it is incorrect to call  as a  because the object is already constructed by the time  is called. But! I have always been under the impression that the  is called only after the object is constructed because it is essentially used  to initialized the data members of the instance which wouldn't make sense if the object didn't exist by the time  was called? (coming from C++/Java background)If you have a class  then:Construction of a Python object is simply allocation of a new instance followed by initialization of said instance.Personally, I find \" is not a constructor\" to be pretty fine hair-splitting. is called when a new object is requested. It is supposed to use its arguments to assign attributes on the new object, such that the required invariants for normal operation of the object are set up. The object is already a valid pre-existing place to store attributes by the time the code in  begins running.  (other than the ones that all objects possess).A C++ constructor is called when a new object is requested. It is supposed to use its arguments to assign to fields on the new object, such that the required invariants for normal operation of the object are set up. The object is already a valid pre-existing place to store fields by the time the code in the constructor begins running. A Java constructor is called when a new object is requested. It is supposed to use its arguments to assign to fields on the new object, such that the required invariants for normal operation of the object are set up. The object is already a valid pre-existing place to store fields by the time the code in the constructor begins running. The major difference between an  method and a C++/Java constructor is in that last sentence I've highlighted, and that's just the difference between the static nature of Java/C++ and the dynamic nature of Python. I don't think this warrants calling them fundamentally different concepts that must not be referred to by the same word.I think the main reason Pythonistas don't like to refer to  as a constructor is that people  of C++/Java constructors as \"making a new object\", because that's what they seem to do when you call them. But there's really two things going on when you call a constructor; a new object is created and then the constructor is called to initialise it. In C++/Java the \"create a new object\" part of that is invisible, whereas that can be exposed/customised in Python (via the  method).So while the role of the  method is extremely similar to the role of a C++/Java constructor, some people prefer to emphasise the fact that this isn't the whole process by saying that \" is not a constructor\".Constructor  an instance and can fail. But  does not return an instance. Even when  raises and exception,  is called to delete the instance.This can be seen here: on the other hand, returns an instance. In \"Programming Python : Introduction to Computer Science\" from John Zelle says,\n\". Python calls this method to initialize a new object. The role of  is to provide initial values for the instance variables of an object.\"From  Regarding the answert given by Ben, I would argue here, that most languages don't follow that defintion (completely).Furthermore:Bottomline (for me):  appears to be the constructor, not  although -by all practical means  does part of what most people think a constructor will do.  is not a constructor, but for reasons that won't matter to you as you're learning Python. It behaves the way you're used to constructors behaving in C++ and Java."},
{"body": "I'm writing some test cases for my application using Python's . Now I need to compare a list of objects with a list of another objects to check if the objects from the first list are what I'm expecting.How can I write a custom  method? What should it do? Should it raise an exception on failure? If yes, which exception? And how to pass the error message? Should the error message be a unicode string or a bytestring?Unfortunately, the  doesn't explain how to write custom assertion methods.If you need a real-world example for this, continue reading.The code I'm writing is somewhat like this:This approach makes it extremely easy to describe the expected values of each object in a very compact way, and without needing to actually create full objects.However... When one object fails the assertion, no further objects are compared, and this makes debugging a bit more difficult. I would like to write a custom method that would unconditionally compare all objects, and then would display all objects that failed, instead of just the first one.I use the multiple inheritance in these cases. For example:First. I define a class with methods that will incorporate.Now I define a class that inherits from unittest.TestCase and CustomAssertionYou should create your own TestCase class, derived from unittest.TestCase.  Then put your custom assert method into that test case class.  If your test fails, raise an AssertionError.  Your message should be a string.  If you want to test all objects in the list rather than stop on a failure, then collect a list of failing indexes, and after the loop over all the objects, build an assert message that summarizes your findings."},
{"body": "I have an array of distances  and I need the indices of the sorted array (for example , for ). Is there a function in Numpy to do that?Yes, there's the  function or  method. It does exactly what you're asking for. You can also call  as a method on an  object like so: .Here's a link to the documentation: "},
{"body": "I have written many scrapers but I am not really sure how to handle infinite scrollers. These days most website etc, Facebook, Pinterest has infinite scrollers.You can use selenium to scrap the infinite scrolling website like twitter or facebook. Step 1 : Install Selenium using pip Step 2 : use the code below to automate infinite scroll and extract the source codeStep 3 : Print the data if required.Most sites that have infinite scrolling do (as Lattyware notes) have a proper API as well, and you will likely be better served by using this rather than scraping.But if you must scrape...Such sites are using JavaScript to request additional content from the site when you reach the bottom of the page. All you need to do is figure out the URL of that additional content and you can retrieve it. Figuring out the required URL can be done by inspecting the script, by using the Firefox Web console, or by using a .For example, open the Firefox Web Console, turn off all the filter buttons except Net, and load the site you wish to scrape. You'll see all the files as they are loaded. Scroll the page while watching the Web Console and you'll see the URLs being used for the additional requests. Then you can request that URL yourself and see what format the data is in (probably JSON) and get it into your Python script.Finding the url of the ajax source will be the best option but it can be cumbersome for certain sites. Alternatively you could use a headless browser like  from  and send keyboard events while reading the data from the DOM tree.  has a nice and simple api."},
{"body": "I'd like to have a child class modify a class variable that it inherits from its parent.I would like to do something along the lines of:and ideally have:I could do:but then every time I instantiate an instance of Child, \"world\" gets appended to the list, which is undesired.  I could modify it further to:but this is still a hack because I must instantiate an instance of Child before it works.Is there a better way?Assuming you want to have a separate list in the subclass, not modify the parent class's list (which seems pointless since you could just modify it in place, or put the expected values there to begin with):Note that this works independently of inheritance, which is probably a good thing.You should not use mutable values in your class variables. Set such values on the  instead, using the  instance initializer:Otherwise what happens in that the  list is shared among not only the instances, but with the subclasses as well.In any case, you'll have to avoid modifying mutables of parent classes even if you do desire to share state among instances through a mutable class variable; only  to a name would create a new variable:where a   variable is created for the  class. By using assignment, you've created a new list instance and the  mutable is unaffected.Do take care with nested mutables in such cases; use the  module to create deep copies if necessary."},
{"body": "How can you  the parent loop of say two nested loops in Python?I know you can avoid this in the majority of cases but can it be done in Python?I would go with 5 every time.Here's a bunch of hacky ways to do it:You use  to break out of the inner loop and continue with the parent"},
{"body": "Are there functions for conversion between different coordinate systems?For example, Matlab has  for conversion from cartesian to polar coordinates. Seems like it should be in numpy or scipy.Using numpy, you can define the following:The existing answers can be simplified:Or even:Note these also work on arrays!If your coordinates are stored as complex numbers you can use If you can't find it in numpy or scipy, here are a couple of quick functions and a point class:There is a better way to write polar(), here it is: Thinking about it in general, I would strongly consider hiding coordinate system behind well-designed abstraction. Quoting Uncle Bob and his book:With interface like that any user of Point class may choose convenient representation, no explicit conversions will be performed. All this ugly sines, cosines etc. will be hidden in one place. Point class. Only place where you should care which representation is used in computer memory."},
{"body": "I have been trying to understand python metaclasses, and so have been going through some sample code. As far as I understand it, a Python metaclass can be any callable. So, I can have my metaclass likeHowever, I have seen a lot of people write their metaclasses in the following way:As far as I can see, these would both do the same thing. Is there any reason to use the base class instead? Is it customary?There are subtle differences, mostly relating to inheritance.  When using a\nfunction as a metaclass, the resulting class is really an instance of ,\nand can be inherited from without restriction; however, the metaclass function\nwill never be called for such subclasses.  When using a subclass of  as a\nmetaclass, the resulting class will be an instance of that metaclass, as will\nany of its subclasses; however, multiple inheritance will be restricted.Illustrating the differences:Note that when defining sub1 and sub2, no metaclass functions were called.\nThey will be created exactly as if c1 and c2 had no metaclasses, but instead\nhad been manipulated after creation.Note the differences already: M1 was called when creating Sub1, and both\nclasses are instances of M1.  I'm using  for the actual creation here,\nfor reasons which will become clear later.This is the major restriction on multiple inheritance with metaclasses.\nPython doesn't know whether M1 and M2 are compatible metaclasses,\nso it forces you to create a new one to guarantee that it does what you need.This is why I used  in the metaclass  functions: so each one\ncan call the next one in the MRO.Certain use cases might need your classes to be of type , or might want\nto avoid the inheritance issues, in which case a metaclass function is probably\nthe way to go.  In other cases, the type of the class might be truly important,\nor you might want to operate on all subclasses, in which case subclassing\n would be a better idea.  Feel free to use the style that fits best in\nany given situation."},
{"body": "In Python, and in general - does a  operation on a file object imply a  operation?Yes. It uses the underlying  function which does that for you ().NB:  and  won't ensure that the data is actually secure on the disk. It just ensures that the OS has the data == that it isn't buffered inside the process.You can try sync or fsync to get the data written to the disk.filehandle.close does not necessarily flush. Surprisingly, filehandle.flush doesn't help either---it still can get stuck in the OS buffers when Python is running. Observe this session where I wrote to a file, closed it and Ctrl-Z to the shell command prompt and examined the file:Subsequently I can reopen the file, and that necessarily syncs the file (because, in this case, I open it in the append mode). As the others have said, the sync syscall (available from the os package) should flush all buffers to disk but it has possible system-wide performance implications (it syncs all files on the system)."},
{"body": "I have a raw image where each pixel corresponds to a 16 bits unsigned integer. I am trying to read using the PIL Image.fromstring() function as in the following code:The PIL documentation informs that the first argument of the fromstring() function is 'mode'. However, looking at the documentation and googling I wasn't able to find details about what that argument really means (I believe that it is related to the color space or something like that). Does anyone knows where I can find a more detailed reference about the fromstring() function and what the mode argument means?The specific documentation is at :I'm not sure what \"L\" stands for, but \"RGBA\" stands for Red-Green-Blue-Alpha, so I presume RGBX is equivalent to RGB (edit: upon testing this isn't the case)? CMYK is Cyan-Magenta-Yellow-Kelvin, which is another type of colorspace. Of course I assume that if you know about PIL you also know about colorspaces. If not,  has a great article.As for what it really means (if that's not enough): pixel values will be encoded differently for each colorspace. In regular RGB you have 3 bytes per pixel - 0-254, 0-254, 0-254. For Alpha you add another byte to each pixel. If you decode an RGB image as RGBA, you'll end out reading the R pixel to the right of the first pixel as your alpha, which means you'll get the G pixel as your R value. This will be magnified depending on how large your image, but it will really make your colors go wonky. Similarly, trying to read a CMYK encoded image as RGB  (or RGBA) will make your image look very much not like it's supposed to. For instance, try this with an image:And you'll see what the different modes do - try it with a variety of input images: png with alpha, png without alpha, bmp, gif, and jpeg. It's kinda a fun experiment, actually.If all else fails, you can always read the source code. For PIL, the downloads are .You never said exactly what format the pixel data in the 16 bits unsigned integers was in, but I'd guess it's something like RRRRRGGGGGGBBBBBB, (5-bits Red, 6-bits Green, 5-bits Blue), or RRRRRGGGGGBBBBBA (5-bits Red, 5-bits Green, 5-bits Blue, 1-bit Alpha or Transparency). I didn't see support for those formats after a very quick peek at the some of the sources myself, but can't say one way or the other for sure. On the same web page where the PIL downloads are, they mention that one can send questions to the Python Image SIG mailing list and provide a link for it. That might be a better source than asking here.Hope this helps.This is an old question, but this might help someone in the future. One of the problems with the original code snippet is that in , the  part works for  mode.This works for me:"},
{"body": "Is there anything similar to Python  for Java or JVM Languages?From what I understand, virtualenv enables you to have separate library installation paths, effectively separate \"virtual\" Python installations.Java doesn't have the concept of a \"system-wide installed\" library: It always searches the classpath for the libraries to be loaded. Since the classpath can be (and needs to be!) defined for each application, each application can pick-and-choose which libraries and which versions it wants to load.If you go down one level deeper and have a single application that somehow needs two different versions of the same library at the same time, then you can do even that with some classpath trickery. It can get complicated, but it's definitely possible (OSGi is one example where this is supported, even Tomcat with two separate webapplications does this).I've seens some references to security in the virtualenv description: Java has a pretty thorough security system built in. In server applications it's often turned off because it's just easier to configure this way, but you can easily configure what exactly a Java application is allowed to do.Build tools like Ant, Maven, and gradle are the the closest thing to  or .The concept of virtualenv is done by the classpath. So there is no real need of virtualenv for JavaI know this may be a little late , but Groovy/Java has gvm  which is the Groovy version of Ruby's renv.I would respectfully agree with Gautam K, luthur. Dependency and package version management for projects is not the  same as an isolated self-contained virtual environment to maintain different project.My 2 cents\n-WI have also been looking for a similar solution to simplify switching context between projects that use different Maven versions/settings and/or Java SDKs without having to modify  and  settings every time.To this end, I developed a solution that helps execute  commands with the appropriate configuration based on  settings (stored in a  folder). See: Be aware that this only helps if you're using Maven to build and/or run your project.Maven, you can explicitly specify which packages you would use in a java projectI'm confused by the assertion that \"Java doesn't have the concept of a 'system-wide installed' library\". What would you call the jar files in $JAVA_HOME/jre/lib and $JAVA_HOME/jre/lib/ext?Regardless of whether or not Java \"needs\" a tool like virtualenv, it seems that something that allowed you to quickly switch between different Java environments (e.g. Java 6 with such-and-such security extensions, Java 7, etc.) would be handy - even if all it was actually doing under the covers was manipulating the PATH, JAVA_HOME, and CLASSPATH env variables.Java as a language does not need the sandboxing features of virtualenv but a JVM Language like Jython can have VirtualEnv to use different environments without any conflict.It is outlined in  Quote:So when using Jython different frameworks and packages can be used without any conflict with global packages."},
{"body": "So I have a dict passed from a web page.  I want to build the query dynamically based on the dict.  I know I can do:However, that only works when the values are an exact match.  I need to do 'like' filtering.  My best attempt using the  attribute:Not sure how to build the query from there.  Any help would be awesome.You're on the right track!First thing you want to do different is access attributes using , not ;  will always do the right thing, even when (as may be the case for more convoluted models) a mapped attribute isn't a column property.The other missing piece is that you can specify  more than once, and just replace the old query object with the result of that method call.  So basically:"},
{"body": "I have a matrix  and I want 2 matrices  and  such that  contains the upper triangular elements of A (all elements above and not including diagonal) and similarly for (all elements below and not including diagonal). Is there a  method to do this?e.g Try  (triangle-upper) and  (triangle-lower).Use the  of  and  to return a copy of a matrix with the elements above or below the k-th diagonal zeroed.In case that you want to extract the upper triangle values to a flat vector,\nyou can do something like:"},
{"body": "Why does first line prints True, but second prints False? And neither enters operator ?I am using Python 2.6You need to define  too. For exampleWill work as expected.As a general rule, any time you implement  you should implement a  such that for all  and  such that , .Set __contains__ makes checks in the following order:The relevant C source code is in Objects/setobject.c::set_lookkey() and in Objects/object.c::PyObject_RichCompareBool().Sets and dictionaries gain their speed by using  as a fast approximation of full equality checking. If you want to redefine equality, you usually need to redefine the hash algorithm so that it is consistent.The default hash function uses the identity of the object, which is pretty useless as a fast approximation of full equality, but at least allows you to use an arbitrary class instance as a dictionary key and retrieve the value stored with it if you pass exactly the same object as a key. But it means if you redefine equality and  redefine the hash function, your objects will go into a dictionary/set without complaining about not being hashable, but still won't actually work the way you expect them to.See  for more details.A tangential answer, but your question and my testing made me curious. If you ignore the set operator which is the source of your  problem, it turns out your question is still interesting.Thanks to the help I got on , I was able to chase the in operator through the source code to it's root. Near the bottom I found the PyObject_RichCompareBool function which indeed tests for identity (see the comment about \"Quick result\") before testing for equality.So unless I misunderstand the way things work, the technical answer to your question is first identity and then equality, through the equality test itself. Just to reiterate, that is not the source of the behavior you were seeing but just the technical answer to your question.If I misunderstood the source, somebody please set me straight.Sets seem to use hash codes, then identity, before comparing for equality. The following code:outputs:What happens seems to be:"},
{"body": "I have a rather big program, where I use functions from the  module in different files. I would like to be able to set the random seed once, at one place, to make the program always return the same results. Can that even be achieved in ?The main python module that is run should  and call  - this is shared between all other imports of  as long as somewhere else doesn't reset the seed.In the beginning of your application call  making sure x is always the same. This will ensure the sequence of pseudo random numbers will be the same during each run of the application.Jon Clements pretty much answers my question. However it wasn't the real problem:\nIt turns out, that the reason for my code's randomness was the numpy.linalg SVD because it does not always produce the same results for badly conditioned matrices !!So be sure to check for that in your code, if you have the same problems!You can guarantee this pretty easily by using your own random number generator.Just pick three largish primes (assuming this isn't a cryptography application), and plug them into a, b and c:\na = ((a * b) % c)\nThis gives a feedback system that produces pretty random data.  Note that not all primes work equally well, but if you're just doing a simulation, it shouldn't matter - all you really need for most simulations is a jumble of numbers with a pattern (pseudo-random, remember) complex enough that it doesn't match up in some way with your application.Knuth talks about this."},
{"body": "Alright, I know how to print variables and strings.  But how can I print something like \"My string\" card.price (it is my variable). I mean, here is my code:\n.By printing multiple values separated by a comma:The  will output each expression separated by spaces, followed by a newline.If you need more complex formatting, use the :or by using the older and semi-deprecated .Assuming you use Python 2.7 (not 3): (as mentioned above). (using ) (by joining lists)There are a lot of ways to do the same, actually. I would prefer the second one.Something that (surprisingly) hasn't been mentioned here is simple concatenation.Example:Result:Additionally, as of Python 3, the  method is deprecated. Don't use that."},
{"body": "I have some escaped strings that need to be unescaped. I'd like to do this in Python.For example, in python2.7 I can do this:How do I do it in Python3?  This doesn't work:My goal is to be abel to take a string like this:And turn it into:After I do the conversion, I'll probe to see if the string I have is encoded in UTF-8 or UTF-16.You'll have to use  instead:If you  with a  object instead (equivalent to the python 2.7 unicode) you'll need to encode to bytes first, then decode with .If you need bytes as end result, you'll have to encode again to a suitable encoding ( for example, if you need to preserve literal byte values; the first 255 unicode code points map 1-on-1).Your example is actually UTF-16 data with escapes. Decode from , back to  to preserve the bytes, then from  (UTF 16 little endian without BOM):The old \"string-escape\" codec maps bytestrings to bytestrings, and there's been a lot of debate about what to do with such codecs, so it isn't currently available through the standard encode/decode interfaces.BUT, the code is still there in the C-API (as ), and this is still exposed to Python via the undocumented  and .These functions return the transformed  object, plus a number indicating how many bytes were processed... you can just ignore the latter.You can't use  on byte strings (or rather, you can, but it doesn't always return the same thing as  does on Python 2) \u2013 beware!This function implements  using a regular expression and custom replacement logic."},
{"body": "I'd like to make a migration for a Flask app. I am using Alembic.However, I receive the following error.Online, I read that it has something to do with this. \nUnfortunately, I don't quite understand how to get the database up to date and where/how I should write the code given in the link. If you have experience with migrations, can you please explain this for meThanksAfter creating a migration, either manually or as , you must apply it with .  If you used  from a shell, you can use  to indicate that the current state of the database represents the application of all migrations.I had to delete some of my migration files for some reason. Not sure why. But that fixed the problem, kind of.One issue is that the database ends up getting updated properly, with all the new tables, etc, but the migration files themselves don't show any changes when I use automigrate.If someone has a better solution, please let me know, as right now my solution is kind of hacky."},
{"body": "I want to print the whole dataframe, but I don't want to print the indexBesides, one column is datetime type, I just want to print time, not date.The dataframe looks like:I want it print asOr possibly:"},
{"body": "Are there any Mixed Integer Linear Programming(MILP) solver for Python?Can GLPK python solve MILP problem? I read that it can solve Mixed integer problem.\nI am very new to linear programming problem. So i am rather confused and cant really differentiate if Mixed Integer Programming is different from Mixed Integer Linear programming(MILP).  is a python modeling interface that hooks up to solvers like the open source , , ,  and (open source).You can also use  to model the optimization problem and then call an external solver, namely CPLEX, Gurobim GLPK and the AMPL solver library.You can also call GLPK from ,  or .Yet another modelling language is , which has a python interface for MIP solvers (for linear programs only).All the above solvers solve , while some of them (CPLEX, GUROBI and XRESS-MP for sure) can solve  and  (and also conic programs but this probably goes beyond the scope of this question). MIP refers to Mixed integer programs, but it is commonly used to refer to linear programs only. To make the terminology more precise, one should always refer to MILP or MINLP (Mixed integer non-linear programming).Note that CPLEX and GUROBI have their own python APIs as well, but they (and also) XPRESS-MP are commercial products, but free for academic research.  is similar to Pulp above but interfaces with the COIN-OR solvers CBC and CGL and CLP.Note that : the latter are falling behind the former by a large margin.  is . Its python interface, PySCIPOpt, is .Also, have a look at .Finally, if you are interested at a simple constraint solver (not optimization) then have a look at .I hope this helps! Two more solvers and python interfaces that fell into my radar: , which appears to be one of the fastest non-commercial MIP solvers, has a  that has quite . Note, however, that the Python API does not include the advanced functionality that comes together with the native . I particularly like the , which demonstrates an array of models used in Operations Management, on top of some small-scale implementations. It is a very interesting introductory manual in its own right, regardless of which solver/API one may want to make use of., which include a multitude of functionalities, such as It has extensive documentation of several traditional OR problems and simple implementations. I could not find a complete Python API documentation, although there exist some examples . It is somewhat unclear to me how other solvers hook up on the interface and whether methods of these solvers are available."},
{"body": "I'm trying to plot a ROC curve using seaborn (python).\nWith matplotlib I simply use the function :where  and  are two lists of paired values.Is there a simple counterparts of the plot function in seaborn? I had a look at the gallery but I didn't find any straightforward method.Since seaborn also uses matplotlib to do its plotting you can easily combine the two. If you only want to adopt the styling of seaborn the  function should get you started:Result:"},
{"body": "This is sort of a follow-up to  thread.Sample input strings:Desired outputs:The number of -like items is .Currently, I'm doing it through :Would it be possible to combine these two  calls into a single one?In other words, I want to replace something at the beginning of the string and then multiple similar things after, all of that in one go.I've looked into  - it's ability to  looks very promising, tried using  but failed to make it work.You can indeed use the regex module and repeated captures. The main interest is that you can check the structure of the matched string:Please do not do this in any code I have to maintain.You are trying to parse syntactically valid Python.  Use  for that.  It's more readable, easier to extend to new syntax, and won't fall apart on some weird corner case. Working sample:Prints:You could do this.  Though I don't think it's very readable.  And doing it this way could get unruly if you start adding more patterns to replace.  It takes advantage of the fact that the replacement string can also be a function.You can use  and a simple string formatting:"},
{"body": "For my program I have a lot of places where an object can be either a string or a list containing strings and other similar lists. These are generally read from a JSON file. They both need to be treated differently. Right now, I am just using isinstance, but that does not feel like the most pythonic way of doing it, so does anyone have a better way of doing it? No need to import modules, ,   and  (versions before 3 -- there's no  in 3!) will do the job for you.From :Since Python3 no longer has  or , in this case ( where you are expecting either a list or a string) it's better to test against as that is compatible with both Python2 and Python3Using :On Python>=2.3 a string may be a  or  type. To check both cases:From Python 3 only one string type exists, so instead of  you should use :Another method, using the practice of \"It's better to ask forgiveness than permission,\" duck-typing being generally preferred in Python, you could try to do what you want first, e.g.:As I like to keep things simple, here is the shortest form that is compatible with both 2.x and 3.x:You can use types module:"},
{"body": "Can this Python code be shortened and still be readable using itertools and sets?I can think of this only:An alternative to  is to use the  method of standard dictionaries:This relies on the fact that lists are mutable, so what is returned from setdefault is the same list as the one in the dictionary, therefore you can append to it.You can use a .may be a bit slow but works"},
{"body": "In Python, I have a list:I want to identify the item that occurred the highest number of times. I am able to solve it but I need the fastest way to do so. I know there is a nice Pythonic answer to this.  Here is a  solution that will work with Python versions 2.5 and above:Note if \nthen there are six 4s and six 7s.   However, the result will be   i.e. six 4s.For older Python versions (< 2.7), you can use  to get the  class.In your question, you asked for the fastest way to do it.  As has been demonstrated repeatedly, particularly with Python, intuition is not a reliable guide: you need to measure.  Here's a simple test of several different implementations:The results on my machine:So it appears that the  solution is not the fastest.  And, in this case at least,  is faster.  is good but you pay a little bit for its convenience;  it's slightly faster to use a regular  with a .What happens if the list is much bigger?  Adding  to the test above and reducing the repeat count to 200:Now  is the clear winner. So perhaps the cost of the 'get' method and the loss of the inplace add adds up (an examination of the generated code is left as an exercise).But with the modified test data, the number of unique item values did not change so presumably  and  have an advantage there over the other implementations.  So what happens if we use the bigger list but substantially increase the number of unique items?  Replacing the initialization of L with:So now  is clearly faster than the  solutions but still slower than the  versions of  and .The point of these examples isn't to produce an optimal solution.  The point is that there often isn't  optimal general solution.  Plus there are other performance criteria.  The memory requirements will differ substantially among the solutions and, as the size of the input goes up, memory requirements may become the overriding factor in algorithm selection.Bottom line: it all depends and you need to measure.I am surprised no-one has mentioned the simplest solution, with the key :Example:This works in Python 3 or 2, but note that it only returns the most frequent item and not also the frequency. Also, in the case of a  (i.e. joint most frequent item) only a single item is returned.I find the  approach is about twice as fast as :Perhaps the  methodI obtained the best results with  from  module with this function using Python 3.5.2:Output:Test with  from  module.I used this script for my test with :Output (The best one):I want to throw in another solution that looks nice and is fast for  lists.You can benchmark this with the code provided by Ned Deily which will give you these results for the smallest test case:But beware, it is inefficient and thus gets  slow for large lists!"},
{"body": "I have a model some thing like thisNow I would like to do a calculation  on the database level. Using Django Aggregation I can have the sum for each field but not the summation of multiplication of fields.  for  please follow the answer provided by @kmmbvnrit's possible using Django ORM:here's what you should do:Note: if the two fields are of different types, say  & , the type you want to return should be passed as the first parameter of It's a late answer, but I guess it'll help someone looking for the same.With Django 1.8 and above you can now pass an expression to your aggregate:Constants are also available, and everything is combinable:The solution depends on Django version.Do you have several options:Overwriting:"},
{"body": "I want to play my song (mp3) from python, can you give me a simplest command to do that?This is not correct:You may try this, simplistic but not the best method maybe.Please note that the support for  is limited ()For installation instructions, visit Grab the , vlc.py, which provides full support for libVLC and pop that in site-packages.  Then:And you can stop it with:That module offers plenty beyond that (like pretty much anything the VLC media player can do), but that's the simplest and most effective means of playing one MP3.You could play with os.path a bit to get it to find the path to the MP3 for you, given the filename and possibly limiting the search directories.Full documentation and pre-prepared modules are available .  Current versions are Python 3 compatible.You are trying to play a  as if it were a .You could try using  to convert it to  format, and then feed that into pyAudio.Alternatively, use , as mentioned in the other answer.As it wasn't already suggested here, but is probably one of the easiest solutions:It depends on any mpg123 compliant player, which you get e.g. for Debian using:or A simple solution:cheers...See also Another quick and simple option...Now you might need to make some slight changes to make it work. For example, if the player needs extra arguments or you don't need to specify the full path. But this is a simple way of doing it.If you're working in the Jupyter (formerly IPython) notebook, you canAt this point, why not mentioning :It's the best solution I found.(I needed to install , on Raspbian)Code excerpt loosely based on:\n"},
{"body": "I have: I essentially want to input  and return  but I keep getting  to return instead.  Not sure what's wrong! like demented hedgehog said. But you said you can't ...Your problem is your code searches only for the first character of your search string which(the first one) is at index 2.You are basically saying if  is in , increment  until  which returned 3 when I tested it but it was still wrong. Here's a way to do it.It produced the following output:There's a builtin method on string objects to do this in python you know?Python is a \"batteries included language\" there's code written to do most of what you want already (whatever you want).. unless this is homework :)Edit:  returns -1 if the string cannot be found."},
{"body": "I need some guidance in working out how to plot a block of histograms from grouped data in a pandas dataframe. Here's an example to illustrate my question:In my ignorance I tried this code command:which failed with the error message \"TypeError: cannot concatenate 'str' and 'float' objects\"Any help most appreciated.I'm on a roll, just found an even simpler way to do it using the  keyword in the hist method:That's a very handy little shortcut for quickly scanning your grouped data!For future visitors, the product of this call is the following chart:\nYour function is failing because the groupby dataframe you end up with has a hierarchical index and two columns (Letter and N) so when you do  it's trying to make a histogram of both columns hence the str error.This is the default behavior of pandas plotting functions (one plot per column) so if you reshape your data frame so that each letter is a column you will get exactly what you want.The  is just to shove the current index into a column called .  Then  will take your data frame, collect all of the values  for each  and make them a column.  The resulting data frame as 400 rows (fills missing values with ) and three columns ().   will then produce one histogram per column and you get format the plots as needed.One solution is to use matplotlib histogram directly on each grouped data frame.  You can loop through the groups obtained in a loop.  Each group is a dataframe.  And you can create a histogram for each one."},
{"body": "I'm learning python and I'm not sure of understanding the following statement : \"The function (including its name) can capture .\"It's the part that is in bold that I don't understand the meaning in terms of programming. The quote comes from How to think like a computer scientist, 3 edition.Thank you !Abstraction is a core concept in all of computer science. Without abstraction, we would still be programming in machine code or worse not have computers in the first place. So IMHO that's a really good question. something means to  to things, so that the name captures the core of what a function or a whole program does.One example is given in the book you reference, where it says Forget about the turtles for a moment and just think of drawing a square. If I tell you to draw a square (on paper), you immediately know what to do: You can do this without further questions because you know by heart what a  is, without me telling you step by step. Here, the word  is the  of \"draw a rectangle with all sides of the same length\".But wait, how do you know what a  is? Well, that's another abstraction for the following:Of course it goes on and on - , , ,  are all  of well-known concepts.Now, imagine each time you want a rectangle or a square to be drawn you have to give the full definition of a rectangle, or explain lines, parallel lines, perpendicular lines and connecting lines -- it would take far too long to do so.That's the first  they make talking and getting things done much easier. The second power of abstractions comes from the nice property of : once you have defined abstractions, you can  two or more abstractions to form a new, larger abstraction: say you are tired of drawing squares, but you really want to draw a . Assume we have already defined the , so then we can define:Next, you want a village:Oh wait, we want a city -- and we have a new concept :and so on...If in the course of planning your program (a process known as ), you find good abstractions to the problem you are trying to solve, your programs become shorter, hence easier to write and - maybe more importantly - easier to read. The way to do this is to try and grasp the major concepts that define your problems -- as in the (simplified) example of drawing a , this was  and , to draw a  it was . In programming, we define abstractions as functions (and some other constructs like classes and modules, but let's focus on functions for now). A function essentially  a set of single statements, so a function essentially is an abstraction -- see the examples in your book for details.In programming, abstractions can make or break productivity. That's why often times, commonly used functions are collected into  which can be reused by others. This means you don't have to worry about the details, you only need to understand how to use the ready-made abstractions. Obviously that should make things easier for you, so you can work faster and thus be more productive::Imagine there is a graphics library called \"nicepic\" that contains pre-defined    functions for all abstractions discussed above: rectangles, squares, triangles, house, village. Say you want to create a program based on the above abstractions that paints a nice picture of a house, all you have to write is this:So that's just two lines of code to get something much more elaborate. Isn't that just wonderful?Hope this helps.A great way to understand abstraction is through abstract classes.Say we are writing a program which models a house. The house is going to have several different rooms, which we will represent as objects. We define a class for a Bathroom, Kitchen, Living Room, Dining Room, etc.However, all of these are Rooms, and thus share several properties (# of doors/windows, square feet, etc.) BUT, a Room can never exist on it's own...it's always going to be some  of room.It then makes sense to create an abstract class called Room, which will contain the properties all rooms share, and then have the classes of Kitchen, Living Room, etc, inherit the abstract class Room.The concept of a room is  and only exists in our head, because any room that  exists isn't just a room; it's a bedroom or a living room or a classroom.We want our code to thus represent our \"mental chunking\". It makes everything a lot neater and easier to deal with.The best way to to describe something is to use examples:A function is nothing more than a series of commands to get stuff done. Basically you can organize a chunk of code that does a single thing. That single thing can get re-used over and over and over through your program.Now that your function does this thing, you should name it so that it's instantly  identifiable as to what it does. Once you have named it you can re-use it all over the place by simply calling it's name.Then to use that function you can just do something like:What happens if we wanted this to bark 4 times? Well you could write bark(); 4 times.Or you could modify your function to accept some type of input, to change how it works.Then we could just call it once:When we start talking about Object Oriented Programming (OOP) abstraction means something different. You'll discover that part later :) As defined on Basically it is removing the details of the problem. For example, to draw a square requires several steps, but I just want a function that draws a square.Let's say you write a function which receives a bunch of text as parameter, then reads credentials in a config file, then connects to a SMTP server using those credentials and sends a mail using that text. The function should be named , not  because it is more easy to represent what it does this way, to yourself and when presenting the API to coworkers or users... even though the 2nd name is more accurate, the first is a better abstraction. is a very important concept both in hardware and software.  We the human can not remember all the things all the times. For example, if your friend speaks 30 random numbers quickly and asks you to add them all, it won't be possible for you. Reason? You might not be able to keep all those numbers in mind. You might write those numbers on a paper even then you will be adding right most digits one by one ignoring the left digits at one time and then ignoring the right most digits at the other time having added the right most ones.It shows that at one time we the human can  at some particular issue while ignoring those which are already solved and moving focus towards what are left to be solved.Here is how abstraction works in programming.Below is the world's famous hello world program in C language:This is the simplest and usually the first computer program a programmer writes. When you compile and run this program on command prompt, the output may appear like this: gcc -o hello hello.cAnd it converted your English like C language code to binary code and you could run that code by giving command:./helloSo, for writing an application in C program, you never need to know how C compiler converts C language code to binary code. So you used GCC compiler as an abstraction.I did not expand the answer to abstraction of operating system, kernel, firmware and hardware for the sake of simplicity.While doing programming, you can use abstraction in a variety of ways to make your program simple and easy.Example 1: You can use a  to abstract value of PI 3.14159 in your program because PI is easy to remember than 3.14159 for the rest of programExample 2: You can write a  which returns square of a given number and then anyone, including you, can use that function by giving it input as parameters and getting a return value from it.Example 3: In an Object-oriented programming (OOP), like Java, you may define an  which encapsulates data and methods and you can use that object by invoking its methods. Example 4: Many applications provide you  which you use to interact with that application. When you use API methods, you never need to know how they are implemented. So abstraction is there.Through all the examples, you can realize the importance of abstraction and how it is implemented in programming. One key thing to remember is that abstraction is contextual i.e. keeps on changing as per context"},
{"body": "Following on from my previous question, , I have now come across a problem regarding the timezone, and it turns out that it's not always going to be \"+0200\". So when strptime tries to parse it as such, it throws up an exception.I thought about just chopping off the +0200 with [:-6] or whatever, but is there a real way to do this with strptime?I am using Python 2.5.2 if it matters.It looks like this is implemented only in >= 2.6, and I think you have to manually parse it.I can't see another solution than to remove the time zone data:No, but since your format appears to be an RFC822-family date, you can read it much more easily using the  library instead:(7200 = timezone offset from UTC in seconds)You can use the  library which is very useful:As far as I know,  doesn't recognize numeric time zone codes. If you know that the string is always going to end with a time zone specification of that form (+ or - followed by 4 digits), just chopping it off and parsing it manually seems like a perfectly reasonable thing to do.It seems that %Z corresponds to time zone names, not offsets.For example, given:I can parse:Although it seems that it doesn't do anything with the time zone, merely observing that it exists and is valid:I suppose if you wished, you could locate a mapping of offsets to names, convert your input, and then parse it.  It might be simpler to just truncate your input, though."},
{"body": "I'm writing Python code. I want to check if numpy and wxpython are installed on machine. How to do that??You can try importing them and then handle the ImportError if the module doesn't exist.I think you also may use thisThe traditional method for checking for packages in Python is \"it's better to beg forgiveness than ask permission\", or rather, \"it's better to catch an exception than test a condition.\"In the numpy README.txt file, it saysThis should be a sufficient test for proper installation. If you use eclipse, you simply type \"import numpy\" and eclipse will \"complain\" if doesn't find. "},
{"body": "I'm trying to perform an element wise divide in python, but if a zero is encountered, I need the quotient to just be zero.For example:I could always just use a for-loop through my data, but to really utilize numpy's optimizations, I need the divide function to return 0 upon divide by zero errors instead of ignoring the error.Unless I'm missing something, it doesn't seem  can return values upon errors. Does anyone have any other suggestions on how I could get the best out of numpy while setting my own divide by zero error handling?Building on @Franck Dernoncourt's answer, fixing -1 / 0:Building on the other answers, and improving on:Code:Output:In numpy v1.7+, you can take advantage of the \"where\" option for .  You can do things in one line and you don't have to deal with the errstate context manager.In this case, it does the divide calculation anywhere 'where' b does not equal zero.  When b does equal zero, then it remains unchanged from whatever value you originally gave it in the 'out' argument.Try doing it in two steps. Division first, then replace.The  line is optional, and just prevents numpy from telling you about the \"error\" of dividing by zero, since you're already intending to do so, and handling that case.This is a neat trick  more than they hate warnings:You can also replace based on , only if the array dtypes are floats, as per :One answer I found searching a related question was to manipulate the output based upon whether the denominator was zero or not.Suppose  and  have been initialized, but  has some zeros. We could do the following if we want to compute  safely.In this case, whenever I have a divide by zero in one of the cells, I set the cell to be equal to , which in this case would be zeroFootnote: In retrospect, this line is unnecessary anyways, since  is instantiated to zero. But if were the case that , this operation would do something."},
{"body": "I'm new to Python and programming in general (a couple of weeks at most).Concerning Python and using modules, I realise that functions can imported using .So instead of typingI can saywhich I find simplifies things a great deal. Now, say I have a bunch of  that I want to use across modules and I have them all defined in one python module. How can I, using a similar method as mentioned above or an equally simple one, import these variables. I don't want to use  and then be required to prefix all my variables with .The following situation would by ideal:a.pyb.pyOutput:You gave the solution yourself:  will work just fine. Python does not differentiate between functions and variables in this respect.Just for some context, most linters will flag  with a warning, because it's prone to namespace collisions that will cause headaches down the road.Nobody has noted yet that, as an alternative, you can use theform and then use  and  directly (without the  prefix). The  form is more future proof because you can easily see when one import will be overriding another.Also note that \"variables\" aren't different from functions in Python in terms of how they're addressed -- every identifier like  or  is pointing at some kind of object. The identifier  is pointing at a string object,  is pointing at a function object, and  is pointing at an integer object. When you tell Python:you're saying \"take those objects pointed at by  and  within module  and point at them in the current scope with the same identifiers\".Similarly, if you want to point at them with different identifiers on import, you can use theform. The same function object gets pointed at, except in the current scope the identifier pointing at it is  whereas in module  the identifier pointing at it is .Like others have said,will also import the modules variables.However, you need to understand that you are  importing variables, just references to objects. Assigning something else to the imported names in the importing module  affect the other modules.Example: assume you have a module  containing the following code:Then you have two other modules,  and  which both do the following:In each module, two names,  and  are created, pointing to the objects  and , respectively.Now, if somewhere in  you assign something else to the global name :the name  in  and the name  in  will still point to the object .So  will work if you want read-only globals, but it won't work if you want read-write globals. If the latter, you're better off just importing  and then either getting the value () or setting the value () prefixed by the module.You didn't say this directly, but I'm assuming you're having trouble with manipulating these global variables.If you manipulate global variables from inside a function, you must declare them globalIf you don't do that, then  will just create a local variable and assign it 15, while the global a stays 10"},
{"body": "I was having trouble implementing , so I copied the code right off of the documentation:and I got:instead of:as is shown in the doc.I'm using Python 2.6 on Windows 7What's going on?Yes it does, it works exactly as documented.  a new namedtuple, it does not modify the original, so you need to write this:See here:  for more information.A tuple is immutable.  returns a new tuple with your modifications: returns a new tuple; the original is unchanged.It looks to me as if namedtuple is immutable, like its forebear, tuple. returns a new  of the same type but with values changed."},
{"body": "How do I access a private attribute of a parent class from a subclass (without making it public)?My understanding of Python convention isOptions for if you control the parent classIf you don't control itSome language designers subscribe to the following assumption:These language designers will feel tempted to protect programmers from each other by introducing a  specifier into their language.\nShortly later they recognize that this is often too inflexible and introduce  as well.Language designers such as Python's Guido van Rossum, in contrast, assume that programmers are responsible adults and capable of good judgment (perhaps not always, but typically).\nThey find that everybody should be able to access the elements of a program if there is a need to do that, so that the language does not get in the way of doing the right thing.\n(The only programming language that can successfully get in the way of doing the  thing is the  language)Therefore,  in Python means something like \"The designer of this module is doing some non-obvious stuff with this attribute, so please do not modify it and stay away from even reading it if you can -- suitable ways to access relevant information have been provided.\"In case you are not able to stay away from accessing  (such as in special cases in a subclass), you simply access it.Using  and    to do what you wante.gif the variable name is \"__secret\" and the class name is \"MyClass\" you can access it like this on an instance named \"var\"var._MyClass__secretThe convention to suggest/emulate protection is to name it with a leading underscore: self._protected_variable = 10Of course, anybody can modify it if it really wants.Make an accessor method, unless I am missing something:"},
{"body": "I'm trying to get this Python MYSQL update statement correct(With Variables):Any ideas where I'm going wrong?It :You can  do it with basic string manipulation,but . As it's so easy (and similar) to do it the . Do it correctly. The only thing you should be careful, is that some database backends don't follow the same convention for string replacement (SQLite comes to mind).You've got the syntax all wrong:For more, .This is the right way\uff1aP.S. Don't forget , or it won't workNeither of them worked for me for some reason.I figured it out that for some reason python doesn't read %s. So use (?) instead of %S in you SQL Code.And finally this worked for me."},
{"body": "I can't find the correct way to install a local directory as a python package using pip.As you can see pip just copied over the package to site-packages. How can I avoid this, and use the package directly from its source folder?I'm trying to integrate django-pipeline into my Django project, but I want to add support for Django 1.4 first, so I forked and cloned my fork. works by installing packages to your .  If you just want to  your module regularly, all you have to do is add the directory path to your  environmental variable.I can also just use:orIf you're working in a venv, you can do this:env/bin/pip install git+file:///path/to/your/git/repoOr with a branch:env/bin/pip install git+file:///path/to/your/git/repo@mybranch"},
{"body": "I've got mssql 2005 running on my personal computer with a database I'd like to run some python scripts on. I'm looking for a way to do some really simple access on the data. I'd like to run some select statements, process the data and maybe have python save a text file with the results.Unfortunately, even though I know a bit about python and a bit about databases, it's very difficult for me to tell, just from reading, if a library does what I want. Ideally, I'd like something that works for other versions of mssql, is free of charge and licensed to allow commercial use, is simple to use, and possibly works with ironpython.I use  with cPython (I don't know if it'll work with IronPython though).  It'll be pretty familiar to you if you've used Hibernate/nHibernate.  If that's a bit too verbose for you, you can use , which is a thin layer on top of SQL Alchemy.  To use either one of those, you'll need , but that's a pretty simple install.Of course, if you want to write straight SQL and not use an ORM, you just need pyodbc.Everyone else seems to have the cPython -> SQL Server side covered. If you want to use IronPython, you can use the standard ADO.NET API to talk to the database:If you've already got IronPython, you don't need to install anything else.Lots of docs available  and .pyodbc comes with Activestate Python, which can be downloaded from .  A minimal odbc script to connect to a SQL Server 2005 database looks like this:I also successfully use  with CPython. (With and without SQLAlchemy). can be used with either CPython or IronPython. I have been very pleased with it. () works under PyPy, Ironpython and CPython. shows a Hello World sample of accessing mssql in Python.PyPyODBC has almostly same usage as pyodbc, as it can been seen as a re-implemenation of the pyodbc moudle. Because it's written in pure Python, it can also run on IronPython and PyPy.Actually, when switch to pypyodbc in your existing script, you can do this:I've used  with standard python and liked it. Probably easier than the alternatives mentioned if you're  looking for basic database access.Sample .If you are want the quick and dirty way with CPython (also works for 3.X python):Install PYWIN32 after you install python Import the following library:\nimport odbcI created the following method for getting the SQL Server odbc driver (it is slightly different in naming depending on your version of Windows, so this will get it regardless):Note: if you use the above function, you'll need to also import these two libraries: winreg and reThen you use the odbc API 1 information as defined here: Your connection interface string should look something like this (assuming you are using my above method for getting the ODBC driver name, and it is a trusted connection):This method has many down sides.  It is clumsy because of only supporting ODBC API 1, and there are a couple minor bugs in either the API or the ODBC driver that I've run across, but it does get the job done in all versions of CPython in Windows."},
{"body": "I know Django 1.1 has some new aggregation methods. However I couldn't figure out equivalent of the following query:Is it possible with Django 1.1's Model Query API or should I just use plain SQL?If you are using Django 1.1 beta (trunk):Django 1.1 does support aggregation methods like count. You can find .To answer your question, you can use something along the lines of:This will need slight tweaking depending on your actual model.Edit: The above snippet generates aggregations on a per-object basis. If you want aggregation on a particular field in the model, you can use the  method: is needed because fields that are in the default ordering are automatically selected even if they are not explicitly passed to . This call to  clears any ordering and makes the query behave as expected.Also, if you want to count the field that is used for grouping (equivalent to ), you can use:"},
{"body": "I'm trying to find a way to lazily load a module-level variable.Specifically, I've written a tiny Python library to talk to iTunes, and I want to have a  module variable. Unfortunately, iTunes won't tell you where its download folder is, so I've written a function that grabs the filepath of a few podcast tracks and climbs back up the directory tree until it finds the \"Downloads\" directory.This takes a second or two, so I'd like to have it evaluated lazily, rather than at module import time.Is there any way to lazily assign a module variable when it's first accessed or will I have to rely on a function?You can't do it with modules, but you can disguise a class \"as if\" it was a module, e.g., in , code...:Now anybody can ... and get in fact your  instance. The  is there to let you access anything else in  that may be more convenient for you to code as a top-level module object, than inside !_)I used Alex' implementation on Python 3.3, but this crashes miserably:\nThe codeis not correct because an  should be raised, not a .\nThis crashed immediately under Python 3.3, because a lot of introspection is done\nduring the import, looking for attributes like ,  etc.Here is  the version that we use now in our project to allow for lazy imports\nin a module. The  of the module is delayed until the first attribute access\nthat has not a special name:The module now needs to define an  function. This function can be used\nto import modules that might import ourselves:The code can be put into another module, and it can be extended with properties\nas in the examples above.Maybe that is useful for somebody.I think you are correct in saying that a function is the best solution to your problem here. \nI will give you a brief example to illustrate.The expensive operation will be executed on import if it is at module level. There is not a way to stop this, short of lazily importing the entire module!!You will be following best practice by using this method.Recently I came across the same problem, and have found a way to do it.With this , You can define a  method for the object, and the object will be initialized lazily, example code looks like:the  object above will have exactly the same methods whatever  object have, for example, you can do:complete code with tests (works in 2.7) can be found here:If that variable lived in a class rather than a module, then you could overload getattr, or better yet, populate it in init."},
{"body": "I have been learning a bit of Python 2 and Python 3 and it seems like Python 2 is overall better than Python 3. So that's where my question comes in. Are there any good reasons to actually switch over to python 3?On the whole, and even in most details, Python3 is better than Python2.The only area where  is .  What makes Python great is not only its intrinsic characteristics as a language and its rather extensive standard library, but also the existence of a whole \"\" of libraries which support so many specific applications of the language.\nSeveral such libraries are, at the moment not fully ported to Python 3.x and this sometimes results in keeping people developing under Python 2.x.: Application developers won't move to 3.x till the libraries \"get there\", libraries developers would rather only maintain one branch and are waiting in an attempt to time the porting to Py3k in a way that they can put the their Py2.x branches in maintenance shortly thereafter.This situation is somewhat of a testimony the satisfaction people have of Python 2.x (or phrased more negatively, to the lack of truly  incentives for a move to 3.x; while Py3k is better and poised for better things yet, as-is, it doesn't have any features that would prompt a move to 3.x \"\".)  This said,  I believe . \nTo back this up, I was about to mention the likelihood that  be only ported to Py3k-only, providing some strong incentive for the move.  But Alex Martelli has started answering this question, and is using this example.  Obviously Alex speaks first-hand of these roadmap questions, please get it from the Master!:  Be sure to use the  version (currently 3.1.2,  soon 3.2.x will replace it as the most recent  version).  Beware that some folks (like me) occasionally use the expression  to reference the generic name for all Py3k (or even for the current version thereof).  The short lived 3.0 version  is now \"defunct\" and of no interest but maybe forensic specialists ;-)As other answers mention, the only real (and crucial) current advantage of Python 2 over Python 3 is that the former already has a huge wealth of third-party extensions (and auxiliary tools, such as IDEs and the like), which the latter is only gradually moving towards.  This is a situation that's gradually improving, as existing extensions are ported to support Python 3; sooner or later somebody will release a Py3-only extension or tool that's important to you, and that might tip you over to actually using Py3 for a new project (and that will happen for many different values of \"you\";-).Python core development has mostly shifted to the Python 3 area -- while Python 2.7 will be out soon, I'm not sure how many future important enhancements, if any, will keep getting backported to Python 2 versions.  In particular, I doubt that Unladen Swallow will be -- if that's the case, then at some point CPython 3 will gain an important speed advantage over CPython 2, which will be the tipping-over factor for other new projects (and ports of existing projects) yet.Not sure what gives you the impression that \"Python 2 is over all better than Python 3\", in terms, of course, of just \"Python proper\", i.e., the parts that the Python Software Foundation is releasing (core code, libraries, docs...): I strongly disagree with this assessment.  Python 3 is better in terms of simplicity (cruft removal) and in having a few features that enable future third-party extensions and tools (such as better metaclass interaction, and syntax for parameter annoutation); I cannot find, in fact, any advantages for Python 2 in terms of \"Python proper\".  The \"ecosystem\" around Python, as already discussed, is another thing, but that's already progressing (at its own pace, of course, which is definitely  under the control of the Python Software Foundation, but rather of many external groups of developers and a few firms).For example, PyQt is already available for Python 3 (as well as 2), and you can use PostgreSQL (a splendid open-source SQL database engine, much more standard than MySql) via . So, if the third party extensions you're pining for are (say) wxpython and mysqldb, perhaps you can try these alternatives (I'm biased, of course, since I've long preferred Qt to wxWidgets, and PostgreSQL to MySQL, on purely technical grounds -- but the prompt porting of the related Python extensions to Python 3 does also hint that these extensions are being more actively and vigorously developed, so that might be another nudge to try them;-).Python 3 is going to be the new standard going forward. As no major sweeping changes are planned to Python 3 anytime soon, more people will eventually be moving to it. So... although there are many Python 2 applications around now, eventually  many of these applications will be migrated up. There is even at tool for this, .Also, what makes you say that python 2 is better than 3? There were many language improvements made in Python 3, and even IDLE is improved. The main thing holding Python 3.x back right now is the lack of third-party libraries.  I'll be converting my code as soon as SciPy gets ported.More iterators (in things like ) will be a big boost for web applications.The core team will put more work into the new version. New books might focus on python 3 (see Dive into Python), but the real work is still done in python 2.Sooner or later, the big libraries (numpy, wx, django) will be ported. Until those big three switch, I can't see many people using python 3. But those aren't impossible projects to port.Once the big libraries are ported, the community will face a real choice. That's when it will start to catch on."},
{"body": "I'm trying to create a histogram with argument normed=1For instance:I expected that the sum of the bins would be 1. But instead, one of the bin is bigger then 1. What this normalization did? And how to create a histogram with such normalization that the integral of the histogram would be equal 1?See my other post for how to make the sum of all bins in a histogram equal to one:\nCopy & Paste:where myarray contains your dataAccording to   This is from numpy doc, but should be the same for pylab.So simply normalization is done according to the documentation like:I think you are confusing bin heights with bin contents. You need to add the contents of each bin, i.e. height*width for all bins. That should = 1.I had the same problem, and while solving it another problem came up: how to plot the the normalised bin frequences as percentages with ticks on  values. I'm posting it here in case it is useful for anyone. In my example I chose 10% (0.1) as the maximum value for the y axis, and 10 steps (one from 0% to 1%, one from 1% to 2%, and so on). The trick is to set the ticks at the  counts (which are the output list  of the ) that will next be transformed into percentages using the  class. Here's what I did:Before normalisation: the y axis unit is number of samples within the bin intervals in the x axis:\nAfter normalisation: the y axis unit is frequency of the bin values as a percentage over all the samples\nIn order to normalize a sequence, you have to take into account the bin size.\nAccording to the  , the default number of bin is 10. Consequently, the bin size is , that is 0.41.\nIf , then the heights of the bar is such that the sum, multiplied by 0.41, gives 1. This is what happens when you integrate.I think that you want the sum of the histogram, not its integral, to be equal to 1. In this case the quickest way seems:There is also an analogue in numpy - : \nOne of the parameters is \"density\", If you set , the output will be normalised."},
{"body": "I'm going through Zed Shaw's Learn Python The Hard Way and I'm on lesson 26. In this lesson we have to fix some code, and the code calls functions from another script. He says that we don't have to import them to pass the test, but I'm curious as to how we would do so. | And here are the particular lines of code that call on a previous script:It depends on how the code in the first file is structured.If it's just a bunch of functions, like:Then you could import it and use the functions as follows:oror, to import  the symbols defined in first.py:Note: This assumes the two files are in the same directory.It gets a bit more complicated when you want to import symbols (functions, classes, etc) in other directories or inside modules.It's worth mentioning that (at least in python 3), in order for this to work, you must have a file named  in the same directory."},
{"body": "How can I insert some seed data in my first migration? If the migration is not the best place for this, then what is the best practice?Alembic has, as one of its operation, . The documentation gives the following example (with some fixes I've included):Note too that the alembic has an  operation, which is just like the normal  function in SQLAlchemy: you can run any SQL you wish, as the documentation example shows:Notice that the table that is being used to create the metadata that is used in the  statement is defined directly in the schema. This might seem like it breaks  (isn't the table already defined in your application), but is actually quite necessary. If you were to try to use the table or model definition that is part of your application, you would break this migration when you make changes to your table/model in your application. Your migration scripts should be set in stone: a change to a future version of your models should not change migrations scripts. Using the application models will mean that the definitions will change depending on what version of the models you have checked out (most likely the latest). Therefore, you need the table definition to be self-contained in the migration script.Another thing to talk about is whether you should put your seed data into a script that runs as its own command (such as using a Flask-Script command, as shown in the other answer). This can be used, but you should be careful about it. If the data you're loading is test data, then that's one thing. But I've understood \"seed data\" to mean data that is required for the application to work correctly. For example, if you need to set up records for \"admin\" and \"user\" in the \"roles\" table. This data SHOULD be inserted as part of the migrations. Remember that a script will only work with the latest version of your database, whereas a migration will work with the specific version that you are migrating to or from. If you wanted a script to load the roles info, you could need a script for every version of the database with a different schema for the \"roles\" table. Also, by relying on a script, you would make it more difficult for you to run the script between migrations (say migration 3->4 requires that the seed data in the initial migration to be in the database). You now need to modify Alembic's default way of running to run these scripts. And that's still not ignoring the problems with the fact that these scripts would have to change over time, and who knows what version of your application you have checked out from source control.Migrations should be limited to schema changes only, and not only that, it is important that when a migration up or down is applied that data that existed in the database from before is preserved as much as possible. Inserting seed data as part of a migration may mess up pre-existing data.As most things with Flask, you can implement this in many ways. Adding a new command to Flask-Script is a good way to do this, in my opinion. For example:So then you run:MarkHildreth has supplied an excellent explanation of how alembic can handle this. However, the OP was specifically about how to modify a flask-migration migration script. I'm going to post an answer to that below to save people the time of having to look into alembic at all.\nMiguel's answer is accurate with respect to normal database information. That is to say, one should follow his advice and absolutely not use this approach to populate a database with \"normal\" rows. This approach is specifically for database rows which are required for the application to function, a kind of data which I think of as \"seed\" data.Flask migrate generates migration scripts at . These scripts are run in order on a database in order to bring it up to the latest version. The OP includes an example of one of these auto-generated migration scripts. In order to add seed data, one must manually modify the appropriate auto-generated migration file. The code I have posted above is an example of that. Very little. You will note that in the new file I am storing the table returned from  for  in a variable called . We then operate on that table using  to create a few example rows."},
{"body": "What's the easiest way to determine which version of  is installed?As of flask 0.7 (June 28th, 2011), a  attribute can be found on the flask module.Keep in mind that because prior to flask 0.7 there was no  attribute, the preceding code will result in an attribute error on those older versions.For versions older than flask 0.7, you might be able to determine it using pkg_resources as shown below:This won't work 100% though. It depends on the user having the pkg_resources library installed (it might come by default with a Linux distribution's python installation, but since it's not part of the standard library you can't be positive), and also that the user installed flask in a way that pkg_resources can find it (for example, just copying the full flask source code into your directory puts it out of the range of pkg_resources).Via the python interpreter.If flask was installed via pip or easy_install, you can always use the 'pip freeze' command.More general way of doing it is :It will list all installed python packages and their versions.\nIf you want to see just flask then try :      using dpkg:output:"},
{"body": "I am following a previous thread on how to plot confusion matrix in Matplotlib. The script is as follows:I would like to change the axis to show string of letters, say (A, B, C,...) rather than integers (0,1,2,3, ..10). How can one do that. Thanks.musaHere's what I'm guessing you want:\nJust use  and .E.g."},
{"body": "Guys, I just started python recently and get confused with the optional parameters, say I have the program like this:If I create A twiceand print their buildsI found they are using the exactly same object,But it is not what I want, since if  changed some internal state of builds, the one in  object will also be changed.Is it possible to recreate this optional parameters each time by using this optional parameters syntax?You need to understand how default values work in order to use them effectively.Functions are objects.  As such, they have attributes.  So, if I create this function:I've created an object.  Here are its attributes:One of them is .  That sounds promising, what's in there?That's a tuple that contains the function's default values.  If a default value is an object, the tuple contains an instance of that object.  This leads to some fairly counterintuitive behavior if you're thinking that  adds an item to a list, returning a list containing only that item if no list is provided:But if you know that the default value is an object instance that's stored in one of the function's attributes, it's much less counterintuitive:Knowing this, it's clear that if you want a default value of a function's parameter to be a new list (or any new object), you can't simply stash an instance of the object in .  You have to create a new one every time the function is called:you need to do the following:it's a very wide-spread error, using mutable parameters as a default arguments. there are plenty of dupes probably on SO.Yes; default parameters are evaluated only at the time when the function is defined.One possible solution would be to have the parameter be a  rather than an instance, a la"},
{"body": "I'm making a game link site, where users can post links to their \nfavorite web game. \nWhen people post games they are supposed to check what category the \ngame falls into. \nI decided to allow many categories for each game since some games can \nfall into many categories. \nSo the question is, how do I handle this in my view? \nAnd how can I show it as Checkboxes, where at least one has to be \nchecked? \nAnd how can I show this as checkboxes in the Admin as well? Thanks! that should fix your view, but i'm not sure about the admin. still looking... will edit if i find anything.Here is how I solved it (Edit: and the admin thing)(It was the queryset part I couldn't find..)And that's all the code needed to save the data.Edit:\nhere is a solution for the adminIt's kind of quirky in looks, but works!\nIf someone finds a way to make it more \"clean\" please post!Cheers!+1 for the admin part in the answer @Gr\u00e9tar J\u00f3nsson. Just a minor correction:In the \"Models:\" section you also need to import models:(A bit silly that you need reputation to comment directly on other peoples posts)"},
{"body": "Has anyone had issues setting up a debug configuration for Django project in PyCharm Community Edition?\nCommunity Edition of the IDE is lacking the project type option on project setup and then when I am setting up Debug or Run config it asks me for a script it should run. What script would it be for Django, manage.py?\nThanks in advanceYes you can."},
{"body": "I have the following code:this will printwhile I'd like it to printIs any way there to do this without having to writePS: img.size is a PIL image. Dunno if that matters anything in this case.Might be a nicer way, but this should workThe  way would be using a list comprehension:Another way could be:Solution:Explanation:  make direct scalar multiplication possible. Hence the  called  here is converted to an . I assume you wish to keep using the , hence we convert the  back to a .This solution is to avoid the explicit and verbose  loop. I do not know whether it is faster or whether the exact same thing happens in both cases.There is probably a simpler way than this, butWill do nearly what you want, although it prints as a list rather than a tuple. Wrap the  call inside  if you want it to print as a tuple (parentheses rather than square brackets).If you have this problem more often and with larger tuples or lists then you might want to use the  library, which allows you to do all kinds of mathematical operations on arrays. However, in this simple situation this would be complete overkill.You can try something like this:It will give you a new list with all the elements you have in the tuple multiplied by 10In line with the previous answers but using numpy:adding nothing but variety.."},
{"body": "Is it preferred to do:orSame thing for \"is not\"  is different than . is true if and only if  -- that is,   and  have to be one and the same object (with the same s).For all built-in Python objects (like strings, lists, dicts, functions, etc.), if ,  then  is also True. However, this is not guaranteed in general. Strictly speaking,  is true if and only if  returns True. It is possible to define an object  with a  method which always returns False, for example, and this would cause  to return False, even if . So the bottom line is,   and   are completely different tests.Consider this for example:PS. Instead ofit is more Pythonic to writeAnd similarly,can be replaced with compares the identities of the two objects, and is asking  It is equivalent to . uses the equality operator and asks the looser question  For user defined types it is equivalent to .The  special method should represent 'equalness' for the objects, for example a class representing fractions would want 1/2 to equal 2/4, even though the 'one half' object couldn't have the same identity as the 'two quarters' object.Note that it is not only the case that  does not imply , but also the reverse is true. One is not in general a more stringent requirement than the other. Yes, this means that you can have  return  if you really want to, for example:In practice though  is  always a more specific comparison than .as others have already said,  (and ) are only when you actually  that a pair of variables are referring to exactly the same object. in most cases, you really don't care at all, so you would use  and .however, what you may start to notice, if you look at a lot of Python code, is that  (and ) are more likely to be used when comparing against , , and . the main reason for this is that those objects are singletons, meaning there is exactly one instance of each of those values. why does that matter? well, this leads to another reason... speed.with  and , the interpreter has to pull up both referred objects in order to make a comparison (of whether they're the same or not), while  and  simply just check the values of the objects they're referring to. with this said, you can see that  the latter pair will perform faster because you don't have to fetch the objects themselves in order to make the comparison. here's a speed test from a couple of years back where we concluded that for one-offs, it's not a big deal, but if it's called a gazillion times in a tight loop, well, it will start to add up.bottom line is that you can use object identity comparisons for checking against , , and , and everything else should use straight-up equality operators. we won't get into interned integers nor bound instance methods, or anything like that here. :-)Depends.  and  do identity comparison, which is good for , , or making sure that two objects are the same. Otherwise, use  or ."},
{"body": "Is there a better way to print the + sign of a digit on positive numbers?0 should return 0 without +.Here is .** Update** If for whatever reason you can't use the  operator, you don't need a function:Use the It's recommended over the  operator"},
{"body": "on my computerbut I get into problems when I run some python programs. my guess is (or at least I want to try this) that there is some backward compatibility issues, and I want to run those python scripts with which is also installed on my system but I do not know how to make it as the (temporary) default python. The python script starts withand I am using arch linux.You can use Enjoy!Just call the script using something like python2.7 or python2 instead of just python.So:instead of:What you could alternatively do is to replace the symbolic link \"python\" in /usr/bin which currently links to python3 with a link to the required python2/2.x executable. Then you could just call it as you would with python 3.You don't want a \"temporary default Python\"You want the 2.7 scripts to start withAnd you want the 3.2 scripts to begin with There's really no use for a \"default\" Python.  And the idea of a \"temporary default\" is just a road to absolute confusion.Remember.You could use :To stop using python2,  or .Use python command to launch scripts, not shell directly. E.g.AFAIK this is the recommended method to workaround scripts with bad env interpreter line."},
{"body": "I cannot seem to find a good simple explanation of what python does differently when running with the -O or optimize flag.  statements are completely eliminated, as are statement blocks of the form  (so you can put your debug code in such statements blocks and just run with  to avoid that debug code).With , in addition, docstrings are also eliminated.From :So in other words, almost nothing.From As answered in :python -O does the following currently:and when called as python -OOI don't know why everyone forgets to mention the  issue; perhaps it is because I'm the only one using it :) An  construct creates no bytecode at all when running under , and I find that very useful."},
{"body": "If you happen to havein the middle of your program (or module), you would get the warning:I understand why  is discouraged in general (namespace invisibility),\nbut there are many situations where it would prove convenient, especially where\ncode is not shared with anyone.So, can anyone explain precisely in detail why  should\nbe prohibited in all possible cases?I believe by \"in the middle of your program\" you are talking about an import  a function definition:This is not allowed because it would make optimizing the body of the function too hard.  The Python implementation wants to know all of the names of function-local variables when it byte-compiles a function, so that it can optimize variable references into operations on the (CPython) virtual machine's operand stack, or at least to local variable-slot operations rather than lookups in outer namespaces.  If you could dump the entire contents of a module into a function's local namespace, then the compiler would have to assume that  name in the function might possibly refer to a module global, because the list of names brought in by  is only known at runtime.Putting   top-level declarations is poor style, but it's allowed: While looking into something else, I discovered that this restriction was introduced in Python 2.1, as a consequence of the  ().  Quoting from the link:This clarifies the Python 3.x vs 2.x behavior discussed in the comments.  It is always contrary to the language specification, but CPython 2.1 through 2.7 only issue an error for  within a function if it might affect the compiler's ability to know whether a variable binds locally or in a containing scope.  In 3.x it has been promoted to an unconditional error. ... and apparently flashk pointed this out years ago in another answer, quoting the same paragraph of \"What's New in Python 2.1\" yet.  Y'all go upvote that now.At any lexical level,  is a \"seemed a good idea at the time\" design decision that has proven a real disaster in real life, with the  exception of handy exploration at the interactive interpreter prompt (even then, I'm not too hot on it --  forces only two extra characters to use qualified names instead [[just an  prefix]],  qualified names are always sharper and more flexible than barenames, not to mention the great usefulness in exploratory interactive situations of having  available for , , and the like!).This bedraggled construct makes it very hard, for the poor person reading the code (often in a doomed attempt to help debug it) to understand where mysteriously-appearing names are coming from -- impossible, if the construct is used more than once on a lexical level; but even when used just once, it forces laborious re-reading of the whole module every time before one can convince oneself that, yep, that bedraggled barename must come from the module.Plus, module authors usually don't go to the extreme trouble needed to \"support\" the horrid construct in question.  If somewhere in your code you have, say, a use of  (and an  at the very top of your module, of course), how do you  that  is the module it should be... or some completely different one (or a non-module) coming from the ?!  Multiply that by all the qualified names you're using, and misery is the only end result -- that, and mysterious bugs requiring long, laborious debugging (usually with the reluctant help of somebody who  \"get\" Python...!-)., a way to add and override arbitrary local names would be even worse.  As an elementary but crucial optimization, the Python compiler looks around the function's body for any assignment or other binding statements on each barename, and deems \"local\" those names it sees thus assigned (the others must be globals or built-ins).  With an  (just like with an  without explicit dicts to use as namespaces), suddenly it becomes a total mystery which names are local, which names are global -- so the poor compiler would have to resort to the slowest possible strategy for each name lookup, using a dict for local variables (instead of the compact \"vector\" it normally uses) and performing up to three dict look-ups for each barename referenced, over and over.Go to any Python interactive prompt.  Type .  What do you see?  The Zen of Python.  What's the last and probably greatest bit of wisdom in that text...?By forcing the use of barenames where qualified names are  vastly preferable, you're essentially doing the very  of this wise recommendation: instead of admiring the greatness and honkingtude of namespaces, and doing more of those, you're  two perfectly good and ready-to-use namespaces (that of the module you're importing, and that of the lexical scope you're importing it in) to make a single, unholy, buggy, slow, rigid, unusable mess.If I could go back and change  early design decision in Python (it's a hard choice, because the use of  and especially  for what Javascript so much more readably calls  is a close second;-), I would retroactively wipe out the  idea from Guido's mind.  No amount of  convenience for exploration at the interactive prompt can balance the amount of evil it's wrought...!-)The  seem to explain why this limitation exists:... it's handy for quick scripts and shell exploring. It's not prohibited at all. It works fine, but you get a warning because it's generally a bad idea (for reasons others have gone into). You can, if you like, suppress the warning; the warnings module is what you want for that.others have given in-depth answers, I'll give a short overview answer of my understanding.. when using from you are making it so you can directly call any function in that module you imported without doing modulename.functioname (you can just call \"functionname\") this creates problems if you have 2 functions of the same name in different modules and also can create confusion when dealing with a lot of functions as you don't know what object/module it belongs to (from point of view of someone looking over already written code that isn't familiar with it)"},
{"body": "What are the new commands & or equivalents of:I know they have changed to , ,  but there doesn't seem to be an example of how to properly use each one. For example, if I use  I get the following: To list:To show:To select:"},
{"body": "Is it possible to have missing values in scikit-learn ? How should they be represented? I couldn't find any documentation about that.The above answer is outdated; the latest release of scikit-learn has a class  that does simple, per-feature missing value imputation. You can feed it arrays containing NaNs to have those replaced by the mean, median or mode of the corresponding feature.I have come across very similar issue, when running the  on data. The presence of NA values were throwing out \"nan\" for predictions. From scrolling around several discussions, the Documentation by Breiman recommends two solutions for continuous and categorical data respectively.According to Breiman the random nature of the algorithm and the number of trees will allow for the correction without too much effect on the accuracy of the prediction. This I feel would be the case if the presence of NA values is sparse, a feature containing many NA values I think will most likely have an affect.I wish I could provide a simple example, but I have found that RandomForestRegressor does  handle nan's gracefully. Performance gets steadily worse when adding features with increasing percentages of nan's. Features that have \"too many\" nan's are completely ignored, even when the nan's indicate very useful information.AFAIK the algorithm will never create a split on the decision \"isnan\" or \"ismissing\". The algorithm will ignore the feature at a particular level of the tree if the feature has only one known value and the rest of the feature's values are nan/unknown.My solution: replace nan's with a single \"clearly out-of-range\" value (like -1.0). This enables the tree to split on the criteria \"known values vs unknown values\". I  tried more advanced imputation techniques (replace with mean/median, predict missing value's \"true\" value, etc.), but the results were mixed. However, there is a problem with replacing with such out-of-range values: known values could get lumped together with the out-of-range value when the algorithm tries to find a good place to split. For example, the minimum known value could get lumped with your out-of-range value (your known 0's get lumped with the -1's that you used to replace your nan values). So your model could change depending on if your out-of-range value is less than the minimum or if it's greater than the maximum (it could get lumped in with the minimum value or maximum value, respectively).Replacing a missing value with a mean/median/other stat may not solve the problem as the fact that the value is missing may be significant. For example in a survey on physical characteristics a respondent may not put their height if they were embarrassed about being abnormally tall or small. This would imply that missing values indicate the respondent was unusually tall or small - the opposite of the median value.What is necessary is a model that has a separate rule for missing values, any attempt to guess the missing value will likely reduce the predictive power of the model. is another python machine learning library that has facilities dedicated to imputation. I have not had a chance to use them, but might be soon, since the simple methods of replacing nan's with zeros, averages, or medians all have significant problems."},
{"body": "I'm trying to figure out a way of deleting (dynamically) subplots in matplotlib. I see they have a  method, but I get the errorI'm surprised that I can't find this anywhere.  Does anyone know how to do this?Wow, ok well I feel really stupid :PIn case anyone else needs it."},
{"body": "In Python 2 there was an error when return was together with yield in function definition. But for this code in Python 3.3there is no error that return is used in function with yield. However when the function  is called then there is thrown exception StopIteration. Why there is not just returned value ? Is this return somehow ignored?This is a new feature in Python 3.3 (as a comment notes, it doesn't even work in 3.2). Much like  in a generator has long been equivalent to ,  in a generator is now equivalent to . For that reason, the exception you're seeing should be printed as , and the value is accessible through the attribute  on the exception object. If the generator is delegated to using the (also new)  syntax, it is the result. See  for details.This prints , but not .The return value is not ignored, but generators only  values, a  just ends the generator, in this case early. Advancing the generator never reaches the  statement in that case.Whenever a iterator reaches the 'end' of the values to yield, a   be raised. Generators are no exception. As of Python 3.3 however, any  expression becomes the value of the exception:Use the  function to advance iterators, instead of calling  directly:"},
{"body": "I can not save the image in this ImageField.You seem to be missing the  argument to the serializer constructor in the your  and  handlers.I think you can use  instead after . The usage of  and  is now pending deprecation in favor of a single  attribute that contains all the parsed data.You can check it from Uploading image files with Django Rest Framework:Hope it helps someone.Following should work if you are posting the image\nas base64 string and your serializer set accordingly and it inherits serializer.Serializer "},
{"body": "I want to implement a function that gives information about all the tables (and their column names) that are present in a database (not only those created with SQLAlchemy). While reading the documentation it seems to me that this is done via reflection but I didn't manage to get something working. Any suggestions or examples on how to do this?start with an engine:quick path to all table /column names, use an inspector:docs: alternatively, use MetaData / Tables:docs: Hey I created a small module that helps easily reflecting all tables in a database you connect to with SQLAlchemy, give it a look: "},
{"body": "What would be an easy expression to process command line arguments if I'm expecting anything like 001 or 999 (let's limit expectations to 001...999 range for this time), and few other arguments passed, and would like to ignore any unexpected?I understand if for example I need to find out if \"debug\" was passed among parameters it'll be something like that:How to find out if 009 or 575 was passed?All those are expected calls:At this point I don't care about calls like that:...first one - because of more than one \"numeric\" argument; second - because of... well, unexpected arguments; third and fourth - because of non-3-digits arguments.As others answered, optparse is the best option, but if you just want quick code try something like this:: Here's an optparse example because so many people are answering optparse without really explaining why, or explaining what you have to change to make it work.The primary reason to use optparse is it gives you more flexibility for expansion later, and gives you more flexibility on the command line.  In other words, your options can appear in any order and usage messages are generated automatically.  However to make it work with optparse you need to change your specifications to put '-' or '--' in front of the optional arguments and you need to allow all the arguments to be in any order.So here's an example using optparse:The differences here with optparse and your spec is that now you can have command lines like:and you can easily add new options by calling parser.add_option()Have a look at the  module.  Dealing with sys.argv yourself is fine for really simple stuff, but it gets out of hand quickly.Note that you may find optparse easier to use if you can change your argument format a little; e.g. replace  with  and  with  or . is your best friend for parsing the command line. Also look into ; it's not in the standard library, though.If you want to implement actual command line switches, give  a look. It's incredibly simple to use, too.Van Gale is largely correct in using the regular expression against the argument.  However, it is NOT absolutely necessary to make everything an option when using optparse, which splits sys.argv into options and arguments, based on whether a \"-\" or \"--\" is in front or not.  Some example code to go through just the arguments:Yes, the args array is parsed much the same way as sys.argv would be, but the ability to easily add options if needed has been added.  For more about optparse, check out the  ."},
{"body": "My deployment script overwrites the media and source directories which means I have to move the uploads directory out of the media directory, and replace it after the upload has been extracted.How can I instruct django to upload to /uploads/ instead of /media/?So far I keep getting django Suspicious Operation errors! :(I suppose another solution might be a symlink?Many thanks,\nToby.I did the following: is defined in my  file: "},
{"body": "There is a snippet of code that I would like to copy and paste into my Python interpreter. Unfortunately due to Python's sensitivity to whitespace it is not straightforward to copy and paste it a way that makes sense. (I think the whitespace gets mangled) Is there a better way? Maybe I can load the snippet from a file. This is just an small example but if there is a lot of code I would like to avoid typing everything from the definition of the function or copy and pasting line by line. You can call execfile(filename). More or less the same as importing a module, except that it skips the module administration part and doesn't require you to add a folder to sys.path.EDIT: To address the original question: copy-pasted code can be executed by calling exec(codestring).You can usually easily and safely do copy-pasting with , through the commands  and .  This is very handy for testing code that you copy from web pages, for instance, or from your editor.  IPython also has a  command that runs a program and leaves you in a Python shell with all the variables that were defined in the program, so that you can play with them.In order to get help on these functions: , etc.You can just import the file into the python interpreter. This will load the class in, and allow you to run the code.For instance, create a file named \"bgcolors.py\" and copy and paste your code inside. Then using the python interpreter, you just type \"import bgcolors\" and you should be able to run it.You can read more here:You can use  which is much better python repl. It has command for getting input from  by using %edit command.  allows you to copy and paste code with proper indentation.The  interface does go to effort to preserve the proper indentation of pasted text. You can simply convert all tabs to spaces and remove  empty lines.\nSo you will be able to paste any code to python console (e.g.: python2.6)There is an inbuilt method call \"indent region & dedent region\" and you can just use it. After you paste a lot of code at once you can select them all and adjust the whitespace.My answer is specifically about copy-pasting into the standard python shell (only tested on linux).Depending on where the code comes from and how it is originally formatted  may or may not matter. In particular about your example snippet - copy-pasted from SO's code-formatted section - it doesn't matter (assuming the code is properly indented to be executable)., however, does cause trouble in the standard python interpreter because it normally is . In your snippet's case the empty line preceeding the  function definition ends/exits the class definition prematurely, so when the  definition line comes in an indentation error is detected:So you just need to pay attention at those empty lines. Your snippet needs just 2 multi-line copy-paste ops to work around that empty line.The only other thing I needed - for copy-pasting just sections of already indented code (say functions from inside classes) - one extra level of indentation to not need to re-do the indentation of the copied code. For that a leading  line prior to pasting the snippet and an  (i.e. empty line) after do the trick:I had this problem recently and ultimately just needed to change my editor's indentation setting from tabs to spaces. (I was running the interpreter using the OSX Terminal.) Once I did this, copy & paste worked fine."},
{"body": "Say I have a dictionary like so:Is there a way that I can switch the keys and values to get:If you are using python 2.7 or 3.x you can use a dictionary comprehension instead:EditAs noted in the comments by JBernardo, for python 3.x you need to use  instead of Use this code (trivially modified) from the accepted answer at :Note that this assumes that the values in the original dictionary are unique.  Otherwise you'd end up with duplicate keys in the resulting dictionary, and that is not allowed.  And, as @wim points out, it also assumes the values are hashable.  See  if you're not sure what is and isn't hashable.Try this:maybe:Sometimes, the condition that the values are all unique will not hold, in which case, the answers above will destroy any duplicate values.The following rolls the values that might be duplicates up into a list:I'm sure there's a better (non-list-comp) method, but I had a problem with the earlier answers, so thought I'd provide my solution in case others have a similar use-case. P.S. Don't expect the dict to remain neatly arranged after any changes to the original! This method is a one-time use only on a static dict - you have been warned!"},
{"body": "This should be easy but I have just started toying with matplotlib and python. I can do a line or a scatter plot but i am not sure how to do a simple step function. Any help is much appreciated.It seems like you want .E.g. If you have non-uniformly spaced data points, you can use the  keyword argument for :Also available are  and .Just draw two lines, one at y=0, and one at y=1, cutting off at whatever  your step function is for?e.g. if you want to step from 0 to 1 at  and plot from  to :I think you want  or equally 's bar method. if not checkout the  for the many styles of plots you can do. Each image comes with example code showing you how to make it using matplotlib.In case someone just wants to  rather than actually plot it:"},
{"body": "Is it somehow possible to have certain output appear in a different color in the IPython Notebook?\nFor example, something along the lines of:The notebook has, of course, its own syntax highlighting. So I would be careful when using colour elsewhere, just to avoid making things harder to read for yourself or someone else (e.g., output should simply be in black, but you get parts in red if there is an exception).But (to my surprise), it appears that you can use ANSI escape codes (even in the browser). At least, I could:On the default Python prompt:In the notebook:(Obviously, I cheated here with the syntax highlighting of SO so that \"red\" is printed in the colour red in both examples. I don't think SO allows a user to set a colour for text.)I wouldn't really know another way to get colours.For more on ANSI escape codes, I'd suggest the . And if you find the above to verbose, you can of course write a wrapper function around this.Not with raw Python print. You will have to define a  on an object and return it  or call . I guess you could overwrite the built-in print to do it automatically...You could inspire from , code , ML discussion .you can use this library  and you can get all other official libraries of python in pypiSee the documentation in pypi.python.org or follow these steps\n1. pip install termcolor\n2. then goto ipython\ntypeThe first argument is what you want to print on console and second argument use that colorHere's a quick hack:[out]:If you just want a single color:  "},
{"body": "If I have 1000+ pdf files need to be merged into one pdf,Execute the above code\uff0cwhen ,An error message\uff1a\nI think this is a bug, If not, What should I do\uff1fI recently came across this exact same problem, so I dug into PyPDF2 to see what's going on, and how to resolve it.Use the  class instead of the  class.  I've tried to provide the following to as closely resemble your content as I could:The way you're using  and  is keeping each file open, and eventually causing Python to generate IOError 24.  To be more specific, when you add a page to the , you are adding references to the page in the open  (hence the noted IO Error if you close the file).  Python detects the file to still be referenced and doesn't do any garbage collection / automatic file closing despite re-using the file handle.  They remain open until  no longer needs access to them, which is at  in your code.To solve this, create copies in memory of the content, and allow the file to be closed.  I noticed in my adventures through the PyPDF2 code that the  class already has this functionality, so instead of re-inventing the wheel, I opted to use it instead.  I learned, though, that my initial look at  wasn't close enough, and that it only created copies .My initial attempts looked like the following, and were resulting in the same IO Problems:Looking at the PyPDF2 source code, we see that  requires  to be passed, and then uses the  function, passing in it's last page as the new files position.  does the following with  (before opening it with :We can see that the  option does accept a string, and when doing so, assumes it's a file path and creates a file object at that location.  The end result is the exact same thing we're trying to avoid.  A  object holding open a file until the file is eventually written!However, if we either make a file object of the file path string  a  object of the path string  it gets passed into , it will automatically create a copy for us as a  object, allowing Python to close the file.I would recommend the simpler , as others have reported that a  object may stay open in memory, even after calling .Hope this helped! I assumed you were using , not .  If you aren't, I highly recommend switching, as PyPDF is no longer maintained with the author giving his official blessings to Phaseit in developing PyPDF2.  If for some reason you cannot swap to PyPDF2 (licensing, system restrictions, etc.) than  won't be available to you.  In that situation you can re-use the code from PyPDF2's  function (provided above) to create a copy of the file as a  object, and use that in your code in place of the file object.  Previous recommendation of using  changed based on comments .The pdfrw package reads each file all in one go, so will not suffer from the problem of too many open files.   is an example concatenation script.The relevant part -- assumes  is a list of input filenames, and  is an output file name:Disclaimer:  I am the primary pdfrw author.It maybe just what it says, you are opening to many files.\nYou may explicitly use  in the loop, or use the  statement. So that each opened file is properly closed. The problem is that you are only allowed to have a certain number of files open at any given time. There are ways to change this (), but I don't think you need this.What you could try is closing the files in the for loop:"},
{"body": "If I catch a , how can I tell what lookup failed?Take the current exception (I used it  in this case); then for a  the first argument is the key that raised the exception. Therefore we can do:With that, you have the offending key stored in .It should be noted that  works in Python 2 but not Python 3, so it shouldn't be used.Not sure if you're using any modules to assist you - if the JSON is coming in as a dict, one can use  towards a useful end. takes two arguments - the first being the  you want, the second being the value to return if that key does not exist.If you import the  module you can get exception info with like this:"},
{"body": "I have a pandas dataframe with mixed type columns, and I'd like to apply sklearn's min_max_scaler to some of the columns.  Ideally, I'd like to do these transformations in place, but haven't figured out a way to do that yet.  I've written the following code that works:I'm curious if this is the preferred/most efficient way to do this transformation.  Is there a way I could use df.apply that would be better?  I'm also surprised I can't get the following code to work:If I pass an entire dataframe to the scaler it works:I'm confused why passing a series to the scaler fails.  In my full working code above I had hoped to just pass a series to the scaler then set the dataframe column = to the scaled series.  I've seen this question asked a few other places, but haven't found a good answer.  Any help understanding what's going on here would be greatly appreciated!I am not sure if previous versions of  prevented this but now the following snippet works perfectly for me and produces exactly what you want without having to use Like this?As it is being mentioned in pir's comment - the  method will produce the following warning:Converting your columns to numpy arrays should do the job (I prefer StandardScaler):You can do it using   only:"},
{"body": "I am trying to use some AOP in my Python programming, but I do not have any experience of the various libraries that exist. So my question are:I've found some, but I don't know how they compare:In which context will I use these? I have two applications, written in Python, which have typically methods which compute taxes and other money things. I'd like to be able to write a \"skeleton\" of a functionality, and customize it at runtime, for example changing the way local taxes are applied (by country, or state, or city, etc.) without having to overload the full stack.Another AOP library for python would be . It is currently the most powerful (as far as I know).Its features are:It also has other goodies such as some special descriptors (see the documentation)See S.Lott's link about Python decorators for some great examples, and see the .Python had AOP since the beginning, it just didn't have an impressive name.\nIn Python 2.4 the decorator syntax was added, which makes applying decorators very nice syntactically.Maybe if you want to apply decorators based on rules you would need a library, but if you're willing to mark the relevant functions/methods when you declare them you probably don't.Here's an example for a simple caching decorator (I wrote it for ):Using annotations is not really AOP, because the weaving process is somewhat hard-coded.There are several AOP frameworks in Python (I counted and compared 8 of them, of which  was the clear winner).I'm going to publish a paper with my findings on one of the next conferences, including a real-life industry use case.In Python, aspect-oriented programming typically consists of dynamically modifying classes and instances at runtime, which is commonly referred to as monkeypatching.  In an answer to another AOP question, I summarized some of these .I'd start with the .  Much of that is AOP kind of stuff.  What about the BSD-licensed ?"},
{"body": "How do I set the timezone of a datetime instance that just came out of the datastore?When it first comes out it is in UTC.  I want to change it to EST.I'm trying, for example:When a Book is retrieved, I want to set its tzinfo immediately:Where  for my EST objectHowever I get:I've seen a number of answers that recommend pytz and python-dateutil, but I really want an answer to this question.'s objects are immutable, so you never change any of their attributes -- you make a  object with some attributes the same, and some different, and assign it to whatever you need to assign it to.I.e., in your case, instead ofyou have to codeIf you're receiving a datetime that's  EST, but doesn't have its tzinfo field set, use  to assign a tzinfo without modifying the time.  (Your database should be doing this for you.)If you're receiving a datetime that's in UDT, and you want it in EST, then you need astimezone.  In the vast majority of cases, your database should be storing and returning data in UDT, and you shouldn't need to use replace (except possibly to assign a UDT tzinfo).What you want is right there in the .output:"},
{"body": "If I have a string like:and I want:What do the replacement tokens have to be? (I know that my example above is incorrect; I'm just trying to express my goal.)is another true but long answer. Just to show you another viewpoint about the issue ;)Python 3 has exactly that syntax, except the  operator is now the  method.  has also been added to Python 2.6+ to smooth the transition to Python 3. See  for more details.It cannot be done with a tuple in older versions of Python, though. You can get close by using mapping keys enclosed in parentheses. With mapping keys the format values must be passed in as a dict instead of a tuple.From the :"},
{"body": "Do I need to add a db_index to \"content\"? Or would that automatically be indexed because it's a foreign key?Unless specified otherwise, an index will be created for a .  Relevant source code:"},
{"body": "In Python 2.7, dictionaries have both an  method and a  method (and similar pairs for values and items), giving two different ways to lazily iterate over the keys of the dictionary. The  method provides the principal feature of , with  effectively equivalent to . Additionally, objects returned  have convenient set-like features. There thus are strong reasons to favor  over . What about the other direction? Apart from compatibility with earlier versions of Python, are there any ways in which  would be preferable to ? Would anything be lost by just always using ? No, there's no advantage to  over , in the same way that there's no advantage to  over either of them.  is only around for back compatibility. Indeed, in Python 3,  is the only behaviour that still exists, and it has been renamed to  - the  method is actually a backport of the Python 3 behaviour.A dictionary view updates as the dictionary does, while an iterator does not necessarily do this.This means if you work with the view, change the dictionary, then work with the view again, the view will have changed to reflect the dictionary's new state.Example:When changes are made to the size, an exception will be thrown:It's also worth noting you can only iterate over a keyiterator once:Functionality-wise, as you have observed, views are better. Compatibility-wise, they're worse.Some performance metrics, taken from Python 2.7.2 on a 64-bit Ubuntu machine:Dealing with an empty dictionary:Constructing the view is slightly more expensive, but consuming the view is significantly faster than the iterator (a bit over twice as fast).Dealing with a thousand-element dictionary:Same results, though less marked; constructing the view is very slightly more expensive, but consuming it is quite definitely faster (15% faster).For fair comparison with  and ,  is distinctlyfaster: it's a trade-off; better functionality (which you will rarely use) and performance (which will only  rarely be significant enough to worry you\u2014if you're caring about such performance matters, you're probably already in the region of needing to work with numpy/scipy) versus better compatibility and muscle memory usage.Personally, unless already depending on 2.7-only functionality or unless I am absolutely controlling the runtime environment, I would avoid dictionary views in Python 2 code. Even in these cases, my fingers still want to type  instead of , so I let 'em!As the name (and ) indicate, the ,  and  methods return a  of the current elements in the dictionary meaning that if the dictionary changes, so does the view; views  . In the general case  views are set-like, and  views are only set-like if the values are hashable.In what cases would be better to use the standard methods ,  and  ? You mentioned a very important one: backwards compatibility. Also, when you need to have a simple list of all keys, values or items  (not a set-like, not an iterator), when you need to modify the returned list without modifying the original dictionary and when you need a snapshot of a dictionary's keys, values or items at a moment in time, independent of any posterior modifications over the dictionary.And what about ,  and ? They're a suitable alternative when you need a one-shot, constant-space, lazy iterator snapshot of a dictionary's contents that will tell you if the dictionary got modified while iterating (via a ), also they're very important for backwards compatibility."},
{"body": "The move in recent versions of Python to passing a  function to  from the previous  function is making it trickier for me to perform complex sorts on certain objects.For example, I want to sort a set of objects from newest to oldest, with a set of string tie-breaker fields. So I want the dates in reverse order but the strings in their natural order. With a comparison function I can just reverse the comparison for the date field compared to the string fields. But with a key function I need to find some way to invert/reverse either the dates or the strings.It's easy (although ugly) to do with numbers - just subtract them from something - but do I have to find a similar hack for dates (subtract them from another date and compare the timedeltas?) and strings (...I have no idea how I'd reverse their order in a locale-independent way).I know of the existence of  but it is described as being . This implies that I should be able to do what I want with the key method - but how?The slow-but-elegant way to do this is to create a value wrapper that has reversed ordering:If you don't have , you'd have to implement all 6 comparisons, e.g.:The most generic way to do this is simply to sort separately by each key in turn. Python's sorting is always stable so it is safe to do this:will (assuming the relevant definitions for the key functions) give you the data sorted by descending date and ascending tiebreakers.Note that doing it this way is slower than producing a single composite key function because you will end up doing two complete sorts, so if you can produce a composite key that will be better, but splitting it out into separate sorts gives a lot of flexibility: given a key function for each column you can make any combination of them and specify reverse for any individual column.For a completely generic option:and for completeness, though I really think it should be avoided where possible:The reason I think you should avoid this you go back to having  calls to the comparison function compared with  calls to the key function (or  calls when you do the sorts twice).I think the docs are incomplete.  I interpret the word \"primarily\" to mean that there are still reasons to use cmp_to_key, and this is one of them.   was removed because it was an \"attractive nuisance:\" people would gravitate to it, even though  was a better choice.But your case is clearly better as a  function, so use  to implement it.Sort twice, once on each key and once reversed.(Python  is ; that is, it doesn't change the order of the original list unless it has to.)For String, you can use some commonly acknowledged maximum value (such as 2^16 or 2^32) and use chr(), unicode(), ord() to do the math, just like for integers.In one of my work, I know I deal with strings in utf8 and their ordinals are below 0xffff, so I wrote:x is of type: (string, int), so what I get is, to abuse the SQL:"},
{"body": "I'm currently learning Python and Classes and I have a basic question, but I didn't find any answer to it. Let's say I have this dummy classI want to run simultaneously resolve_domain and generate_website_thumbnail and when the threads are finished I want to print the IP and the thumbnail.  I know I should use threads, maybe something like thisBut should I use them outside the Class? Should I write another Class to handle Threads?What is the  to do that?If you call them from the class, it is as simple as:"},
{"body": "Before I ask,  will be my last options, this script will be used across Windows and Linux and I'd prefer to have a coded out method of doing this than leaving this to the end user to complete.Is there a library for Python that I can use to schedule tasks? I will need to run a function once every hour, however, over time if I run a script once every hour and use .sleep, \"once every hour\" will run at a different part of the hour from the previous day due to the delay inherent to executing/running the script and/or function.What is the  way to schedule a function to run at a specific time of day (more than once)  using a Cron Job or scheduling it with Task Scheduler?out:(From Animesh Pandey's answer below)Maybe this can help: Here's a small piece of code from their documentation:To run something every 10 minutes past the hour.  For  < 3.0, see .For  > 3.0One option is to write a C/C++ wrapper that executes the python script on a regular basis. Your end-user would run the C/C++ executable, which would remain running in the background, and periodically execute the python script. This may not be the best solution, and may not work if you don't know C/C++ or want to keep this 100% python. But it does seem like the most user-friendly approach, since people are used to clicking on executables. All of this assumes that python is installed on your end user's computer.Another option is to use cron job/Task Scheduler but to put it in the installer as a script so your end user doesn't have to do it.The Python standard library does provide  and  for this task. But this means your scheduler script will have be running all the time instead of leaving its execution to the OS, which may or may not be what you want.On the version posted by sunshinekitty called \"Version < 3.0\" , you may need to specify apscheduler 2.1.2 .  I accidentally had version 3 on my 2.7 install, so I went:It worked correctly after that.  Hope that helps."},
{"body": "I try to extract all files from .zip containing subfolders in one folder.  I want all the files from subfolders extract in only one folder without keeping the original structure. At the moment, I extract all, move the files to a folder, then remove previous subfolders.  The files with same names are overwrited.Is it possible to do it before writing files?Here is a structure for example:At the end I whish this:What can I add to this code ?if I rename files path from zip_file.namelist(), I have this error:This opens file handles of members of the zip archive, extracts the filename and copies it to a target file (that's how  works, without taken care of subdirectories).Just extract to bytes in memory,compute the filename, and write it there yourself,\ninstead of letting the library do it - -mostly, just use the \"read()\" instead of \"extract()\" method:"},
{"body": "I'm trying to write a short program that will read in the contents of e-mails within a folder on my exchange/Outlook profile so I can manipulate the data. However I'm having a problem finding much information about python and exchange/Outlook integration. A lot of stuff is either very old/has no docs/not explained. I've tried several snippets but seem to be getting the same errors. I've tried Tim Golden's code:However I get an error:Not sure what my profile name is so I tried with:to be prompted but that didn't work either (same error). Also tried both with Outlook open and closed and neither changed anything.I had the same problem you did - didn't find much that worked. The following code, however, works like a charm.I have created my own iterator to iterate over Outlook objects via python. The issue is that python tries to iterates starting with Index[0], but outlook expects for first item Index[1]...  To make it more Ruby simple, there is below a helper class Oli with following\nmethods:  .items() - yields a tuple(index, Item)... \n.prop() - helping to introspect outlook object exposing available properties (methods and attributes)    \nI had the same issue. Combining various approaches from the internet (and above) come up with the following approach (checkEmails.py)For concistency I include also the code for the FileWriter class (found in FileWrapper.py). I needed this because \ntrying to pipe UTF8 to a file in windows did not work.Enjoy."},
{"body": "I have a Django webapp. I have installed the debug_toolbar middleware and module.\nHowever, my webapps don't have the debug toolbar pull-out.How do I actually see the debug toolbar? Is there something more I need to do?\nDo I need to use a particular template for my webapp? I have followed all the steps in the README, but that is not enough -- there seems to be some other dependency, or something else I'm missing.Also, when looking at the set of URL patterns for my webapp, the  prefix is  found among the recognized patterns. I've put a log in urls.py in debug_toolbar to make sure that modules is getting loaded by the activated debug_toolbar application, and it is.This has me totally mystified, and I can find no Google or README on what to do to make this actually show up, or what the requirements are, so any pointer you can provide would be great!Edit: It turns out, I was testing this with a SSH tunnel from the machine running the browser to the machine running the Django/Apache. In this case, the IP address actually seen for the remote machine was not what I thought it was, so the list of \"good\" IPs did not contain the browser's apparent remote machine. Fixing that fixed the problem!(Edit note: lapis updated the configs above to match the names used by the current (at the time of this update, 1.3.2) version of the Django Debug Toolbar.  Per , the original versions (that used e.g. debug_toolbar.panels.sql.SQLDebugPanel vs debug_toolbar.panels.sql.SQLPanel as in 1.3.2) were correct when this question was original answered.)(note: after Django 1.10,  should be .)"},
{"body": "I want to get the directory where the file resides. For example the full path is:I could do it like this:But the example above relies on specific directory separator and is not really pretty. Is there a better way?You are looking for this:Related functions:"},
{"body": "I have a 2D array containing integers (both positive or negative). Each row represents the values over time for a particular spatial site, whereas each column represents values for various spatial sites for a given time.So if the array is like:The result should beNote that when there are multiple values for mode, any one (selected randomly) may be set as mode.I can iterate over the columns finding mode one at a time but I was hoping numpy might have some in-built function to do that. Or if there is a trick to find that efficiently without looping.Check  (inspired by @tom10's comment):Output:As you can see, it returns both the mode as well as the counts. You can select the modes directly via :Output:This is a tricky problem, since there is not much out there to calculate mode along an axis.  The solution is straight forward for 1-D arrays, where  is handy, along with  with the  arg as .  The most common n-dimensional function I see is scipy.stats.mode, although it is prohibitively slow- especially for large arrays with many unique values.  As a solution, I've developed this function, and use it heavily:Result:Some benchmarks:EDIT: Provided more of a background and modified the approach to be more memory-efficientExpanding on , applied to finding the mode of the data where you may need the index of the actual array to see how far away the value is from the center of the distribution.Remember to discard the mode when len(np.argmax(counts)) > 1, also to validate if it is actually representative of the central distribution of your data you may check whether it falls inside your standard deviation interval."},
{"body": "I've done some searching and can't figure out how to filter a dataframe by , however I'm wondering if there is a way to do the reverse: filter a dataframe by that set's compliment. eg: to the effect of . Can this be done through a  method?You can use the invert (~) operator (which acts like a not for boolean data):I was having trouble with the not (~) symbol as well, so here's another way from another :I had to get rid of the NULL values before using the command recommended by Andy above. An example:Now running the command:I get the following error:I got rid of the NULL values using dropna() or fillna() first and retried the command with no problem."},
{"body": " has been released, and I am wondering, if a similar solution is available, or coming for Python?RubyMotion isn't a new cross-platform framework (like Kivy, Papaya, Rhodes, etc.) but rather an implementation of the Ruby programming language ON TOP OF the native iOS language Objective-C. Because of this, a RubyMotion object  an Obj-C Object, but you get to use the dynamic Ruby syntax and idioms to deal with it, instead of the static-oriented Objective-C approaches. You must still learn the Apple SDK, you just talk to it with \"toll-free\" Ruby code. With Titatnium, for example, you learn that platform's APIs which work on multiple OSes, albeit at a cost of UI mismatches and performance penalties.MacRuby is the desktop progenitor of RubyMotion, and the Python analogue to it would be PyObjC. The  for PyObjC looks dormant to me (last updated in late 2009) so someone would have to first revive it, and then make similar modifications (replacing garbage-collection style memory management with some form of automatic reference counting and binary compilation) in order to build a similar Python solution for iOS.I wouldn't expect it to drop out of the sky anytime soon, but surprises are always welcome.Depends what you are after and what for. Papaya do what they call a social engine (for games really). But you can write with Python for that. "},
{"body": "I'm working on a Python script that uses the scissor character (9986 - \u2702) and I'm trying to port my code to Mac, but I'm running into this error.The scissor character shows up fine when run from IDLE (Python 3.2.5 - OS X 10.4.11 iBook G4 PPC) and the code works entirely fine on Ubuntu 13.10, but when I attempt to run this in the terminal I get this error/traceback:and the code that is giving me the problem: Doesn't this signal that the terminal doesn't have the capability to display that character? I know this is an old system, but it is currently the only system I have to use. Could the age of the OS is interfering with the program?I've read over these questions:What's causing this error? Is it the age of the system/OS, the version of Python, or some programming error?:\nThis error crops up later with this duplicate issue (just thought I'd add it as it is within the same program and is the same error): I went into the terminal character settings and it does in fact support that character (as you can see in this screenshot:when I insert it into terminal it prints out this:  and when I press  I get this:  Ran commands as @J.F. Sebastian asked::: Tried the \"hackerish\" solution provided by @PauloBu:As you can see, this caused one (Yay!) scissor, but I am now getting a new error. Traceback/error: Added results of @PauloBu's fix::And his fix for his fix:When Python prints and output, it automatically encodes it to the target medium. If it is a file, UTF-8 will be used as default and everyone will be happy, but if it is a terminal, Python will figure out the encoding the terminal is using and will try to encode the output using that one.This means that if your terminal is using  as encoding, Python is trying to encode  char to ascii. Of course, ascii doesn't support it so you get Unicode decode error.This is why . Explicit is better than implicit remember? To fix your code you may do:This seems a bit hackerish. You can also set PYTHONIOENCODING=utf-8 before executing the script. I'am uncomfortable with both solutions. Probably your console doesn't support utf-8 and you see gibberish. But your program will be behaving correctly.What I strongly recommend if you  need to show correct output on your console is to set your console to use another encoding, one that support  character. (utf-8 perhaps). On Linux, that can be achieve by doing: . On Windows you change the console's code page with . Just figure out how to set utf8 in yours and IMHO that'll be the best solution.I suggest you to take a read at the docs to see what's going on under the hood with  function and with : Hope this helps! output suggests that you should change your  settings e.g., set .The first error might be due to you are trying to decode a string that is already Unicode. Python 2 tries to encode it using a default character encoding ()  decoding it using (possibly) different character encoding. The error happens on the  step:It looks like you are running your script using Python 2 instead of Python 3. You would get:different error otherwise.Just drop the  call:The second issue is due to printing a Unicode string into a pipe:Set appropriate for your purpose  environment variable:If  is a  object then leave the  calls. The fix is the same:My locale is set to de_AT.UTF-8 but these lines in \nwere missing:logout / login and your problem should be solvedTo verify if all locales are set correctly type  in your terminalThe output should be similar to this:in the first line of your file .py you need to add this string, :# --and you can also try this:print (\"|\\t \",unichr(9986),\"PySnipt'd\",unichr(9986),\"\\t|\")"},
{"body": "In Python, I know that the value  returns for a given object is supposed to be the same for the lifetime of that object. But, out of curiosity, what happens if it isn't? What sort of havoc would this cause?I know  and  would behave strangely, and dicts and sets would act odd because of that. You also might end up with \"orphaned\" values in the dict/set.What else could happen? Could it crash the interpreter, or corrupt internal structures?Your main problem would indeed be with dicts and sets. If you insert an object into a dict/set, and that object's hash changes, then when you try to retrieve that object you will end up looking in a  spot in the dict/set's underlying array and hence won't find the object. This is precisely why dict keys should always be immutable.Here's a small example: let's say we put  into a dict, and 's initial hash is 3. We would do something like this (a slight simplification but gets the point across):Now let's say the hash of  changes to . If we want to retrieve  from the dict, we'll look at spot , but there's nothing there! This will cause a false negative when querying the data structure. In reality, each element of the array above could have a \"value\" associated with it in the case of a dict, and there could be multiple elements in a single spot (e.g. a ). Also, we'd generally take the hash value modulo the size of the array when deciding where to put the element. Irrespective of all these details, though, the example above still accurately conveys what could go wrong when the hash code of an object changes. No, this won't happen. When we say an object's hash changing is \"dangerous\", we mean dangerous in the sense that it essentially defeats the purpose of hashing and makes the code difficult if not impossible to reason about. We don't mean dangerous in the sense that it could cause crashes.There's a great post on Github about it: . \nFirst, you need to know that Python expects (quoted from the article): Here's the code example which show the problem of a variant hash, with a slightly different class example, but the idea remains the same: but unexpected behavior will happen with dict/set and everything based on object hash."},
{"body": "For example, I may use  or  or just , in which case the default compiler (say, ) will be used. How can I get the compiler name inside my setup.py (e. g. ,  and , respectively)?UPD.: I don't need the default compiler, I need the one that is  going to be used, which is not necessarily the default one. So far I haven't found a better way than to parse  to see if there's a  string there.This is an expanded version of Luper Rouch's answer that worked for me to get an openmp extension to compile using both mingw and msvc on windows. After subclassing build_ext you need to pass it to setup.py in the cmdclass arg. By subclassing build_extensions instead of finalize_options you'll have the actual compiler object to look into, so you can then get more detailed version information. You could eventually set compiler flags on a per-compiler, per-extension basis:You can subclass the  command.Once  method has been called, the compiler type is stored in  as a string (the same as the one passed to the 's  option, e.g. 'mingw32', 'gcc', etc...).import distutils.ccompilercompiler_name = distutils.ccompiler.get_default_compiler()You can use self.distribution.get_command_obj('build_ext') to get build_ext instance,\nand then get the compiler_type"},
{"body": "Can anyone suggest any good payment processing libraries for python/django?The most developed Django solution is  with support for Authorize.Net, TrustCommerce, CyberSource, PayPal, Google Checkout, and Protx.The new kid on the Django block is  which looks like only support for PayPal at the moment, and even that may not be complete.For general Python the main player is There is  which currently supports only PayPal. It can be used with any existing application without changing it's code. Basically, it can use  as order.Perhaps you can find some usefull code hints/modules looking at Satchmo: \nDjango paypal is very cool. I've used in on couple of my projects. It is relatively easy to integrate with an existing website. \nSatchmo is good, if you want a full internet store, but if you want to sell just couple items from your website, which is devoted to something else, you will find Satchmo to be very heavy (a lot of dependencies to install, really complicates your admin). You may want to take a look at  which isn't django-specific but works nicely with it or in plain ole python.I created Paython: Supports a few different processors:"},
{"body": "I'm trying to generate an XML document with namespaces, currently with Python's xml.dom.minidom:The namespace is saved ( is ), but why is it missing in the output?I expect:or is defined as:so\u2026yields:\u2026not quite there\u2026yields:alternatively:wich produces:It looks like you'd have to do it manually.  shows no indication that namespaces would get any special treatment.EDIT: This answer is targeted at  only, since the OP used it in the question. I do not indicate that it was impossible to use XML namespaces in Python generally. ;-)This feature is already proposed; a patch is . See Tomalak's answer (in short: Manually add the  attribute) for a workaround."},
{"body": "I'm working on several Python projects who run on various versions of Python. I'm hoping to set up my vim environment to use ropevim, pyflakes, and pylint but I've run into some issues caused by using a single vim (compiled for a specific version of Python which doesn't match the project's Python version).I'm hoping to build vim into each of my virtualenv directories but I've run into an issue and I can't get it to work. When I try to build vim from source, despite specifying the Python config folder in my virtualenv, the system-wide Python interpreter is always used.Currently, I have Python 2.6.2 and Python 2.7.1 installed with several virtualenvs created from each version. I'm using Ubuntu 10.04 where the system-default Python is 2.6.5. Every time I compile vim and call  it returns .Results in the following in config.log:It should be . Is there any way to specify the Python interpreter for vim to use?NOTE: I can confirm that /virtualenv/project/bin appears at the front of  environment variable.I'd recommend building vim against the 2 interpreters, then invoking it using the shell script I provided below to point it to a particular virtualenv.I was able to build vim against Python 2.7 using the following command (2.7 is installed under $HOME/root):Your virtualenv is actually a thin wrapper around the Python interpreter it was created with --   is a symlink to .So if you created it with the system interpreter, VIM will probe for this and ultimately link against the real interpreter, using the system  by default, even though configure will show the virtualenv's path:: Since your system vim is most likely compiled against your system python, you don't need to rebuild vim for each virtualenv: you can just drop a shell script named  in your virtualenv's bin directory, which extends the PYTHONPATH before calling system vim:Contents of :When that is invoked, the virtualenv's sys.path is inserted:For what it's worth, and no one seems to have answered this here, I had some luck using a command line like the following:vi_cv_path_python=/usr/bin/python26 ./configure --includedir=/usr/include/python2.6/  --prefix=/home/bcrowder/local --with-features=huge --enable-rubyinterp --enable-pythoninterp --disable-selinux --with-python-config-dir=/usr/lib64/python2.6/configI would like to give a similar solution to crowder's that works quite well for me.Imagine you have Python installed in /opt/Python-2.7.5 and that the structure of that folder isand you would like to build vim with that version of Python. All you need to do isThus, just by explicitly giving  variable to  the script will deduce everything on it's own (even the config-dir).This was tested multiple times on vim 7.4+ and lately with .I was having this same issue with 3 different versions of python on my system.for me the easiest thing was to change my  env variable so that the folder that has the version of python I wanted was (in my case ) was found before another."},
{"body": "Should be a simple question, but I'm unable to find an answer anywhere. The  operator in python is a documented as a bitwise inversion operator.  Fine. I have noticed seemingly schizophrenic behavior though, to wit:In the first 4 examples, I can see that python is implementing (as documented) , with the input treated as an int . Hence, for a scalar boolean,  is not treated as a logical negation. Not that the behavior is identical on a numpy array defined with boolean values by with an int type.Why does  then work as a logical negation operator on a boolean array (Also notice: ?)?It is extremely annoying that I must use  on a scalar, but  won't work to negate an array. Then for an array, I must use , but  won't work to negate a scalar... is implemented through the  special method, which is required to return either  or , so it can't give the required result.  Instead the  operator is used, which is implemented through the  special method.  For the same reason,  and  are used in place of  and . aimed to allow overloading of boolean operators but was rejected because of excessive overhead (it would e.g. complicate  statements).   suggests a general syntax for \"elementwise\" operators, which would provide a more general solution, but has been deferred.  It appears that the current situation, while awkward, is not painful enough to force change. when called on a scalar returns a value of type , not .   is also the type you get when extracting a scalar value from an array of bool dtype.  If you use  and  in place of  and  you will get consistent behaviour under ."},
{"body": "How do I transform a directed acyclic graph into a hash value such that any two isomorphic graphs hash to the same value?  It is acceptable, but undesirable for two isomorphic graphs to hash to different values, which is what I have done in the code below.  We can assume that the number of vertices in the graph is at most 11.I am particularly interested in Python code.Here is what I did.  If  is a mapping from node to descendants (not children!), then I relabel the nodes according to a modified topological sort (that prefers to order elements with more descendants first if it can).  Then, I hash the sorted dictionary.  Some isomorphic graphs will hash to different values, especially as the number of nodes grows.I have included all the code to motivate my use case.  I am calculating the number of comparisons required to find the median of 7 numbers.  The more that isomorphic graphs hash to the same value the less work that has to be redone.  I considered putting larger connected components first, but didn't see how to do that quickly.To effectively test for graph isomorphism you will want to use . Specifically for Python there is the wrapper , but I can't attest its quality (to compile it correctly I had to do some simple patching on its ). If this wrapper is doing everything correctly, then it simplifies nauty a lot for the uses you are interested and it is only a matter of hashing  -- which will be the same value for isomorphic graphs.Some quick tests showed that  is giving the same certificate for every graph (with same amount of vertices). But that is only because of a minor issue in the wrapper when converting the graph to nauty's format. After fixing this, it works for me (I also used the graphs at  for comparison). Here is the short patch which also considers the modifications needed in : Therefore there is currently no known (worst case sub-exponential) solution to guarantee that two isomorphic directed acyclic graphs will yield the same hash. Only if the mapping between different graphs is known - for example if all vertices have unique labels - one could efficiently guarantee matching hashes.Okay, let's brute force this for a small number of vertices. We have to find a representation of the graph that is independent of the ordering of the vertices in the input and therefore guarantees that isomorphic graphs yield the same representation. Further this representation must ensure that no two non-isomorphic graphs yield the same representation.The simplest solution is to construct the adjacency matrix for all n! permutations of the vertices and just interpret the adjacency matrix as n bit integer. Then we can just pick the smallest or largest of this numbers as canonical representation. This number completely encodes the graph and therefore ensures that no two non-isomorphic graphs yield the same number - one could consider this function a . And because we choose the smallest or largest number encoding the graph under all possible permutations of the vertices we further ensure that isomorphic graphs yield the same representation.How good or bad is this in the case of 11 vertices? Well, the representation will have 121 bits. We can reduce this by 11 bits because the diagonal representing loops will be all zeros in an acyclic graph and are left with 110 bits. This number could in theory be decreased further; not all 2 remaining graphs are acyclic and for each graph there may be up to 11! - roughly 2 - isomorphic representations but in practice this might be quite hard to do. Does anybody know how to compute the number of distinct directed acyclic graphs with n vertices?How long will it take to find this representation? Naively 11! or 39,916,800 iterations. This is not nothing and probably already impractical but I did not implement and test it. But we can probably speed this up a bit. If we interpret the adjacency matrix as integer by concatenating the rows from top to bottom left to right we want many ones (zeros) at the left of the first row to obtain a large (small) number. Therefore we pick as first vertex the one (or one of the vertices) with largest (smallest) degree (indegree or outdegree depending on the representation) and than vertices connected (not connected) to this vertex in subsequent positions to bring the ones (zeros) to the left.There are likely more possibilities to prune the search space but I am not sure if there are enough to make this a practical solution. Maybe there are or maybe somebody else can at least build something upon this idea.How good does the hash have to be? I assume that you do  want a full serialization of the graph. A hash rarely guarantees that there is no second (but different) element (graph) that evaluates to the same hash. If it is very important to you, that isomorphic graphs (in different representations) have the same hash, then only use values that are invariant under a change of representation. E.g.:All these informations can be gathered in O(#nodes) [assuming that the graph is stored properly]. Concatenate them and you have a hash. If you prefer you can use some well known hash algorithm like  on these concatenated informations. Without additional hashing it is a  (it allows to find similar graphs), with additional hashing it is  and fixed in size if the chosen hash algorithm has these properties.As it is, it is already good enough to register any added or removed connection. It might miss connections that were changed though ( instead of ).This approach is modular and can be extended as far as you like. Any additional property that is being included will reduce the number of collisions but increase the effort necessary to get the hash value. Some more ideas:You requested \"a unique hash value\" and clearly I cannot offer you one. But I see the terms \"hash\" and \"unique to every graph\" as mutually exclusive (not entirely true of course) and decided to answer the \"hash\" part and not the \"unique\" part. A \"unique hash\" () basically needs to be a full serialization of the graph (because the amount of information stored in the hash has to reflect the total amount of information in the graph). If that is really what you want just define some unique order of nodes (eg. sorted by own outdegree, then indegree, then outdegree of children and so on until the order is unambiguous) and serialize the graph in any way (using the position in the formentioned ordering as index to the nodes).Of course this is much more complex though.Imho, If the graph could be topologically sorted, the very straightforward solution  exists.I will describe an algorithm to hash an arbitrary directed graph, not taking into account that the graph is acyclic. In fact even counting the acyclic graphs of a given order is a very complicated task and I believe here this will only make the hashing significantly more complicated and thus slower. A unique representation of the graph can be given by the neighbourhood list. For each vertex create a list with all it's neighbours. Write all the lists one after the other appending the number of neighbours for each list to the front. Also keep the neighbours sorted in ascending order to make the representation unique for each graph. So for example assume you have the graph:What I propose is that you transform this to , here the curly brackets being only to visualize the representation, not part of the python's syntax. So the list is in fact: .Now to compute the unique hash, you need to order these representations somehow and assign a unique number to them. I suggest you do something like a lexicographical sort to do that. Lets assume you have two sequences  and  , Here c and a are the number of neighbours for each vertex and b_i_j and d_k_l are the corresponding neighbours. For the ordering first compare the sequnces  and  and if they are different use this to compare the sequences. If these sequences are different, compare the lists from left to right first comparing lexicographically  to  and so on until the first missmatch.In fact what I propose to use as hash the lexicographical number of a word of size  over the alphabet that is formed by all possible selections of subsets of elements of . The neighbourhood list for a given vertex is a letter over this alphabet e.g.  is the subset consisting of two elements of the set, namely  and .The (set of possible ) for the set  would be(ordered ): First number like above is the number of elements in the given subset and the remaining numbers- the subset itself. So form all the 3  from this alphabet and you will get all the possible directed graphs with 3 vertices.Now the number of subsets of the set  is  and thus the number of  of this alphabet is . Now we code each directed graph of  nodes with a  with exactly   from this  and thus the number of possible hash codes is precisely: . This is to show that the hash code grows  fast with the increase of . Also this is the number of possible different directed graphs with  nodes so what I suggest is optimal hashing in the sense it is bijection and no smaller hash can be unique. There is a linear algorithm to get a given subset number in the the lexicographical ordering of all subsets of a given set, in this case . Here is the code I have written for coding/decoding a subset in number and vice versa. It is written in  but quite easy to understand I hope. For the hashing you will need only the code function but as the hash I propose is reversable I add the decode function - you will be able to reconstruct the graph from the hash which is quite cool I think:Also this code stores the result in  variable,  which is only enough for graphs with less than 64 elements. All possible hashes of graphs with 64 nodes will be . This number has about 1280  so maybe is a big number. Still the algorithm I describe will work really fast and I believe you should be able to hash and 'unhash' graphs with a lot of vertices.Also have a look at .I'm not sure that it's 100% working, but here is an idea:Let's code a graph into a string and then take its hash.To produce the same hash for isomorphic graphs before concatenation in step3 just sort the hashes (e.g. in lexicographical order).For hash of a graph just take hash of its root (or sorted concatenation, if there are several roots). While I hoped that the resulting string will describe graph without collisions,  found that sometimes non-isomorphic graphs will get the same hash. That happens when a vertex has several parents - then it \"duplicated\" for every parent. For example, the algorithm does not differentiate a \"diamond\" {A->B->C,A->D->C} from the case {A->B->C,A->D->E}.I'm not familiar with Python and it's hard for me to understand how graph stored in the example, but here is some code in C++ which is likely convertible to Python easily:I am assuming there are no common labels on vertices or edges, for then you could put the graph in a canonical form, which itself would be a perfect hash.  This proposal is therefore based on isomorphism only.For this, combine hashes for as many simple aggregate characteristics of a DAG as you can imagine, picking those that are quick to compute.  Here is a starter list:\nLet me be more explicit.  For 1, we'd compute a set of triples  (where no two triples have the same , values), signifying that there are  nodes with in-degree  and out-degree .  You'd hash this set of triples or better yet use the whole set arranged in some canonical order e.g. lexicographically sorted.  For 2, we compute a set of quintuples  signifying that there are  edges from nodes with in degree  and out degree , to nodes with  and  respectively.  Again hash these quintuples or else use them in canonical order as-is for another part of the final hash.Starting with this and then looking at collisions that still occur will probably provide insights on how to get better.When I saw the question, I had essentially the same idea as @example. I wrote a function providing a graph tag such that the tag coincides for two isomorphic graphs. This tag consists of the sequence of out-degrees in ascending order. You can hash this tag with the string hash function of your choice to obtain a hash of the graph. I expressed my proposal in the context of @NeilG's original question. The only modification to make to his code is to redefine the  function as:Years ago, I created a simple and flexible algorithm for exactly this problem (finding duplicate structures in a database of chemical structures by hashing them).I named it \"Powerhash\", and to create the algorithm it required two insights. The first is the power iteration graph algorithm, also used in PageRank. The second is the ability to replace power iteration's inside step function with anything that we want. I replaced it with a function that does the following on each step, and for each node:On the first step, a node's hash is affected by its direct neighbors. On the second step, a node's hash is affected by the neighborhood 2-hops away from it. On the Nth step a node's hash will be affected by the neighborhood N-hops around it. So you only need to continue running the Powerhash for N = graph_radius steps. In the end, the graph center node's hash will have been affected by the whole graph.To produce the final hash, sort the final step's node hashes and concatenate them together. After that, you can compare the final hashes to find if two graphs are isomorphic. If you have labels, then add them in the internal hashes that you calculate for each node (and at each step).For more on this you can look at my post here:The algorithm above was implemented inside the \"madIS\" functional relational database. You can find the source code of the algorithm here:With suitable ordering of your descendents (and if you have a single root node, not a given, but with suitable ordering (maybe by including a virtual root node)), the method for hashing a tree ought to work with a slight modification.Example code in , the modification would be to sort children in some deterministic order (increasing hash?) before hashing the parent.Even if you have multiple possible roots, you can create a synthetic single root, with all roots as children."},
{"body": "I managed to override the look of a  Widget in the django admin interface with two different ways:in :This way is a bit of an overkill, since it will change all the TextField for that\nmodel.in :in Both solutions resize the . However in both solutions the actual size of the \ntext area is more than 1 row (actually 2 rows). Here is how the rendered HTML looks like:And here is a screentshot:According to :So, my questions are: This is a browser-specific problem.According to the thread :You can set a  attribute of the textarea:Works for me - tested on Firefox v. 23 and Chrome v. 29.Hope that helps."},
{"body": "I have a dataframe where some cells contain lists of multiple values. Rather than storing multiple\nvalues in a cell, I'd like to expand the dataframe so that each item in the list gets its own row (with the same values in all other columns). So if I have:How do I convert to long form, e.g.:The index is not important, it's OK to set existing\ncolumns as the index and the final ordering isn't\nimportant.A bit longer than I expected:If you want sequential index, you can apply  to the result.:you can also use  and  for this:last, if you need you can sort base on the first the first three columns.Trying to work through Roman Pekar's solution step-by-step to understand it better, I came up with my own solution, which uses  to avoid some of the confusing stacking and index resetting. I can't say that it's obviously a clearer solution though:Output (obviously we can drop the original samples column now):"},
{"body": "Dependency analysis programs help us organize code by controlling the dependencies between modules in our code. When one module is a circular dependency of another module, it is a clue to find a way to turn that into a unidirectional dependency or merge two modules into one module.What is the best dependency analysis tool for Python code?I recommend using  for creating graphical dependency graphs of Python projects. It detects dependencies nicely enough to immediately see areas for refactorisation. Its usage is pretty straightforward if you read a little bit of documentation.Of course, you can omit the graph-creation step and receive a dependency dictionary in a file instead.I don't know what is the  dependency analysis tool.  You could look into  \u2013 it's a module in the standard library that determines the set of modules imported by a script.Of course, with python you have problems of conditional imports, and even potentially scripts calling  directly, so it may not find everything.  This is why tools like py2exe need special help to cope with packages like PIL.PyStructure \u2013 Automated Structure and Dependency Analysis of Python CodeThis is used for PyDev's refactoring features. I'm not sure about the best tool, but dependency analysis is best done using extracting and scanning the python files of the project.Here is one such tool using the above technique : "},
{"body": "How can I turn the above into a function where I pass the name of the class?\nIn the above example, Subscriber is a class name. I want to do something like this: Thanks,\nNeal Walters If you pass the class object directly, as in your code between \"like this\" and \"or\",\nyou can get its name as the  attribute.Starting with the name (as in your code  \"or\") makes it REALLY hard (and not unambiguous) to retrieve the class object unless you have some indication about where the class object may be contained -- so why not pass the class object instead?!A slight variation of Ned's code that I used.  This is a web application, \nso I start it by running the get routine via a URL: .  I didn't see the need of the . Output: "},
{"body": "I have tens of millions of rows to transfer from multidimensional array files into a PostgreSQL database. My tools are Python and psycopg2. The most efficient way to bulk instert data is using . However, my data are mostly 32-bit floating point numbers (real or float4), so I'd rather not convert from real \u2192 text \u2192 real. Here is an example database DDL:Here is where I'm at with Python using strings (text):Is there an equivalent that could work using a binary mode? I.e., keep the floating point numbers in binary? Not only would this preserve the floating point precision, but it could be faster.(Note: to see the same precision as the example, use )Here is the binary equivalent of COPY FROM for Python 3:I rewrote the above approach to writing the files for COPY. My data in Python is in NumPy arrays, so it makes sense to use these. Here is some example  with with 1M rows, 7 columns:On my database, I have two tables that look like:and another similar table named .Here are some simple helper functions to prepare the data for COPY (both text and binary formats) by using the information in the NumPy record array:This how I'm using the helper functions to benchmark the two COPY format methods:Here is the output from the last two  commands:So both the NumPy \u2192 file and file \u2192 database steps are way faster with the binary approach. The obvious difference is how Python prepares the COPY file, which is really slow for text. Generally speaking, the binary format loads into the database in 2/3 of the time as the text format for this schema.Lastly, I compared the values in both tables within the database to see if the numbers were different. About 1.46% of the rows have different values for column , and this fraction increases to 6.17% for  (probably related on the random method that I used). The non-zero absolute differences between all 70M 32-bit float values range between 9.3132257e-010 and 7.6293945e-006. These small differences between the text and binary loading methods are due to the loss of precision from the float \u2192 text \u2192 float conversions required for the text format method. is my version. Based on Mike's version. Its very ad-hoc but there are two pros:"},
{"body": "I've recently started experimenting with using Python for web development. So far I've had some success using Apache with mod_wsgi and the Django web framework for Python 2.7. However I have run into some issues with having processes constantly running, updating information and such.I have written a script I call \"daemonManager.py\" that can start and stop all or individual python update loops (Should I call them Daemons?). It does that by forking, then loading the module for the specific functions it should run and starting an infinite loop. It saves a PID file in  to keep track of the process. So far so good. The problems I've encountered are:Any useful answer on how to  run infinite Python processes, hopefully also shedding some light on the above problems, I will acceptI'm using Apache 2.2.14 on an Ubuntu machine.\nMy Python version is 2.7.2I'll open by stating that this is  way to manage a long running process (LRP) -- not de facto by any stretch.In my experience, the best possible product comes from concentrating on the specific problem you're dealing with, while delegating supporting tech to other libraries. In this case, I'm referring to the act of backgrounding processes (the art of the double fork), monitoring, and log redirection. My favorite solution is Using a system like supervisord, you basically write a conventional python script that performs a task while stuck in an \"infinite\" loop.Writing your script this way makes it simple and convenient to develop and debug (you can easily start/stop it in a terminal, watching the log output as events unfold). When it comes time to throw into production, you simply define a supervisor config that calls your script (here's the  example for defining a \"program\", much of which is optional: ).Supervisor has  of configuration options so I won't enumerate them, but I will say that it specifically solves the problems you describe:I assume you are running Unix/Linux but you don't really say. I have no direct advice on your issue. So I don't expect to be the \"right\" answer to this question. But there is something to explore here.First, if your daemons are crashing, you should fix that. Only programs with bugs should crash. Perhaps you should launch them under a debugger and see what happens when they crash (if that's possible). Do you have any trace logging in these processes? If not, add them. That might help diagnose your crash.Second, are your daemons providing services (opening pipes and waiting for requests) or are they performing periodic cleanup? If they are periodic cleanup processes you should use cron to launch them periodically rather then have them run in an infinite loop. Cron processes should be preferred over daemon processes. Similarly, if they are services that open ports and service requests, have you considered making them work with INETD? Again, a single daemon (inetd) should be preferred to a bunch of daemon processes.Third, saving a PID in a file is not very effective, as you've discovered. Perhaps a shared IPC, like a semaphore, would work better. I don't have any details here though.Fourth, sometimes I need stuff to run in the context of the website. I use a cron process that calls wget with a maintenance URL. You set a special cookie and include the cookie info in with wget command line. If the special cookie doesn't exist, return 403 rather than performing the maintenance process. The other benefit here is login to the database and other environmental concerns of avoided since the code that serves normal web pages are serving the maintenance process.Hope that gives you ideas. I think avoiding daemons if you can is the best place to start. If you can run your python within mod_wsgi that saves you having to support multiple \"environments\". Debugging a process that fails after running for days at a time is just brutal.You should consider Python processes as able to run \"forever\" assuming you don't have any memory leaks in your program, the Python interpreter, or any of the Python libraries / modules that you are using. (Even in the face of memory leaks, you might be able to run forever if you have sufficient swap space on a 64-bit machine. Decades, if not centuries, should be doable. I've had Python processes survive just fine for nearly two years on limited hardware -- before the hardware needed to be moved.)Ensuring programs restart when they die used to be very simple back when Linux distributions used  -- you just add a new line to the  and  would spawn your program at boot and re-spawn it if it dies. (I know of no mechanism to replicate this functionality with the new  -replacement that many distributions are using these days. I'm not saying it is impossible, I just don't know how to do it.)But even the  mechanism of years gone by wasn't as flexible as some would have liked. The  package by DJB is one example of process control-and-monitoring tools intended to keep daemons living forever. The  suite provides another similar tool, though it might provide too much \"extra\" functionality to be justified for this task.  is another option."},
{"body": "Here is the thing, I have a proxy holding the reference to a remote module, and I put some of these proxies to the  such that I can use it just like local modules. But some other objects are put in the  module at the remote environment (like a magic variable for convenience of debugging or referencing). I don't want to reference these vars like , and I have to either replace the local  (which seems not working for replace  or to hook the global name finding rules.\nHow? For a module you can just overload a  to do this. But in a interactive interpreter like , who is the main module or how to do this?   As pointed out by @Nizam Mohamed, yes I can get the  module, but still I can't modify the name lookup role of it.   For now I just iterate all the  and if there is a name that isn't in the local . I add the name to local's . But it's not so dynamic compare to a name lookup rule say if I can't find the name in local  try the remote one. is a similar discussion.And  question gives a simulation of module by replace it with a object in . But this won't work for  name lookup, I've also tried to replace the  with a custom one that will first use the original lookup followed by a custom one when failed. But global name lookup of  never called into the  even  returns the desired value,  or  never returns one.As @asmeurer said, you can write a simple AST transformer to \"hook\" the variable name lookup. The base class  provide a  method that you can manipulate. You just need to overload this method to redefine those variables existing in the remote module but not locally.The following module can be used as an :testAST.pydummyModule.pyUsage:The benefit of this method is that any modification to the remote module takes effect immediately because the lookup is done in the language level and no object is copied and cached.One drawback of this method is not being able to handle dynamic lookup. If that's important for you, maybe the  hook is more suitable.There is a way to get a list of all names the module will use. Although it does not modify the lookup mechanism, I believe it solves your problem.Here is some code (hopefully understandable enough) you can put in a module, let's call it :You can then import it to do magic:There are two downsides, however. First, dynamic lookups will fails:Although that particular case can be solved by looking at the strings in , it will not work with more advanced dynamic lookups like The second downside is that it will not work in functions:This is because in that case, the name  is in \u00a0instead of the module's .One possible solution would be to modify  to try to find all functions of the module and alter their code, but it won't work with functions defined inside functions, etc.An other solution is to call  in the function:"},
{"body": "So I have a few Python C extensions I have previously built for and used in 32 bit Python running in Win7. I have now however switched to 64 bit Python, and I am having issues building the C extension with MinGW-w64.I made the changes to distutils as per , but I am getting some weird errors suggesting something is wrong:I have googled around quite a bit to find information, but it's not easy to find a definite answer. Could someone shed some light on this? What further changes should I do to be able to successfully build C extensions for 64 bit Python in Win7?EDIT:After some helpful pointers in cgohlke's comments below I managed to generate . However after following the advice on  (2nd to last) I still had a the  error. After some serious Google-fu I managed to trip over  telling me to rename the  line to . After that everything worked swimmingly.This worked for me with Python 3.3 :voila!"},
{"body": "Experimenting with some code and doing some microbenchmarks I just found out that using the  function on a string containing an integer number is a factor 2 faster than using  on the same string.It gets even stranger when testing  which runtime is shorter than the bare .I tested the code under Windows 7 running cPython 2.7.6 and Linux Mint 16 with cPython 2.7.6. I have to add that only Python 2 is affected, Python 3 shows a way smaller (not remarkable) difference between the runtimes.I know that the information I get by such microbenchmarks are easy to misuse, but I'm curious why there is such a difference in the functions' runtime.I tried to find the implementations of  and  but I can not find it in the sources. has lots of bases.*, 0*, 0x*, 0b*, 0o* and it can be long, it takes time to determine the base and other thingsif the base is set, it saves a lot of timeas @Martijn Pieters metions the code the  and "},
{"body": "The setuptools documentation only states:In practical terms, what is the performance benefit gained? Is it worth investigating if my projects are zip-safe, or are the benefits generally minimal?Zip files take up less space on disk, which also means they're more quickly read from disk. Since most things are I/O bound, the overhead in decompressing the packaging may be less than the overhead in reading a larger file from disk. Moreover, it's likely that a single, small-ish zip file is stored sequentially on disk, while  a collection of smaller files may be more spread out. On rotational media, this also increases read performance by cutting down the number of seeks. So you generally optimize your disk usage at the cost of some CPU time, which may dramatically improve your  and load times.There are several advantages, in addition to the ones already mentioned.Reading a  large .egg file (and unzipping it) may be significantly faster than loading multiple (potentially a lot of) smaller .py files, depending on the storage medium/filesystem on which it resides.Some filesystem have a large block size (e.g., 1MB), which means that dealing with small files can be expensive. Even though your files are small (say, 10KB), you may actually be loading a 1MB block from disk when reading it. Typically, filesystems combine multiple small files in a large block to mitigate this a bit.On filesystems where access to file metadata is slow (which sometimes happens with shared filesystems, like NFS), accessing a large amount of files may be very expensive too.Of course, zipping the whole bunch also helps, since that means that less data will have to be read in total.Long story short: it may matter a lot if your filesystem is more suited for a small amount of large files."},
{"body": "Is there a way to use  to organize multiple files into a ?Reason:  Modules are easier to use than packages, because they don't have as many layers of namespace.Normally it makes a package, this I get.  Problem is with a package, 'import thepackage' gives me an empty namespace.  Users must then either use \"from thepackage import *\" (frowned upon) or know exactly what is contained and manually pull it out into a usable namespace.What I want to have is the user do 'import thepackage' and have nice clean namespaces that look like this, exposing functions and classes relevant to the project for use.The maintainer's job would be to avoid defining the same name in different files, which should be easy when the project is small like mine is.It would also be nice if people can do  and have it retrieve the class, rather than a module containing the class.This is easy if all my code is in one gigantic file, but I like to organize when things start getting big. What I have on disk looks sort of like this:I only separate them so my files aren't huge and unnavigable.  They are all related, though someone (possible me) may want to use the classes by themselves without importing everything.I've read a number of suggestions in various threads, here's what happens for each suggestion I can find for how to do this:If I , I cannot import anything because Python doesn't descend into the folder from sys.path.If I , when I  it's an empty namespace with nothing in it. None of my files imported, which makes it more difficult to use.If I , I can use the (frowned upon?)  syntax, but all of my classes are behind unnecessary namespace barriers again. The user has to (1) know they should use  instead of , (2) manually reshuffle classes until they can reasonably obey line width style constraints.If I , I get closer but I have namespace conflicts (?) and extra namespaces for things I didn't want to be in there.  In the below example, you'll see that:.Also someone importing just the data abstraction class would get something different than they expect when they do 'from doit_tools import JobInfo':So, is this just a wrong way to organize Python code? If not, what is a correct way to split related code up but still collect it in a module-like way?Maybe the best case scenario is that doing 'from doit_tools import JobInfo' is a little confusing for someone using the package?Maybe a python file called 'api' so that people using the code do the following?:============================================Examples in response to comments:Take the following package contents, inside folder 'foo' which is in python path.Do this: and  are there cluttering things up, and also have extra copies of e.g. doit() underneath their namespaces.If I have a class named Data inside a file named Data.py, when I do 'from Data import Data' then I get a namespace conflict because Data is a class in the current namespace that is inside module Data, somehow is also in the current namespace.  (But Python seems to be able to handle this.)You can sort of do it, but it's not really a good idea and you're fighting against the way Python modules/packages are supposed to work. By importing appropriate names in  you can make them accessible in the package namespace.  By deleting module names you can make them inaccessible.  (For why you need to delete them, see ).  So you can get close to what you want with something like this (in ):However, this will break subsequent attempts to .  In general, you can't import anything from a  without making  accessible as an importable reference to that module (although with the  you can block ).More generally, by splitting up your code by class/function you are working against the Python package/module system.  A Python module should generally contain stuff you want to import as a unit.  It's not uncommon to import submodule components directly in the top-level package namespace for convenience, but the reverse --- trying to hide the submodules and allow access to their contents  through the top-level package namespace --- is going to lead to problems.  In addition, there is nothing to be gained by trying to \"cleanse\" the package namespace of the modules.  Those modules are supposed to be in the package namespace; that's where they belong.Define  in the  e.g.:Real-world example: .You have some misconception about how Python packages work:You need  file in Python versions older than Python 3.3 to mark a directory as containing a Python package.It doesn't prevent the import:It works as expected.(1) Your users (in most cases) should  use  outside an interactive Python shell.(2) you could use  to break a long import line:It is upto you to resolve namespace conflicts (different objects with the same name). The name can refer to any object: integer, string, package, module, class, functions, etc. Python can't know what object you might prefer and even if it could it would be inconsistent to ignore some name bindings in this particular case with respect to the usage of name bindings in all other cases.To mark names as non-public you could prefix them with  e.g., .python is not java. Module file name does not need to be the same as class name. In fact python recommend using all lower case for module file name.Also \"from math import sqrt\" will only add sqrt to namespace, not math.There are perfectly valid reasons to hide the sub-structure of a package (not only when debugging). Amongst them are  and . When trying to do a rapid prototype with a package it is extremely annoying having to interrupt the train of thought just to look up the utterly useless information what the exact sub-module for a specific function or class might be.When everything is available at the top level of a package, The idiom:displays the , not just some measly module names.You can always turn off sub-module imports for production code, or to clean up the package modules after development.The following is the best way I have come up with so far. It maximizes convenience while trying not to suppress valid errors. See also the .Define package name and sub-modules to be imported to avoid error-prone duplication:Use relative imports when available (this is imperative, see ):Try importing the package, including .\nThis happens when executing a module file as script with the\npackage in the search path (e.g. )As a last resort, try importing the sub-modules directly.\nThis happens when executing a module file as script inside the\npackage directory without the package in the search path\n(e.g. ).Construct  (leaving out modules), unless it has been imported\nbefore:Here is the function :"},
{"body": "On Ubuntu 16.04 with virtualenv 15.0.1 and Python 3.5.2 (both installed with ) when I create and activate new Python virtual environment withI get the following output:Indeed  lists these 4 packages:Though, I'd expect  (without ) to omit these implicitly installed packages. It does omit some of them, but not :(Same btw. for )While this is consistent with the help texthaving  in the  output doesn't seem very useful and might even be harmful. (I suspect it's why running  from  uninstalls pkg-resources from the virtual environment, subtly breaking the environment thereby.)  As far as I remember, it didn't list it on Ubuntu 14.04 (with Python 3.4).According to , this is a bug resulting from Ubuntu providing incorrect metadata to pip. So, no there does not seem to be a good reason for this behaviour. I filed a follow-up bug with Ubuntu. I had the same problem.\nwith commands It just worked."},
{"body": "I have two zip files, both of them open well with Windows Explorer and 7-zip. However when i open them with Python's zipfile module [ zipfile.ZipFile(\"filex.zip\") ], one of them gets opened but the other one gives error \"\".I've made sure that the latter one is a valid Zip File by opening it with 7-Zip and looking at its properties (says 7Zip.ZIP). When I open the file with a text editor, the first two characters are \"PK\", showing that it is indeed a zip file.I'm using Python 2.5 and really don't have any clue how to go about for this. I've tried it both with Windows as well as Ubuntu and problem exists on both platforms. Traceback from Python 2.5.4 on Windows:Basically when the  function is called for getting data from End of Central Directory\" record, the comment length checkout fails [ endrec[7] == len(comment) ]. The values of locals in the  function are as following:files named file can confuse python - try naming it something else. if it  wont work, try this code:I run into the same issue. My problem was that it was a gzip instead of a zip file. I switched to the class  and it worked like a charm.astronautlevel's solution works for most cases, but the compressed data and CRCs in the Zip can also contain the same 4 bytes. You should do an  (not ), seek to pos+20 and then add write  to the end of the file (tell zip applications that the length of the 'comments' section is 0 bytes long).Show the full traceback that you got from Python -- this may give a hint as to what the specific problem is.  What software produced the bad file, and on what platform?Update: Traceback indicates having problem detecting the \"End of Central Directory\" record in the file -- see function _EndRecData starting at line 128 of C:\\Python25\\Lib\\zipfile.pySuggestions:\n(1) Trace through the above function\n(2) Try it on the latest Python\n(3) Answer the question above.\n(4) Read  and anything else found by  that appears to be relevantI had the same problem and was able to solve this issue for my files, see my answer at \nHave you tried a newer python, or if that is too much trouble, simply a newer zipfile.py? I have successfully used a copy of zipfile.py from Python 2.6.2 (latest at the time) with Python 2.5 in order to open some zip files that weren't supported by Py2.5s zipfile module."},
{"body": "I would like to put some logging statements within test function to examine some state variables.I have the following code snippet:I get the following output:Notice that only the logging messages from the  block get transmitted to the console.Is there a way to force pytest to emit logging to console from test methods as well?Works for me, here's the output I get: [snip -> example was incorrect]Edit: It seems that you have to pass the  option to py.test so it won't capture stdout. Here (py.test not installed), it was enough to use  .For your code, all you need is to pass  in  to :See the py.test documentation on ."},
{"body": "I noticed Pandas now has .  Currently, I create s like this:Is there a way to create a  with a  or ? Converting to dense format kills RAM badly. Thanks!A direct conversion is not supported ATM. Contributions are welcome!Try this, should be ok on memory as the SpareSeries is much like a csc_matrix (for 1 column)\nand pretty space efficientAs of pandas v 0.20.0 you can use the  constructor.An example from :A much shorter version:"},
{"body": "Python is a \"whitespace delimited\" language.  However, the use of semicolons  allowed.  For example, the following works but is frowned upon:I've been using python for several years now, and the only time I have ever used semicolons is in generating one-time command-line scripts with python:or adding code in comments on SO (i.e. \"you should try \")I also noticed in  that the semicolon can also be used to make one line  blocks, as inwhich is convenient for the two usage examples I gave (command-line scripts and comments).The above examples are for communicating code in paragraph form or making short snippets, but not something I would expect in a production codebase.  Here is my question: in python, is there ever a reason to use the semicolon in a production code?  I imagine that they were added to the language solely for the reasons I have cited, but its always possible that Guido had a grander scheme in mind.  No opinions please; I'm looking either for examples from existing code where the semicolon was useful, or some kind of statement from the python docs or from Guido about the use of the semicolon. is the official style guide and says:(See also the examples just after this in the PEP.)While I don't agree with everything PEP 8 says, if you're looking for an authoritative source, that's it.  You should use multi-statement lines only as a last resort.  ( is a good example of such a last resort, because you have no way to use actual linebreaks in that case.)I use semicolons in code all of the time. Our code folds across lines quite frequently, and a semicolon is a definitive assertion that a statement is ending.See? They're quite helpful to readability."},
{"body": "model example)If 72 and '2011-08-07' is already storedraises the question is why  raises the , thats the idea of using \n.Not sure if this is a bug, I opened a ticket It appears your problem is with there being more columns you're not including in your , see i.e.  on a Django mailing list.You need to use the  parameter of  as described in the , or specify values for all columns, for  to match correctly."},
{"body": "I've started working on a project with loads of unused legacy code in it.  I was wondering if it might be possible to use a tool like coverage in combination with a crawler (like the django-test-utils one) to help me locate code which isn't getting hit which we can mark with deprecation warnings.  I realise that something like this won't be foolproof but thought it might help.I've tried running coverage.py with the django debug server but it doesn't work correctly (it seems to just profile the runserver machinery rather than my views, etc).We're improving our test coverage all the time but there's a way to go and I thought there might be a quicker way.Any thoughts?Thanks.You can run the development server under coverage if you use the --noreload switch: is great tool for static code analysis (among others things it will detect unused imports, variables or arguments)."},
{"body": "I'm familiar with  C function. I've been using this function for many purpouses. Most of them, if not all, for reading and writting to pipes, files, etc... I must say that I've never used the error list, but this is not involved in the key question.Does python  behaves as the following?It turns out to me that  on python behaves a different way despite the  interface to C . It seems that  returns the very first time a file is ready for reading. If you read the file letting being bytes on its queue, calling  will block. But, if you call  again after a previous call to  was returned without any read call between these two calls,  will return as expected. For example:If this is the behaviour of  in python, I'm okay with that, I could handle it. Not what I expected though, but it's fine, I know what I can do with it.But, if this is not the behaviour of  I would appreciate someone to tell me what I'm doing wrong. What I read about  is what the python doc says: \"select() returns if any file in the read|write|error list is ready for read|write|error.\". That's ok, no lies there. Maybe the questions should be: Python's  gets passed through as a  system call as you are expecting, but the problem you have with it blocking is a different issue, probably relating to buffering.  Just to satify yourself that  is doing the right thing, try reading/writing a file on the file system rather than using a special device such as a joystick.You probably want to change your  call.  Pythons  call will by default use buffered reads, so even if you do a  it will likely read more data from the input file and buffer the results.  You need to set the  option to  so that the joystick device is opened unbuffered.Suggestions for you to try:Can I ask a stupid question - are you sure there are really 8 bytes left?Device nodes don't necessarily behave like normal files.  It might be that you have to read the entire struct input_event in a single read() system call.  (And if you don't read enough, the rest gets thrown away).  A bit like recvmsg() on datagram sockets."},
{"body": "There are lots of questions about using numpy in cython on this site, a particularly useful one being .However, the cython/numpy interface api , in particular with ensuring the passing of memory-contiguous arrays.What is the best way to write a wrapper function in cython that:My try is below:This code compiles but is not necessarily optimal. Do you have any suggestions on improving the snippet above?You've basically got it right. First, hopefully optimization shouldn't be a big deal. Ideally, most of the time is spent inside your C++ kernel, not in the cythnon wrapper code.There are a few stylistic changes you can make that will simplify your code. (1) Reshaping between 1D and 2D arrays is not necessary. When you know the memory layout of your data (C-order vs. fortran order, striding, etc), you can see the array as just a chunk of memory that you're going to index yourself in C++, so numpy's ndim doesn't matter on the C++ side -- it's just seeing that pointer. (2) Using cython's address-of operator , you can get the pointer to the start of the array in a little cleaner way -- no explicit cast necessary -- using .So this is my edited version of your original snippet:"},
{"body": "All the docs I've seen imply that you  be able to do that, but there isn't anything official w/r/t ulong64/uint64 fields. There are a few off-the-shelf options that look quite promising in this arena:... all of which  like they  store a number like that. Except NONE OF THEM WILL COMMIT, much like every single rom-com character portrayed by Hugh Grant. My primary criterion is that it works with Django's supported backends, without any  type of special-case nonsense. After that, there is the need for speed -- this is for a model field in an visual-database application that will index image-derived data (e.g. perceptual hashes and extracted keypoint features), allowing ordering and grouping by the content of those images.So: is there a good Django extension or app that furnishes some kind of  that will suit my purposes?And, barring that: If there is a simple and reliable way to use Django's stock ORM to store unsigned 64-bit ints, I'd like to know it. Look, I'm no binary whiz; I have to do two's complement on paper -- so if this method of yours involves some bit-shifting trickery, don't hesitate to explain what it is, even if it strikes you as obvious. Thanks in advance.Although I did not test it, but you may wish to just subclass . The original  looks like that ():Derived  may looks like this:Although you should test it thoroughly, before using it. If you do, please share the results :):I missed one thing - internal database representation. This is based on value returned by  and the definition of the column type is stored eg.  in case of MySQL backend and determined . It looks like overwriting  will give you control over how the field is represented in the database. However, you will need to find a way to return DBMS-specific value in  by checking  argument."},
{"body": "It seems common practice in Flask to start like this:And then import and use  and  everywhere.  But when you create  like this, it grabs configuration from the app, and it seems that this configuration can't ever be overridden once it happens.  There are some pages on Flask's website about making application factories, but it's not clear how I would be able to still use  and  everywhere if I did that.How do I write a script to test my Flask application with a different database?  How should I structure my application to make this possible?  Do I have to use s ?Your instinct to use environment variables is correct. However, there is some danger of running unit tests with the wrong db. Also, you may not want to  with every request and everywhere you want to use . You can use a config directory and environment variables which you set explicitly. This is the best I've come up with so far.so, the config files may be:andSo, I can explicitly set the db in my base TestCase:Then, the , for me:Then, in the other files, where I know what config I want to run, potentially:andMaybe .. best so far :P.You won't want to make connecting to the db happen at import time.  Go ahead and configure your app at import time because you can always tweak the configuration in your tests before attempting to test or run your app.  In the example below you'll have your db connection behind some functions that use the application config so in a unittest you can actually change the db connection to point to a different file and then go ahead and connect explicitly in your setup.Say you have a myapp package containing myapp.py which looks like:Your test file myapp/test_myapp.py will look like this:Of course if you'd like to use SQLAlchemy you'll have to update the connect_db and init_db functions appropriately but hopefully you get the idea.First, instead of instantiating Flask app directly in your script, you use an . It means you create a function that takes your config file as parameter, and return the instantiated app object. Then, you create the global SQLAlchemy object without parameter, and you configure it when creating the app, .To run the app, you simply do something like:To run unittests, you can do something like:In my case, I'm using SQLAlchemy directly with scoped_session instead of Flask-SQLAlchemy.\nI did the same, but with ."},
{"body": "I often find myself doing this:Is there a more concise way to do this in Python? I am thinking of something along the lines ofYou can use :If you've got  as a dependency already,  will do the trick ...Use :From the docs:It depends on what is inside the loop.  If dealing with lists, you may be able to use a For the more general case, see  on itertools. "},
{"body": "I'm probably doing something very stupid, but I'm stumped.I have a dataframe and I want to replace the values in a particular column that exceed a value with zero. I had thought this was a way of achieving this:If I copy the channel into a new data frame it's simple:this does exactly what I want, but seems not to work with the channel as part of the original dataframe.Thanks is advance.BenTry indexer works okay for pandas version prior to 0.20.0, but since pandas 0.20.0, the  indexer is , so you should avoid using it. Instead, you can use  or  indexers. In this case you can solve your problem by: helps you to select the rows in which  is , while  sets the value 0 to the selected rows where holds in the column which name is ."},
{"body": "Is there a standard body or a specific normative way how time-related things should be  (like ICU for Unicode-related tasks) or is this currently a \"best-effort\", depending on how much effort, time and money language and library implementers want to spend?Is there a specific and complete implementation which could serve as a example for how time-related things should be handled?Which existing library would you consider as a bad, a decent or a good example?These classes are a complete rewrite of JodaTime trying to fix the design flaws of / as well as JodaTime.JSR 310 tries to provide a comprehensive model for date and time, which is type-safe and self-documenting. It is interoperable with existing classes, but also considers XML- and DBMS-based use-cases.\nThe classes are final, immutable, thread-safe and cannot be modified after construction. Instances are created via a rich set of Factory methods which can cache things in the background.The API has some \"machine-oriented\" classes and some \"human-oriented\" classes:For a point of time comparable to an Unix or Java timestamp. Actually there are ,  and  which enable people to exactly choose which definition of time they need i. e. \"day-based\", \"linear, without leap seconds\" etc.A time range not necessarily associated with a specific Date or Calendar.There is a rich collection of classes handling different use-cases like Date-only, Time-only, Time and Date, with and without Timezones, with and without DST.,  (,  compatibility),  (,  compatibility), ,  (,  compatibility), ,  (,  compatibility)Periods represent a time span like \"5 days\" that can be added and subtracted from a date/time.Matchers enable queries like \"is this date in the year 2006?\" or \"is this day the last day of this year\".Adjusters come to the rescue if you have want to make more complex changes, like \"Give me the last day of the month!\" or \"The second Tuesday after Christmas, please!\".Resolvers allow users to define what should happen if a certain date is not valid, like February 31st 2010:It is possible to serialize these classes and deserialize them using either the current timezone data or the timezone data when they were serialized.\nAdditionally, rules from different timezones can be compared: It is possible to find out if DST rules have changed, e. g. between version 2010e and 2010f for Dates in London or Moscow and decide what should be done if a Time is in a gap or overlap.Although everything is based on , simple calendars for Hebrew, Hijrah, Japanese, ThaiBuddist, etc. time systems are provided. returns ISO8601 and patterns like those in  and more advanced are supported.\nThe first problem is that dates are not linked to time but to astronomical position of Earh, Moon, etc. + regularity/periodicity of human activity. The time is also subjective and relative or even relativistic and measured either astronomically or or atomically.    \n [4] has issued   which, like other international standards,  is recommendation and is based on already established practices.\nIt is (subjectively) based only on Gregorian Calendar [5] and on proleptic one (projected backwards to well before it was actually invented so is of limited use in dealing with historic dates) [5a].   The World Calendar Association [1d] initiated the introduction of new World Calendar since 2012 [1b-1d] which would make useless already existing date libraries. Again, the same main problem, see on it further on.    The most covering, I ever saw, date-time-related comparison in IT systems is [2] between BIG8 DBMS (IBM DB2, Informix, Ingres, InterBase, Microsoft SQL Server, MySQL, Oracle, and Sybase).\nThis and all other surveys show that     in all-all systems, frameworks is that their date/time datatypes do not permit to include .\nWithout which they are mainly half- useless - what is the sense of milliseconds in SQL Server datetime2 values in 7th century? At that time there was even no clocks measuring time with accuracy of minutes (Galileo Galilei used, for ex., his heart beats to measure intervals of time in his experiments) as well as Gregorian Calendar was not even invented.          Just fast illustrations:   ==== Cited:\n[1] New World Calendar\n[1a]\n\n[1b] \n[1c] \n[1d] [2] Peter Gulutzan,Trudy Pelzer. SQL Performance Tuning: Dates in SQL\n[3] SqlDateTime.MinValue != C# DateTime.MinValue, why?\n  [4]\nInternational Organization for Standardization\n\n[4a] ISO 8601 Data elements and interchange formats \u2014 Information interchange \u2014 Representation of dates and times\n   [5]\nGregorian Calendar\n\n[5a] Proleptic Gregorian calendar\n\n[5b] Gregorian Calendar Adoption\n\n[6]\n  I don't think there's a single standard to such things at the moment, however there are multiple standards which such things may conform to: ISO 8601 for example.'s own date/time handling is a cross-language (C/C++ and Java) and multi-platform library.It handles dates and times internally, typically, using for a single time a UDate (C/C++) or a java.util.Date/long (Java), as number of milliseconds since 1-1-1970, or a Calendar object which is specific to the type of calendar (Gregorian vs Hijri, etc).   Durations are available for formatting.   Leap years are calculated as part of calendar systems, and leap seconds are assumed to be handled by the underlying operating system.   DST/Timezone data is kept up to date with 'the tz database' sometimes referred to by its author's surname, Olson.Hope this has answered your question some as regards ICU.I haven't used it in a while, but from past experience I'd say that  is a pretty good example.While probably not the first choice for many fast paced projects today, the expressive power of C++ still seems to be a very good match for a complex problem domain like date/time, so combined with the high quality peer review process to pass for becoming an official Boost C++ library I'd hope that the library at hand could , albeit not as a , see below.The library is documented  well, so I could probably assemble the entire answer from quotes, but I'll try to extract some fragments according to the template suggested by  instead - nonetheless I'm going to start with an entire quote:The library supports 3 basic temporal types:You can get a pretty intuitive overview on how the domain concepts relate to each other in section .An important part of my original decision to evaluate the library has been the available documentation of design goals and necessary tradeoffs in light of the complex problem domain, which seems to outline the real world expertise that has been put into the library - you can read more about it in the following two sections:There is full support for all kind of calculations and conversions one could think of, as far as I'm concerned - see the headings in section  for an initial impression.This is definitely the weak spot concerning your specification, despite the library being specifically designed with extensibility in mind:However, I'm not aware of any implementations of other calendar/time systems than the ones included, see  for the current implementations of:This is fully supported and one of the strong points of the library due to the respective power of the underlying C++ I/O system, see  - the stream oriented C++ I/O has both merit and issues depending on your needs and expectations, but this topic is discussed elsewhere on this site.This is  via compatibility with , which is  oriented though, usually meaning a  or so; i.e. databases are not explicitly supported as in your  example.You mention Python in an earlier comment.  Python's builtin  support  is pretty practical, but you have to use a third-party timezone database such as   to make it close to complete.  And, as the pytz docs mention, you may still have problems with adding deltas to times right around DST transitions if you aren't careful.It was once the case that eGenix's  was the way to go if datetime didn't do it for your application, particularly for string to timestamp conversions, but  seems to be popular these days (I haven't used it though).To answer your \"Good Example\" question, take a look at  - 's port of the  libraries for Java to .Net"},
{"body": "I am creating a program for analyzing and generating queries.  I was curious if there currently exists a method within SQLite such that I could query the time taken for a query to process?  I am unable to modify my install in any way, so this method needs to work out of the box.  I am writing my tool in python, and although I guess I could use the timer class to time execution -this method will not work when I am connecting to remote machines (and return a consistent timing.) From within the sqlite3 command-line program you can do:This will print the CPU time taken for the query."},
{"body": "I use Python 2.7 and matplotlib. I have a *.txt data file :first column of my file (numbers) should be on axis Y in my bar chart, and the second column from my file (dates) should be on axis OX in my histogram. I only know how to read the file:I did read a matplotlib docs but it still doesn't help me. I would also like to add dates I read to my bar chart, to make it look likeCould someone please help me?You're talking about histograms, but this doesn't quite make sense. Histograms and bar charts are different things. An histogram would be a bar chart representing the sum of values per year, for example. Here, you just seem to be after bars.Here is a complete example from your data that shows a bar of for each required value at each date:You need to parse the date with  and set the x-axis to use dates (as described in ).If you're not interested in having the x-axis show a linear time scale, but just want bars with labels, you can do this instead:EDIT: Following comments, for all the ticks, and for them to be centred, pass the range to  (and move them by half the bar width):This code will do what you're looking for. It's based on examples found  and .The  call is particularly useful for making the x-axis labels readable.First, what you are looking for is a  or , not really a histogram. A histogram is made from a frequency distribution of a continuous variable that is separated into bins. Here you have a column against separate labels.To make a bar diagram with matplotlib, use the  method. Have a look at  of the matplotlib documentation that explains very well with examples and  how to do it.If it is possible though, I would just suggest that for a simple task like this if you could avoid writing code that would be better. If you have any spreadsheet program this should be a piece of cake because that's exactly what they are for, and you won't have to 'reinvent the wheel'. The following is the plot of your data in Excel:I just copied your data from the question, used the text import wizard to put it in two columns, then I inserted a column diagram."},
{"body": "I need to create a symlink for every item of dir1 (file or directory) inside dir2. dir2 already exists and is not a symlink. In Bash I can easily achieve this by: But in python using os.symlink I get an error:I know I can use  and run  command. I don't want that solution. I'm also aware that workarounds using  or  are possible, but I want to know if it is possible to do this using . creates a single symlink. creates multiple symlinks (if its last argument is a directory, and there's more than one source). The Python equivalent is something like:So, how does it work when you do ? Your  makes that work, by turning the wildcard into multiple arguments. If you were to just  the  command with a wildcard, it would look for a single source literally named  in , not all files in that directory.The Python equivalent is something like (if you don't mind mixing two levels together and ignoring a lot of other cases\u2014tildes, env variables, command substitution, etc. that are possible at the shell):You can't do that with  alone\u2014either part of it\u2014because it doesn't do that. It's like saying \"I want to do the equivalent of  using  without filtering on the name.\" Or, for that matter, I want to do the equivalent of  without the shell globbing for me.\"The right answer is to use , or , or  plus a regex, or whatever you prefer.Do  use , because that does a  filesystem walk, so it's not even close to shell  expansion. is a shell extension pattern, which in your case designates \"all files starting with \". But it's your  to the files it matches. Not the  command's.But  is not a shell, it's an OS call - hence, it doesn't support shell extension patterns. You'll have to do that work in your script. To do so, you can use , or . As indicated in the other answer, the appropriate call will depend on what you want to do. ( wouldn't be the equivalent of )To convince yourself: run this command on an Unix machine in your terminal: . You'll see that it's the shell that's doing the matching.  As suggested by @abarnert it's the shell that recognizes  and replaces it with all the items insside dir1. Therefore I think using  is the best choice:"},
{"body": "I have the following problem, I want to create my own colormap (red-mix-violet-mix-blue) that maps to values between -2 and +2 and want to use it to color points in my plot.\nThe plot should then have the colorscale to the right.\n\nThat is how I create the map so far. But I am not really sure if it mixes the colors.That way I map the colors to the values.Then I plot it:\nMy problems are:\n1. I can't plot the color scale.\n2. I am not completely sure if my scale is creating a continues (smooth) colorscale.There is an illustrative example of .\nThe docstring is essential for understanding the meaning of\n. Once you get that under your belt, you might use a  like this:Although the  format gives you a lot of flexibility, I find for simple\ngradients its format is rather unintuitive. Here is a utility function to help\ngenerate simple LinearSegmentedColormaps:By the way, the plots one point for every call to . This will work for a small number of points, but will become extremely slow for many points.  can only draw in one color, but  can assign a different color to each dot. Thus,  is the way to go.If you want to automate the creating of a custom divergent colormap commonly used for , this module combined with @unutbu method worked well for me.The high and low values can be either string color names or rgb tuples.  This is the result using the :\n"},
{"body": "I would like my default display for IPython notebook code cells to include line numbers. I learned from  that I can toggle this with ctrl-M L, which is great, but manual. In order to include line numbers by default, I would need to add something to my ipython_notebook_config.py file. Unless I've missed something, there is not an explanation of how to do this in the documentation. In your  file (location depends on your OS) put If you can't find custom.js, you can just search for it, but generally it will be in your profile_default folder. If it doesn't exist, create the file at If for whatever reason that doesn't work, you can always edit the  file in the  in the same way. In the latest Jupyter versions, they have  the place to make config changes. So basically, in the Jupyter update, they've removed the concept of profiles, so the  file location is now , depending on where your  folder is. So if you don't have a  folder or the  file, just create them, then put these lines into the newly created file:The above is for setting line numbers to  at the same time. Code, Markdown and Raw cells will all get line numbers if you do this. If you want line numbers , there is a simpler approach. Select a code cell, open the Chrome/Firefox JavaScript console, type the following lines:Then reload the page. These changes persist because Jupyter will create a json config file in  to store them. This method is from  of the documentation, so read the docs for more config changes that you can make.In the latest version of IPython Notebook (v3.1.0), go to  and add these lines:The  line alone will not work as it needs to load the IPython.Cell object before it tries this. Adding this line alone will cause an undefined error in the console. You need to encase it in the event handler as shown.@William-Denman's code might have worked for an earlier version, but now you will need to do this.Also, in the latest IPython/Jupyter, which I'm running using the , I couldn't find the  file within the profile folder. I found it (after  searching) in . I don't know if this is a WinPython thing or a Jupyter thing. If someone has Jupyter (latest version) installed normally (using pip or whatever) and can still find the  file in the profile folder, please comment."},
{"body": "With  when a log scale is specified for an axis, the default method of labeling that axis is with numbers that are 10 to a power eg. 10^6. Is there an easy way to change all of these labels to be their full numerical representation? eg. 1, 10, 100, etc.Note that I do not know what the range of powers will be and want to support an arbitrary range (negatives included).Sure, just change the formatter.For example, if we have this plot:You could set the tick labels manually, but then the tick locations and labels would be fixed when you zoom/pan/etc.  Therefore, it's best to change the formatter:I've found that using  is great if all your tick values are greater than or equal to 1. However, if you have a tick at a number , the  prints the tick label as .I've used this  function with  to set numbers  to their integer value, and numbers  to their decimal value, with the minimum number of decimal places required (i.e. , etc). It assumes that you are only setting ticks on the  values.For clarity, here's that lambda function written out in a more verbose, but also more understandable, way:regarding these questionsyou can solve those issue like this with MINOR formatter:in my application I'm using this format scheme, which I think solves most issues related to log scalar formatting; the same could be done for data > 1.0 or x axis formatting: which yields:then to create a general use machine:yields:"},
{"body": "I would like to make a decorator which could be used with or without a parameter :\nSomething like this :In my code, only the use of decorator with parameter is working: How to proceed to have both working (with and without parameter)?I found an example, you can use  or : nice!If you want to take parameters to your decorator, you need to  call it as a function:Otherwise, you need to try to detect the difference in parameters--in other words, you need to magically guess what the caller means.  Don't create an API that needs to guess; consistently say what you mean to begin with.In other words, a function should either be a decorator, or a decorator factory; it shouldn't be both.Note that if all you want to do is store a value, you don't need to write a class.If you don't mind relying on using named arguments, I made something similar to what you need:This works because only one non keyword argument, the function decorated is passed to the decorator.Notice that I also used the arguments passed to the decorated function, in this case 'self'.You have to detect if the argument to the decorator is a function, and use a simple decorator in that case. And then you need to hope that you never need to pass only a function to the parametrized decorator.This would work.If a decorator function @invocation isn't passed any explicit arguments, it is called with the function defined in the following  . If it  passed arguments, then it is first called with them and then the result of  preliminary call (which must itself also be a callable) is called with the function being defined. Either way, the return value of the last or only call is bound to the defined function name."},
{"body": "I have a NumPy array of  values. I want to count how many of these values are in a specific range say x<100 and x>25. I have read about the counter, but it seems to only be valid for specif values not ranges of values. I have searched, but have not found anything regarding my specific problem. If someone could point me towards the proper documentation I would appreciate it. Thank youI have tried thisBut it just gives me the numbers in between 25 and 99. \nThe data I am using was created by another program. I then used a script to read the data and store it as a list. I then took the list and turned it in to an array using array(r).The result of running If your array is called , the number of elements fulfilling  isThe expression  results in a Boolean array with the same shape as  with the value  for all elements that satisfy the condition.  Summing over this Boolean array treats  values as  and  values as .You could use . Here's a basic usage example:In your particular case, it would look something like this:Additionally, when you have a list of strings, you have to explicitly specify the type, so that  knows to produce an array of floats instead of a list of strings.Building on Sven's good approach, you can also do the more direct:This first creates an array of booleans with one boolean for each input number in array , and then count the number of non-False (i.e. True) values (which gives the number of matching numbers).Note, however, that this approach is twice as slow as Sven's  approach, on an array of 100k numbers (NumPy 1.6.1, Python 2.7.3)\u2013about 300\u00a0\u00b5s versus 150\u00a0\u00b5s.Sven's answer is the way to do it if you don't wish to further process matching values.\nThe following two examples return copies with only the matching values:Or: Example interpreter session:The above examples use a \"bit-wise and\" (&) to do an element-wise computation along the two boolean arrays which you create for comparison purposes.\nAnother way to write Sven's excellent answer, for example, is:  The boolean arrays contain  values when the condition matches, and  when it doesn't.\nA bonus aspect of boolean values is that  is equivalent to 1 and  to 0.  I think @Sven Marnach answer is quite nice, because it operates in on the numpy array itself which will be fast and efficient (C implementation).  I like to put the test into one condition like , so I would probably do it something like this:"},
{"body": "I need to convert time value strings given in the following format to seconds.I am using eg:Do I need to use regex to do this ?I tried to use time module,but   threw Can someone tell me how this can be solved?Edit:I thinkgives the correct value..I was using the wrong moduleFor Python 2.7:A little more pythonic way I think would be:Output is 263Sec.I would be interested to see if anyone could simplify it further.It looks like you're willing to strip fractions of a second... the problem is you can't use '00' as the hour with There is always parsing by handTo get the , you should subtract : above implies the input is less than a day, to support the time difference more than a day:To emulate  on Python 2.6:without imports"},
{"body": "I recently switch to Celery 3.0. Before that I was using  in order to integrate Celery with Flask. Although it had many issues like hiding some powerful Celery functionalities but it allowed me to use the full context of Flask app and especially Flask-SQLAlchemy.In my background tasks I am processing data and the SQLAlchemy ORM to store the data. The maintainer of Flask-Celery has dropped support of the plugin. The plugin was pickling the Flask instance in the task so I could have full access to SQLAlchemy.I am trying to replicate this behavior in my tasks.py file but with no success. Do you have any hints on how to achieve this?extensions.pyapp.pyOnce you've set up your app this way, you can run and use celery without having to explicitly run it from within an application context, as all your tasks will automatically be run in an application context if necessary, and you don't have to explicitly worry about post-task teardown, which is an important issue to manage (see other responses below).I prefer to run all of celery within the application context by creating a separate file that invokes celery.start() with the application's context.  This means your tasks file doesn't have to be littered with context setup and teardowns.  It also lends itself well to the flask 'application factory' pattern.extensions.pytasks.pyapp.pyRunCelery.pyIn your tasks.py file do the following:Nevermind that didn't work. I ended up having an argument in my Flask app factory to not run db.init_app(app) if Celery was calling it. Instead the workers will call it after Celery forks them. I now see several connections in my MySQL processlist. By doing this, we are able to maintain database connection per-worker.If you want to run your task under flask context, you can subclass :"},
{"body": "I have two classes,  and . They look a little bit like this:This error is pointing to Field's :I expected Background () to be called first.  To pass \"a, b\" to Fields (), Field to assign a and b then to assign a list with three 0's in it to field.  Then for Background's () to continue, to then call its own buildField() and override self.field with a list containing c.It seems I don't fully understand super(), however i was unable to find a solution to my issue after looking at similar inheritance problems on the web and around here.I expected behavior like c++ where a class can override a method that was inherited.  How can i achieve this or something similar.Most issues I found related to this were people using double underscores.  My experience with inheritance with super is using the inherited class () to just pass different variables to the super class.  Nothing involving overwriting anything.Coming from a C++ perspective, there might be two misconceptions here.First, overriding a method with a different signature does not overload it like in C++. If one of your Background objects tries to call buildField with no arguments, the original version from Field will not be called -- it has been completely hidden.The second issue is that if a method defined in the superclass calls buildField, the subclass version will be called. In python,  methods are bound dynamically, like a C++  method.Field's  expected to be dealing with an object that had a buildField method taking  no arguments. You used the method with an object that has a buildField method taking one argument.The thing with  is that it doesnt change the type of the object, so you shouldn't change the signature of any methods that the superclass' methods might call.So far, so good.Ah. This is where we get the error. Even though this line occurs within ,  is an instance of . so  finds 's  method, not 's.Since  expects 2 arguments instead of 1, raises an error.So how do we tell Python to call 's  method instead of 's?The purpose of  (naming an attribute with double underscores) is to solve this exact problem.The method name  is \"mangled\" to  inside  so inside ,calls , which is 's  method. While similarly, inside  calls 's  method.Actually  is getting called.. But So, the first statement of  is invoking the  init method.. and passing the  as argument.. Now this  is actually a reference of .. Your  method is actually invoking the one in the Background class.. This is because, the  here is instance of  class(Try printing  in your  method of ).. As you passed it while invoking the  method, from  class.. As you are not passing any value.. So, only value passed is the implicit .The  will invoke:in . However,  here refers to the  instance, and  is in fact calling  of , which is why you get that error.It seems to be that your code should be better written as:If you can't allow the base constructor to finish then it signals that the design is flawed. It is therefore much better to separate  to belong to the class by using  or , if you  call these methods in your constructor. However, if your base class constructor does not invoke any instance method from within, you can then safely overwrite any method of this base class. is talked about but it sounds like to me And also it sounds like  properties:Let me explain:I do not know your business logic but sometimes by-passing super class's  method gave me more control:Has more clean syntax."},
{"body": "How would I take an RGB image in Python and convert it to black OR white? Not grayscale, I want each pixel to be either fully black (0, 0, 0) or fully white (255, 255, 255).Is there any built-in functionality for this in the popular Python image processing libraries? If not, would the best way be just to loop through each pixel, see if it's closer to white or black, and if it's closer to white set it to white, if it's closer to black set it to black?Convert to grayscale and then scale to white or black (whichever is closest).Original:Result:Install  if you haven't already: (or PIL) can help you work with images effectively.Alternatively, you can use  with . You'll need to install numpy:Numpy needs a copy of the array to operate on, but the result is the same.Using  you can convert it directly to black and white. It will look like it has shades of grey but your brain is tricking you! (Black and white near each other look like grey)Original:Converted:I would suggest converting to grayscale, then simply applying a threshold (halfway, or mean or meadian, if you so choose) to it.Using  you can convert it directly to black and white. It will look like it has shades of grey but your brain is tricking you! (Black and white near each other look like grey)Original:Converted:And you can use  (in the standard library) to convert rgb to  and use the lightness value to determine black/white:"},
{"body": "I have access to numpy and scipy and want to create a simple FFT of a dataset. I have two lists one that is y values and the other is timestamps for those y values. What is the simplest way to feed these lists into a scipy or numpy method and plot the resulting FFT?I have looked up examples, but they all rely on creating a set of fake data with some certain number of data points, and frequency, etc. and doesn't really show how to do it with just a set of data and the corresponding timestamps.I have tried the following example:But when i change the argument of fft to my data set and plot it i get extremely odd results, it appears the scaling for the frequency may be off. i am unsure.Here is a pastebin of the data i am attempting to FFT\nWhen i do an fft on the whole thing it just has a huge spike at zero and nothing elseHere is my code:spacing is just equal to  So I run a functionally equivalent form of your code in an IPython notebook:I get what I believe to be very reasonable output.It's been longer than I care to admit since I was in engineering school thinking about signal processing, but spikes at 50 and 80 are exactly what I would expect. So what's the issue?The problem here is that you don't have periodic data. You should always inspect the data that you feed into  algorithm to make sure that it's appropriate.The important thing about fft is that it can only be applied to data in which the timestamp is uniform. i.e. uniform sampling in time (like what oyu have shown above). In case of non-uniform sampling\nIf that is not the case, please use a function for fitting the data. There are several tutorials and functions to choose from:\n   If fitting is not an option, you can directly use some form of interpolation to interpolate data to a uniform sampling. When you have uniform samples, you will only have to wory about the time delta () of your samples. In this case, you can directly use the fft functionsThis should solve your problem. The high spike that you have is due to the DC (non-varying, i.e. freq = 0) portion of your signal. It's an issue of scale. If you want to see non-DC frequency content, for visualization, you may need to plot from the offset 1 not from offset 0 of the FFT of the signal.Modifying the example given above by @PaulHThe output plots:\nAnother way, is to visualize the data in log scale:Using:Will show:\nJust as a complement to the answers already given, I would like to point out that often it is important to play with the size of the bins for the FFT. It would make sense to test a bunch of values and pick the one that makes more sense to your application. Often, it is in the same magnitude of the number of samples. This was as assumed by most of the answers given, and produces great and reasonable results. In case one wants to explore that, here is my code version:the output plots:\n"},
{"body": "I wonder what is the time complexity of pop method of list objects in Python (in CPython particulary). Also does the value of N for list.pop(N) affects the complexity? Pop() for the last element ought to be O(1) since you only need to return the element referred to by the last element in the array and update the index of the last element.  I would expect pop(N) to be O(N) and require on average N/2 operations since you would need to move any elements beyond the Nth one, one position up in the array of pointers.Yes, it is O(1) to pop the last element of a Python list, and O(N) to pop an arbitrary element (since the whole rest of the list has to be shifted).Here's a great article on how Python lists are stored and manipulated: "},
{"body": "I need to load an XML file and put the contents into an object-oriented structure. I want to take this:and turn it into something like this:It'll have a more complicated structure than that and I can't hard code the element names. The names need to be collected when parsing and used as the object properties.Any thoughts on what would be the best way to go about this?It's worth to have a look at Or the other way around to build xml structures:I've been recommending this more than once today, but try  (easy_install BeautifulSoup).David Mertz's  would seem to do this for you. Documentation's a bit hard to come by, but there are a few IBM articles on it, including .Creating xml from objects in this way is a different matter, though.How about this#outputIf googling around for a code-generator doesn't work, you could write your own that uses XML as input and outputs objects in your language of choice.It's not terribly difficult, however the three step process of Parse XML, Generate Code, Compile/Execute Script does making debugging a bit harder.There are three common XML parsers for python: xml.dom.minidom, elementree, and BeautifulSoup.IMO, BeautifulSoup is by far the best.  "},
{"body": "I would like to automate the response for some question prompted by some programs, like mysql  prompting for a password, or apt asking for a 'yes' or ... when I want to rebuild my haystack index with a ./manage.py rebuild_index.For MySQL, I can use the --password= switch, and I'm sure that apt has a 'quiet' like option. But how can I pass the response to other programs ?Why can't you just use ?For example, for an automated auto accept, just use , that just outputs a neverending stream of .If you are looking for a user to confirm an operation, use the confrim method.Or if you are looking for a way to get input from the user, use the prompt method.The development version of Fabric (1.0a) now supports interaction with remote programs.\nThose both methods are valid and works.I choose the first one, because I didn't want to have any interaction with my deployment system.So here is the solution I used:WARNING: This will irreparably remove EVERYTHING from your search index.\nYour choices after this are to restore from backups or rebuild via the  command.\nAre you sure you wish to continue? [y/N] \nRemoving all documents from your index because you said so.\nAll documents removed.\nIndexing 27 Items.\n"}