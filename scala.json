[
{"body": "I have just started to look at the  which is coming in the imminent  release. Those familiar with the library from 2.7 will notice that the library, from a usage perspective, has changed little. For example......would work in either versions. : in fact it's fantastic. However, those previously unfamiliar with Scala and  now have to make sense of method signatures like:For such simple functionality, this is a daunting signature and one which I find myself struggling to understand.  (or /C/C++/C#) - I don't believe its creators were aiming it at that market - but I think it is/was certainly feasible for Scala to become the next Ruby or Python (i.e. to gain a significant commercial user-base)  (mistakenly in my opinion) for what he saw as its overcomplicated type-system. I worry that someone is going to have a field day spreading  with this API (similarly to how Josh Bloch scared the  out of adding closures to Java). - Despite whatever my wife and coworkers keep telling me, I don't think I'm an idiot: I have a good degree in mathematics from the , and I've been programming commercially for almost 12 years and in  for about a year (also commercially).. This question is subjective but it is a genuine question, I've made it CW and I'd like some opinions on the matter.I hope it's not a \"suicide note\", but I can see your point. You hit on what is at the same time both a strength and a problem of Scala: its . This lets us implement most major functionality in libraries. In some other languages, sequences with something like  or  would be built in, and nobody has to see all the hoops the compiler has to go through to make them work smoothly. In Scala, it's all in a library, and therefore out in the open.In fact the functionality of  that's supported by its complicated type is pretty advanced. Consider this:See how you always get the best possible type? If you map s to s you get again a , but if you map s to s, you get a general . Both the static type and the runtime representation of map's result depend on the result type of the function that's passed to it. And this works even if the set is empty, so the function is never applied! As far as I know there is no other collection framework with an equivalent functionality. Yet from a user perspective this is how things are  to work.The problem we have is that all the clever technology that makes this happen leaks into the type signatures which become large and scary. But maybe a user should not be shown by default the full type signature of ? How about if she looked up  in  she got:The docs would not lie in that case, because from a user perspective indeed map has the type . But  also has a more general type which can be inspected by clicking on another link.We have not yet implemented functionality like this in our tools. But I believe we need to do this, to avoid scaring people off and to give more useful info. With tools like that, hopefully smart frameworks and libraries will not become suicide notes. I do not have a PhD, nor any other kind of degree neither in CS nor math nor indeed any other field. I have no prior experience with Scala nor any other similar language. I have no experience with even remotely comparable type systems. In fact, the only language that I have more than just a superficial knowledge of which even  a type system is Pascal, not exactly known for its sophisticated type system. (Although it  have range types, which AFAIK pretty much no other language has, but that isn't really relevant here.) The other three languages I know are BASIC, Smalltalk and Ruby, none of which even have a type system.And yet, I have no trouble at all understanding the signature of the  function you posted. It looks to me like pretty much the same signature that  has in every other language I have ever seen. The difference is that this version is more generic. It looks more like a C++ STL thing than, say, Haskell. In particular, it abstracts away from the concrete collection type by only requiring that the argument is , and also abstracts away from the concrete return type by only requiring that an implicit conversion  function exists which can build  out of that collection of result values. Yes, that is quite complex, but it really is only an expression of the general paradigm of generic programming: do not assume anything that you don't actually have to.In this case,  does not actually  the collection to be a list, or being ordered or being sortable or anything like that. The only thing that  cares about is that it can get access to all elements of the collection, one after the other, but in no particular order. And it does not need to know what the resulting collection is, it only needs to know how to build it. So, that is what its type signature requires.So, instead ofwhich is the traditional type signature for , it is generalized to not require a concrete  but rather just an  data structurewhich is then further generalized by only requiring that a function exists that can  the result to whatever data structure the user wants:I admit that the syntax is a bit clunkier, but the semantics are the same. Basically, it starts from which is the traditional signature for . (Note how due to the object-oriented nature of Scala, the input list parameter vanishes, because it is now the implicit receiver parameter that every method in a single-dispatch OO system has.) Then it generalized from a concrete  to a more general Now, it replaces the  result collection with a function that , well, really just about anything.Which I really believe is not  hard to understand. There's really only a couple of intellectual tools you need:None of these three should give any professional or even hobbyist programmer a serious headache.  has been a standard function in pretty much every language designed in the last 50 years, the fact that different languages have different syntax should be obvious to anyone who has designed a website with HTML and CSS and you can't subscribe to an even remotely programming related mailinglist without some annoying C++ fanboy from the church of St. Stepanov explaining the virtues of generic programming.Yes, Scala  complex. Yes, Scala has one of the most sophisticated type systems known to man, rivaling and even surpassing languages like Haskell, Miranda, Clean or Cyclone. But if complexity were an argument against success of a programming language, C++ would have died long ago and we would all be writing Scheme. There are lots of reasons why Scala will very likely not be successful, but the fact that programmers can't be bothered to turn on their brains before sitting down in front of the keyboard is probably not going to be the main one.Same thing in C++:Well, I can understand your pain, but, quite frankly, people like you and I -- or pretty much any regular Stack Overflow user -- are not the rule.What I mean by that is that... most programmers won't care about that type signature, because ! They don't read documentation.As long as they saw some example of how the code works, and the code doesn't fail them in producing the result they , they won't ever look at the documentation. When that fails, they'll look at the documentation and expect to see  at the top.With these things in mind, I think that:Alas, Java programmers are much into power tools, so, in answering that, I have just revised my expectation of mainstream Scala adoption. I have no doubt at all that Scala will become a mainstream language. Not C-mainstream, but perhaps Perl-mainstream or PHP-mainstream.Speaking of Java, did you ever replace the class loader? Have you ever looked into what that involves? Java can be scary, if you look at the places framework writers do. It's just that most people don't. The same thing applies to Scala, IMHO, but early adopters have a tendency to look under each rock they encounter, to see if there's something hiding there.Yes, but it will also prevent people from being put off.  I've considered the lack of collections that use higher-kinded types to be a major weakness ever since Scala gained support for higher-kinded types.  It make the API docs more complicated, but it really makes usage more natural.Some probably will.  I don't think Scala is accessible to many \"professional\" developers, partially due to the complexity of Scala and partly due to the unwillingness of many developers to learn.  The CTOs who employ such developers will rightly be scared off.Absolutely.  It makes collections fit much better with the rest of the language and the type system, even if it still has some rough edges.I'm not using it commercially.  I'll probably wait until at least a couple revs into the 2.8.x series before even trying to introduce it so that the bugs can be flushed out.  I'll also wait to see how much success EPFL has in improving its development a release processes.  What I'm seeing looks hopeful, but I work for a conservative company.One the more general topic of \"is Scala too complicated for mainstream developers?\"...Most developers, mainstream or otherwise, are maintaining or extending existing systems.  This means that most of what they use is dictated by decisions made long ago.  There are still plenty of people writing COBOL.Tomorrow's mainstream developer will work maintaining and extending the applications that are being built today.  Many of these applications are not being built by mainstream developers.  Tomorrow's mainstream developers will use the language that is being used by today's most successful developers of new applications.One way that the Scala community can help ease the fear of programmers new to Scala is to focus on practice and to teach by example--a lot of examples that start small and grow gradually larger.  Here are a few sites that take this approach:After spending some time on these sites, one quickly realizes that Scala and its libraries, though perhaps difficult to design and implement, are not so difficult to use, especially in the common cases.I have an undergraduate degree from a cheap \"mass market\" US university, so I'd say I fall into the middle of the user intelligence (or at least education) scale :) I've been dabbling with Scala for just a few months and have worked on two or three non-trivial apps.Especially now that IntelliJ has released their fine IDE with what IMHO is currently the best Scala plugin, Scala development is relatively painless: As a newbie, I continue to struggle with the terse and idiomatic syntax. Method calls without parameters don't need parentheses except where they do; cases in the match statement need a fat arrow (  ), but there are also places where you need a thin arrow (  ). Many methods have short but rather cryptic names like  or  - I can get my stuff done if I flip enough manual pages, but some of my code ends up looking like Perl or line noise. Ironically, one of the most popular bits of syntactic shorthand is missing in action: I keep getting bitten by the fact that  doesn't define a  method.This is just my opinion: I feel like Scala has the power of C++ combined with the complexity and readability of C++. The syntactic complexity of the language also makes the API documentation hard to read.Scala is very  and brilliant in many respects. I suspect many an academic would love to program in it. However, it's also full of cleverness and gotchas, it has a much higher learning curve than Java and is harder to read. If I scan the fora and see how many developers are still struggling with the finer points of Java, . No company will be able to justify sending its developers on a 3 week Scala course when formerly they only needed a 1 week Java course.I think primary problem with that method is that the  goes without any explanation.  Even though I know what implicit arguments are there's nothing indicating how this affects the call.  Chasing through the scaladoc only leaves me more confused (few of the classes related to  even have documentation).I think a simple \"there must be an implicit object in scope for  that provides a builder for objects of type  into the return type \" would help somewhat, but it's kind of a heady concept when all you really want to do is map 's to 's.  In fact, I'm not sure that's right, because I don't know what the type  means, and the documentation for  certainly gives no clue at all.So, I'm left with two options, neither of them pleasant:I get that Scala is essentially exposing the guts of how these things work and that ultimately this is provide a way to do what oxbow_lakes is describing.  But it's a distraction in the signature.I'm a Scala beginner and I honestly don't see a problem with that type signature. The parameter is the function to map and the implicit parameter the builder to return the correct collection. Clear and readable.The whole thing's quite elegant, actually. The builder type parameters let the compiler choose the correct return type while the implicit parameter mechanism hides this extra parameter from the class user. I tried this:That's polymorphism done right. Now, granted, it's not a mainstream paradigm and it will scare away many. But, it will also attract many who value its expressiveness and elegance.Unfortunately the signature for map that you gave is an incorrect one for map and there is indeed legitimate criticism.The first criticism is that by subverting the signature for map, we have something that is more general. It is a common error to believe that this is a virtue by default. It isn't. The map function is very well defined as a covariant functor Fx -> (x -> y) -> Fy with adherence to the two laws of composition and identity. Anything else attributed to \"map\" is a travesty.The given signature is something else, but it is not map. What I suspect it is trying to be is a specialised and slightly altered version of the \"traverse\" signature from the paper, The Essence of the Iterator Pattern. Here is its signature:I shall convert it to Scala:Of course it fails -- it is not general enough! Also, it is slightly different (note that you can get map by running traverse through the Identity functor). However, I suspect that if the library writers were more aware of library generalisations that are well documented (Applicative Programming with Effects precedes the aforementioned), then we wouldn't see this error.Second, the map function is a special-case in Scala because of its use in for-comprehensions. This unfortunately means that a library designer who is better equipped cannot ignore this error without also sacrificing the syntactic sugar of comprehensions. In other words, if the Scala library designers were to destroy a method, then this is easily ignored, but please not map!I hope someone speaks up about it, because as it is, it will become harder to workaround the errors that Scala insists on making, apparently for reasons that I have strong objections to. That is, the solution to \"the irresponsible objections from the average programmer (i.e. too hard!)\" is not \"appease them to make it easier for them\" but instead, provide pointers and assistance to become better programmers. Myself and Scala's objectives are in contention on this issue, but back to your point.You were probably making your point, predicting specific responses from \"the average programmer.\" That is, the people who will claim \"but it is too complicated!\" or some such. These are the Yegges or Blochs that you refer to. My response to these people of the anti-intellectualism/pragmatism movement is quite harsh and I'm already anticipating a barrage of responses, so I will omit it.I truly hope the Scala libraries improve, or at least, the errors can be safely tucked away in a corner. Java is a language where \"trying to do anything useful\" is so incredibly costly, that it is often not worth it because the overwhelming amount of errors simply cannot be avoided. I implore Scala to not go down the same path.I totally agree with both the question and Martin's answer :). Even in Java, reading javadoc with generics is much harder than it should be due to the extra noise. This is compounded in Scala where implicit parameters are used as in the questions's example code (while the implicits do very useful collection-morphing stuff).I don't think its a problem with the language per se - I think its more a tooling issue. And while I agree with what J\u00f6rg W Mittag says, I think looking at scaladoc (or the documentation of a type in your IDE) - it should require as little brain power as possible to grok what a method is, what it takes and returns. There shouldn't be a need to hack up a bit of algebra on a bit of paper to get it :)For sure IDEs need a nice way to show all the methods for any variable/expression/type (which as with Martin's example can have all the generics inlined so its nice and easy to grok). I like Martin's idea of hiding the implicits by default too.To take the example in scaladoc...When looking at this in scaladoc I'd like the generic block [B, That] to be hidden by default as well as the implicit parameter (maybe they show if you hover a little icon with the mouse) - as its extra stuff to grok reading it which usually isn't that relevant. e.g. imagine if this looked like...nice and clear and obvious what it does. You might wonder what 'That' is, if you mouse over or click it it could expand the [B, That] text highlighting the 'That' for example. Maybe a little icon could be used for the [] declaration and (implicit...) block so its clear there are little bits of the statement collapsed? Its hard to use a token for it, but I'll use a . for now...So by default the 'noise' of the type system is hidden from the main 80% of what folks need to look at - the method name, its parameter types and its return type in nice simple concise way - with little expandable links to the detail if you really care that much.Mostly folks are reading scaladoc to find out what methods they can call on a type and what parameters they can pass. We're kinda overloading users with way too much detail right how IMHO.Here's another example...Now if we hid the generics declaration its easier to readThen if folks hover over, say, A1 we could show the declaration of A1 being A1 <: A. Covariant and contravariant types in generics add lots of noise too which can be rendered in a much easier to grok way to users I think.I don't know how to break it to you, but I have a PhD from Cambridge, and I'm using 2.8 just fine.More seriously, I hardly spent any time with 2.7 (it won't inter-op with a Java library I am using) and started using Scala just over a month ago. I have some experience with Haskell (not much), but just ignored the stuff you're worried about and looked for methods that matched my experience with Java (which I use for a living). So: I am a \"new user\" and I wasn't put off - the fact that it works like Java gave me enough confidence to ignore the bits I didn't understand.(However, the reason I was looking at Scala was partly to see whether to push it at work, and I am not going to do so yet.  Making the documentation less intimidating would certainly help, but what surprised me is how much it is still changing and being developed (to be fair what surprised me most was how awesome it is, but the changes came a close second).  So I guess what I am saying is that I'd rather prefer the limited resources were put into getting it into a final state - I don't think they were expecting to be this popular this soon.)Don't know Scala at all, however a few weeks ago I could not read Clojure. Now I can read most of it, but can not write anything yet beyond the most simplistic . I suspect Scala is no different. You need a good book or course depending on how you learn. Just reading the  declaration above, I got  1/3 of it. I believe the bigger problems are not the syntax of these languages, but adopting and internalizing the  that make them usable in everyday production code. For me Java was not a huge leap from C++, which was not a huge leap from C, which was not a leap at all from Pascal, nor Basic etc... But coding in a functional language like Clojure  a huge leap (for me anyway). I guess in Scala you can code in Java style or Scala style. But in Clojure you will create quite the mess trying to keep your imperative habits from Java.Scala has a lot of crazy features (particularly where implicit parameters are concerned) that look very complicated and academic, but are designed to make things easy to use. The most useful ones get syntactic sugar (like  which means that an object of type A has an implicit conversion to an object of type B) and a well-documented explanation of what they do. But most of the time, as a client of these libraries you can ignore the implicit parameters and trust them to do the right thing.I don't think it is the main factor that will affect how popular Scala will become, because Scala has a lot of power and its syntax is not as foreign to a Java/C++/PHP programmer as Haskell, OCaml, SML, Lisps, etc..But I do think Scala's popularity will plateau at less than where Java is today, because I also think the next mainstream language must be much simplified, and the only way I see to get there is pure immutability, i.e. declarative like HTML, but Turing complete. However, I am biased because I am developing such a language, but I only did so after ruling out over a several month study that Scala could not suffice for what I needed.I don't think Scala's reputation will suffer from the Haskell complex. But I think that some will put off learning it, because for most programmers, I don't yet see a use case that forces them to use Scala, and they will procrastinate learning about it. Perhaps the highly-scalable server side is the most compelling use case.And, for the mainstream market, first learning Scala is not a \"breath of fresh air\", where one is writing programs immediately, such as first using HTML or Python. Scala tends to grow on you, after one learns all the details that one stumbles on from the start. However, maybe if I had read Programming in Scala from the start, my experience and opinion of the learning curve would have been different.Definitely.I am using Scala as the initial platform of my new language. I probably wouldn't be building code on Scala's collection library if I was using Scala commercially otherwise. I would create my own category theory based library, since the one time I looked, I found Scalaz's type signatures even more verbose and unwieldy than Scala's collection library. Part of that problem perhaps is Scala's way of implementing type classes, and that is a minor reason I am creating my own language.I decided to write this answer, because I wanted to force myself to research and compare Scala's collection class design to the one I am doing for my language. Might as well share my thought process.The 2.8 Scala collections use of a builder abstraction is a sound design principle. I want to explore two design tradeoffs below.Additionally I am carrying in my mind an incomplete idea that some of Scala's tradeoffs are due to trying to be both an mutable and immutable language, unlike for example Haskell or the language I am developing. This concurs with Tony Morris's comment about for comprehensions. In my language, there are no loops and no mutable constructs. My language will sit on top of Scala (for now) and owes much to it, and this wouldn't be possible if Scala didn't have the general type system and mutability. That might not be true though, because I think Odersky & Moors (\"Fighting Bit Rot with Types\") are incorrect to state that Scala is the only OOP language with higher-kinds, because I verified (myself and via Bob Harper) that Standard ML has them. Also appears SML's type system may be equivalently flexible (since 1980s), which may not be readily appreciated because the syntax is not so much similar to Java (and C++/PHP) as Scala. In any case, this isn't a criticism of Scala, but rather an attempt to present an incomplete analysis of tradeoffs, which is I hope germane to the question. Scala and SML don't suffer from Haskell's inability to do , which is critical and I understand is why so many functions in the Haskell Prelude are repeated for different types.It seems necessary to state ones degree here: B.A. in Political Science and B.ed in Computer Science.To the point:Scala is difficult, because its underlying programming paradigm is difficult. Functional programming scares a lot of people. It is possible to build closures in PHP but people rarely  do. So no, not this signature but all the rest will put people off, if they do not have the specific education to make them value the power of the underlying paradigm.If this education is available, everyone can do it. Last year I build a chess computer with a bunch of school kids in SCALA! They had their problems but they did fine in the end.I would not be worried. I have a maths degree from Oxford too! It took me a while to 'get' the new collections stuff. But I like it a lot now that I do. In fact, the typing of 'map' was one of the first big things that bugged me in 2.7 (perhaps since the first thing I did was subclass one of the collection classes).Reading Martin's paper on the new 2.8 collections really helped explain the use of implicits, but yes the documentation itself definitely needs to do a better job of explaining the role of different kind of implicits within method signatures of core APIs.My main concern is more this: when is 2.8 going to be released? When will the bug reports stop coming in for it? have scala team bitten off more than they can chew with 2.8 / tried to change too much at once?I'd really like to see 2.8 stabilised for release as a priority before adding anything else new at all, and wonder (while watching from the sidelines) if some improvements could be made to the way the development roadmap for the scala compiler is managed.What about error messages in use site?And what about when comes the use case one needs to integrate existing types with a custom one that fits a DSL. One have to be well educated on matters of association, precedence, implicit conversions, implicit parameters, higher kinds, and maybe existential types.It's very good to know that mostly it's simple but it's not necessarily enough. At least there must be one guy who knows this stuff if widespread library is to be designed."},
{"body": "I've seen in many examples that sometimes a Seq is being used, while other times is the List...Is there any difference, other than the former one being a Scala type and the List coming from Java?In Java terms, Scala's  would be Java's , and Scala's  would be Java's .Note that  is a , which is equivalent to Java's , but with the equivalent of up-and-coming defender methods. Scala's  is an abstract class that is extended by  and , which are the concrete implementations of .So, where Java's  is an , Scala's  is an implementation.Beyond that, Scala's  is immutable, which is not the case of . In fact, Java has no equivalent to immutable collections (the read only thing only guarantees the new object cannot be changed, but you still can change the old one, and, therefore, the \"read only\" one).Scala's  is highly optimized by compiler and libraries, and it's a fundamental data type in functional programming. However, it has limitations and it's inadequate for parallel programming. These days,  is a better choice than , but habit is hard to break. is a good generalization for sequences, so if you program to interfaces, you should use that. Note that there are actually three of them: ,  and , and it is the latter one that is the \"default\" imported into scope.There's also  and . The latter methods run in parallel where possible, which the former is parent to  and  both, being a suitable generalization for when parallelism of a code doesn't matter. They are both relatively newly introduced, so people doesn't use them much yet.In Scala, a List inherits from Seq, but implements ; here is the proper definition of  : [Note:  is a tad bit more complex, in order to fit in with and make use of Scala's very powerful collection framework.]A  is an Iterable that has a defined order of elements. Sequences provide a method  for indexing, ranging from 0 up to the length of the sequence. Seq has many subclasses including Queue, Range, List, Stack, and LinkedList.A  is a Seq that is implemented as an immutable linked list. It's best used in cases with last-in first-out (LIFO) access patterns."},
{"body": "Given a variable with type , how do I cast it to  in Scala?The preferred technique is to use pattern matching.  This allows you to gracefully handle the case that the value in question is  of the given type:This block replicates the semantics of the  method, but with greater flexibility.  For example, you could provide different branches for various types, effectively performing multiple conditional casts at the same time.  Finally, you don't  need to throw an exception in the catch-all area, you could also return  (or preferably, ), or you could enter some fallback branch which works without .In short, this is really the way to go.  It's a little more syntactically bulky than , but the added flexibility is almost always worth it."},
{"body": "Suppose we want to write a macro that defines an anonymous class with some type members or methods, and then creates an instance of that class that's statically typed as a structural type with those methods, etc. This is possible with the macro system in 2.10.0, and the type member part is extremely easy:(Where  is a  that provides my  method.)This macro lets us specify the name of the anonymous class's type member as a string literal:Note that it's appropriately typed. We can confirm that everything's working as expected:Now suppose that we try to do the same thing with a method:But when we try it out, we don't get a structural type:But if we stick an extra anonymous class in there:It works:This is extremely handy\u2014it lets you do things like , for example\u2014but I don't understand why it works, and the type member version works, but not . I know this , but does it make any sense? Is there an cleaner way to get a structural type (with the methods on it) from a macro?This question is answered in duplicate by Travis . There are links to the issue in the tracker and to Eugene's discussion (in the comments and mailing list).In the famous \"Skylla and Charybdis\" section of the type checker, our hero decides what shall escape dark anonymity and see the light as a member of the structural type.There are a couple of ways to trick the type checker (which do not entail Odysseus's ploy of hugging a sheep).  The simplest is to insert a dummy statement so that the block doesn't look like an anonymous class followed by its instantiation.If the typer notices that you're a public term that isn't referenced by the outside, it will make you private."},
{"body": "Are there any best-practice guidelines on when to use  (or case objects) vs extending Enumeration in Scala?They seem to offer some of the same benefits.One big difference is that s come with support for instantiating them from some  String. For example:Then you can do:This is useful when wishing to persist enumerations (for example, to a database) or create them from data residing in files. However, I find in general that enumerations are a bit clumsy in Scala and have the feel of an awkward add-on, so I now tend to use s. A  is more flexible than an enum:So now I have the advantage of...As  pointed out (with some corrections to ease reading):To follow up on the other answers here, the main drawbacks of s over s are:Case objects already return their name for their toString methods, so passing it in separately is unnecessary.  Here is a version similar to jho's (convenience methods omitted for brevity):Objects are lazy; by using vals instead we can drop the list but have to repeat the name:If you don't mind some cheating, you can pre-load your enumeration values using the reflection API or something like Google Reflections.  Non-lazy case objects give you the cleanest syntax:Nice and clean, with all the advantages of case classes and Java enumerations.  Personally, I define the enumeration values outside of the object to better match idiomatic Scala code:\nA new  has been created which is far superior to the solution I outline below. I strongly recommend using this new .  Whoohoo!\nThere are three basic patterns for attempting to reproduce the Java  within a Scala project. Two of the three patterns; directly using Java  and , are not capable of enabling Scala's exhaustive pattern matching. And the third one; \"sealed trait + case object\", does...but has  resulting in inconsistent ordinal index generation.  I have created a solution with two classes;  and , located in this . I didn't post the code into this thread as the file for Enumeration was quite large (+400 lines - contains lots of comments explaining implementation context).\n\n\n\nThe question you're asking is pretty general; \"...when to use  vs extending \". And it turns out there are MANY possible answers, each answer depending on the subtleties of the specific project requirements you have. The answer can be reduced down to three basic patterns.  To start, let's make sure we are working from the same basic idea of what an enumeration is. Let's define an enumeration mostly in terms of the :Next, let's look at boiled down versions of the three most common solution patterns posted:\n\n\n Actually directly using  pattern (in a mixed Scala/Java project):\nThe following items from the enumeration definition are not available:  For my current projects, I don't have the benefit of taking the risks around the Scala/Java mixed project pathway. And even if I could choose to do a mixed project, item 7 is critical for allowing me to catch compile time issues if/when I either add/remove enumeration members, or am writing some new code to deal with existing enumeration members.\n\n\n Using the \"\" pattern:\nThe following items from the enumeration definition are not available:  It's arguable it really meets enumeration definition items 5 and 6. For 5, it's a stretch to claim it's efficient. For 6, it's not really easy to extend to hold additional associated singleton-ness data.\n\n\n Using the  pattern (inspired by ):\nThe following items from the enumeration definition are not available (happens to be identical to the list for directly using the Java Enum):  Again for my current projects, item 7 is critical for allowing me to catch compile time issues if/when I either add/remove enumeration members, or am writing some new code to deal with existing enumeration members.  So, given the above definition of an enumeration, none of the above three solutions work as they do not provide everything outlined in the enumeration definition above:  Each of these solutions can be eventually reworked/expanded/refactored to attempt to cover some of each one's missing requirements. However, neither the Java  nor the  solutions can be sufficiently expanded to provide item 7. And for my own projects, this is one of the more compelling values of using a closed type within Scala. I strongly prefer compile time warnings/errors to indicate I have a gap/issue in my code as opposed to having to glean it out of a production runtime exception/failure.In that regard, I set about working with the  pathway to see if I could produce a solution which covered all of the enumeration definition above. The first challenge was to push through the core of the JVM class/object initialization issue (covered in detail in ). And I was finally able to figure out a solution.As my solution is two traits;  and , and since the  trait is over +400 lines long (lots of comments explaining context), I am forgoing pasting it into this thread (which would make it stretch down the page considerbly). For details, please jump directly to the .Here's what the solution ends up looking like using the same data idea as above (fully commented version ) and implemented in .\nThis is an example usage of a new pair of enumeration traits I created (located in ) to implement all of the capabilities desired and outlined in the enumeration definition.  One concern expressed is that the enumeration member names must be repeated ( in the example above). While I did minimize it down to a single repetition, I couldn't see how to make it even less due to two issues:  Given these two issues, I had to give up trying to generate an implied ordering and had to explicitly require the client define and declare it with some sort of ordered set notion. As the Scala collections do not have an insert ordered set implementation, the best I could do was use a  and then runtime check that it was truly a set. It's not how I would have preferred to have achieved this.  And given the design required this second list/set ordering , given the  example above, it was possible to add  and then forget to add  to . So, there is a runtime check to verify that the list is not only a set, but contains ALL of the case objects which extend the . That was a special form of reflection/macro hell to work through.\n\n\nPlease leave comments and/or feedback on the .The advantages of using case classes over Enumerations are:The advantages of using Enumerations instead of case classes are:So in general, if you just need a list of simple constants by name, use enumerations.  Otherwise, if you need something a bit more complex or want the extra safety of the compiler telling you if you have all matches specified, use case classes.UPDATE: The code below has a bug, described .  The test program below works, but if you were to use DayOfWeek.Mon (for example) before DayOfWeek itself, it would fail because DayOfWeek has not been initialized (use of an inner object does not cause an outer object to be initialized).  You can still use this code if you do something like  in your main class, forcing initialization of your enums, or you can use chaotic3quilibrium's modifications.  Looking forward to a macro-based enum!If you wantthen the following may be of interest.  Feedback welcome.In this implementation there are abstract Enum and EnumVal base classes, which you extend.  We'll see those classes in a minute, but first, here's how you would define an enum:Note that you have to use each enum value (call its apply method) to bring it to life.  [I wish inner objects weren't lazy unless I specifically ask for them to be.  I think.]We could of course add methods/data to DayOfWeek, Val, or the individual case objects if we so desired.And here's how you would use such an enum:Here's what you get when you compile it:You can replace \"day match\" with \"( day: @unchecked ) match\" where you don't want such warnings, or simply include a catch-all case at the end.When you run the above program, you get this output:Note that since the List and Maps are immutable, you can easily remove elements to create subsets, without breaking the enum itself.Here is the Enum class itself (and EnumVal within it):And here is a more advanced use of it which controls the IDs and adds data/methods to the Val abstraction and to the enum itself:I have a nice simple lib here that allows you to use sealed traits/classes as enum values without having to maintain your own list of values. It relies on a simple macro that is not dependent on the buggy .Another disadvantage of case classes versus Enumerations when you will need to iterate or filter across all instances. This is a built-in capability of Enumeration (and Java enums as well) while case classes don't automatically support such capability.In other words: \"there's no easy way to get a list of the total set of enumerated values with case classes\".If you are serious about maintaining interoperability with other JVM languages (e.g. Java) then the best option is to write Java enums.  Those work transparently from both Scala and Java code, which is more than can be said for  or case objects.  Let's not have a new enumerations library for every new hobby project on GitHub, if it can be avoided!I've seen various versions of making a case class mimic an enumeration.  Here is my version:Which allows you to construct case classes that look like the following:Maybe someone could come up with a better trick than simply adding a each case class to the list like I did.  This was all I could come up with at the time.Update March 2017: as commented by , the  PR has been closed. (next generation compiler for Scala) will take the lead, though  and .Note: there is now (August 2016, 6+ years later) a proposal to remove : I've been going back and forth on these two options the last few times I've needed them.  Up until recently, my preference has been for the sealed trait/case object option.1) Scala Enumeration Declaration2) Sealed Traits + Case ObjectsWhile neither of these really meet all of what a java enumeration gives you, below are the pros and cons:Scala EnumerationPros:\n-Functions for instantiating with option or directly assuming accurate (easier when loading from a persistent store)\n-Iteration over all possible values is supported Cons:\n-Compilation warning for non-exhaustive search is not supported (makes pattern matching less ideal)Case Objects/Sealed traitsPros:\n-Using sealed traits, we can pre-instantiate some values while others can be injected at creation time\n-full support for pattern matching (apply/unapply methods defined)Cons:\n-Instantiating from a persistent store - you often have to use pattern matching here or define your own list of all possible 'enum values'What ultimately made me change my opinion was something like the following snippet:The  calls were hideous - using enumeration instead I can simply call the withName method on the enumeration as follows:So I think my preference going forward is to use Enumerations when the values are intended to be accessed from a repository and case objects/sealed traits otherwise.I prefer  (it's a matter of personal preference). To cope with the problems inherent to that approach (parse string and iterate over all elements), I've added a few lines that are not perfect, but are effective.I'm pasting you the code here expecting it could be useful, and also that others could improve it.For those still looking how to get :\nYou can just reference the case object after declaring it to instantiate it:"},
{"body": "What's the best way to parse command-line parameters in Scala?\nI personally prefer something lightweight that does not require external jar.Related:For most cases you do not need an external parser. Scala's pattern matching allows consuming args in a functional style. For example:will print, for example:This version only takes one infile. Easy to improve on (by using a List).Note also that this approach allows for concatenation of multiple command line arguments - even more than two!The above generates the following usage text:This is what I currently use. Clean usage without too much baggage.\n(Disclaimer: I now maintain this project)I realize that the question was asked some time ago, but I thought it might help some people, who are googling around (like me), and hit this page. looks quite promising as well.Features (quote from the linked github page):And some example code (also from that Github page):I like  over arguments for relatively simple configurations.This is largely a shameless clone of .  It turns out that JewelCLI is Scala-friendly in that it doesn't require JavaBean style methods to get automatic argument naming.JewelCLI is a .  It uses Proxied Interfaces Configured with Annotations to dynamically build a type-safe API for your command-line parameters.An example parameter interface :An example usage of the parameter interface :Save copies of the files above to a single directory and download the  to that directory as well.Compile and run the example in Bash on Linux/Mac OS X/etc.:Compile and run the example in the Windows Command Prompt:Running the example should yield the following output:here is mine too! (a bit late in the game though)As opposed to  it is entirely mutable... but wait! That gives us a pretty nice syntax:And a simple way to run it:You can do a lot more of course (multi-commands, many configuration options, ...) and has no dependency.I'll finish with a kind of distinctive feature, the default usage (quite often neglected for multi commands):\nI think scala-optparse-applicative is the most functional command line parser library in Scala.There's also  (disclaimer: I created it):I am from Java world, I like  because its simple, specification is more readable( thanks to annotations) and produces nicely formatted output.Here is my example snippet:I liked the slide() approach of joslinm just not the mutable vars ;) So here's an immutable way to that approach:I've just found an extensive command line parsing library in scalac's scala.tools.cmd package.See I've attempted generalize @pjotrp's solution by taking in a list of required positional key symbols, a map of flag -> key symbol and default options:I based my approach on the top answer (from dave4420), and tried to improve it by making it more general-purpose. It returns a  of all command line parameters\nYou can query this for the specific parameters you want (eg using ) or convert the values into the types you want (eg using ).Example:Gives:another library: Here's a  that is easy to use.  It automatically formats help text, and it converts switch arguments to your desired type. Both short POSIX, and long GNU style switches are supported. Supports switches with required arguments, optional arguments, and multiple value arguments. You can even specify the finite list of acceptable values for a particular switch.  Long switch names can be abbreviated on the command line for convenience.  Similar to the option parser in the Ruby standard library.I like the clean look of this code... gleaned from a discussion here:\nI just created I understand that solution has two major flaws that may distract you: It eliminates the freedom (i.e. the dependence on other libraries, that you value so much) and redundancy (the DRY principle, you do type the option name only once, as Scala program variable and eliminate it second time typed as command line text).I'm going to pile on.  I solved this with a simple line of code.  My command line arguments look like this:This creates an array via Scala's native command line functionality (from either App or a main method):I can then use this line to parse out the default args array:Which creates a map with names associated with the command line values:I can then access the values of named parameters in my code and the order they appear on the command line is no longer relevant.  I realize this is fairly simple and doesn't have all the advanced functionality mentioned above but seems to be sufficient in most cases, only needs one line of code, and doesn't involve external dependencies.I'd suggest to use . There's a scala-port but the Java implementation  works just fine and seems to be better maintained. Here's an example:This is what I cooked. It returns a tuple of a map and a list. List is for input, like input file names. Map is for switches/options.will returnSwitches can be \"--t\" which x will be set to true, or \"--x 10\" which x will be set to \"10\". Everything else will end up in list.This will generate the following usage:How to parse parameters without an external dependency. Great question! You may be interested in . Picocli is specifically designed to solve the problem asked in the question: it is a command line parsing framework in a single file, so you can . This lets users run picocli-based applications . It works by annotating fields so you write very little code. Quick summary:The usage help message is easy to customize with annotations (without programming). For example: ()I couldn't resist adding one more screenshot to show what kind of usage help messages are possible. Usage help is the face of your application, so be creative and have fun!Disclaimer: I created picocli. Feedback or questions very welcome. It is written in java, but let me know if there is any issue using it in scala and I'll try to address it.I have never liked ruby like option parsers. Most developers that used them never write a proper  for their scripts and end up with pages long options not organized in a proper way because of their parser.I have always preferred Perl's way of doing things with Perl's .I am working on a scala implementation of it. The early API looks something like this:So calling  like this:Would print:And return:The project is hosted in github .As everyone posted it's own solution here is mine, cause I wanted something easier to write for the user : The gist contains a code file, plus a test file and a short example copied here:There is not fancy options to force a variable to be in some bounds, cause I don't feel that the parser is the best place to do so.Note : you can have as much alias as you want for a given variable.Here is mine 1-linerIt drops 3 mandatory arguments and gives out the options. Integers are specified like notorious  java option, jointly with the prefix. You can parse binaries and integers as simple asNo need to import anything.Poor man's quick-and-dirty one-liner for parsing key=value pairs:"},
{"body": "Sometimes when I read articles in the Scala ecosystem I read the term \"lifting\" / \"lifted\". Unfortunately, it is not explained what that exactly means. I did some research, and it seems that lifting has something to do with functional values or something like that, but I was not able to find a text that explains what lifting actually is about in a beginner friendly way. There is additional confusion through the  framework which has lifting in its name, but it doesn't help answer the question.There are a few usages:Remember a  is a function defined for some subset of the domain  (as specified by the  method). You can \"lift\" a  into a . That is, a function defined over the  of  but whose values are of type This is done by the explicit invocation of the method  on .You can \"lift\" a method invocation into a function. This is called  (thanks to Ben James for this). So for example:We lift a method into a function by applying the Note the fundamental difference between methods and functions.  is an  (i.e. it is a ) of the (function) type A  (as defined by ) is some \"container\" (I use the term  loosely),  such that, if we have an  and a function , then we can get our hands on an  (think, for example,  and the  method)We can encode this property as follows:This is isomorphic to being able to \"lift\" the function  into the domain of the functor. That is:That is, if  is a functor, and we have a function , we have a function . You might try and implement the  method - it's pretty trivial.As  says below (and I've just realized that this would have saved me from writing a ton of unnecessary code), the term \"lift\" also has a meaning within . Recall that a monad transformers are a way of \"stacking\" monads on top of each other (monads do not compose).So for example, suppose you have a function which returns an . This can be converted to the monad transformer . Now you may wish to \"lift\" some other value an  perhaps to that it is also a . You could either write this:Or this:this begs the question: . The answer would be \"to take advantage of composition possibilities\". Let's say you have a function Another usage of  that I've come across in papers (not necessarily Scala-related ones) is overloading a function from  with  (or sets, multisets, ...). This is often used to simplify formalisations because it then doesn't matter whether  is applied to an individual element or to multiple elements.This kind of overloading is often done declaratively, e.g.,oror imperatively, e.g.,Note any collection that extends  (as pointed out by oxbow_lakes) may be lifted; thus for instancewhich turns a partial function into a total function where values not defined in the collection are mapped onto ,Moreover,This shows a neat approach to avoid  exceptions."},
{"body": "I've just started learning Scala, and the first thing I'm going to implement is a tiny web application. I've been using  for the last year to implement server-side software, but I've never wrote web applications before. It will be a great experience.Are there web-frameworks for Scala except for ?Don't get me wrong, Lift looks awesome. I just want to know how many frameworks there are so that I can then choose between them. It's always a good to have a choice, but I the only thing I found was Lift.I'm very interested in Scala, but I have not used it yet, so with that caveat, the frameworks I am aware of that are not mentioned in  (Lift, Sweet, Slinky) are:I wrote a  about this.To summarise, some of the options are:I finally found that none were suitable for me, and developed my own little \"framework\". (It is not open-source yet).I like Lift ;-)Play is my second choice for Scala-friendly web frameworks.Wicket is my third choice.Following is a dump of frameworks. It doesn't mean I actually used them:Try , which also support Scala.One very interesting web framework with commercial deployment is , inspired by Ruby's Sinatra. Here's an  about it.I find Unfiltered very interesting .It's mentioned in IttayD's list. Here is a presentation about it  \nand the video Also here there is an article with more info It must be noted that there is also a considerable interest in  and . Wicket fits Scala suprisingly well. If you want to take advantage of the very mature Wicket project and its ecosystem (extensions) plus the concise syntax and productivity advantage of Scala, this one may be for you! See also: is pretty sweet.It is now production ready. It incorporates: a cool template framework,automatic reloading of source files upon safe, a composable action system, akka awesomeness, etc.Its part of the .Having used it for two projects, I can say that it works pretty smoothly and it should be something to consider next time you are looking to learn new web frameworks.I tend to use JAX-RS using  (you can write nice resource beans in Scala, Java or Groovy) to write RESTul web applications. Then I use  for the rendering the views using one of the various template languages (, ,  (Scala Server Pages), , etc.).There's a new web framework, called . From the site:The Scala Pages web framework is likely to appeal to web programmers who come from a Java background and want to program web applications in Scala. The emphasis is on OOP rather than functional programming.Prikrutil, I think we're on the same boat. I also come to Scala from Erlang. I like  a lot so I decided to created a Scala web framework inspired by it.Take a look at .  is quite extensive. From README:Xitrum is an async and clustered Scala web framework and web server on top of Netty and Hazelcast:Hazelcast also gives:Follow the  for a quick start.There's also Pinky, which used to be on bitbucket but got transfered to .By the way, github is a great place to search for Scala projects, as there's a lot being put there.I'd like to add my own efforts to this list.  You can find out more information here:It's in early development and I'm still working on it aggressively.  It includes features like:Any and all feedback is much appreciated.UPDATE: 2011-09-078, I just posted a major update to version 0.9.1.  There's more info at  which includes a screencast.Both Sweet and Slinky seem to be unmaintanted for about a year. Sweet Maven repo sweetsoftwaredesign.com is dead so there's even no way to download dependencies.<>:Spiffy is a web framework using Scala, Akka (a Scala actor implementation), and the Java Servlet 3.0 API. It makes use of the the async interface and aims to provide a massively parallel and scalable environment for web applications. Spiffy's various components are all based on the idea that they need to be independent minimalistic modules that do small amounts of work very quickly and hand off the request to the next component in the pipeline. After the last component is done processing the request it signals the servlet container by \"completing\" the request and sending it back to the client.<>You could also try . It was designed to be a Java-framework but I have successfully used it with Scala also without difficulties. It is a component based framework and has similar properties as Lift or Tapestry.I have stumbled upon your question a few weeks back, but since then also learned about . This is a nice, minimal framework that is therefore easy to learn, and it has pretty good documentation available as well.Beside it's minimal-ness, it also claims to work well with other libraries and lets you use your own implementation of things when you need it.Java EE would work since Scala is built on top of the Java platform."},
{"body": "I read  (part of ). In that post he stated:But he didn't explain anything about it. What was he trying to say?Jim has got this pretty much covered in , but I'm posting a briefing here for reference.First, let's see what the Scala Specification tell us. Chapter 3 (types) tell us about  (3.2.9) and  (3.3.1). Chapter 4 (basic declarations) speaks of  (4.1),  (4.2) and  (4.6). Chapter 6 (expressions) speaks of  (6.23) and  (6.7). Curiously, function values is spoken of one time on 3.2.9, and no where else.A  is (roughly) a type of the form , which is a shorthand for the trait  in the standard library.  and  have function types, and function types can be used as part of value, variable and function declarations and definitions. In fact, it can be part of a method type.A  is a . That means there is  value - no object, no instance - with a method type. As mentioned above, a  actually has a . A method type is a  declaration - everything about a  except its body. and  are  and  declarations, including both  - which can be, respectively,  and . Note that, on the JVM, these (method values) are implemented with what Java calls \"methods\".A   is a  declaration, including  and . The type part is the Method Type, and the body is an . This is also implemented on the JVM with what Java calls \"methods\".Finally, an  is an instance of a  (ie, an instance of the trait ), and a  is the same thing! The distinction is that a Method Value is created from methods, either by postfixing an underscore ( is a method value corresponding to the \"function declaration\" () ), or by a process called , which is like an automatic cast from method to function.That is what the specs say, so let me put this up-front:  It leads to too much confusion between so-called , which is a part of the program (chapter 4 -- basic declarations) and , which is an expression, and , which is, well a type -- a trait.The terminology below, and used by experienced Scala programmers, makes one change from the terminology of the specification: . Or even method declaration. Furthermore, we note that  and  are also methods for practical purposes.A  is an object that includes one of the  traits, such as , , , etc. It might be including  as well, which actually extends .Let's see the type signature for one of these traits:This trait has one abstract method (it has a few concrete methods as well):And that tell us all that there is to know about it. A  has an  method which receives  parameters of types , , ..., , and returns something of type . It is contra-variant on the parameters it receives, and co-variant on the result.That variance means that a  is a subtype of . Being a subtype means it can be used  it. One can easily see that if I'm going to call  and expect an  back, either of the two types above would work.Now, what is the  of a method and a function? Well, if  is a function and  is a method local to the scope, then both can be called like this:These calls are actually different, because the first one is just a syntactic sugar. Scala expands it to:Which, of course, is a method call on object . Functions also have other syntactic sugars to its advantage: function literals (two of them, actually) and  type signatures. For example:Another similarity between a method and a function is that the former can be easily converted into the latter:Scala will expand , assuming  type is  into (Scala 2.7):On Scala 2.8, it actually uses an  class to reduce class sizes.Notice that one can't convert the other way around -- from a function to a method.Methods, however, have one big advantage (well, two -- they can be slightly faster): they can receive . For instance, while  above can necessarily specify the type of  it receives ( in the example),  can parameterize it:I think this pretty much covers everything, but I'll be happy to complement this with answers to any questions that may remain.One big practical difference between a method and a function is what  means.   only ever returns from a method.  For example:Returning from a function defined in a method does a non-local return:Whereas returning from a local method only returns from that method.Let Say you have a ListDefine a MethodDefine a FunctionMethod Accepting ArgumentDefining Function with valArgument to function is OptionalArgument to Method is MandatoryCheck the following  that explains passing other differences with examples like other example of diff with Method Vs Function, Using function as Variables, creating function that returned functionFunctions don't support parameter defaults. Methods do. Converting from a method to a function loses parameter defaults. (Scala 2.8.1)"},
{"body": "I was reading . When is it better to use abstract types?For example,rather that generics, for example,You have a good point of view on this issue here:  \nA Conversation with Martin Odersky, Part III\nby Bill Venners and Frank Sommers  (May 18, 2009)Update (October2009): what follows below has actually been illustrated in this new article by Bill Venners:\n (see summary at the end)(Here is the relevant extract of the first interview, May 2009, emphasis mine)There have always been two notions of abstraction: In Java you also have both, but it depends on what you are abstracting over.\nIn Java you have abstract methods, but you can't pass a method as a parameter.\nYou don't have abstract fields, but you can pass a value as a parameter.\nAnd similarly you don't have abstract type members, but you can specify a type as a parameter.\nSo in Java you also have all three of these, but there's a distinction about what abstraction principle you can use for what kinds of things. And you could argue that this distinction is fairly arbitrary.We decided to have the .\nSo you can have abstract fields as well as value parameters.\nYou can pass methods (or \"functions\") as parameters, or you can abstract over them.\nYou can specify types as parameters, or you can abstract over them.\nAnd what we get conceptually is that we can model one in terms of the other. At least in principle, we can express every sort of parameterization as a form of object-oriented abstraction. So in a sense you could say Scala is a more orthogonal and complete language.What, in particular,  we talked about before.\nOne standard problem, which has been around for a long time, is the problem of animals and foods.\nThe puzzle was to have a class  with a method, , which eats some food.\nThe problem is if we subclass Animal and have a class such as Cow, then they would eat only Grass and not arbitrary food. A Cow couldn't eat a Fish, for instance.\nWhat you want is to be able to say that a Cow has an eat method that eats only Grass and not other things.\nActually, you can't do that in Java because it turns out you can construct unsound situations, like the problem of assigning a Fruit to an Apple variable that I talked about earlier.The answer is that .\nYou say, my new Animal class has a type of , which I don't know.\nSo it's an abstract type. You don't give an implementation of the type. Then you have an  method that eats only .\nAnd then in the  class I would say, OK, I have a Cow, which extends class , and for .\nSo .Indeed you can. You could parameterize class Animal with the kind of food it eats.\nBut , and usually, what's more, in .\nAt the 1998 ECOOP, Kim Bruce, Phil Wadler, and I had a paper where we showed that .\nSo there are very good reasons not to do parameters, but to have these abstract members, because they don't give you this quadratic blow up.  asks in the comments:I am not sure the relationship is that different between using abstract types or generics. \nWhat is different is:To understand what Martin is speaking about when it comes to \"explosion of parameters, and usually, what's more, in \", and its subsequent quadratically growth when abstract type are modeled using generics, you can consider the paper \"\" written by... Martin Odersky, and Matthias Zenger for OOPSLA 2005, referenced in the  (finished in 2007).Relevant extracts(Note: Family polymorphism has been proposed for object-oriented languages as a solution to supporting reusable yet type-safe mutually recursive classes.\nA key idea of family polymorphism is the notion of families, which are used to group mutually recursive classes)(Note, from Peter Canning, William Cook, Walter Hill, Walter Olthoff paper:\nBounded quantification was introduced by Cardelli and Wegner as a means of typing functions that operate uniformly over all subtypes of a given type.\nThey defined a simple \"object\" model and used bounded quantification to type-check functions that make sense on all objects having a specified set of \"attributes\".\nA more realistic presentation of object-oriented languages would allow objects that are elements of .\nIn this context, bounded quantification no longer serves its intended purpose. It is easy to find functions that makes sense on all objects having a specified set of methods, but which cannot be typed in the Cardelli-Wegner system.\nTo provide a basis for typed polymorphic functions in object-oriented languages, we introduce F-bounded quantification)There are two principal forms of abstraction in programming languages: The first form is typical for functional languages, whereas the second form is typically used in object-oriented languages.  Traditionally, Java supports parameterization for values, and member abstraction for operations.\nThe more recent Java 5.0 with generics supports parameterization also for types.The arguments for including generics in Scala are two-fold:  In a system with bounded polymorphism, rewriting abstract type into generics might entail a . (Bill Venners)(emphasis mine)Example:I had the same question when I was reading about Scala.The advantage to using generics is that you are creating a family of types. Nobody will need to subclass \u2014they can just use , , etc.If you use an abstract type, then people will be forced to create a subclass. People will need classes like , , etc.You need to decide which is better for your particular need.You may use abstract types in conjunction with type parameters to establish custom templates.Let's assume you need to establish a pattern with three connected traits:in the way that arguments mentioned in type parameters are AA,BB,CC itself respectfullyYou may come with some kind of code:which would not work in this simple way because of type parameter bonds. You need to made it covariant to inherit correctlyThis one sample would compile but it sets strong requirements on variance rules and can not be used in some occasionsThe compiler will object with bunch of variance check errorsIn that case you may gather all type requirements in additional trait and parametrize other traits over itNow we can write concrete representation for the described pattern, define left and join methods in all classes and get right and double for freeSo, both abstract types and type parameters are used for creating abstractions. They both have weak and strong point. Abstract types are more specific and capable to describe any type structure but is verbose and require to explicit specified. Type parameters can create bunch of types instantly but gives you additional worry about inheritance and type bounds.They give synergy to each other and can be used in conjunction for creating complex abstractions that can't be expressed with only one of them."},
{"body": "You can find the following on the web:So with this in mind, it is difficult to distinguish between ,  and , therefore the question above.Let me make up for starting some of this confusion by pitching in with some disambiguation.  I like to use the analogy to the value level to explain this, as people tend to be more familiar with it.Value constructors are usually called \"functions\" or \"methods\". These \"constructors\" are also said to be \"polymorphic\" (because they can be used to construct \"stuff\" of varying \"shape\"), or \"abstractions\" (since they abstract over what varies between different polymorphic instantiations). In the context of abstraction/polymorphism, first-order refers to \"single use\" of abstraction: you abstract over a type once, but that type itself cannot abstract over anything. Java 5 generics are first-order.The first-order interpretation of the above characterizations of abstractions are:To emphasize there's no abstraction involved (I guess you could call this \"zero-order\", but I have not seen this used anywhere), such as the value  or the type , we usually say something is a \"proper\" value or type. A proper value is \"immediately usable\" in the sense that it is not waiting for arguments (it does not abstract over them). Think of them as values that you can easily print/inspect (serializing a function is cheating!). A proper type is a type that classifies values (including value constructors), type constructors do not classify any values (they first need to be applied to the right type arguments to yield a proper type). To instantiate a type, it's necessary (but not sufficient) that it be a proper type. (It might be an abstract class, or a class that you don't have access to.)\"Higher-order\" is simply a generic term that means repeated use of polymorphism/abstraction. It means the same thing for polymorphic types and values. Concretely, a higher-order abstraction abstracts over something that abstracts over something. For types, the term \"higher-kinded\" is a special-purpose version of the more general \"higher-order\". Thus, the higher-order version of our characterization becomes:Thus, \"higher-order\" simply means that when you say \"abstracting over X\", you really mean it! The  that is abstracted over does not lose its own \"abstraction rights\": it can abstract all it wants. (By the way, I use the verb \"abstract\" here to mean: to leave out something that is not essential for the definition of a value or type, so that it can be varied/provided by the user of the abstraction as an argument.)Here are some examples (inspired by Lutz's questions by email) of proper, first-order and higher-order values and types:Where the used classes were defined as:To avoid the indirection through defining classes, you need to somehow express anonymous type functions, which are not expressible directly in Scala, but you can use structural types without too much syntactic overhead (the -style is due to  afaik):In some hypothetical future version of Scala that supports anonymous type functions, you could shorten that last line from the examples to:(On a personal note, I regret ever having talked about \"higher-kinded types\", they're just types after all! When you absolutely need to disambiguate, I suggest saying things like \"type constructor parameter\", \"type constructor member\", or \"type constructor alias\", to emphasize that you're not talking about just proper types.)ps: To complicate matters further, \"polymorphic\" is ambiguous in a different way, since a polymorphic type sometimes means a universally quantified type, such as , which is a proper type, since it classifies polymorphic values (in Scala, this value can be written as the structural type )(This answer is an attempt to decorate Adriaan Moors answer by some graphical and historical information.)Higher kinded types are part of Scala since 2.5. The  of ordinary types like  and , whose instances are values, is . The kind of unary type constructors like  is ; binary type constructors like  have () kind , and so on. You can view types like  and  as type-level functions: they take one or more types, and return a type.A function is  if it has an  greater than 1, where the order is (informally) the nesting depth, to the left, of function arrows:So, long story short, a  type is just a type-level higher-order function.I would say: A higher kinded type  a type constructor. E.g. consider Here  is a \"higher kinded type\" (as used in the \"Generics of a Higher Kind\" paper). It is not a concrete (\"first-order\") type constructor like  (which abstracts over proper types only). It abstracts over all unary (\"first-order\") type constructors (as denoted with ).Or to put it in another way: In Java, we have clearly type constructors (e.g. ), but we have no \"higher kinded types\", because we can't abstract over them (e.g. we can't write the  interface defined above - at least not ).The term \"higher order (type constructor) polymorphism\" is used to describe systems that support \"higher kinded types\"."},
{"body": "The use of symbol literals is not immediately clear from what I've read up on Scala. Would anyone care to share some real world uses?  Is there a particular Java idiom being covered by symbol literals?  What languages have similar constructs?  I'm coming from a Python background and not sure there's anything analogous in that language.What would motivate me to use 'HelloWorld vs \"HelloWorld\"?ThanksIn Java terms, symbols are interned strings. This means, for example, that reference equality comparison ( in Scala and  in Java) gives the same result as normal equality comparison ( in Scala and  in Java):  will return true, while  might not, depending on JVM's whims (well, it should for literals, but not for strings created dynamically in general).Other languages which use symbols are Lisp (which uses  like Scala), Ruby (), Erlang and Prolog (; they are called atoms instead of symbols).I would use a symbol when I don't care about the structure of a string and use it purely as a name for something. For example, if I have a database table representing CDs, which includes a column named \"price\", I don't care that the second character in \"price\" is \"r\", or about concatenating column names; so a database library in Scala could reasonably use symbols for table and column names.If you have plain strings representing say method names in code, that perhaps get passed around, you're not quite conveying things appropriately. This is sort of the Data/Code boundary issue, it's not always easy to the draw the line, but if we were to say that in that example those method names are more code than they are data, then we want something to clearly identify that.A Symbol Literal comes into play where it clearly differentiates just any old string data with a construct being used in the code. It's just really there where you want to indicate, this isn't just some string data, but in fact in some way part of the code. The idea being things like your IDE would highlight it differently, and given the tooling, you could refactor on those, rather than doing text search/replace.This  discusses it fairly well.Python mantains an internal global table of  with the names of all variables, functions, modules, etc. With this table, the interpreter can make faster searchs and optimizations. You can force this process with the  function ( in python3).Also, Java and Scala automatically use  for faster searchs. With scala, you can use the  method to force the intern of a string, but this process don't works with all strings. Symbols benefit from being guaranteed to be interned, so a single reference equality check is both sufficient to prove equality or inequality."},
{"body": "Often in the Scala literature, I encounter the phrase \"abstract over\", but I don't understand the intent.  , Martin Odersky writesAs another example, in the  paper, I have read that first order generics \"abstract over types\", while monads \"abstract over type constructors\".  And we also see phrases like this in the .  To quote one of many such examples:Even relevant stack overflow questions use this terminology.  So... what does \"abstract over\" actually mean?In algebra, as in everyday concept formation, abstractions are formed by grouping units by some essential characteristics and omitting their specific other characteristics. The abstraction is unified under a single symbol or word denoting their similarities. We say that we  the differences, but this really means we're  by the similarities.For example, consider a program that takes the sum of the numbers , , and :This program is not very interesting, since it's not very abstract. So we can  the specific numbers, by integrating all lists of numbers under a single symbol :And we don't particularly care that it's a List either. List is a specific type constructor (takes a type and returns a type), but we can  the type constructor by specifying which essential characteristic we want (that it can be folded):And we can have implicit  instances for  and any other thing we can fold. What's more, we can  both the operation and the type of the operands:Now we have something quite general. The method  will fold any  given that we can prove that  is foldable and that  is a monoid or can be mapped into one. For example:We have  monoids and foldables.To a first approximation, being able to \"abstract over\" something means that instead of using that something directly, you can make a parameter of it, or otherwise use it \"anonymously\".  Scala allows you to abstract over types, by allowing classes, methods, and values to have type parameters, and values to have abstract (or anonymous) types.Scala allows you to abstract over actions, by allowing methods to have function parameters.Scala allows you to abstract over features, by allowing types to be defined structurally.Scala allows you to abstract over type parameters, by allowing higher-order type parameters.Scala allows you to abstract over data access patterns, by allowing you to create extractors.Scala allows you to abstract over \"things that can be used as something else\", by allowing implicit conversions as parameters.  Haskell does similarly with type classes.Scala doesn't (yet) allow you to abstract over classes.  You can't pass a class to something, and then use that class to create new objects.   Other languages do allow abstraction over classes.  (\"Monads abstract over type constructors\" is only true in a very restrictive way.  Don't get hung up on it until you have your \"Aha!  I understand monads!!\" moment.)The ability to abstract over some aspect of computation is basically what allows code reuse, and enables the creation of libraries of functionality.  Scala allows many more sorts of things to be abstracted over than more mainstream languages, and libraries in Scala can be correspondingly more powerful.An abstraction is a sort of generalization. Not only in Scala but many languages there is a need to have such mechanisms to reduce complexity(or at least create a hierarchy that partitions information into easier to understand pieces).A class is an abstraction over a simple data type. It is sort of like a basic type but actually generalizes them. So a class is more than a simple data type but has many things in common with it.When he says \"abstracting over\" he means the process by which you generalize. So if you are abstracting over methods as parameters you are generalizing the process of doing that. e.g., instead of passing methods to functions you might create some type of generalized way to handle it(such as not passing methods at all but building up a special system to deal with it).In this case he specifically means the process of abstracting a problem and creating a oop like solution to the problem. C has very little ability to abstract(you can do it but it gets messy real quick and the language doesn't directly support it). If you wrote it in C++ you could use oop concepts to reduce the complexity of the problem(well, it's the same complexity but the conceptualization is generally easier(at least once you learn to think in terms of abstractions)).e.g., If I needed a special data type that was like an int but, lets say restricted I could abstract over it by creating a new type that could be used like an int but had those properties I needed. The process I would use to do such a thing would be called an \"abstracting\".Here is my narrow show and tell interpretation. It's self-explanatory and runs in the REPL.The other answers give already a good idea of what kinds of abstractions exist. Lets go over the quotes one by one, and provide an example:Pass function as a parameter:  Clearly  is passed as parameter here.  itself abstracts over a function that does a certain specialiced thing with each list element.  specifies a type paramter (String). You could write a collection type which uses abstract type members instead: . One difference is that you have to write  but , so the element type is kind of \"hidden\" in the second method.In Swing an event just \"happens\" out of the blue, and you have to deal with it here and now. Event streams allow you to do all the plumbing an wiring in a more declarative way. E.g. when you want to change the responsible listener in Swing, you have to unregister the old and to register the new one, and to know all the gory details (e.g. threading issues). With event streams, the  of the events becomes a thing you can simply pass around, making it not very different from a byte or char stream, hence a more \"abstract\" concept.The Buffer class above is already an example for this."},
{"body": "In various Scala literature I see some self-type annotations using \"this\" and others using \"self\":Is there any real difference between using \"this\" or \"self\"?  Does it matter at all what name you use?  Is this just as valid?All three forms are valid, and have the effect that  is assumed as the type of  in class . The variantsintroduce  (respectively, ) as an alias for  in trait . This is useful for accessing the  reference from an inner class. I.e. you could then use  instead of  when accessing the  reference of the trait  from a class nested within it. The third variant,does not introduce an alias for ; it just sets the self type.There is a difference in that  always refers to the object defined by the innermost template.So, if you call your self-type , you could still refer to it as  (unless, of course, you are in an inner template in which case  will refer to the object defined by it \u2013 and unless you don\u2019t give the inner template\u2019s self-type the same name) but obviously not the other way round."},
{"body": "I have a file  in .How can I read that file into a new  in my test  in ?Resources are meant to be accessed using the special  style methods that Java provides. Given your example of  being in , you can access it in a test like so:Of course that  is now just a normal Scala IO object so you can do anything you want with it, like reading the contents and using it for test data.There are other methods to get the resource as well (for example as a stream). For more information look at the  methods on the .sbt copies files from  to .You can access the resources in your tests as follows:It does assume that  was directly under the folder . Add any subdirectories, otherwise.Another alternative (especially if you need to access resource as a ); is to obtain it's path via:as has been pointed out in And in cases where  does not work (don't know nor care when or why exactly),  from Google Guava usually doesTo know where you are in file system during test, you can do something like this in a dummy test:Then, when you know your path, in your test you can use it as:"},
{"body": "I need to build a JSON string, something like this:I need to be able to add rows to the , something like What is the closest library/solution to this?Unfortunately writing a JSON library is the Scala community's version of coding a todo list app.There are quite a variety of alternatives. I list them in no particular order, with notes:\u00a7 = has Scalaz integration, \u00b1 = supports interop with Jackson In  we use json4s with the Jackson back-end; we've had good experiences with Argonaut too.Lift-json is at version 2.6 and it works really well (and is also very well supported, the maintainer is always ready to fix any bugs users may find.\nYou can find examples using it on the The maintainer (Joni Freeman) is always reachable on the  list. There are also other users on the mailing list  who are very helpful as well.As @Alexey points out, if you want to use the library with other Scala version, say , change  and use  as follows:You can check the  site to find out the latest version as time goes by.I suggest using , it supports most basic type conversions:Number 7 on the list is Jackson, not using Jerkson. It has support for Scala objects, (case classes etc).Below is an example of how I use it.This makes it very simple. In addition is the XmlSerializer and support for JAXB Annotations is very handy. This blog post describes it's use with JAXB Annotations and the Play Framework.Here is my current JacksonMapper.Maybe I've late a bit, but you really should try to use json library from play framework.\nYou could look at . In current 2.1.1 release you could not separately use it without whole play 2, so dependency will looks like this:It will bring you whole play framework with all stuff on board. But as I know guys from Typesafe have a plan to separate it in 2.2 release. So, there is standalone  from 2.2-snapshot. Here is a basic implementation of writing and then reading  file using . is a very flexible JSON parser library in Scala. It also allows generation of custom ASTs; you just need to supply it with a small trait to map to the AST.Worked great for a recent project that needed a little bit of JSON parsing.You should check .\nIt just works and is much easier to use than most of the existing alternatives in Scala. It is fast, has many features and integrations with some other libs (jodatime, json4s DOM api...).All that without any fancy unecessary code like implicits, custom readers/writers for basic cases, ilisible API due to operator overload...Using it is as easy as:Disclaimer: I am Gensons author, but that doesn't meen I am not objective :)@AlaxDean's #7 answer,  is the only one that I was able to get working quickly with sbt and intellij. Actually json4s also took little time but dealing with a raw AST is not what I wanted. I got argonaut to work by putting in a single line into my build.st:And then a simple test to see if it I could get JSON:And then Make sure you are familiar with  which is just a value that can also be null (null safe I guess). Argonaut makes use of  so if you see something you don't understand like the symbol  (an or operation) it's probably Scalaz. Rapture seems to be missing in the list of the answers.\nIt can be obtained from  and allows you (among other thing) to:I don't want to copy/paste Rapture examples from it's page. A nice presentation about Rapture's features was given by Jon Pretty at SBTB 2014: You can try this:\nIt's simple, and has only one scala file with less than 300 lines code.There are samples:I use PLAY JSON library\nyou can find the mavn repo for only the JSON library not the whole framework hereA very good tutorials about how to use them, are available here:Let me also give you the  version:Play released its module for dealing with JSON independently from Play Framework, Made a blog post about that, check it out at Using case classes, and  (already included in Play Framework) you case convert between json and case classes with a simple one-liner implicitI use  which has the big advantage that it will handle nested case classes automatically:Add this to your  to use uPickle:"},
{"body": "I've heard that Scala has path-dependent types. It's something to do with inner-classes but what does this actually mean and why do I care?My favorite example:So, the type of  is dependent on the instance of  from which it was instantiated. There are all sort of things that can be accomplished with this, giving a sort of type safety that is dependent on values and not types alone.This might sound like dependent types, but it is more limited. For example, the type of  is dependent on the value of . Above, the last line doesn't work because the type of  is , while 's type is . Note that one can use another identifier with the same type of , so it is not the   that is associated with the type. For example, the following works:"},
{"body": "There are several ways to construct an immutable list in Scala (see contrived example code below). You can use a mutable ListBuffer, create a  list and modify it, use a  method, and probably others that I don't know about.Instinctively, I use the ListBuffer, but I don't have a good reason for doing so. Is there a preferred or idiomatic method for creating a list, or are there situations that are best for one method over another? is a mutable list which has constant-time append, and constant-time conversion into a . is immutable and has constant-time prepend and linear-time append.How you construct your list depends on the algorithm you'll use the list for and the order in which you get the elements to create it.For instance, if you get the elements in the opposite order of when they are going to be used, then you can just use a  and do prepends. Whether you'll do so with a tail-recursive function, , or something else is not really relevant.If you get the elements in the same order you use them, then a  is most likely a preferable choice, if performance is critical.But, if you are not on a critical path and the input is low enough, you can always  the list later, or just , or  the input, which is linear-time.What you  do is use a  and append to it. This will give you much worse performance than just prepending and reversing at the end.And for simple cases: :) Uhmm.. these seem too complex to me. May I proposeorYou want to focus on immutability in Scala generally by eliminating any vars.\nReadability is still important for your fellow man so:Try:You probably don't even need to convert to a list in most cases :)The indexed seq will have everything you need:That is, you can now work on that IndexedSeq:I always prefer List and I use \"fold/reduce\" before \"for comprehension\".  However, \"for comprehension\" is preferred if nested \"folds\" are required.  Recursion is the last resort if I can not accomplish the task using \"fold/reduce/for\".so for your example, I will do:before I do:Note: I use \"foldRight(:\\)\" instead of \"foldLeft(/:)\" here because of the order of \"_\"s.  For a version that does not throw StackOverflowException, use \"foldLeft\" instead.Using , like this,The Scala collection classes are going to be redesigned as of Scala 2.8, so be prepared to change the way you create lists very soon.What is the forward compatible way of creating a List? I have no idea since I haven't read the 2.8 docs yet.As a new scala developer i wrote small test to check list creation time with suggested methods above. It looks like  (for ( p <- ( 0 to x ) ) yield p) toList  the fastest approach.  just an example that uses collection.breakOut"},
{"body": "How do you provide overloaded constructors in Scala?It's worth explicitly mentioning that Auxiliary Constructors in Scala must either call the primary constructor (as in landon9720's) answer, or another auxiliary constructor from the same class, as their first action. They cannot simply call the superclass's constructor explicitly or implicitly as they can in Java. This ensures that the primary constructor is the sole point of entry to the class.As of Scala 2.8.0 you can also have default values for contructor- and method parameters. Like thisParameters with default values must come after the ones with no default values in the parameter list.While looking at my code, I suddenly realized that I did kind of an overload a constructor. I then remembered that question and came back to give another answer:In Scala, you can\u2019t overload constructors, but you can do this with functions.Also, many choose to make the  function of a companion object a factory for the respective class.Making this class abstract and overloading the  function to implement-instantiate this class, you have your overloaded \u201cconstructor\u201d:Note that I explicitly define each  to return , else it would return a duck-typed .I thought may be  (2008-11-11) could add more information.Try this"},
{"body": "Both are BDD (Behavior Driven Development) capable unit test frameworks for Scala written in Scala. And   may also involve the  framework. But what does Specs offer ScalaTest doesn't? What are the differences?Specs and ScalaTest are both good tools with happy users, but they differ in several ways. You will probably want to pick one as your main testing tool in Scala, but need not give up the other because you can use pieces of both. If you like ScalaTest's  syntax and specs' Mockito syntax, for example, you can put both jar files in your classpath and use both at the same time. Here I'll try and capture the main design philosophy differences I've noticed between specs and ScalaTest.Probably the main philosophical difference between the tools is that specs is designed for Behavior-Driven Development (BDD), whereas ScalaTest is more general. ScalaTest provides traits that you can mix together to get the behavior you prefer in your test classes, including BDD, and you can also easily define your own behavior if you want something different.ScalaTest supports BDD through its , , , , and  traits, and also has traits that you can mix in to get a nice matcher syntax. If you like \"should\", you mix in ShouldMatchers. If you like \"must\", you mix in . But if you like BDD but don't like matcher syntax, you can just use one of ScalaTest's Spec traits without mixing in a matchers trait. Specs has a Specification class that you extend, and you must use the word \"must\" in your matcher expressions. A big philosophical difference that is evident here is that ScalaTest gives you a lot more choices. To make this space of choice easier to navigate, I provide a decision tree here:The matcher syntax is also different between ScalaTest and specs. In ScalaTest I tried to see how far I could go with operator notation, and ended up with matcher expressions that read very much like English sentences, with spaces between the words. Specs matcher syntax runs words together more with camel case.Specs has more matchers than ScalaTest, and that I think reflects a difference in design attitude. I actually cut probably 2/3 of the matcher syntax I built and considered for release. I will add more matchers in future releases, but wanted to be sure I knew users actually wanted something before I added it. However ScalaTest's matchers includes a dynamic property matcher syntax takes up some of that slack. For example in Specs you can write on a :This will invoke the  and make sure it is true. ScalaTest does not have any special matchers for  currently, but in ScalaTest, you could just use a dynamic check like this:Anytime you pass a symbol in after , it will use reflection to look for (in this case) a method or field named  or a method named . There's also a way to make this static, by defining a  (which requires only 2 or 3 lines of code usually). So basically in ScalaTest I try to provide more functionality with less API.Another general design attitude difference between specs and ScalaTest\ninvolves implicit conversions. By default you get only one implicit conversion when you use ScalaTest, which is the one that puts the  operator on everything. (If you need to, you can \"turn off\" this implicit conversion with one line of code. The only reason you would need to do that is if you were trying to test something that has its own  operator, and you get a conflict.) ScalaTest defines many other implicit conversions, but to use them you need to explicitly \"invite\" them into your code by mixing in a trait or doing an import. When you extend class  in specs I think you pretty much get dozens of implicit conversions by default. I'm not sure how much that will matter in practice, but I figure people will want to test code that uses their own implicits, and sometimes there may be a conflict between the test framework's implicits and those of the production code. When that happens I think it may be easier to work around the problem in ScalaTest than specs.Another difference in design attitude that I've noticed is comfort with operators. One goal I had was that any programmer looking at someone else's test code that uses ScalaTest would be able to guess what the meaning was without looking anything up in the ScalaTest documentation. I wanted ScalaTest client code to be drop dead obvious. One way that goal manifested itself is that ScalaTest is very conservative about operators. I only define five operators in ScalaTest:That's it. So these things pretty much look like what mean. If you see in someone else's code:My hope is that you won't need to run to the API documentation to guess what that  means. By contrast, specs is much freer with operators. Nothing wrong with that, but it is a difference. Operators can make code more concise, but the tradeoff is you may have to run to the documentation when you find things like  , , , , , or  (which all have special meanings in Specs) in your colleague's test code.One other philosophical difference is that I do try and make it just slightly easier in ScalaTest to use a functional style when you need to share a fixture, whereas Specs by default continues the tradition of the  and  approach popularized by JUnit, in which you reassign vars before each test. However if you want to test that way, it is also very easy in ScalaTest. You just need to mix in the  trait.For more insight into ScalaTest, you can watch the \"Get Higher with ScalaTest\" presentation I gave at the 2009 Devoxx conference here:The main differences are (mostly from a specs point of view :-) ):This is certainly a very partial and biased comparison and many other differences exist (and the libraries are still evolving, ...). At the end of the day I think that it really depends on your testing/specifying style. If it's simple (simple specification structure, setups, expectations, ...) then both libraries will appear very similar. Otherwise, both have their take on how things should be done. As a last example of this you can have a look at tagging: in  and in .I hope this helps.As far as I know, barring a few highly specialized features, it comes down to personal preference according to the style.IDE support may be another pointI've been trying to get Specs to work with Eclipse through JUnit, and I found the official solution to be a bit \"hacky\". Specs setup: ScalaTest's integration (also through JUnit) with seems a bit less hacky. Still, I haven't got any of them to work as well as JUnit and Java.ScalaTest setup: If one decision factor is the compile time, We're currently using specs2 in our project, but suffer from slow compile times in tests. I just finished a POC on moving to scalatest and saw compile times drop by a factor of about 0.82 just by switching the 2 frameworks in some of our sources.The main differences between ScalaTest and Specs2 are:"},
{"body": "I'm trying to represent a function that takes no arguments and returns no value (I'm simulating the setTimeout function in JavaScript, if you must know.)doesn't compile, saying \" `val' parameters may not be call-by-name\"compiles, but has to be invoked strangely, instead ofI have to do thisWhat also works isbut is invoked in an even-less-sensible way(What would a variable of type Unit be?)  What I  of course is a constructor that can be invoke the way I would invoke it if it were an ordinary function:Give baby his bottle!The  notation stands for call-by-name, which is one of the  parameters can be passed. If you aren't familiar with them, I recommend taking some time to read that wikipedia article, even though nowadays it is mostly call-by-value and call-by-reference.What it means is that what is passed is  for the value name inside the function. For example, take this function:If I call it like thisThen the code will execute like thisThough that raises the point of what happens if there's a identifier name clash. In traditional call-by-name, a mechanism called capture-avoiding substitution takes place to avoid name clashes. In Scala, however, this is implemented in another way with the same result -- identifier names inside the parameter can't refer to or shadow identifiers in the called function.There are some other points related to call-by-name that I'll speak of after explaining the other two.The syntax  stands for the type of a . That is, a function which takes no parameters and returns something. This is equivalent to, say, calling the method  -- it takes no parameters and returns a number.It is interesting, however, that this syntax is very similar to the syntax for a , which is the cause for some confusion. For example,is an anonymous function literal of arity 0, whose  isSo we could write:It is important not to confuse the type with the value, however.This is actually just a , whose first parameter is of type . Other ways to write it would be  or . The thing is... this is unlikely to ever be what one wants. The  type's main purpose is indicating a value one is not interested in, so doesn't make sense to  that value.Consider, for instance,What could one possibly do with ? It can only have a single value, so one need not receive it. One possible use would be chaining functions returning :Because  is only defined on , and the functions we are chaining are returning , we had to define them as being of type  to be able to chain them.The first source of confusion is thinking the similarity between type and literal that exists for 0-arity functions also exists for call-by-name. In other words, thinking that, becauseis a literal for , thenwould be a literal for . It is not. That is a , not a literal. Another source of confusion is that  type's  is written , which looks like a 0-arity parameter list (but it is not).The  modifier makes implicit  out of each argument to the constructor.  Hence (as someone noted) if you remove  you can use a call-by-name parameter.  The compiler could probably allow it anyway, but it might surprise people if it created  instead of morphing into .When you change to  now your case just takes a function rather than a call-by-name parameter.  Obviously the function can be stored in  so there's no problem.The easiest way to get what you want ( where a call-by-name parameter is used to pass a lambda) is probably to skip the  and explicitly create the  that you couldn't get in the first place:In use:"},
{"body": "As noted in , I'm using Scalaz 7 iteratees to process a large (i.e., unbounded) stream of data in constant heap space.My code looks like this: I seem to have run into a memory leak, but I'm not familiar enough with Scalaz/FP to know whether the bug is in Scalaz or in my code. Intuitively, I expect this code to require only (on the order of)  times the -size space.Note: I found  in which an  was encountered, but my code is not using .I ran some tests to try and isolate the problem. To summarize, the leak only appears to arise when both  and  are used.Code for the tests:This will come as little consolation for anyone who's stuck with the older  API, but I recently verified that an equivalent test passes against the . This is a newer stream processing API that is intended to replace .For completeness, here's the test code:This should work with any value for the  parameter (provided you're willing to wait long enough) -- I tested with 2^14 32MiB arrays (i.e., a total of half a TiB of memory allocated over time)."},
{"body": "If I have a collection  of type  and there is a property  on  (of type , say), what is the best way to do a ?One way is the following:But now I need a  map. Is there a better way of doing this so that it's in 1 line and I end up with an  Map? (Obviously I could turn the above into a simple library utility, as I would in Java, but I suspect that in Scala there is no need)You can usebut be aware that this needs 2 traversals.You can construct a Map with a variable number of tuples.  So use the map method on the collection to convert it into a collection of tuples and then use the : _* trick to convert the result into a variable argument.In addition to @James Iry's solution, it is also possible to accomplish this using a fold.  I suspect that this solution is slightly faster than the tuple method (fewer garbage objects are created):Another solution (might not work for all types)this avoids the creation of the intermediary list, more info here:\nWhat you're trying to achieve is a bit undefined.\nWhat if two or more items in  share the same ? Which item will be mapped to that  in the map?The more accurate way of looking at this is yielding a map between  and all  items that have it:This could be easily achieved with :If you still want the original map, you can, for instance, map  to the first  that has it:Works well and is very intuitivThis can be implemented immutably and with a single traversal by folding through the collection as follows.The solution works because adding to an immutable Map returns a new immutable Map with the additional entry and this value serves as the accumulator through the fold operation.The tradeoff here is the simplicity of the code versus its efficiency. So, for large collections, this approach may be more suitable than using 2 traversal implementations such as applying  and .For what it's worth, here are two  ways of doing it:This is probably not the most efficient way to turn a list to map, but it makes the calling code more readable. I used implicit conversions to add a  method to List:Calling code example:Note that because of the implicit conversion, the caller code needs to import scala's implicitConversions.This works for me:The Map has to be mutable and the Map has to be return since adding to a mutable Map does not return a map."},
{"body": "Since Scala 2.7.2 there is something called  which is a workaround for Java's type erasure. But how does  work exactly and why / when do you need to use it?The blog post  by Jorge Ortiz explains some of it, but it doesn't explain how to use it together with .Also, what is , what's the difference with ?I have some code (part of a larger program, can't easily include it here) that has some warnings with regard to type erasure; I suspect I can solve these by using manifests, but I'm not sure exactly how.The compiler knows more information about types than the JVM runtime can easily represent.  A Manifest is a way for the compiler to send an inter-dimensional message to the code at runtime about the type information that was lost.This is similar to how the Kleptonians have left encoded messages in fossil records and the \"junk\" DNA of humans.  Due to limitations of lightspeed and gravitational resonance fields, they are unable to communicate directly.  But, if you know how to tune into their signal, you can benefit in ways you cannot imagine, from deciding what to eat for lunch or which lotto number to play.It isn't clear if a Manifest would benefit the errors you are seeing without knowing more detail.One common use of Manifests is to have your code behave differently based on the static type of a collection.  For example, what if you wanted to treat a List[String] differently from other types of a List:A reflection-based solution to this would probably involve inspecting each element of the list.A context bound seems most suited to using type-classes in scala, and is well explained here by Debasish Ghosh:\nContext bounds can also just make the method signatures more readable.  For example, the above function could be re-written using context bounds like so:Not a complete answer, but regarding the difference between  and , you can find an example in the :Example:(See this )A Manifest was intended to reify generic types that get type-erased to run on the JVM (which does not support generics). However, they had some serious issues: they were too simplistic, and were unable to fully support Scala's type system. They were thus  in Scala 2.10, and are replaced with s (which are essentially what the Scala compiler itself uses to represent types, and therefore fully support Scala types). For more details on the difference, see:Before 2013-01-04, .Let's also chck out  in  sources (), we see:So with regards to following example code:we can see that the    searches for an implicit  which satisfies the  you provide in our example code it was .  So when you call something like:you are checking if the current  which you defined in your function is of type  and as the  is a function of type  it would search for a specific  and it would find if there is such an implicit."},
{"body": "Which build tool is the best for Scala? What are the pros and cons of each of them? How to I determine which one of them to use in a project?We're using Maven to build Scala projects at work because it integrates well with our CI server.  We could just run a shell script to kick off a build, of course, but we've got a bunch of other information coming out of Maven that we want to go into CI.  That's about the only reason I can think of to use Maven for a Scala project.Otherwise, just use SBT.  You get access to the same dependencies (really the best part about maven, IMHO).  You also get the incremental compilation, which is huge.  The ability to start up a shell inside of your project, which is also great.  ScalaMock only works with SBT, and you're probably going to want to use that rather than a Java mocking library.  On top of that, it's  easier to extend SBT since you can write full scala code in the build file, so you don't have to go through all the rigamarole of writing a Mojo.In short, just use SBT unless you really need tight integration into your CI server.The question is in danger of just generating lots of opinions; it would be better to have a clear list of requirements or a description of your environment, previous knowledge, etc.FWIW, there are more opinions in .My 2c are: Go with sbt if you don't have specific requirementsSome people hate sbt because it uses Ivy for dependency management (I can't comment on its pros and cons, but most of the time it is a non-issue), some people hate sbt because you specify the build file in terms of a Scala DSL instead of XML. Some people were disappointed that sbt's format changed from v0.7 to v0.10, but obviously, migration won't affect you if you start from scratch."},
{"body": "This is a follow-up to the  to my previous question.Suppose I need to map each item  of  to  with function  and generate .Obviously I cannot just call  on the list but I can use the list . The zipper is a cursor to move around a list. It provides access to the current element () and its neighbors. Now I can replace my  with   and pass this new function  to  method of . The  works like this: it calls that  with the zipper, then moves the zipper, calls with the  \"moved\" zipper, moves the zipper again and so on, and so on ... until the zipper reaches the end of the list.  Finally, the  returns a new zipper of type , which can be transformed to the list and so the problem is solved.Now note the symmetry between  and  That's why  is a  and  is a .Does it make sense ?A  can be viewed as a comonad just as well (in multiple ways), while a  can be cast as a monad (also in many ways). The difference is in whether you are conceptually focused on \"appending\" data constructively to a state machine (that's what the  interface is about), or \"extracting\" state from it \"deconstructively\" (that's what the  does).It is not easy to answer the question, stated as \"does this understanding make sense\", however. In some sense it does, in another it does not."},
{"body": "Given the following RDD:where keys appear either twice (k1) or once (k2), never more than that. I want to get:One approach is to use reduceByKey:However, it's too slow for arrays with 7B elements. \nIs there any faster approach on this specific case?What (probably) takes the time here is shuffling the data: when you want to group two or more records together, they must reside within the same partition, so Spark has to first  the records so that all records with same key are in a single partition.Now, even if each key has two records at most, this shuffle will have to take place,  - for example, if you loaded this RDD from HDFS and you somehow know that each key resides on a single file part to begin with. In that (unlikely) case, you can use  to perform the grouping yourself on each partition separately, thus saving the shuffle:None of this is special to the case where the maximum repetition of each key is 2, by the way. "},
{"body": "I'm learning  and  at the same time and I got stuck on understanding the syntax used to inintialize the  in the Boot.scala:What exactly is the meaning of the SiteMap parameter?\nI see that the value  is a list of Menu. What is the ?\nAt first I thought it is a method on the List, but I am unable to find such definition...OK, after my colleague mentioned to me, that he encountered this secret incantation in the  book, I did a search in my copy and found it described in . (Though you need to search with space between the colon and underscore :-/ ) There is a one sentence to explain it as:I find the  to be more helpful.So  is  a type declaration that tells the compiler to treat  as  (aka variable-length argument list \u2014 )."},
{"body": "I am attempting to use JSON to send data between the browser and my app.I am attempting to use Lift 1.0 to create and parse JSON strings, but for some reason I am unable to parse the JSON I just constructed:How do I programmatically construct a valid JSON message in Scala/Lift that can also be parsed again?You are using Lift 1.0's , which produces JSON with single-quoted strings and attempting to parse it with scala's parser, which only supports double-quoted strings.It is important to realize that there are multiple definitions for JSON.Are single-quoted strings valid in JSON?Lift and Scala provide many ways to parse JSON, sometimes with differing behavior between versions.The strings accepted by these parsers are not equivalent.Here are some comments and examples of the various methods to product and parse JSON strings.example:example:example:example:example:example:example:example:Take a look at .  It's really nice to use and it leverages some of the new tools from  and .  Plus, you can use it from .Taken from the :"},
{"body": "I'm working my way through , and though I'm tempted to look at things from the perspective of Python, I don't want to program \"Python in Scala.\"I'm not quite sure what to do as far as control flow goes: in Python, we use  to death, and we love it. A very similar construct exists in Scala which Odersky calls a for , probably to distinguish it from the Java for loop. Also, Scala has a  attribute (I guess it would be an attribute, I don't know enough about Scala to name it correctly) for iterable data types. It doesn't seem like I can use  to do much more than call one function for each item in the container, though.This leaves me with a few questions. First, are for expressions important/heavily used constructs in Scala like they are in Python, and second, when should I use  instead of a for expression (other than the obvious case of calling a function on each item of a container)?I hope I'm not being terribly ambiguous or overbroad, but I'm just trying to grok some of the design/language fundamentals in Scala (which seems very cool so far).python uses  in list comprehensions and generator expressions. Those are  similar to the scala  expression:Each construct can take a number of generators/iterators, apply filters expressions and yield a combined expression. In python the  is roughly equivalent to:In scala  is roughly equivalent to:If you love the python  syntax, you'll likely love the scala  expression.Scala has some additional syntax and translation twists. Syntax wise  can be used with braces so that you can put statements on separate lines. You can also perform value assignments.Also  really performs a match . If there is no match those elements are ignored from the iteration. I think  has an important place in the language. I can tell from the fact there is a whole chapter (23) about it in the book you're reading!Yes, Scala for comprehensions (as they are commonly known) are used a lot, but they are really just syntactic sugar for a particular combination of methods, and many prefer to call these methods directly instead of using the syntactic sugar.To better understand Scala for comprehensions, please refer to . In particular, you'll see that  is the same thing as .Now, you mention that you don't seem much use with  method, but I'll point out that almost all of the methods of Scala collections are (or can be) implemented with just . See the documentation for  -- all of its methods can be implemented with only .Note that Scala's  bears no resemblance to Python's  -- you can look up  question too.With its support for nested iteration, filters, and transformation, I'd say Scala's  is one of the strong points of the language and very central. I tend to favor it over using ,  and .The foreach is a functional style while the for is an imperative style.  If you've ever done any lisp or scheme, you're already familiar with functional programming.  If you haven't then it might be a bit confusing at first.  The first thing I would do is read up on the closure syntax which are anonymous functions you pass into things like foreach.   Once you understand that it will all make more sense.Your questions are largely answered by the following:To summaraize: It's largely stylistic.  Personally, I favor the functional methodology, but prefer the succinctness of comprehensions when dealing with nested loops."},
{"body": "Are there any libraries written for Scala enabling Functional Reactive Programming?See also Odersky et al.'s paper \"\". It explains the library , which was developed for the paper.There's reactive -- .\nThe repository currently contains two projects. reactive-core is a standalone FRP library. reactive-web builds on it to make it very easy to make very dynamic and interactive Lift webapps. Functional Reactive Programming library for the JVM, developed by Netflix.I don't Scala so don't know how good these are but here is a blog with comments that talk about FRP in scala:  and  is the reddit that shows a stackoverflow conversation that led me to the above link.There is  by Li Haoyi. In short, it aims to be a simpler, easy-to-use and more interoperable reimagination of (parts of) Scala.React.Don't be misled by the Rx suffix. Scala.Rx has little to do with Reactive Extensions from .NET. Scala.Rx does not focus so much on asynchrony and event streams as rather time-varying values and the expression of functional dependencies with automatic change propagation.Coursera  course promotes .There is scala-reactive: It is inspired by Microsoft's Reactive Extensions library, with an Observable trait taking the place of IObservable, and tailored to the Scala collections API as much Rx is to the LINQ API."},
{"body": "Just wondering if anyone has experience with the three.  I have used read through some RoR and used Django.  They seem fairly easy to use.  Is lift \"easy\" like these two are?  I know easy is subjective and has no context here.  I mean in a very high level and general sense.I'm currently working on a series of projects in lift, so I'll give my personal experiences.Its a very capable framework - and I find it makes lighter work of application development than the equivalent in rails or django. However, you will need a reasonable understanding of scala in order to get started on sophisticated app development as documentation is existent but limited, and not centralised.Lift itself has cherry-picked a lot of features from existent frameworks - rails and django included - to form, imho, a best of breed framework. It also uses some novel techniques of its own that truly mean you can take a frontend and build an application around it relatively quickly.Following the tutorials, you can create the classic \"todo\" application demonstrated by many frameworks with little to no experience within 2-3 minutes. But I would say that you need to have a reasonable knowledge of Java, and an understanding of scala in order to get the most out of lift.Setting up a comfortable development environment isn't exactly painless, due to spotty scala support in the mainstream IDEs.If you have a weekend you can dedicate to  and  I would highly recommend taking the plunge.As a more direct answer to your question, in terms of \"ease\", I would say that if you are unfamiliar with Java, scala, ruby and python, then of the three, your best bet would be rails - as ruby is a beautiful language, nigh on self-explanatory, well documented, introduces the majority of OO concepts, and is very easy to learn.  Additionally, I know of many people that have actually learned ruby solely through developing in rails.I have to disagree with the other answers that support Lift over RoR and Django.   Lift is incredibly capable, and it is very flexible.  But anyone who thinks it is easier to grok and get rolling with than Django and RoR isn't being pragmatic.  For one there is a lot more documentation on Django and RoR.  Their are training classes, way more books, lots of web entries on solving common problems.  Django has a huge number of plug-ins,etc.   I know it might not seem like these things matter but they do.  When it's hard to hire, hard to train, you have to solve common problems over again, etc it's a real drain.  I'm a fan of lift, and I think one day my recommendation might not be true.  But if your life is dependent on it you might want to take the safe course and go RoR/Django.  I'd have to say it's a bit more difficult to get started with, due to the complexities of the Java/Scala ecosystem (maven? gradle? .war files? Tomcat? Jetty? ) and the necessity of compilation. The exception is if you have Java development experience, or especially Java web development experience, in which case I'm sure all of this stuff is an old hat to you.Once you've gotten past that and have a workflow, it's great. Scala is a fantastic language, and lift is an engaging framework.If ease of start up is your only concern then you may want to go with RoR or Django.  But if long term performance, scalability and maintainability are significant, if you are capable of learning scala, which is a bit more complicated but quite elegant and enjoyable in my experience, then Lift or Play will probably yield greater longevity and adaptability long term, which may greatly out weigh the slightly larger startup time on a significant project.  Take a look at some of the    "},
{"body": "I am a beginner in Scala.  I installed Scala IDE in eclipse and now I want to run my application programme.  It never shows \"run as Scala application\", instead it shows \"run as Java application\" or \"Java applet\"I opened \"run configuration\" and clicked on \"Scala application\" and my project name is \"test\" and second column is of \"Class Main\".  What do I have to fill in?  I filled in \"Main.Scala\", but it states \"could not find mainmain class main.scala\".Can you help me with running this project?If you want to run the whole project it should have a \"main class\", in any of your Scala objects you should be defining:From there it should be \"calling\" the rest of your objects to do whatever the whole project does and in the \"Class Main\" column you should specify the fully qualified name of your object. For instance, if you defined the main in a class called \"Start\" in the package \"starter\", in the \"Class Main\" field you should state \"starter.Start\".But on the other hand if you only want to run a Scala object it should extend App, if it doesn't extend App, Scala IDE won't add the \"Run as Scala Application...\":Right click your project and check the \"Scala Compiler\" settings. Check the \"Project Specific\" checkbox and try checking if you can run your Scala object (which should extend App). make sure your declared  in your source code matches the  under your source directory.in this case, a sourcefile declaring package \"greeter\" will auto-run as scala if the source file is indeed under src/greeter/Hello.scala (and not just under src/Hello.scala)Its a common mistake that doesn't get highlighted by the syntax checker.If you installed Scala plugin for Eclipse, open the Scala perspective. Then right-click on your project and select \"Add Scala Nature\" in \"Configure\" menu.You should now be able to run your Scala applications. Download from this link Restart Eclipse, create Scala Project, then create Scala Object and type this.Run as Scala ApplicationI had issues with the Scala IDE for Eclipse running Scala applications that extend , but running objects with a proper main method, i.e.  always works fine for me.If it is the  you run the Scala IDE for eclipse after setting it up and creating your project, all the thing you need is to just save your project and restart the IDE. At the next start, the \"run as Scala Application\" is appeared and can be used.I was having similar issue. Make sure your java and scala files are not in the same package. I changed the package names and it worked for meUnless you have a strong reason why you need Eclipse, could I recommend that you try IntelliJ?Version 10 was just released earlier today, and the (free) community edition is perfectly happy working with the IntelliJ Scala plugin.Just a pointer ..\nI had faced same difficulty .\nBeing experienced from JAVA , instead of creating a Spark object I was creating spark class that why I was not getting this option .Hope my experience helps . Right click on the Project --> Click on Run Configurations --> In the Run Configurations window select the \"Scala Application\" Try to run the eclipse command of the sbt tool inside your project directory, this will build your scala project for the eclipse IDE. Then you will have no problem to configure your run configuration, It might even be done for you automatically.done! now import your project into Eclipse's workspaceI had this issue using an Eclipse Luna Scala IDE. No of the above solution made it possible to compile my Main.scala file.The problem was the following: My project only referenced the JRE System library but no Scala library. I carried out the following steps:Then, go to the Main.scala file that is lying somewhere in your project folder and contains your main function. If you right-click file, \"Scala Application\" should appear under \"Run as\"."},
{"body": "I have read that Scala's type system is weakened by Java interoperability and therefore cannot perform some of the same powers as Haskell's type system.  Is this true?  Is the weakness because of type erasure, or am I wrong in every way?  Is this difference the reason that Scala has no typeclasses?The big difference is that Scala doesn't have Hindley-Milner global type inference and instead uses a form of local type inference, requiring you to specify types for method parameters and the return type for overloaded or recursive functions.This isn't driven by type erasure or by other requirements of the JVM.  All possible difficulties here can be overcome, and have been, just consider Jaskell - H-M inference doesn't work in an object-oriented context.  Specifically, when type-polymorphism is used (as opposed to the ad-hoc polymorphism of type classes).  This is crucial for strong interop with other Java libraries, and (to a lesser extent) to get the best possible optimisation from the JVM.It's not really valid to state that either Haskell or Scala has a stronger type system, just that they are different.  Both languages are pushing the boundaries for type-based programming in different directions, and each language has unique strengths that are hard to duplicate in the other.Scala's type system is different from Haskell's, although Scala's concepts are sometimes directly inspired by Haskell's strengths and its knowledgeable community of researchers and professionals.Of course, running on a VM not primarily intended for functional programming in the first place creates some compatibility concerns with existing languages targeting this platform.\nBecause most of the reasoning about types happens at compile time, the limitations of Java (as a language and as a platform) at runtime are nothing to be concerned about (except Type Erasure, although exactly this bug seems to make the integration into the Java ecosystem more seamless).As far as I know the only \"compromise\" on the type system level with Java is a special syntax to handle Raw Types. While Scala doesn't even allow Raw Types anymore, it accepts older Java class files with that bug.\nMaybe you have seen code like  (or the longer equivalent ). This is a compatibility feature with Java, but is treated as an existential type internally too and doesn't weaken the type system.Scala's type system does support , although in a more verbose way than Haskell. I suggest reading this paper, which might create a different impression on the relative strength of Scala's type system (the table on page 17 serves as a nice list of very powerful type system concepts).Not necessarily related to the power of the type system is the approach Scala's and Haskell's compilers use to infer types, although it has some impact on the way people write code.\nHaving a powerful type inference algorithm can make it worthwhile to write more abstract code (you can decide yourself if that is a good thing in all cases).In the end Scala's and Haskell's type system are driven by the desire to provide their users with the best tools to solve their problems, but have taken different paths to that goal.another interesting point to consider is that Scala directly supports the classical OO-style. Which means, there are  relations (e.g. List is a subclass of Seq). And this makes type inference more tricky. Add to this the fact that you can mix in traits in Scala, which means that a given type can have multiple supertype relations (making it yet more tricky)Scala does not have , although it may be possible to  in certain cases.I only have little experenice with Haskell, but the most obvious thing I note that Scala type system different from Haskell is the type inference.In Scala, there is no global type inference, you must explicit tell the type of function arguments.For example, in Scala you need to write this:instead of This may cause problem when you need generic version of add function that work with all kinds of type has the \"+\" method. There is a workaround for this, but it will get more verbose.But in real use, I found Scala's type system is powerful enough for daily usage, and I almost never use those workaround for generic, maybe this is because I come from Java world.And the limitation of explicit declare the type of arguments is not necessary a bad thing, you need document it anyway.Well are they Turing reducible?See Oleg Kiselyov's page \n...\nOne can implement the lambda calculus in Haskell's type system.  If Scala can do that, then in a sense Haskell's type system and Scala's type system compute the same types.  The questions are: How natural is one over the other?  How elegant is one over the other?"},
{"body": "What is the difference between  and  Scala class?The difference is that all subclasses of a sealed class (whether it's abstract or not) must be in the same file as the sealed class.As , all  subclasses of a sealed class (abstract or not) must be in the same file. A practical consequence of this is that the compiler can warn if the pattern match is incomplete. For instance:If the  is , then the compiler warns unless that last line is uncommented."},
{"body": "If I wanted to port a Go library that uses Goroutines, would Scala be a good choice because its inbox/akka framework is similar in nature to coroutines?Nope, they're not.  Goroutines are based on the theory of Communicating Sequential Processes, as specified by Tony Hoare in 1978.  The idea is that there can be two processes or threads that act independently of one another but share a \"channel,\" which one process/thread puts data into and the other process/thread consumes.  The most prominent implementations you'll find are Go's channels and Clojure's , but at this time they are limited to the current runtime and cannot be distributed, even between two runtimes on the same physical box.CSP evolved to include a static, formal process algebra for proving the existence of deadlocks in code.  This is a really nice feature, but neither Goroutines nor  currently support it.  If and when they do, it will be extremely nice to know before running your code whether or not a deadlock is possible.  However, CSP does not support fault tolerance in a meaningful way, so you as the developer have to figure out how to handle failure that can occur on both sides of channels, and such logic ends up getting strewn about all over the application.Actors, as specified by Carl Hewitt in 1973, involve entities that have their own mailbox.  They are asynchronous by nature, and have location transparency that spans runtimes and machines - if you have a reference (Akka) or PID (Erlang) of an actor, you can message it.  This is also where some people find fault in Actor-based implementations, in that you have to have a reference to the other actor in order to send it a message, thus coupling the sender and receiver directly.  In the CSP model, the channel is shared, and can be shared by multiple producers and consumers.  In my experience, this has not been much of an issue.  I like the idea of proxy references that mean my code is not littered with implementation details of how to send the message - I just send one, and wherever the actor is located, it receives it.  If that node goes down and the actor is reincarnated elsewhere, it's theoretically transparent to me.Actors have another very nice feature - fault tolerance.  By organizing actors into a supervision hierarchy per the OTP specification devised in Erlang, you can build a domain of failure into your application.  Just like value classes/DTOs/whatever you want to call them, you can model failure, how it should be handled and at what level of the hierarchy.  This is very powerful, as you have very little failure handling capabilities inside of CSP.Actors are also a concurrency paradigm, where the actor can have mutable state inside of it and a guarantee of no multithreaded access to the state, unless the developer building an actor-based system accidentally introduces it, for example by registering the Actor as a listener for a callback, or going asynchronous inside the actor via Futures.Shameless plug - I'm writing a new book with the head of the Akka team, Roland Kuhn, called Reactive Design Patterns where we discuss all of this and more.  Green threads, CSP, event loops, Iteratees, Reactive Extensions, Actors, Futures/Promises, etc.  Expect to see a MEAP on Manning by early next month.Good luck!There are two questions here:This is an easy question, since Scala is a general purpose language, which is no worse or better than many others you can choose to \"port goroutines\". There are of course many  on why Scala is better or worse  (e.g.  is mine), but these are just opinions, and don't let them stop you. \nSince Scala is general purpose, it \"pretty much\" comes down to: everything you can do in language X, you can do in Scala. If it sounds too broad.. how about  :)The only similarity (aside the nitpicking) is they both have to do with concurrency and message passing. But that is where the similarity ends.Since Jamie's answer gave a good overview of Scala actors, I'll focus more on Goroutines/core.async, but with some actor model intro.Where a \"worry free\" piece is usually associated with terms such as: , , , etc.. Without going into grave details how actors work, in two simple terms actors have to do with:Think \"talking processes\" where each process has a reference and a function that gets called when a message arrives.There is much more to it of course (e.g. check out , or ), but the above two is a good start.Where it gets interesting with actors is.. implementation. Two big ones, at the moment, are Erlang OTP and Scala AKKA. While they both aim to solve the same thing, there are some differences. Let's look at a couple: The point of the above is not to say that one is better than the other, but it's to show that purity of the actor model as a concept depends on its implementation.Now to goroutines..As other answers already mentioned, goroutines take roots in , which is a \"formal language for describing patterns of interaction in concurrent systems\", which by definition can mean pretty much anything :)I am going to give examples based on , since I know internals of it better than Goroutines. But  was built after the Goroutines/CSP model, so there should not be too many differences conceptually.The main concurrency primitive in core.async/Goroutine is a . Think about a  as a \"queue on rocks\". This channel is used to \"pass\" messages. Any process that would like to \"participate in a game\" creates or gets a reference to a  and puts/takes (e.g. sends/receives) messages to/from it.Most of work that is done on channels usually happens inside a \"\" or \"\", which \"\" ().It is a lot easier to convey with a visual. Here is what a blocking IO execution looks like:You can see that threads mostly spend time waiting for work. Here is the same work but done via \"Goroutine\"/\"go block\" approach:Here 2 threads did all the work, that 4 threads did in a blocking approach, while taking the same amount of time.The kicker in above description is: \"threads are \" when they have no work, which means, their state gets \"offloaded\" to a state machine, and the actual live JVM thread is free to do other work ( for a great visual): in core.async, channel  used outside of \"go block\"s, which will be backed by a JVM thread without parking ability: e.g. if it blocks, it blocks the real thread.Another huge thing in \"Goroutines\"/\"go blocks\" is operations that can be performed on a channel. For example, a  can be created, which will close in X milliseconds. Or select/ function that, when used in conjunction with many channels, works like a \"are you ready\" polling mechanism across different channels. Think about it as a socket selector in non blocking IO. Here is an example of using  and  together:This code snippet is taken from , where it sends the same request to all three: Yahoo, Bing and Google, and returns a result from the fastest one,  times out (returns a timeout message) if none returned within a given time. Clojure may not be your first language, but you can't disagree on how  this implementation of concurrency looks and feels.  You can also merge/fan-in/fan-out data from/to many channels, map/reduce/filter/... channels data and more. Channels are also first class citizens: you can pass a channel to a channel..Since core.async \"go blocks\" has this ability to \"park\" execution state, and have a very sequential \"look and feel\" when dealing with concurrency, how about JavaScript? There is no concurrency in JavaScript, since there is only one thread, right? And the way concurrency is mimicked is via 1024 callbacks.But it does not have to be this way. The above example from  is in fact written in ClojureScript that compiles down to JavaScript. Yes, it will work on the server with many threads and/or in a browser: the code can stay the same.Again, a couple of implementation differences [there are more] to underline the fact that theoretical concept is not exactly one to one in practice:I hope the above shed some light on differences between the actor model and CSP. Not to cause a flame war, but to give you yet another perspective of let's say Rich Hickey:\"I remain unenthusiastic about actors. They still couple the producer with the consumer. Yes, one can emulate or implement certain kinds of queues with actors (and, notably, people often do), but since any actor mechanism already incorporates a queue, it seems evident that queues are more primitive. It should be noted that Clojure's mechanisms for concurrent use of state remain viable, and channels are oriented towards the flow aspects of a system.\"()However, in practice, Whatsapp is based on Erlang OTP, and it seemed to sell pretty well.It isn't quite true that you could do the exact same things that you do with goroutines in Akka. Go channels are often used as synchronization points. You cannot reproduce that directly in Akka. In Akka, post-sync processing has to be moved into a separate handler (\"strewn\" in jamie's words :D). I'd say the design patterns are different. You can kick off a goroutine with a , do some stuff, and then  to wait for it to finish before moving on. Akka has a less-powerful form of this with , but  isn't really the Akka way IMO.Chans are also typed, while mailboxes are not. That's a big deal IMO, and it's pretty shocking for a Scala-based system. I understand that  is hard to implement with typed messages, but maybe that indicates that  isn't very Scala-like. I could say that about Akka generally. It often feels like its own thing that happens to run on Scala. Goroutines are a key reason Go exists.Don't get me wrong; I like the actor model a lot, and I generally like Akka and find it pleasant to work in. I also generally like Go (I find Scala beautiful, while I find Go merely useful; but it is quite useful).But fault tolerance is really the point of Akka IMO. You happen to get concurrency with that. Concurrency is the heart of goroutines. Fault-tolerance is a separate thing in Go, delegated to  and , which can be used to implement quite a bit of fault tolerance. Akka's fault tolerance is more formal and feature-rich, but it can also be a bit more complicated.All said, despite having some passing similarities, Akka is not a superset of Go, and they have significant divergence in features. Akka and Go are quite different in how they encourage you to approach problems, and things that are easy in one, are awkward, impractical, or at least non-idiomatic in the other. And that's the key differentiators in any system.So bringing it back to your actual question: I would strongly recommend rethinking the Go interface before bringing it to Scala or Akka (which are also quite different things IMO). Make sure you're doing it the way your target environment means to do things. A straight port of a complicated Go library is likely to not fit in well with either environment.These are all great and thorough answers. But for a simple way to look at it, here is my view. Goroutines are a simple abstraction of Actors. Actors are just a more specific use-case of Goroutines.You could implement Actors using Goroutines by creating the Goroutine aside a Channel. By deciding that the channel is 'owned' by that Goroutine you're saying that only that Goroutine will consume from it. Your Goroutine simply runs an inbox-message-matching loop on that Channel. You can then simply pass the Channel around as the 'address' of your \"Actor\" (Goroutine).But as Goroutines are an abstraction, a more general design than actors, Goroutines can be used for far more tasks and designs than Actors.A trade-off though, is that since Actors are a more specific case, implementations of actors like Erlang can optimize them better (rail recursion on the inbox loop) and can provide other built-in features more easily (multi process and machine actors)."},
{"body": "I am using SBT 0.12.0. I have read other answers on stackoverflow and followed them, however none of them helps, for example:Am I missing something? How do I set heap size for sbt 0.12, when doing both testing and ?You need , here's what I use in my .bash_profile:\nTo get your 2G heap space you can use this:Older versions of  contain bugs that override these settings, use  for latest  for Mac (assuming brew install) (IDK for Linux). As of March 2015, if you are using sbt on  with  then you should edit the file e.g.\"sbt -mem 23000 run\" works for me.I have found the solution. No matter how you specify JVM heap size, it will never work because SBT executable already has it overridden.There is a line in SBT executable which says:So I edited the file:Remove the  line.Now when you run SBT, it will no longer override your JVM heap size settings. You can specify heap size settings using @Noan's answer.Or alternatively:I was looking to solve a problem like this on Mac OS X with a homebrew install of SBT. If you installed SBT via homebrew, you're in the clear since the  file looks likeThis means that any settings you put in  will stick (your -Xmx will take precedence). Furthermore, the first line of the script will execute any commands in  if it exists so it may be a better place to put your SBT options if you are playing with them quite a bit. You won't have to  every time you make a change to On windows, for sbt 0.13.9.2, you need to set  to the jvm options you want.The  script loads its defaults from  into  but will use  instead if set.Relevant excerpts from :. . . (skip) . . ."},
{"body": "I want to implement it like this:  (returns 3).scala collections do have : A somewhat cleaner version of one of the other answers is:giving a  with a count for each item in the original sequence:The question asks how to find the count of a specific item. With this approach, the solution would require mapping the desired element to its count value as follows:I had the same problem as Sharath Prabhal, and I got another (to me clearer) solution :With as result :If you want to use it like  you have to implement it using an .Using , given e.g.then all of these (in the order of less simplified to more simplified)yieldgivesIt is interesting to note that the map with default 0 value, intentionally designed for this case demonstrates the worst performance (and not as concise as )producesIt is curious that most concise  is faster than even mutable map!I ran into the same problem but wanted to count multiple items in one go..Here is another option:"},
{"body": "What are some good tutorials on fold left?I am trying to implement a method for finding the boudning box of rectangle, circle, location and the group which all extends Shape. Group is basically an array of ShapesI got the bounding box computed for all three except the Group one. So now for the bounding box method I know I should be using map and fold left for Group, but I just can't find out the exact syntax of creating it.Group bounding box is basically the smallest bounding box with all the shapes enclosed.Now that you've edited to ask an almost completely different question, I'll give a different answer.  Rather than point to a tutorial on maps and folds, I'll just give one.In Scala, you first need to know how to create an anonymous function.  It goes like so, from most general to more specific:Here are some examples:Now, the  method of lists and such will apply a function (anonymous or otherwise) to every element of the map.  That is, if you havethenproducesThere are all sorts of reasons why this might be useful.  Maybe you have a bunch of strings and you want to know how long each is, or you want to make them all upper case, or you want them backwards.  If you have a function that does what you want to  element, map will do it to all elements:So, that's map in general, and in Scala.But what if we want to collect our results?  That's where fold comes in ( being the version that starts on the left and works right).Suppose we have a function , that is, it takes a B and an A, and combines them to produce a B.  Well, we could start with a B, and then feed our list of A's into it one at a time, and at the end of it all, we'd have some B.  That's exactly what fold does.   does it starting from the left end of the list;  starts from the right.  That is,produceswhere  is, of course, your initial value.So, maybe we have a function that takes an int and a string, and returns the int or the length of the string, whichever is greater--if we folded our list using that, it would tell us the longest string (assuming that we start with 0).  Or we could add the length to the int, accumulating values as we go.Let's give it a try.Okay, fine, but what if we want to know  is the longest?  One way (perhaps not the best, but it illustrates a useful pattern well) is to carry along both the length (an integer)  the leading contender (a string).  Let's give that a go:Here,  is now a tuple of type , and  is the first part of that tuple (an Int).But in some cases like this, using a fold isn't really want we want.  If we want the longer of two strings, the most natural function would be one like .  How do we apply that one?Well, in this case, there is a default \"shortest\" case, so we could fold the string-max function starting with \"\".  But a better way is to use .  As with fold, there are two versions, one that works from the left, the other which works from the right.  It takes no initial value, and requires a function .  That is, it takes two things and returns one of the same type.  Here's an example with a string-max function:Now, there are just two more tricks.  First, the following two mean the same thing:Notice how the second is shorter, and it sort of gives you the impression that you're taking  and doing something to the list with it (which you are).  ( is the same as , but you use it like so: Second, if you only refer to a variable once, you can use  instead of the variable name and omit the  part of the anonymous function declaration.  Here are two examples:At this point, you should be able to create functions and map, fold, and reduce them using Scala.  Thus, if you know how your algorithm should work, it should be reasonably straightforward to implement it.The basic algorithm would go like this:Now you have to write  and , which is a pure algorithms problem more than a language problem.If the shapes all had the same center, implementing  would be easier. It would go like this:As for , which takes two shapes and combine them, it will usually be a rectangle, but can be a circle under certain circunstances. Anyway, it is pretty straight-forward, once you have the algorithm figured out.A bounding box is usually a rectangle.  I don't think a circle located at (-r,-r) is the bounding box of a circle of radius r....Anyway, suppose you have a bounding box b1 and another b2 and a function  that computes the bounding box of b1 and b2.Then if you have a  set of shapes in your group, you can use  to compute the whole bounding box of a list of bounding boxes by combining them two at a time until only one giant box remains.  (The same idea can be used to reduce a list of numbers to a sum of numbers by adding them in pairs.  And it's called  because it works left to right across the list.)Suppose that  is a list of bounding boxes of each shape.  (Hint: this is where  comes in.)  ThenYou'll need to catch the empty group case separately, however.  (Since it has a no well-defined bounding box, you don't want to use folds; folds are good for when there is a default empty case that makes sense.  Or you have to fold with , but then your combining function has to understand how to combine  with , which is probably not worth it in this case--but very well might be if you were writing production code that needs to elegantly handle various sorts of empty list situations.)"},
{"body": "A discussion came up at work recently about Sets, which in Scala support the  method and how this can lead to bugs, e.g.I think it's pretty clear that s  shouldn't support a  operation, since the elements are not ordered. However, it was suggested that the problem is that  isn't really a functor, and shouldn't have a  method. Certainly, you can get yourself into trouble by mapping over a set. Switching to Haskell now,and now in ghciSo  fails to satisfy the functor lawIt can be argued that this is not a failing of the  operation on s, but a failing of the  instance that we defined, because it doesn't respect the substitution law, namely that for two instances of  on A and B  and a mapping  thenwhich doesn't hold for  (e.g. consider ).Is the substition law a sensible law for the  type that we should try to respect? Certainly, other equality laws are respected by our  type (symmetry, transitivity and reflexivity are trivially satisfied) so substitution is the only place that we can get into trouble.To me, substition seems like a very desirable property for the  class. On the other hand, some comments on a  includeThese three are all pretty well known in the Haskell community, so I'd be hesitant to go against them and insist on substitability for my  types!Another argument against  being a  - it is widely accepted that being a  allows you to transform the \"elements\" of a \"collection\" while preserving the shape. For example, this quote on the Haskell wiki (note that  is a generalization of )and in Real World HaskellClearly, any functor instance for  has the possibility to change the shape, by reducing the number of elements in the set.But it seems as though s really should be functors (ignoring the  requirement for the moment - I see that as an artificial restriction imposed by our desire to work efficiently with sets, not an absolute requirement for any set. For example, sets of functions are a perfectly sensible thing to consider. In any case, Oleg  how to write efficient Functor and Monad instances for  that don't require an  constraint). There are just too many nice uses for them (the same is true for the non-existant  instance).Can anyone clear up this mess? Should  be a ? If so, what does one do about the potential for breaking the Functor laws? What should the laws for  be, and how do they interact with the laws for  and the  instance in particular?I'm afraid that this is a case of taking the \"shape\" analogy as a defining condition when it is not.  Mathematically speaking, there is such a thing as the power set functor.  :The function P(f) ( in the power set functor) does not preserve the size of its argument set, yet this is nonetheless a functor.If you want an ill-considered intuitive analogy, we could say this: in a structure like a list, each element \"cares\" about its relationship to the other elements, and would be \"offended\" if a false functor were to break that relationship.  But a set is the limiting case: a structure whose elements are indifferent to each other, so there is very little you can do to \"offend\" them; the only thing is if a false functor were to map a set that contains that element to a result that doesn't include its \"voice.\"(Ok, I'll shut up now...) I truncated the following bits when I quoted you at the top of my answer:Here's I'd remark that  is a kind of  , not a \"generalization\" of it.  One of the key facts about any  (or, actually, about , which  extends) is that it requires that the elements of any structure have a linear order\u2014you can turn any  into a list of its elements (with ).Another, less obvious fact about  is that the following functions exist (adapted from ):A  instance for sets would violate the proposed law, because all non-empty sets would have the same \u2014the set whose  is .  From this it should be easy to prove that whenever you try to  a set you would only ever get the empty set or a singleton back.Lesson?   \"preserves shape\" in a very specific, stronger sense than  does.Set is \"just\" a functor (not a ) from the subcategory of Hask where  is \"nice\" (i.e. the subcategory where congruence, substitution, holds). If constraint kinds were around from  then perhaps set would be a  of some kind.Well, Set can be treated as a covariant functor, and as a contravariant functor; usually it's a covariant functor. And for it to behave regarding equality one has to make sure that whatever the implementation, it does.Regarding Set.zip - it is nonsense. As well as Set.head (you have it in Scala). It should not exist."},
{"body": "If I have something like a  and I want to convert this into a , the standard way is to use :Now  is just an identity function. I would have thought there'd be some way to do:However, I can't get this to work as you can't generify an . I tried a few things to no avail; has anyone got something like this to work?There's an identity .A for expresion is nicer, I suppose:I tried to figure out why the the type parameter (Option[String]) is needed. The problem seems to be the type conversion from Option[T] to Iterable[T].If you define the identity function as:the type parameter can be omitted.FWIW, on Scala 2.8 you just call  on it.  has it mostly covered for Scala 2.7. He only missed one alternative way of using that identity:It won't work with operator notation, however (it seems operator notation does not accept type parameters, which is good to know).You can  call  on Scala 2.7 (on a , at least), but it won't be able to do anything without a type. However, this works:You could just give the type inferencer a little help:"},
{"body": "Lets say I have an already functioning Play 2.0 framework based application in Scala that serves a URL such as:which responds with a listing of all known birthdaysI now want to enhance this by adding the ability to restrict results with optional \"from\" (date) and \"to\" request params such as(dates here interpreted as yyyyMMdd)My question is how to handle the request param binding and interpretation in Play 2.0 with Scala, especially given that both of these params should be optional. Should these parameters be somehow expressed in the \"routes\" specification? Alternatively, should the responding Controller method pick apart the params from the request object somehow? Is there another way to do this?Encode your optional parameters as  (or , but you\u2019ll have to implement your own ):And declare the following route:A maybe less clean way of doing this for java users is setting defaults:And in the controllerOne more problem, you'll have to repeat the defaults whenever you link to your page in the templateHere's Julien's example rewritten in java, using F.Option: (works as of play 2.1)Route:You can also just pick arbitrary query parameters out as strings (you have to do the type conversion yourself):In Addition to Julien's answer. If you don't want to include it in the routes file.You can get this attribute in the controller method using RequestHeaderThis will give you the desired request parameters, plus keep your routes file clean.My way of doing this involves using a custom . This way I express parameters in routes as:The code for Period looks like this.} is just a convienence method i use in my controllers if I want to apply date filtering to the query. Obviously you could use other date defaults here, or use some other default than null for start and end date in the  method.For optional Query parameters, you can do it this wayIn routes file, declare APIYou can also give some default value, in case API doesn't contain these query params it will automatically assign the default values to these paramsIn method written inside controller Application these params will have value  if no default values assigned else default values."},
{"body": "I feel a bit insecure about using actors in Scala. I have read documentation about how to do stuff, but I guess I would also need some DON'T rules in order to feel free to use them.\nI think I am afraid that I will use them in a wrong way, and I will not even notice it. Can you think of something, that, if applied, would result in breaking the benefits that Scala actors bring, or even erroneous results? .I know this doesn't really answer the question, but you should at least take heart in the fact that message-based concurrency is much less prone to wierd errors than shared-memory-thread-based concurrency.I presume you have seen the actor guidelines in , but for the record:"},
{"body": "Could you guys please explain to me how to set main class in SBT project ? I'm trying to use version 0.13.My directory structure is very simple (unlike SBT's documentation). In the root folder I have  with following contentAnd I have subfolder  with single file  which contains following codeI'm able to compile it by calling  but  returnsPS.I'm shocked that after reading SBT's documentation and 15 similar questions on StackOverflow I couldn't make SBT project run. It's basic functionality which should be brain-dead simple and clear in first 10 seconds of looking at SBT's web-site.PS2.And I'd like to ask Typesafe employees to improve documentation of their product instead of downvoting questions of people who are investing significant time in switching to new platform.You need to put your application's source in ,  is for build definition code.Here is how to specify main classTry to use an object instead of classbecause you need to run main method neither main classFor custom modules in SBT (0.13), just enter on the SBT console:  to switch scope to moduleX, as define in Built.scala. All main classes within that scope will be detected automatically. Otherwise you get the same error of no main class detected. \nFor God's sake, SBT does not tell you that the default scope is not set. It has nothing to do with default versus non default source folders but only with SBT not telling anything that it doesn't know which module to use by default. : PLEASE add a default output like:at the end of SBT start to lower the level of frustration while using SBT on multi module projects..... If you have multiple main methods in your project you can add the following line to your build.sbt file:If you want to specify the class that will be added to the manifest when your application is packaged as a JAR file, add this line to your build.sbt file:You can also specify main class using run-main command in sbt and activator to run:orI had the same issue: was mode following the tutorial at , and in my opinion, as a build tool 's interaction and error messages can be quite misleading to a newcomer.It turned out, hours of head scratching later, that I missed the critical  line in the example each time. :-("},
{"body": "So, if I have an actor, I can give it a name. But, can I access that name internally? Example:Now, I can pass its name as a constructor parameter. But, that seems like unnecessary duplication if there is a way to get the name internally... as it was set when I instantiated the actor using system.actorOf. API docs didn't seem to have anything.From an  you can use  to get the ."},
{"body": "How convert it to immutable?The immutable hierarchy doesn't contain a MultiMap, so you won't be able to use the converted structure with the same convenient syntax.  But if you're happy to deal with key/valueset pairs, then:If you just want a mutable , you can just use  in 2.8 or  in 2.7.But if you want the whole structure to be immutable--including the underlying set!--then you have to do more: you need to convert the sets along the way.  In 2.8:In 2.7:You can use  to convert an a mutable map into immutable in Scala 2.8 and later versions.Looking at definition of  from documentation:You can just to the following"},
{"body": "I've downloaded  and I want to try out few things in the  using this library. How do I achieve this? Of course, you can use scala -cp whatever and manually manage your dependencies. But that gets quite tedious, especially if you have multiple dependencies.A more flexible approach is to use  to manage your dependencies. Search for the library you want to use on . Algebird for example is available by simply . Then create a build.sbt referring to that library, enter the directory and enter . It will download all your dependencies and start a scala console session with all dependencies automatically on the classpath.Changing things like the scala version or the library version is just a simple change in the build.sbt. To play around you don't need any scala code in your directory. An empty directory with just the build.sbt will do just fine.Here is a  for using algebird:Edit: often when you want to play around with a library, the first thing you have to do is to import the namespace(s) of the library. This can also be automated in the build.sbt by adding the following line:You can use the scala's  switch to keep jars on the classpath. There are other switches available too, for example,   and  for turning on various warnings. Many more to be found with  and . You can find out more information about these switches with Running  will not import libraries declared with a test scope. To use those libraries in the REPL, start the console with You should be aware, however, that starting the console this way skips compiling your test sources.Source: "},
{"body": "Is there any reason for Scala not support the ++ operator to increment primitive types by default?\nFor example, you can not write:ThanksMy guess is this was omitted because it would only work for mutable variables, and it would not make sense for immutable values.  Perhaps it was decided that the  operator doesn't scream assignment, so including it may lead to mistakes with regard to whether or not you are mutating the variable.I feel that something like this is safe to do (on one line):but this would be a bad practice (in any language):You don't want to mix assignment statements and side effects/mutation.I like 's , but I think the point has to be more strongly made.It would require a new language feature. For instance, let's say we create an  keyword. The type signature would need to be changed as well, to indicate that  is not   a , but  it to whatever field is holding the present object. In Scala spirit of not intruding in the programmers namespace, let's say we do that by prefixing the type with .Then it could be like this:The next problem is that postfix operators suck with Scala rules. For instance:Because of Scala rules, that is the same thing as:Which wasn't the intent. Now, Scala privileges a flowing style: . That mixes well C++/Java traditional syntax of  with functional programming concept of pipelining an input through multiple functions to get the end result. This style has been recently called \"fluent interfaces\" as well.The problem is that, by privileging that style, it cripples postfix operators (and prefix ones, but Scala barely has them anyway). So, in the end, Scala would have to make big changes, and it would be able to measure up to the elegance of C/Java's increment and decrement operators anyway -- unless it really departed from the kind of thing it  support.In Scala, ++ is a valid method, and no method implies assignment. Only  can do that.A longer answer is that languages like C++ and Java treat  specially, and Scala treats  specially, and in an inconsistent way.In Scala when you write  the compiler first looks for a method called  on the Int. It's not there so next it does it's magic on  and tries to compile the line as if it read . If you write  then Scala will call the method  on  and assign the result to... nothing. Because only  means assignment. You could write  but that kind of defeats the purpose.The fact that Scala supports method names like  is already controversial and some people think it's operator overloading. They could have added special behavior for  but then it would no longer be a valid method name (like ) and it would be one more thing to remember.I think the reasoning in part is that  is only one more character, and  is used pretty heavily in the collections code for concatenation.  So it keeps the code cleaner.Also, Scala encourages immutable variables, and  is intrinsically a mutating operation.  If you require , at least you can force all your mutations to go through a common assignment procedure (e.g. ).Of course you can have that in Scala, if you really want:And here you go:The primary reason is that there is not the need in Scala, as in C. In C you are constantly:C++ has added higher level methods for avoiding for explicit loops, but Scala has much gone further providing foreach, map, flatMap foldLeft etc. Even if you actually want to operate on a sequence of Integers rather than just cycling though a collection of non integer objects, you can use Scala range.Because the ++ operator is used by the collection library, I feel its better to avoid its use in non collection classes. I used to use ++ as a value returning method in my Util package package object as so: But I removed it. Most of the times when I have used ++ or + 1 on an integer, I have later found a better way, which doesn't require it.    It is possible if you define you own class which can simulate the desired output however it may be a pain if you want to use normal \"Int\" methods as well since you would have to always use *()Some possible test casesLets define a var: ++i is already short enough: Now i++ can look like this: To use above syntax, define somewhere inside a package object, and then import: Operators chaining is also possible:The above example is similar to this Java (C++?) code: The style could depend, of course.It isn't included because Scala developers thought it make the specification more complex while achieving only negligible benefits and because Scala doesn't have operators at all.You could write your own one like this:But I'm sure you will get into some trouble with precedence not working as you expect. Additionally if i++ would be added, people would ask for ++i too, which doesn't really fit into Scala's syntax."},
{"body": "I'm trying to pick up some scala. Reading through examples I came across this impossible-to-google nugget: What does the triple colon accomplish?Concatenates two lists - To add to , it's important to understand that methods whose names end in a colon are right-associative. So writing  is the same as writing . In this case it doesn't matter since both operands are lists, but in general you'll need this knowledge to find such methods in the scaladocs. It also helps to know that the  have a comprehensive index of all methods (and classes, etc) with symbolic names. You can reach it by clicking on the  in the upper-left corner."},
{"body": "I have code like this:but it throws runtime java.lang.UnsupportedOperationException.I need to declare empty list or empty maps and some where later in the code need to fill them.Scala lists are immutable by default. You cannot \"add\" an element, but you can form a new list by appending the new element in front. Since it is a  list, you need to reassign the reference (so you can't use a val).The operator  creates the new list. You can also use the shorter syntax: In scala don't use the type  but ,  or .If you need to mutate stuff, use  or  instead. However, it would be better to address this statement:Instead of doing that, fill the list with code that returns the elements. There are many ways of doing that, and I'll give some examples:As everyone already mentioned, this is not the best way of using lists in Scala...Per default collections in scala are immutable, so you have a + method which returns a new list with the element added to it.\nIf you really need something like an add method you need a mutable collection, e.g.  which has a += method. "},
{"body": "I am calling a webservice like this:The complete error: According to , it is fixed in the documentation. I needed to add the following import:An alternative option: Since Play 2.4 you can inject  via Guice dependency: injection."},
{"body": "I am looking for chart which shows equivalents in Scala of LINQ methods for IEnumerable:Does anyone know anything of such \"translate\" table?I am only listing out the equivalents of functions from . This is incomplete as of now. I will try to update this later with more.There is no direct equivalent of some functions, but it's fairly easy to roll your own. Here are some such functions.::    :::I don't know anything about C# or LINQ, but is this what you're looking for?There's no method to get an element or a default, but this will work:"},
{"body": "Does Scala support tail recursion optimization?Scala does tail recursion optimisation at compile-time, as other posters have said. That is, a tail recursive function is transformed into a loop by the compiler (a method invoke is transformed into a jump), as can be seen from the stack trace when running a tail recursive function.Try the following snippet:and inspect the stack trace. It will show only one call to the function boom - therefore the compiled bytecode is not recursive. There is a proposal floating around to  - which in my opinion would a great thing to do, as then the JVM could do runtime optimizations, rather than just compile time optimizations of the code - and could possibly mean more flexible tail recursion. Basically a  would behave exactly like a normal method  but will drop the stack of the caller when it's safe to do so - the specification of the JVM states that stack frames must be preserved, so the JIT has to do some static code analysis to find out if the stack frames are never going to be used.The current status of it is . I don't think it will be done in time for Java 7 ( has a greater priority, and the implementation is almost done) but Java 8 might see it implemented.In Scala 2.8 you can use  to mark specific method that you hope the compiler will optimise:If a method can not be optimized you get a warning.Scala 2.7.x supports tail-call optimization for self-recursion (a function calling itself) of final methods and local functions.Scala 2.8 might come with library support for trampoline too, which is a technique to optimize mutually recursive functions.A good deal of information about the state of Scala recursion can be found in .Only in very simple cases where the function is self-recursive.It looks like Scala 2.8 might be improving tail-recursion recognition, though."},
{"body": "I came across this  and did the following experiment with scala 2.10.3.I rewrote the Scala version to use explicit tail recursion:and compared it to the following Java version. I consciously made the functions non-static for a fair comparison with Scala:Here are the results on my computer:This is scala 2.10.3 on (Java HotSpot(TM) 64-Bit Server VM, Java 1.7.0_51).My question is what is the hidden cost with the scala version?Many thanks.Well, OP's benchmarking is not the ideal one. Tons of effects need to be mitigated, including warmup, dead code elimination, forking, etc. Luckily,  already takes care of many things, and has bindings for both Java and Scala. Please follow the procedures on JMH page to get the benchmark project, then you can transplant the benchmarks below there.This is the sample Java benchmark:...and this is the sample Scala benchmark:If you run these on JDK 8 GA, Linux x86_64, then you'll get:Notice we juggle  to see if the effect is local for the particular value of . It is not, the effect is systematic, and Java version being twice as fast.  will shed some light on this. This one is the hottest block in Scala benchmark:...and this is similar block in Java:Notice how in Java version the compiler employed the trick for translating integer remainder calculation into the multiplication and shifting right (see Hacker's Delight, Ch. 10, Sect. 19). This is possible when compiler detects we compute the remainder against the constant, which suggests Java version hit that sweet optimization, but Scala version did not. You can dig into the bytecode disassembly to figure out what quirk in scalac have intervened, but the point of this exercise is that surprising minute differences in code generation are magnified by benchmarks a lot.P.S. So much for ...UPDATE: A more thorough explanation of the effect: I changed the to a and got a significant performance boost, now it seems that both versions perform almost equally [on my system, see update and comments].I have not looked into into the bytecode, but if you use  you can see using  that there is a method (and that version is as slow as the one with the ). So I assume that even a  involves calling a method, and that's not directly comparable with a  in Java.On my system I got these resultsUsing OpenJDK 1.7 on a 32-Bit Linux.In my experience Oracle's JDK on a 64-Bit system does actually perform better, so this probably explains that other measurements yield even better results in favour of the Scala version.As for the Scala version performing better I assume that tail recursion optimization does have an effect here (see Phil's answer, if the Java version is rewritten to use a loop instead of recursion, it performs equally again).I looked at  and edited the Scala version to have  inside :The new Scala version now runs twice as fast as the original Java one:I figured out it is because Java not having tail calls. The optimized Java one with loop instead of recursion runs just as fast:Now my confusion is fully solved:In conclusion, the original Scala version was slow because I didn't declare  to be  (directly or indirectly, as 's  points out). And the original Java version was slow due to lack of tail calls.To make the Java version completely equivalent to your Scala code you need to change it like this.It is slower because the JVM can not optimize the method calls."},
{"body": "Has anyone had success developing a substantial Android app in Scala? Is it a viable option yet? Are there any mature development environments? Given the state of the Scala Eclipse plug-in, it looks as if there is no good IDE support at all other than possibly IntelliJ Ultimate.A few people have posted tutorials describing how to  to sort-of support Scala, and how to to  using Proguard, but beyond that there has been worryingly little discussion about this topic. An interesting article on Android+Scala from the developers behind the Bump app: I'm programming my Android application project in Scala.If you are interested, you may take a look at this:Ya, it is a Chinese website, but you may just take a look at screenshots to get some idea about that Scala can do everything with Android SDK just like Java.The source code of these two android application is hosted on GitHub: Currently I don't use any IDE, because Vim / SBT is far more convenient and lightweight then any IDE I ever used.And if you are using SBT to build your Scala Android application, you don't need worry about those progruard stuff, just install the sbt android-plugin and setup your project.You may read  to learn how to build your Android application with SBT.BTW, I use my own handcraft sbt plugin called  in my project, instead of  metioned in that blog post.There is a new plugin, AndroidProguardScala which makes everything simple :Tested on Windows 7, with Eclipse 3.7, scala 2.9 and without even using the command line or custom ant tools. With it, I have build a real application for my business that is currently published on Google play.I have a scala android tutorial it allows you to use the ant lifecycle targets provided by android. please see here  it also has a link to a github project with everything configured."},
{"body": "Obviously there is the  library but what else can we expect to see in the next release of scala? Any language changes? Is there a website where I can see a roadmap?Martin Odersky announced new features at Devoxx several days ago. I blogged about it (you can also find photos and presentation slides .The smaller items:Some changes which I hope will be included:Martin Odersky just put slides from ScalaDays 2011 on the web.\nLast few pages of the presentation shows the desired direction for Scala.Odersky's talk  at  contained pretty much the latest details about this."},
{"body": "It seems that both Iterator and Stream are lazy and allow you to keep returning elements to your heart's content. What's the difference between the two?Stream  and Iterator does not. You can traverse the same Stream multiple times and get the same result each time. Iterator, on the other hand, can only be traversed once.They are both constructs for accessing a current element, having a yet unknown list of remaining elements (the lazy tail).  is an imperative construct which you can only traverse once. is a functional construct. In theory you can traverse it multiple times (and as others mentioned, it won't recompute the already computed parts), but in practice because Streams are either infinite or very large (that is why you use it in the first place), holding reference to the full stream doesn't make much sense (you run into Out Of Memory pretty easy).Generally it is safer to the mind to avoid plain s. Alternatives are using  of Scalaz which auto-forgets unreferred parts using weak references, or using  (see also ) or ."},
{"body": "In this slide show on  what does the single quote indicate when the message is sent to the pong actor? This defines a literal . See also .It indicates a Symbol.  Eg. cfr  :"},
{"body": "Why won't the Scala compiler apply tail call optimization unless a method is final?For example, this:results inWhat exactly would go wrong if the compiler applied  in a case such as this?Consider the following interaction with the REPL. First we define a class with a factorial method:Now let's override it in a subclass to double the superclass's answer:What result do you expect for this last call? You might be expecting 240.  But no:That's because when the superclass's method makes a recursive call, the recursive call goes through the subclass.If overriding worked such that 240 was the right answer, then it would be safe for tail-call optimization to be performed in the superclass here.  But that isn't how Scala (or Java) works.Unless a method is marked final,  when it makes a recursive call.And that's why @tailrec doesn't work unless a method is final (or private).UPDATE: I recommend reading the other two answers (John's and Rex's) as well.Recursive calls might be to a subclass instead of to a superclass;  will prevent that.  But why might you want that behavior?  The Fibonacci series doesn't provide any clues.  But this does:If the Pretty call was tail-recursive, we'd print out  instead since the extension wouldn't apply.Since this sort of recursion is plausibly useful, and would be destroyed if tail calls on non-final methods were allowed, the compiler inserts a real call instead.Let foo::fact(n, res) denote your routine.  Let baz::fact(n, res) denone someone else's override of your routine.The compiler is telling you that the semantics allow baz::fact() to be a wrapper, that  upcall (?) foo::fact() if it wants to.  Under such a scenario, the rule is that foo::fact(), when it recurs, must activate baz::fact() rather than foo::fact(), and, while foo::fact() is tail-recursive, baz::fact() may not be.  At that point, rather than looping on the tail-recursive call, foo::fact() must return to baz::fact(), so it can unwind itself.Nothing would go wrong. Any language with proper tail call elimination will do this (SML, OCaml, F#, Haskell etc.). The only reason Scala does not is that the JVM does not support tail recursion and Scala's usual hack of replacing self-recursive calls in tail position with  does not work in this case. Scala on the CLR could do this as F# does."},
{"body": " is the same question for older version of Scala, but they say that Eclipse plugin has been improved vastly. Is it the best IDE now? How do different Scala IDE compare today? I've been pretty successful with IDEA 9. I've briefly tried both Netbeans and Eclipse and wasn't able to get what I wanted. Eclipse's code-complete didn't behave as well as I'd have liked, and I couldn't find a way to make Netbeans handle Scala scripts; It'd just complain that the file wasn't a class.To be clear, I've been using IDEA for a few years for Java, so keep that in mind:)For the moment, Scala Plugin in IntelliJ IDEA is the best. It handles Scala 2.8 well. IntelliJ IDEA Community Edition is now free and open source (and works with Scala), so I can't see any reason for not using it.The plugin is still somewhat buggy (many \"false negatives\", i.e. the code without red underscores may not compile successfully; but almost no \"false positives\"), but perfectly usable. The best thing is that you can use IDEA's excellent debugger with Scala (not without some issues, but it actually works!).FSC (Fast Scala Compiler) is also supported in latest builds. A huge time-saver.The plugin development team is quite responsive. Some of the guys work directly in JetBrains and possess intimate knowledge about IDEA platform, so the development progresses fast.JetBrains IDEA's Scala plug-in handles 2.7 and 2.8 equally well.I cannot make any comparisons because I have used only IDEA.Using Eclipse Helios with the dev-version of the new Scala(2.8) plugin, as there isn't an official release yet. That  beta, definitively -- but I can't confirm the frequently expressed opinion that this plugin is outright horrible ;-)I'd say, the experience is already OK-ish, and indeed better than the current state of affairs with the Groovy plugin. OTOH, the experience with plain Java is way more smooth (feels like flying at times), and the current CDT I'd rate somewhat in between.Incremental compile and error highlighting work quite well for me; tweaking a DSL implementation into form just by continuously rewriting your code until the error markers are gone -- without ever having to test-run your program -- is outright fun and just again shows that FP / static typing rocks!Problems encountered from time to time:\n- implicits and nested types in other compilation units (esp. nested / super packages) aren't picked up at times when there are still other errors around; they will be picked up after an full build\n- there seems to be a memory leak in the version I'm using right now (from end august 2010), necessitating to restart the workbench after some hours of work\n- beware when you're using AspectJ, to make sure you get a version of the Scala plugin which relies on a JDT weaving bundle version which also works with AJDTPS: I'm using maven builds in all my projects and generated the eclipse projects with the eclipse-maven-plugin, and then imported them as plain-flat eclipse projects. I can just strongly recommend everyone to keep away from the M2-eclipse plugin (for maven) in its current (2010) state, it makes your workbench painfully slow, is buggy and has lots of almost unpredictable behaviour, because it constantly tries to do magic things behind the scenes (and besides that, the aspectj support is broken since this spring)i use both eclipse and IDEA in summery i prefer to use IDEA rather than eclipseA non-answer: None.Based on what a perceived majority says, IDEA is probably the best Scala IDE today. And it (read: the Scala plugin) sucks. It does not handle fsc well, type inference is a mess, many errors are not shown, a number of non-errors are marked as errors, it is slow (when inspections are turned on), the test runner silently swallows aborting (!= failing) tests, ...So I switched to a simple text editor with syntax highlighting on one and a maximized shell with SBT (simple build tool) on the other screen. Awesome! SBT is responsive (you can let file changes trigger recompilation of affected code and even reruns of tests), manages dependencies very smoothly and has helpful output (esp for tests; using ScalaTest). SBT increased my productivity compared to IDEA a lot.You lose code completion, of course, altough geany offers me identified symbols. But as long as IDEs don't get type inference to work properly code completion does not help, anyway.Some people care a lot about code refactoring. Well, the IDEs apparently don't make a good job there either. Even if they would, I'd rather only open them for this particular task than use them all the time.My experiences clearly point to IntelliJ IDEA:About six months ago, when I started a serious Scala (multi module) project, I had to abandon Eclipse as my favorite Java IDE and switched to IntelliJ (9.0.x). Eclipse Scala IDE was way to buggy and often stopped responding at some point, even for the most simple projects. For CI (Hudson) and command line build, I depend on Maven (with Scala plugin). The Maven dependencies (incl. Scala libs) are picked up nicely by IntelliJ.A few days back I updated to IDEA X (CE) with the current plugin (nightly build) and work became even smoother. Although fsc still terminates after a while when inactive.From what I see, I'd like to add, that there seems to be way more activity on the IntelliJ side to respond to bugs and improve the plugin continuously. Correct me when I'm wrong, but Eclipse Scala IDE development seems almost stalled. Still no 'official' Helios release!NB: Just to provide some context (not bragging, really): The aforementioned project consists of about 25 Scala modules (POMs), 5 Java modules, 325 Scala files with a total of about 360 Scala classes, case classes and traits (> 19 kLOC, including comments). My platform is OS X 10.6, Scala 2.8.1, Java 1.6.UPDATE: After having the need for pretty extensive refactorings (mainly move class, rename package), I discovered that the recent IDEA 10.0.1 plugin 0.4.413 (and probably older versions, too) has quite some problems getting stuff right. I don't want to explain the specifics, but I (almost ever) ended up manually fixing unresolved references or otherwise messed-up code. You can have a look at  to get an idea.For everyone who is really considering doing some serious development with Scala, I strongly recommend to evaluate the IDEs in question beyond the basics. When you are into an agile approach, which in my option requires a painless refactoring support without surprises (especially in multi-module projects), things are pretty tight at the moment.It would be pretty neat, if someone came up with a IDE independent specification-like list of refactorings (and desired outcomes), which could be used to verify an IDE's refactoring support.The officially endorsed and supported (by Typesafe) for Scala 2.9 is Eclipse. The current version is far superior to prior versions and includes a context-aware REPL, full-featured debugger, and even the ability to debug REPL statements.  I think this question needs to be updated and the answers revisited.I think that the best option so far is the ScalaIDE for Eclipse. You can go to the ScalaIDE Web Site and look around to see by yourself. \nStrong points I see about it are: Here below a summary of the main features:Scala IDE provides support for development of Scala applications in the Eclipse platform. Its main target is the support for the Scala language and the integration with the Eclipse Java tools. It provides many of the features Eclipse users have come to expect including:UPDATE: the features and advantages are mentioned on this answer are for version 2.9 and 2.10 of Scala, because it has been already discontinued. see here:\"The 2.0.1 release is only available for Scala 2.9, if you would like to use the Scala IDE with Scala 2.8, please install the 2.0.0 release \"I don't recommend the Scala IDE/Eclipse.  It doesn't have a lot of the features that are even available for Eclipse with Java.  And there are bugs.I am using the latest NetBeans and haven't tried anything else. I've met at least 2 notable bugs in NetBeans while coding in Scala:One: NB occasionally come unable to run a program, hanging on classpath scanning.\nSolution: Create a new project, copy your code there and go on.\nComment: .Two: Sometimes NB  of particular namespaces or classes and complains when you use them.\nSolution: Just ignore and go on - compiler founds no errors and the program works.I'd recommend IDEA's plugin for now.The Scala plugin for NetBeans is quite nice too. It doesn't yet support NetBeans 6.9, the newest release, though, and you still need to download it manually instead of installing it directly from the plugin manager inside NetBeans. However, it integrates better with Maven projects than IDEA's plugin does (this is true for NB and IDEA in general, in my opinion).It partly depends on your style of working, as all the options have strengths and weaknesses.If you need refactoring across mixed java/scala projects, then IntelliJ is your only option.If you want to do any work on the compiler or a compiler plugin, then Eclipse has the advantage of being able to launch a runtime workspace with a custom compiler build, including breakpoints.  It also improved massively for the 2.8 Scala release.Netbeans is a fine choice to go with if you're already very familiar with that platform, the costs of learning a new environment may well outweigh any benefits, and all three solutions are improving rapidly.I haven't tried netbeans scala plugin yet, but I find that Intellij IDEA plugin is at any way much better a scala ide than the Scala eclipse plugin, which is sooooo slow that drives me crazy.\nThough swing applications don't work well with my tiling window manager.try IDEAX the latest community edition of Intellij IDEA (version 10), it has improved scala plugin which has faster code compilation and exceution in addition to that it has \nMaven3 and SBT support with which we can develop Lift applications.IntelliJ IDEA community edition + Scala Plugin + SBT plugin"},
{"body": "For example suppose I haveHow would I do something likeYou can do . You can also do , , and .My prefered way is to add a rounding method, to reduce any potential suprise in the conversion behavior:oror you could do..."},
{"body": "Consider the following Scala code:Now in Java, I would have liked to use  as:However, the above does not work (as expected since Java does not allow functional programming). What is the easiest workaround to pass a function in Java? I would like a generic solution that works with functions having arbitrary number of parameters. You have to manually instantiate a  in Java. Something like:This is taken from .In the  package, there are abstract classes named  and so on for other arities. To use them from Java you only need to override , like this:If you're on Java 8 and want to use Java 8 lambda syntax for this, check out .The easiest way for me is to defined a  interface like:Then modify your scala code, overloading  to accept also  objects such as:You will naturally use the first definition from scala, but still be able to use the second one from java:Here's my attempt at a solution, a little library: You wrap your Java 8 lambda in F(...) and then it's converted to a Scala function."},
{"body": "The Either class seems useful and the ways of using it are pretty obvious. But then I look at the API documentation and I'm baffled:What do I do with a projection and how do I even  the joins?Google just points me to the API documentation.This might just be a case of \"paying no attention to the man behind the curtain\", but I don't think so. I think this is important. and  are the important ones.  is useful without projections (mostly you do pattern matching), but projections are quite worthy of attention, as they give a much richer API. You will use joins much less. is often used to mean \"a proper value or an error\". In this respect, it is like an extended  . When there is no data, instead of , you have an error. \n has a rich API. The same can be made available on , provided we know, in Either, which one is the result and which one is the error.  and  projection says just that. It is the , plus the added knowledge that the value is respectively at left or at right, and the other one is the error. For instance, in , you can map, so  returns an  with  applied to the value of  if it has a one, and still  if  was . On a left projection, it will apply  on the value at left if it is a , and leave it  unchanged if it is a . Observe the signatures: and  are simply the way to say which side is considered the value when you want to use one of the usual API routines. Alternatives could have been: Now the joins. Left and Right means the same thing as for the projections, and they are closely related to . Consider . The signature may be puzzling: and  are technically necessary, but not critical to the understanding, let's simplifyWhat the implicit means is that the method can only be called if  is an . The method is not available on an  in general, but only on an . As with left projection, we consider that the value is at left (that would be right for ). What the join does is flatten this (think ). When one join, one does not care whether the error (B) is inside or outside, we just want Either[C,B]. So Left(Left(c)) yields Left(c), both Left(Right(b)) and Right(b) yield Right(b). The relation with flatMap is as follows:The  equivalent would work on an ,   would yield  both  and  would yield . It can be written o.flatMap(identity). Note that  is isomorphic to  (if you use left projections and joins) and also to  (using right projections).Ignoring the joins for now, projections are a mechanism allowing you to use use an  as a monad. Think of it as extracting either the left or right side into an , but As always, this probably makes more sense with an example. So imagine you have an  and want to convert the  to a  (if present)This will map over the left side of result, giving you an  and  enable you to \"flatten\" a nested :Edit:  shows one example of how you can use the projections, in this case to fold together a sequence of s without pattern matching or calling  or . If you're familiar with how to use  without matching or calling , it's analagous.While curiously looking at the current , I saw that  and  are implemented with pattern matching. However, I stumbled across this  and saw that it used to implement the join methods using projections:My suggestion is add the following to your utility package:In my experience the only useful provided method is fold. You don't really use isLeft or isRight in functional code. joinLeft and joinRight might be useful as flatten functions as explained by Dider Dupont but, I haven't had occasion to use them that way. The above is using Either as right biased, which I suspect is how most people use them. Its like an Option with an error value instead of None.Here's some of my own code. Apologies its not polished code but its an example of using Either in a for comprehension. Adding the map and flatMap methods to Either allows us to use the special syntax in for comprehensions. Its parsing HTTP headers, either returning an Http and Html error page response or a parsed custom HTTP Request object. Without the use of the for comprehension the code would be very difficult to comprehend."},
{"body": "What is the difference between exclamation mark () and question mark () when sending messages to Actors?Shamelessly copied   (look  section for more):From the recipient's point of view, it sees  and  messages the same way. However when receiving a  the value of  will be the reference of the actor who sent the message, whereas for an , the  is set up such that any reply goes to the  created in the actor who did the asking.There is an advantage in , that it is easy to know that the response you're receiving was definitely a result of the message you asked, whereas with Tell, you may need to use unique IDs to achieve a similar result. However with  you need to set a  after which the  will fail if no response is received.In the code below, the same effect is achieved with a  and and ."},
{"body": "The type inference engine of Haskell is much more powerful than Scala's. In Haskell I rarely have to explicitly write the types whereas in Scala the types can only be inferred in expressions but not in method definitions.For example, see following Haskell code snippet:It returns the size of a List. The Haskell compiler can recognize what types are used and what the function definition is. The equivalent Scala code:Or with method definitions:My question is: Why can't I write them like the following?Once again with method definitions:Is it because nobody has implemented it yet? Is the type system of Scala not as powerful as needed for this case? Or are there other reasons?The main reason is that the type system of Scala allows sub-typing, which the  does not support.Haskell does not have sub-typing, so the algorithm works much better there, although many popular type system extensions supported by GHC cause type inference to fail again, forcing you to provide explicit type signatures for some expressions.In the end, it's a trade-off between the power of the type system and the amount of type inference that can be done. Scala and Haskell have simply made different trade-offs.I guess the main reasons have already been given, but I find this quote by Scala's creator Martin Odersky to be particularly informative,Source: comment under post .hammar gave the biggest reason. Here are two others:ConsiderHow could Scala possibly infer the type of the argument? Should it look for every class with an  and  field? What if there are more than 1? In Haskellrecord names are unique, which presents its own problems, but means you can always infer what kind of record is being referred to.In Scala the type of the object being matched can be used either as part of the match, or to decide how matching should be done. So even if all the constructors in the  are for , you  want to pass something other than a list to it, and have it fail.Another thing that doesn't play well with Hindley-Milner type inference is , and related features like default arguments and varargs. That's the reason why it is so hard to write things like  in Haskell (which is trivial in Scala)."},
{"body": "Suppose that I have a string in scala and I want to try to parse a double out of it.  I know that I can just call toDouble and then catch the java num format exception if this fails, but is there a cleaner way to do this?  For example if there was a parseDouble function that returned Option[Double] this would qualify.  I don't want to put this in my own code if it already exists in the standard library and I am just looking for it in the wrong place.Thanks for any help you can provide.Or just Fancy version:Scalaz provides an extension method  on s, which gives a value of type . You can convert it to  if so required.You could try using  which returns an  type.So using the following returns a Left wrapping a  or a Right wrapping a Unfortunately, this isn't in the standard library.  Here's what I use:There's nothing like this not only in Scala, but even in basic Java. Here's a piece code that does it , though:Usage:I'd usually go with an \"in place\" Try:Note, that you can do further calculation in the for and any exception would project into a Failure(ex). AFAIK this is the idiomatic way of handling a sequence of unreliable operations."},
{"body": "(This is a follow up to . The question wasn't answered.)Basically,  says \"I don't know how to create a project under the new sbt. With the old one, I just ran  in a new folder and there was a guided wizard that led me through the setup.\"The  does not explain how to create a new project, it just points to , which also doesn't explicitly say how to create a new project -- only how to write a  file.So I tried first writing a  and then running  in the directory with the  file, but I still don't see a  directory to work with.Could someone post a simple step-by-step (I'm assuming there are like 3 steps at most) guiding how to create a new project under sbt 0.10.X?I found the answer I was looking for at this webpage: .The high-level steps are:I am surprised that noone gave another solution which is the closest to the old way (as mentioned by @dsg) to create a simple project in sbt:\nJust run sbt in your project directory, then issue the following commands in the sbt REPL:Granted, it is only mildly useful as it will just create the build.sbt file (enough to make it a proper sbt project) with the corresponding properties set, and you might as well create the file by hand (I usually prefer to do so myself). It won't create the  directory either.Just a few days ago  () plugin to sbt was released. It intended to dealt exactly with that problem:You can use  to generate project layout using various templatesIn newer versions of sbt, you can just install sbteclipse:then from sbt's console you can run:In version 0.10.x I think this post can help you:I've been using  skeleton. See also my .It was the first one that just worked and I've been a quite happy with it since then.Don't forget the recent sbt 0.13.3 :Example:Check out the GitHub repo . In particular the  file.Just clone the repo, go to that directory, then call the  command. From inside of , you can type  to execute provided code. Have fun!An alternative way to generate the project structure using Intellij: That will create for you the basic skeleton for the sbt project. "},
{"body": "I have a server side implemented in  and  based front end. My services return  and they are handled within Scalatra's  for JSON responses. For isomorphic/server side rendering setup I did not want to change services to be blocking so I started with Scala Future->  conversion . But the dispatcher in Flux would like to have JS Promise. So far I found only rather complicated sounding way around this  Is there any recommended way to deal with this Scala Future -> JS Promise conversion?I will try to answer the Scala Future to JS Promise part of the question.\nAs you haven't provided an example. I will provide one here with the conversion.\nIf we say that we have a Future implemented in Scala this way:then the corresponding code in JavaScript/ES6 could look like this:I know this is not Scala, but I wanted to include it for completeness.\nThis is a mapping of  to  taken from  docs:"},
{"body": "Is there an easy way to get rid of everything getting generated as a result of performing an SBT build? It turns out it creates target directories all over the place. Performing ... doesn't get rid of all.If you use git:This will remove every file not tracked by git.On my system (Ubuntu Linux) with SBT 0.13.5 and some projects from the Coursera Functional Programming course I found the folders all totalled up to 2.1GB for 12 projects due to all the cache files and duplicated Scala downloads.The current SBT commands that work and get  everything cleaned is:This removes the top level \"target\" and \"lib_managed\" folders (23MB down to 3.2MB in this case) but leaves some target folders under project:This is where the Linux find command (also posted by @jack-oconnor) is very helpful:This gets us back down to a mere 444KB for one of my own projects and the 2.1GB goes down to 5.0MB !In windows you won't have as many useful command line options, e.g. no star wildcards in path names, but you can always try and force it with:The best I can do on automatically finding is a DIR command:Obviously this is very important for reproducible builds on an integration server such as Jenkins!Ensure that all files, , are stored within the integration server workspace, by supplying command line arguments such as this to sbt:and then click the Wipe Out Workspace button in Jenkins, or the equivalent in other integration servers. That should definitely do it!Or if you are using a recent version of the sbt launcher script, you can simply add  instead.On Linux or similar, this is better than , as it won't accidentally remove any directory named  that might exist in your source code:If you're running this command within a shell, you'll need to quote the regular expression, for example, for bash:With BSD find (e.g. on Mac OS X) the command will be:"},
{"body": "I am developing an application with  and  that exposes some REST API. These APIs will be used by different applications, web, mobile or desktop, so the OAuth protocol (OAuth2) seems the most suitable.Also I would initially use an external OAuth Provider such as Facebook.My question is: what is the exact flow to authorize the individual REST call? What should I expect on the server side for each call and what I should check with the external provider? With OAuth1 I knew that the client sent the token with all the signed request, but with Oauth2 I think not so, I imagine that if a token is not signed is not trusted and therefore I do not think this is the flow.You could use a module called SecureSocial.This one is quite refined and many people in Play community seem to be aware/using this module.For authorization might be useful.\nFor end to end scala stuff,\nI ported Apache Amber to Play2 Scala, here is the link:\nThe reason to port Apache Amber is:If you want to setup oauth2 server on your site, you can try use my port. It has document.Basically, the standard flow is the following:If you want more details, just ask :-)OAuth is an Authorization Protocol, so if you're looking at a Authentication Solution, this might not be the one. You're question saying the consumer of the API will be various application. This lead to 2 scenarios, To support OAuth Eco-System you need a Key Management System. \nTo, now coming to endpoint you would have to expose, Otherwise simplest/robust solution would be, \nYou can use existing OAuth ecosystem of Apigee.I had the same problem, what I did ( I suppose It's not the best solution) was, to place the methods of the REST server, inside an \"@Security.Authenticated(Secure.class)\" , and,  use a session cookie (which also was registered inside a Hash table in backend). The session cookie was generated after user sign-inandHope this helps I did not try it myself , but how about  module.\nAs in the github repo it says:I hope this helpsYou can try using this template for play that combines OAuth 2 provider with Deadbolt.\nThe OAuth scope maps to the Deadbolt permission and role concept.\nIt uses Redis to store access tokens, and they expire automatically after the time period you configure."},
{"body": "I want to write a backend system for a web site (it'll be a custom search-style service). It needs to be highly concurrent and fast. Given my wish for concurrency, I was planning on using a functional language such as Haskell or Scala.However, speed is also a priority.  results appear to show that Java is almost as fast as C/C++, Scala is generally pretty good, but Haskell ranges from slower to a lot slower for most tasks.Does anyone have any performance benchmarks/experience of using Haskell vs Scala vs Java for performing highly concurrent tasks? Some sites I've seen suggest that Scala has memory leaks which could be terrible for long running services such as this one. What should I write my service in, or what should I take into account before choosing (performance and concurrency being the highest priorities)?ThanksThis question is superficially about performance of code compiled with GHC vs code running on the JVM. But there are a lot of other factors that come into play.There are a million and one other factors that you should consider. Whether you choose Scala, Java, or Haskell, I can almost guarantee that you will be able to meet your performance requirements (meaning, it probably requires approximately the same amount of intelligence to meet your performance requirements in any of those languages). The Haskell community is notoriously helpful, and my limited experience with the Scala community has been much the same as with Haskell. Personally I am starting to find Java rather icky compared to languages that at least have first-class functions. Also, there are a lot more Java programmers out there, causing a proliferation of information on the internet about Java, for better (more likely what you need to know is out there) or worse (lots of noise to sift through). I'm pretty sure performance is roughly the same. Consider other criteria.You should pick the language that you know the best and which has the best library support for what you are trying to accomplish (note that Scala can use Java libraries).  Haskell is very likely adequate for your needs, if you learn enough to use it efficiently, and the same for Scala.  If you don't know the language reasonably well, it can be hard to write high-performance code.My observation has been that one can write moderately faster  more compact high-performance parallel code in Scala than in Haskell.  You can't just use whatever most obviously comes to mind in either language, however, and expect it to be blazing fast.Scala doesn't have actor-related memory leaks any more  if you use the default actors in a case where either you're CPU-limited so messages get created faster than they're consumed, or you forget to process all your messages.  This is a design choice rather than a bug, but can be the wrong design choice for certain types of fault-tolerant applications.  Akka overcomes these problems by using a different implementation of actors.Take a look at the head-to-head comparison. For some problems ghc and java7-server are very close. For equally many, there's a 2x difference, and for only one there's a 5x difference. That problem is k-nucleotide for which the GHC version uses a hand-rolled mutable hashtable since there isn't a good one in the stdlibs. I'd be willing to bet that some of the new datastructures work provides better hashtables than that one now.In any case, if your problem is more like the first set of problems (pure computation) then there's not a big performance difference and if its more like the second (typically making essential use of mutation) then even with mutation you'll probably notice somewhat of a performance difference. But again, it really depends on what you're doing. If you're searching over a large data set, you'll tend to be IO bound. If you're optimizing traversal of an immutable structure, haskell will be fine. If you're mutating a complex structure, then you may (depending) pay somewhat more.Additionally, GHC's lightweight green threads can make certain types of server applications extremely efficient. So if the serving/switching itself would tend to be a bottleneck, then GHC may have the leg up.Speed is well and good to care about, but the real difference is between using any compiled language and any scripting language. Beyond that, only in certain HPC situations are the sorts of differences we're talking about  going to matter.The shootout benchmark assumes the same algorithm is used in all implementations.  This gives the most advantage to C/C++ (which is the reference implementation in most cases) and languages like it.  If you were to use a different approach which suited a different language, this is disqualified.If you start with a problem which more naturally described in Haskell it will perform best in that language (or one very much like it)Often when people talk about using concurrency they forget the reason they are doing it is to make the application faster.  There are plenty of examples where using multiple threads is not much faster or much much slower.  I would start with an efficient single threaded implementation, as profiled/tuned as you can make it and then consider what could be performed concurrently. If its not faster this more than one CPU, don't make it concurrent.IMHO: Performance is your highest priority (behind correctness), concurrency is only a priority in homework exercise.I would say Scala, but then I have been experimenting with Scala so my preference would definitely be Scala. Any how, I have seen quite a few high performance multi-threaded applications written in Java, so I am not sure why this nature of an application would mandate going for FP. I would suggest you write a very small module based on what your application would need in both scala and haskell and measure the performance on your set up. And, may I also add clojure to the mix ? :-) I suspect you may want to stay with java, unless you are looking at benefiting from any other feature of the language you choose.Your specific solution architecture matters - ."},
{"body": "Why were the case classes without a parameter list deprecated from Scala? And why does compiler suggest to use  as parameter list instead?Someone please answer  my second question... :|It is really easy to accidentally use a no-arg case class incorrectly as a pattern.Instead of:Or better: Hopefully the transcript below will demonstrate why an empty parameter list is preferred to the deprecated missing parameter list.A case object would usually still be preferred over an empty parameter list.Without parameters, every instance of the case class is indistinguishable and hence is essentially a constant. Use an object for that case."},
{"body": "I'm pretty sure I'm missing something here, since I'm pretty new to Shapeless and I'm learning, but when is the Aux technique actually ? I see that it is used to expose a  statement by raising it up into the signature of another \"companion\"  definition. but isn't this nearly equivalent to just putting R in the type signature of F ?I see that path-dependent type would bring benefits  one could use them in argument lists, but we know we can't do thus, we are still forced to put an additional type parameter in the signature of . By using the  technique, we are  required to spend additional time writing the companion . From a usage standpoint, it would look to a naive user like me that there is no benefit in using path-dependent types at all. There is only one case I can think of, that is, for a given type-level computation more than one type-level result is returned, and you may want to use only one of them.I guess it all boils down to me overlooking something in my simple example. There are two separate questions here:I'll start with the second question because the answer is more straightforward: the  type aliases are entirely a syntactic convenience. You don't ever  to use them. For example, suppose we want to write a method that will only compile when called with two hlists that have the same length:The  type class has one type parameter (for the  type) and one type member (for the ). The  syntax makes it relatively easy to refer to the  type member in the implicit parameter list, but it's just a convenience\u2014the following is exactly equivalent:The  version has a couple of advantages over writing out the type refinements in this way: it's less noisy, and it doesn't require us to remember the name of the type member. These are purely ergonomic issues, though\u2014the  aliases make our code a little easier to read and write, but they don't change what we can or can't do with the code in any meaningful way.The answer to the first question is a little more complex. In many cases, including my , there's no advantage to  being a type member instead of a type parameter. Because Scala , we need  to be a type parameter for our method if we want to verify that the two  instances have the same  type. At that point, the  on  might as well be a type parameter (at least from our perspective as the authors of ).In other cases, though, we can take advantage of the fact that Shapeless sometimes (I'll talk about specifically  in a moment) uses type members instead of type parameters. For example, suppose we want to write a method that will return a function that will convert a specified case class type into an :Now we can use it like this:And we'll get a nice . If 's  were a type parameter instead of a type member, we'd have to write something like this instead:Scala doesn't support partial application of type parameters, so every time we call this (hypothetical) method we'd have to specify both type parameters since we want to specify :This makes it basically worthless, since the whole point was to let the generic machinery figure out the representation.In general, whenever a type is uniquely determined by a type class's other parameters, Shapeless will make it a type member instead of a type parameter. Every case class has a single generic representation, so  has one type parameter (for the case class type) and one type member (for the representation type); every  has a single length, so  has one type parameter and one type member, etc.Making uniquely-determined types type members instead of type parameters means that if we want to use them only as path-dependent types (as in the first  above), we can, but if we want to use them as if they were type parameters, we can always either write out the type refinement (or the syntactically nicer  version). If Shapeless made these types type parameters from the beginning, it wouldn't be possible to go in the opposite direction.As a side note, this relationship between a type class's type \"parameters\" (I'm using quotation marks since they may not be  in the literal Scala sense) is called a  in languages like Haskell, but you shouldn't feel like you need to understand anything about functional dependencies in Haskell to get what's going on in Shapeless."},
{"body": "I recently followed a way , which reduces the code and the compile time without using Proguard or Treeshake.Following this article, I should be able use the last Eclipse build (3.7), almost the last version of Scala (2.8.1) updated on an emulator version 10, the version 2.8.3 within Eclipse with the provided plug-in.The presented way is to provide a specific ramdisk image version, where we can upload scala libraries, which drastically shrinks the size of the code to upload to the emulator.I followed the steps, created a hello world, added scala nature, added a dummy scala class, moved the Scala builder before the Android Package Installer, everything builds perfectly, but when I launch the apk on a emulator from Eclipse, the application crashes and I get the following error, which looks like the \n (at the end of the document) :If I remove the scala reference in the activity file, it runs well.Here is the TestSinceInstallation.java file:and here is the ComputeSum.scala fileWhat do you think I should do to make this work ? I feel so close to the goal.Here is the solution, to use Android with  and with Scala 3.0.0 without any problems.Now, to create a scala project,You're done.Now good things happen.\nFirst, you can scalafy any activity, and you will get access to scala unique features, such as:Here is an example of some of them.Now after all the previous boilerplate, it is very useful to write concise activities. Note how we can directly operate on ids as if they were views. Disambiguation is needed as  if the methods can be inferred from various structures.Make sure that the name of the scala file matches the main activity contained in it, in this case it should be .Second, to set up a scala project as a library project, to use is as a base for applications having different resources, follow the . Right-click on the scala project that you want as a base library project, , , and check .\nTo create derivated project using this library and for which you can generate an APK, create a new android project, and without adding any scala or androidproguardscala nature, just right-click, , , and add the previous scala project as a library. With the new version of the Android Plug-in, you should go to  and check . This will allow to export the scala library, both in the library project and the main project, even if the main project is not assigned Scala. Using the plug-in  makes it easy to test your android scala project. Just follow the steps for creating a test project, add Scala nature to it. You can even use the new scala debugger, and by adding the  library, you can use should matchers and many other features Scala has."},
{"body": "I'm going to develop new HTTP/REST services using Scala and Akka Actors.I have experience working with Play, but I don't really need a complete web Framework.\nFrom what I read, I think Spray is a suitable choice. \nMy question come from the future of Spray after the new arrived AKKA-HTTP.Does the Spray project will grow independently from the Akka-HTTP project, or are the two project going to be merged into one Akka-HTTTP? What is the impact of this if I start developing with Spray?\nAlso I read that Play will integrate AKKA-HTTP. \nSo I finally wonder if a should not go with Play?Thanks for your help.Spray is production ready, but the development team (Mathias Doenitz) works for Typesafe on Akka-http now.The status of Akka-http is .  There are vague promises of a full release \"within a few months\", but nothing you can take to the bank.Jonas Boner from Typesafe has referred to Akka-http as \"Spray 2.0\".  So don't expect any future versions of Spray and at some point you'll have to make the switch.  I saw Jonas' Akka-http presentation at Scala Days and it looks like porting Spray code to Akka-http should be straight forward as the DSL is mostly unchanged (even though the underlying implementation of the library will be different).To answer your questions specifically: Spray is finished as a separate project, it is being imported into Akka under the name Akka-http (not a merge as Akka didn't have any equivalent before hand).  If you need to start development now go with Spray, if you can afford to work with the inevitable bugs in a preview release go with Akka-http.  Your Spray code will never stop working, but it won't be supported either outside of minor bug fixes.  All new functionality will be added in Akka-http, so instead of updating to Spray 2.0 you update to Akka-http.You have already given answer to your question in your comment. As long as if you don't need to deal with UI I would suggest go with .I don't think you will have impact if you starts with Spray.io as there are many projects running their production environment on Spray.For more details please refer below link.ThanksWell, if you have to learn everything from scratch, I would recommend choosing spray - Akka hhtp documentation is really incomplete and a lot of route directives are not implemented yet on akka.http. I started with akka but I was forced to go to spray...As per Johannes Rudolphsource: "},
{"body": "Scala 2.10 seems to have broken some of the old libraries (at least for the time being) like Jerkson and lift-json.  The target usability is as follows:But I'm having trouble finding good existing ways of generating and deserializing Json that work with Scala 2.10.Are there best practice ways of doing this in Scala 2.10?  is a Java library to process JSON fast. The Jerkson project wraps Jackson, but appears to be abandoned. I've switched to Jackson's  for serialization and deserialization to native Scala data structures.To get it, include the following in your :Then your examples will work verbatim with the following Jackson wrapper (I extracted it from jackson-module-scala test files):Other Scala 2.10 JSON options include Twitter's  based on the Programming Scala book--it's simple, at the cost of performance. There is also , which uses  for parsing. Finally,  looks nice, but it does not easily decouple from the Play project.Mentioning  that wraps jackson, lift-json or its own native implementation as a long term solution:I can heartily recommend  for json support in scala. All you need to configure it to serialize your Customer object is one line:  That will pimp your class to give it an  method which turns it into a string. It will also pimp the string class to give it a method  to parse strings. It handles the options in your class fine. Here is a working class with a passing test and a running main method which you can drop into a git clone of argonaut to see it all working fine: The developers are also helpful and responsive to folks getting started. There is now a fork of Jerkson that supports Scala 2.10 at .So, based on the absence of an error message and the incorrect sample code, I'm suspecting this is more of an issue of just not understanding how the lift-json extraction works. If I've misunderstood, do comment and let me know. So, if I'm right then here's what you need.To serialize:Then to reverse the process you'd do something like:If that's not the part you're having trouble with, then do let me know and I can try to revise my answer to be more helpful."},
{"body": "This should be an easy one.  How do I apply a function to a tuple in Scala? Viz:Many thanks in advance.In Scala 2.7:In Scala 2.8:Following up on the other answer, one could write (tested with 2.11.4):See :"},
{"body": "Kinda silly question, but I used  to get started with the play framework, and now need to see what version I'm using. 2.3 came out with support for docker, but when I put in my , it complains it doesn't know what dockerExposedPorts is, so I'm thinking I might be running 2.2. Type  within the activator console. Alternatively you can look in  for the line In this example, the play version is 2.3.2"},
{"body": "There's something I don't understand about Scala's . It describes the interface for all mutable sequences, yet I don't see methods to append or prepend elements without creating a new sequence. Am I missing something obvious here?There are  and  for append and prepend, respectively, but they create new collections \u2014 in order to be consistent with the behavior of immutable sequences, I assume. This is fine, but why is there no method like  and , like  and  define, for in-place append and prepend? Does it mean that I cannot refer to a mutable seq that's typed as  if I want to do in-place append?Again, I must have missed something obvious, but cannot find what\u2026Mutability for sequences  guarantees that you'll be able to swap out the items for different ones (via the  method), as you can with e.g. primitive arrays.  It does  guarantee that you'll be able to make the sequence larger (that's what the  trait is for) or smaller (). is the abstract trait that contains  and , not ."},
{"body": "When you create a case class, the compiler creates a corresponding companion object with a few of the case class goodies: an  factory method matching the primary constructor, , , and .Somewhat oddly, this generated object extends FunctionN.This is only the case if: Seems like this was  about two years ago. The latest incarnation is .Does anyone use this, or know why it was added? It increases the size of the generated bytecode a little with static forwarder methods, and shows up in the  method of the companion objects:Manually created objects with a single  method are not automatically considered as :The reason why case class companion objects implement FunctionN is that before, case classes generated a class and a factory method, not a companion object. When we added extractors to Scala it made more sense to turn the factory method into a full companion object with apply and unapply methods. But then, since the factory method did conform to FunctionN, the companion object needed to conform, too.[Edit] That said, it would make sense to have companion objects show as their own name, not as \"function\"Well, given that  in Scala:it seems natural that a companion object:is really:So the addition seems to be natural to me (I'm not sure why it seems \"odd\" to you?). As to whether it actually  anything; well, that is for someone smarter than me!Aside from oxbow_lakes's reply about the naturalness of it, it can often be useful to have constructors available as first-class functions, particularly in conjunction with Scala collections higher-order functions. For (a trivial) example,"},
{"body": "I started learning scala a few days ago and when learning it, I am comparing it with other  languages like (, ) which I had some familiarity with. Does Scala has  sequences available?I went through pattern matching in Scala, but is there any concept equivalent to guards with  and all?Yes, it uses the keyword . From the  section of A Tour of Scala, near the bottom:(This isn't mentioned on the  page, maybe because the Tour is such a quick overview.)In Haskell,  is actually just a variable bound to . So it doesn't add any power to the concept of pattern matching. You can get it just by repeating your initial pattern without the guard:Yes, there are pattern guards. They're used like this:Note that instead of an -clause, you simply specifiy the pattern without a guard.The simple answer is no. It is not exactly what you are looking for (an exact match for Haskell syntax). You would use Scala's \"match\" statement with a guard, and supply a wild card, like:I stumbled to this post looking how to apply guards to matches with multiple arguments, it is not really intuitive, so I am adding an random example here. "},
{"body": "Given a function that takes a variable number of arguments, e.g.How can I pass a sequence of arguments to the function? I would like to write:Obviously, this does not work. does the trick. Instead of applying the sequence as one single argument, each element in the sequence will be used as an argument."},
{"body": "What would be good purely functional data structures for text editors? I want to be able to insert single characters into the text and delete single characters from the text with acceptable efficiency, and I would like to be able to hold on to old versions, so I can undo changes with ease.Should I just use a list of strings and reuse the lines that don't change from version to version?A  would probably be a good bet. It is an  so has decent update / prepend / update performance, unlike the  you mention. If you look at , it's the only immutable collection mentioned that has effective constant-time update.I don't know whether this suggestion is \"good\" for sophisticated definitions of \"good\", but it's easy and fun. I often set an exercise to write the core of a text editor in Haskell, linking with rendering code that I provide. The data model is as follows.First, I define what it is to be a cursor inside a list of -elements, where the information available at the cursor has some type . (The  will turn out to be  or .)This  thing is just the backward \"snoc-lists\". I want to keep strong spatial intuitions, so I turn things around in my code, not in my head. The idea is that the stuff nearest the cursor is the most easily accessible. That's the spirit of The Zipper.I provide a gratuitous singleton type to act as a readable marker for the cursor......and I can thus say what it is to be somewhere in a Now, to represent a buffer of multiple lines, we need s above and below the line with the cursor, and a  in the middle, for the line we're currently editing.This  type is all I use to represent the state of the edit buffer. It's a two layer zipper. I provide the students with code to render a viewport on the text in an ANSI-escape-enabled shell window, ensuring that the viewport contains the cursor. All they have to do is implement the code that updates the  in response to keystrokes.where  should return  if the keystroke is meaningless, but otherwise deliver  an updated  and a \"damage report\", the latter being one of(If you're wondering what the difference is between returning  and returning , consider whether you also want the editor to go beep.) The damage report tells the renderer how much work it needs to do to bring the displayed image up to date.The  type just gives a readable dataype representation to the possible keystrokes, abstracting away from the raw ANSI escape sequences. It's unremarkable.I provide the students with a big clue about to go up and down with this data model by offering these pieces of kit:The  function is used to shift focus out of a , giving you an ordinary list, but telling you where the cursor . The corresponding  function attempts to place the cursor at a given position in a list:I offer the students a deliberately incorrect and incomplete definition of which just handles ordinary character keystrokes but makes the text come out backwards. It's easy to see that the character  appears  of . I invite them to fix the bug and add functionality for the arrow keys, backspace, delete, return, and so on.It may not be the most efficient representation ever, but it's purely functional and enables the code to conform concretely to our spatial intuitions about the text that's being edited.We use a text zipper in Yi, a serious text editor implementation in Haskell. The implementation of the immutable state types is described in the following,and other papers.I'd suggest to use  in combination with  which is based on . So you could represent the current state as\nThis gives you  complexity for moving cursor up/down a single line, and since  and  (union) have both  complexity, you'll get  complexity for skipping  lines up/down.You could have a similar zipper structure for  to keep a sequence of character before, at and after the cursor. could be something space-efficient, such as .I've implemented a zipper for this purpose for my  library.  You can take a look here:The Clojure community is looking at  (Relaxed Radix Balanced) as a persistent data strcuture for vectors of data that can be efficiently concatenated / sliced / inserted etc.It allows concatenation, insert-at-index and split operations in O(log N) time.I imagine a RRB Tree specialised for character data would be perfectly suited for large \"editable\" text data structures.The possibilities that spring to mind are:"},
{"body": "Just a simple velocity standalone app based on maven structure. Here is the code snippet written in Scala to render the template  in  folder: when to compile and run the program, I got the following error:Great question - I solved my issue today as follows using Ecilpse:See how first we tell VelocityEngine to look in the classpath. Without this it wouldn't know where to look.I put my .vm under the , then the code is :this works in web project.  In eclipse Velocity.getTemplate(\"my.vm\") works since velocity will look for the .vm file in src/main/resources/ or src/main/resources/templates, but in web project, we have to use Velocity.getTemplate(\"templates/my.vm\");You can just use it like this: It works.Make sure you have a properly configured resource loader. See Velocity documentation for help selecting and configuring a resource loader: you can try to add these code:I do this, and pass!you could use the above code to set the properties for velocity template. You can then give the name of the tempalte file when initializing the Template and it will find if it exists in the classpath.All the above classes come from package org.apache.velocity*While using embedded jetty the property webapp.resource.loader.path should starts with slash:otherwise templates will not be found in ../webapp/templatesYou can also put your templates folder under src/main/resources instead of src/main/java. Works for me and it's a preferable location since templates are not java source.I faced the similar problem with intellij IDEA.\nyou can use this "},
{"body": "Is it possible to match a range of values in Scala?For example: would be  if  was between 0 and 10, but false otherwise. This little bit doesn't work of course, but is there any way to achieve something like it?Guard using :You can use guards:Here's another way to match using a range:With these definitions:You can do checks like these:So the code you need would be:"},
{"body": "How do I check if a path / file exists in Scala similar to Python ? An example below:  Since Java 7 the better way would beWell, sorry I found the answer to my own question: you can use scala library as well :"},
{"body": "I know a lot of Java people have started looking at Scala since it runs on the JVM, and a lot of people in the Microsoft world are looking at F#, but what does Ruby have as a natural functional successor?In a pure FP sense Ruby doesn't lack anything, instead it has too much some may say. A functional language forces the programmer to not use global variables and other idioms so much (although it is possible to use globals in functional languages)There's two  different definitions of what \"functional programming\" means. You can kind-of do the one in Ruby, but you cannot do the other.Those two definitions are:You can kind-of program with first-class functions in Ruby. It has support for first-class functions. In fact, it has  support for them: there is , , , , , blocks,  and  (and probably some others that I forget).All of these , have slightly different syntax, slightly different behavior and slightly different restrictions. For example: the only one of these which is syntactically lightweight enough that you can actually use it densely, is blocks. But blocks have some rather severe restrictions: you can only pass one block to a method, blocks aren't objects (which in an object-oriented language in wich \"everything is an object\" is a  severe restriction) and at least in Ruby 1.8 there are also some restrictions w.r.t parameters.Referring to a method is another thing that is fairly awkward. In  or  for example, I can just say  to refer to the  method of the  object. In Ruby,  is a method , if I want to refer to the  method of , I have to say . And if I now want to  that method, I cannot just say , I have to say  or  or (in Ruby 1.9) .So, first-class functions in Ruby aren't  first-class. They are much better than second-class, and they are \u2122, but they aren't fully first-class.But generally, Rubyists do not leave Ruby just for first-class functions. Ruby's support is good enough that any advantages you might gain from better support in another language usually is eaten up by the training effort for the new language or by something  that you are accustomed to that you must now give up. Like, say RubyGems or tight Unix integration or Ruby on Rails or syntax or \u2026However, the  definition of FP is where Ruby falls flat on its face. If you want to do programming with mathematical functions in Ruby, you are in for a world of pain. You cannot use the absolute majority of Ruby libraries, because most of them are stateful, effectful, encourage mutation or are otherwise impure. You cannot use the standard library for the same reasons. You cannot use the core library. You cannot use any of the core datatypes, because they are all mutable. You could just say \"I don't care that they are mutable, I will simply not mutate them and always copy them\", but the problem is: someone else still can mutate them. Also, because they are mutable, Ruby cannot optimize the copying and the garbage collector isn't tuned for that kind of workload.It just doesn't work.There is also a couple of features that have really nothing to do with functional programming but that most functional languages tend to have, that Ruby is missing. Pattern matching, for example. Laziness also was not that easy to achieve before s were more aggressively used in Ruby 1.9. And there's still some stuff that works with strict s or s but not with lazy s, although there's actually no reason for them to  strictness.And for  definition of FP, it definitely makes sense to leave Ruby behind.The two main languages that Rubyists have been flocking to, are  and . These are both relatively good matches for Ruby, because they are both dynamically typed, have a similar REPL culture as Ruby, and (this is more a Rails thing than a Ruby thing) are also very good on the web. They have still pretty small and welcoming communities, the original language creators are still active in the community, there is a strong focus on doing new, exciting and edgy things, all of which are traits that the Ruby community also has.The interest in Erlang started, when someone showed the original 1993 introduction video \"\" at RubyConf 2006. A couple of high-profile Rails projects started using Erlang, for example  and . Erlang is also easy to master for Rubyists, because it doesn't take purity quite as far as  or . The  of an actor is pretty pure, but the act of sending messages itself is of course a side-effect. Another thing that makes Erlang easy to grasp, is that Actors and Objects are actually the same thing, when you follow .Clojure has been a recent addition to the Rubyist's toolbelt. Its popularity is I guess mostly driven by the fact that the Ruby community has finally warmed up to the idea that JVM \u2260 Java and embraced  and then they started to look around what  interesting stuff there was on the JVM. And again, Clojure is much more pragmatic than both other functional languages like Haskell and other Lisps like  and much simpler and more modern than CommonLisp, so it is a natural fit for Rubyists.Another cool thing about Clojure is that because both Clojure and Ruby run on the JVM, you can  them.The author of \"\" (Stuart Halloway) is a (former?) Rubyist, for example, as is , the author of the  build tool for Clojure.However, Rubyists are also looking at both  (as one of the more pragmatic statically typed FP languages) and Haskell (as one of the more elegant ones). Then there is projects like  and  which are bridges that let you integrate Ruby with Scala and Haskell, respectively. Twitter's decision to move part of their low-level messaging infrastructure first from MySQL to Ruby, then from Ruby to Scala is also pretty widely known. doesn't seem to play any role at all, possibly due to an irrational fear towards all things Microsoft the Ruby community has. (Which, BTW, seems mostly unfounded, given that the F# team has  made versions available for Mono.)Java people are using a language on the JVM and want a more functional one compatible with their runtime, so they go to Scala.C# people are using a language on the CLR and want a more functional one compatible with their runtime, so they go to F#.Ruby people are using a language that's already pretty functional, and they're using it on a number of underlying runtimes (JRuby, IronRuby, MRI, MacRuby, Rubinius, etc...).  I don't think it has a natural functional successor, or even needs one.Any version of Lisp should be fine.Ruby it self is a kind of Functional programming language, So I don't see any special dialects for FP using ruby.In hype level, Haskell.Assuming Ruby people don't just go to the JVM themselves, I think most would adopt Erlang, being another dynamically typed language.Ruby isn't as functional as say Lisp, but it is just functional enough that you can do some functional programming in a good fun way. (unlike trying to do functional programming in something like C#)Also, it actually forces you into functional paradigms in some of its syntax, such as the heavy use of blocks and yield. (which I fell in love with after learning Ruby). "},
{"body": "With the intention of learning and further to this , I've remained curious of the idiomatic alternatives to explicit recursion for an algorithm that checks whether a list (or collection) is ordered.  (I'm keeping things simple here by using an operator to compare and Int as type; I'd like to look at the algorithm before delving into the generics of it)The basic recursive version would be (by @Luigi Plinge): A poor performing idiomatic way would be:An alternative algorithm using fold:It has the drawback that it will compare for all n elements of the list even if it could stop earlier after finding the first out-of-order element. Is there a way to \"stop\" fold and therefore making this a better solution?Any other (elegant) alternatives? By \"idiomatic\", I assume you're talking about McBride and Paterson's \"Idioms\" in their paper . :o)Here's how you would use their idioms to check if a collection is ordered:Here's how this works:Any data structure  where there exists an implementation of , can be traversed with an  functor, or \"idiom\", or \"strong lax monoidal functor\". It just so happens that every  induces such an idiom for free (see section 4 of the paper).A monoid is just an associative binary operation over some type, and an identity element for that operation. I'm defining a  (a semigroup is the same as a monoid, except without the identity element) whose associative operation tracks the lesser of two values and whether the left value is less than the right value. And of course  is just the monoid generated freely by our semigroup.Finally,  traverses the collection type  in the idiom induced by the monoid. The result  will contain true if each value was less than all the following ones (meaning the collection was ordered), or  if the  had no elements. Since an empty  is sorted by convention, we pass  as the second argument to the final  of the .As a bonus, this works for all traversable collections. A demo:A test:And  how you do  traversal to detect if a collection is ordered.This will exit after the first element that is out of order. It should thus perform well, but I haven't tested that. It's also a lot more elegant in my opinion. :)I'm going with this, which is pretty similar to Kim Stebel's, as a matter of fact.In case you missed missingfaktor's elegant solution in the comments :This solution is very readable and will exit on the first out-of-order element.The recursive version is fine, but limited to  (with limited changes, it would work well on ).If it was implemented in the standard library (would make sense) it would probably be done in  and have a completely imperative implementation (see for instance method )You can interrupt the  with a  (in which case you need only the previous element and not boolean all along)but I don't see how it is any better or even idiomatic than an imperative implementation. I'm not sure I would not call it imperative actually. Another solution could be Rather concise, and maybe that could be called idiomatic, I'm not sure. But I think it is not too clear. Moreover, all of those methods would probably perform much worse than the imperative or tail recursive version, and I do not think they have any added clarity that would buy that. Also you should have a look at .To stop iteration, you can use :If you split the List into two parts, and check whether the last of the first part is lower than the first of the second part. If so, you could check in  for both parts. Here the schematic idea, first without parallel: And now with parallel, and using  instead of :my solution combine with missingfaktor's solution and Orderingand you can use your own comparison method. E.g."},
{"body": "So I've got this macro:I've said three times that I want  to return a , and yet I can do the following (in 2.10.0-RC3):The same thing happens if I remove the type parameters on either . If I really want to make sure that whoever's calling  can't see that they're getting a , I have to add a type ascription in the tree itself.This is actually pretty great\u2014it means for example that I can point a macro at a schema of some sort and create an anonymous subclass of some  class with member methods representing terms in the vocabulary, and these will be available on the returned object.I'd like to understand exactly what I'm doing, though, so I have a couple of questions. First, what is the return type on the  method actually for? Is it just available for (optional) documentation? It clearly constrains the return type (e.g., I can't change it to  in this case), and if I remove it entirely I get an error like this:But I  change it to  and still get a statically typed  when I call .Second, is this behavior specified somewhere? This seems like a fairly elementary set of issues, but I haven't been able to search up a clear explanation or discussion.This behavior is underspecified but intended, though it might appear confusing. We plan to elaborate on the role of return type in macro signatures, but at the moment I feel like the flexibility is a good thing to have.Also at times the behavior is inconsistent, e.g. when the macro is caught in the middle of type inference, its static signature will be used (i.e.  in your example), not the type of the actual expansion. That's because macro expansion is intentionally delayed until type inference is done (so that macro implementations get to see inferred types, not type vars). This is a trade-off and not necessarily the best one, so we're planning to revisit it soon: .Another problem in this department is with implicit macros. When the return type of an implicit macro is generic and needs to be inferred from the requested type of an implicit value, bad things happen. This makes it currently impossible to use macros to generate type tags: ."},
{"body": "I have a  of  and I'd like to be able to convert the tuple to a list so that I can then use . Is there any way to do this? Also, I know it's a , though I'm always a fan of generic solutions.Works with any tuple (scala 2.8):Scala 2.7:Not sure how to maintain type info for a general Product or Tuple, but for Tuple2:You could, of course, define similar type-safe conversions for all the Tuples (up to 22).Using Shapeless -Note that type information is not lost using Shapeless's ."},
{"body": "Given this  case class:... and this instanceIs there a reason to prefer this code (with )... over the following?Perhaps I'm missing the meaning/power of ?You only include the  when you want to also deal with the object itself. Hence:Otherwise, there's no real point in including it."},
{"body": "I am an experienced C++ programmer with average Python skills. The reasons I studied Python in the first place were:I think that Python is great and I believe that I have achieved the above goals. I will continue to use it for small projects, scripts and web development.I doubt that I can use it for medium to large projects though. While the dynamic typing is convenient, it allows a certain class of bugs that I find disturbing. Unit testing and linting can alleviate this problem, but static typing completely eliminates it.After looking at some programming languages, I think that Scala looks like a good candidate:\nI like the type inference and it runs on the JVM so it should be available wherever the JVM is available. I can also learn more about functional programming when using it.But... I also have some doubts, and this is where I hope that the Stack Overflow community can help:Finally, what do you think that are some of Scalas disadvantages?Yes. There is quite some movement about Scala on Android. As for J2ME, I saw something in that respect, but not much. There is some code pertaining to J2ME on the source code repository. I'm not sure how viable it is, but it looks to me that there isn't much demand for that.I'll also mention that there is/was a pool on Scala-Lang about the desired target platforms, and J2ME was one of them, very low on the totem pole.As well as you can mix C++ with Java, for whatever that is worth. If you haven't any experience with that, you can just read the Java resources, as anything in them will be applicable with Scala with no changes (aside Scala syntax).Definitely, yes. Scala goes out of it's way to make sure you don't need to program in a functional style. This is the main criticism of Scala from functional folks, as a matter of fact: some do not consider a language functional unless it forces the programmer to write in functional style.Anyway, you can go right on doing things your way. My bet, though, is that you'll pick up functional habits without even realizing they are functional.Perhaps you can look at the  series in my own blog about writing a Matrix class. Even though it looks like standard OO code, it is, in fact, very functional.IDEA (IntelliJ), NetBeans and Eclipse all have good support for Scala. It seems IDEA's is the best, and NetBeans/Eclipse keep frog-jumping each other, though NetBeans has certainly been more stable than Eclipse of late. On the other hand, the support on Eclipse is taking a very promising route that should produce results in the next 6 months or so -- it's just that it's a bumping route. :-)Some interesting signs of Scala tooling for these enviroments is the fact that the Eclipse plugin in development uses AOP to merge more seamlessly with the whole IDE, that the NetBeans plugin is being completely rewritten in Scala, and that there's a Scala Power Pack on IDEA that supports, among other things, translating Java code into Scala code.The EMACS folks have extensive tools for Scala as well, and lots of smaller editors have support for it too. I'm very comfortable with jEdit's support for small programs and scripts, for instance.There is also good Maven support -- in fact, the standard way to install  is to install maven, and then build a Lift archetype. That will pull in an appropriate Scala version. There's an  target that will do triggered recompilation as well.Speaking of recompilation, neither Maven, and particularly nor Ant do a good job at identifying what needs to be recompiled. From that problem sprung  (Simple Build Tool), written in Scala, which solves that problem through the use of Scala compiler plugin. SBT uses the same project layout as Maven, as well as Maven/Ivy repositories, but project configurations are done in Scala code instead of XML -- with support for Maven/Ivy configuration files as well.Very fast. As a purely OO language, Scala already introduces some nice features, comparable to some stuff that's present in C++ but not Java, though they work in different fashion. In that respect, once you realize what such features are for and relate them to C++ stuff, you'll be much ahead of Java programmers, as you'll already know what to do with them.The same thing as Java. You can deploy JARs, WARs, or any other of Java targets, because the scala compiler generate class files. In fact, you use Java's jar to generate a Scala's JAR file from the class files, and the Maven targets for Lift support building WAR files.There is an alternative for script files, though. You can call \"scala\" to run Scala source code directly, similar to a Perl of Shell script. It can also be done on Windows. However, even with the use of a compilation daemon to speed up execution, start up times are slow enough that effective use of Scala in a heavy scripting environment needs something like .As for Scala's disadvantages, take a look at my answer (and other's) in  Stack Overflow question.Scala is an evolving language well worth to invest in, especially if you are coming from Java world. Scala is widely covered at . See this  from Bill Venners and also read about a.Regarding your questions:Scala has full support for imperative programming, writing programs with no FP elements in it is a breeze (however, FP is useful and worth learning anyway).Quickly. There is a number of interesting features in Scala that may be not familiar to people coming from a C++, Java environment, like for example some of the features of the typing system. Some argue that the fact that there is a lot to learn in Scala before you know all of it is a  with the language; I disagree. The presence of those feature is  of the language. The more features the merrier. After all, you don't have to use them all at once, just like you don't have to buy everything that is being sold in the store.I also come from a C++ background, one thing I noticed is that since you will write a lot less code as compared to C++ for a comparable task, your learning will be expedited as you will get more done in the same time period.  This was the same phenomenon that I experienced with Ruby. Actually - if I were you - I'd study programming paradigms instead of languages. Of course you have to study an example language to study the paradigm. Knowing the drawbacks & benefits of different paradigms enables you to view your problems from a different side and makes you a better programmer (even in the languages you already know).Picking up a language of a paradigm already known is a relativly easy task if needed. Since Scala is FP (at least you mentioned it) and C++/Python is OOP, it's a good language for you, I'd say.You should register for this course by the Creator of Scala himself.James Strachan (productive Java open source developer, for those not in the loop) has an interesting discussion of Scala , and why he feels it's a progression from Java (the langauge, not the platform).Scala looks like it's gaining a lot of traction. I don't think it's a flash in the pan, and is currently on my list of languages to learn (partly for the functional aspect)Here's an anecdotal evidence regarding learnability of Scala.In our company, we got several interns from U.Waterloo. They were told to write in Scala; never saw it before.They picked up Scala and Lift remarkably fast; now they are producing Scala code; it may be not perfect, but nobody's perfect.So, the fact that a manager does not know Scala may be not the best argument when you decide on adoption."},
{"body": "How to reduce the amount of trace info the Spark runtime produces?The default is too verbose,How to turn off it, and turn on it when I need.Thanksquoting from '' book.I think that you could just try:It worked for me (Spark 1.4.1).From comments in source code:We can do this in Log4j way also.Set Logging  for packages  and  "},
{"body": "How do I read a large CSV file (> 1 Gb) with a Scala Stream? Do you have a code example? Or would you use a different way to read a large CSV file without loading it into memory first?Just use  as you already stated.That returns an Iterator, which is already lazy (You'd use stream as a lazy collection where you wanted previously retrieved values to be memoized, so you can read them again)If you're getting memory problems, then the problem will lie in what you're doing  getLines.  Any operation like , which forces a strict collection, will cause the problem.I hope you don't mean Scala's  with Stream. This is  what you want. Stream is lazy, but does memoization.I don't know what you plan to do, but just reading the file line-by-line should work very well without using high amounts of memory. should evaluate lazily and should not crash (as long as your file does not have more than 2\u00b3\u00b2 lines, afaik). If it does, ask on #scala or file a bug ticket (or do both).If you are looking to process the large file line-by-line while avoiding requiring the entire file's contents be loaded into memory all at once, then you can use the  returned by .I have a small function, , (containing two sub-functions) which I use for exactly these types of use-cases. The function takes up to four parameters, of which only the first is required. The other parameters have sane default values provided.Here's the function profile (full function implementation is at the bottom):The first parameter, , is required. And it is just any valid instance of  which points to a line-oriented text file, like a CSV.The second parameter, , is optional. And if provided, it must be a function expecting to receive two input parameters; , . And then return an . The function may return a  wrapped  consisting of the valid column values. Or it may return a  which indicates the entire streaming process is aborting early. If this parameter is not provided, a default value of  is provided. This default results in the entire line being returned as a single  value.The third parameter, , is optional. And if provided, it must be a function expecting to receive two input parameters; , . And then return an . The function may return a  wrapped  indicating whether this particular line should be included in the output. Or it may return a  which indicates the entire streaming process is aborting early. If this parameter is not provided, a default value of  is provided. This default results in all lines being included.The fourth and final parameter, , is optional. And if provided, it must be a function expecting to receive two input parameters; , . And then return an . The function may return a  wrapped  consisting of some subset and/or alteration of the existing column values. Or it may return a  which indicates the entire streaming process is aborting early. If this parameter is not provided, a default value of  is provided. This default results in the values parsed by the second parameter, .Consider a file with the following contents (4 lines):The following calling profile......results in this output for  (the unaltered contents of the file):The following calling profile......results in this output for  (each line parsed into the individual column values):The following calling profile......results in this output for  (each line parsed into the individual column values, no header and only the two rows in Irving,Tx):Here's the entire  function implementation:While this solution is relatively succinct, it took me considerable time and many refactoring passes before I was finally able to get to here. Please let me know if you see any ways it might be improved.UPDATE: I have just asked the issue below as . And it now  mentioned below.I had the idea to try and make this even more generic changing the  parameter to  with the new generics-ified function definition below. However, I keep getting the highlight error in IntelliJ \"Expression of type Some[List[String]] doesn't conform to expected type Option[A]\" and wasn't able to figure out how to change the default value so the error goes away.Any assistance on how to make this work would be greatly appreciated."},
{"body": "Say, I have a sequence of strings as an input and I want to get a new immutable  which consists of elements of the input and an item . Here are two methods that I've discovered to be working:My questions are:Use the  (append) operator to append an element to a :Note that some implementations of  are more suitable for appending than others.  is optimised for prepending.  has fast append and prepend operations. is a method on  which requires another  as its parameter - what are the advantages that you see in it accepting other types of sequence? It would have to convert other types to a . If you know that  is efficient for your use case then use  (if you must). If you want polymorphic behaviour then use the generic .There's no instantiation overhead to using ; you don't instantiate it because it's a singleton."},
{"body": "I am learning Scala and I have a Java project to migrate to Scala. I want to migrate it by rewriting classes one-by-one and checking that new class didn't break the project.This Java project uses lots of  and . In new Scala classes I would like to use Scala\u2019s  and  to have good-looking Scala code.The problem is that new classes (those are wtitten in Scala) do not integrate seamelessly with existing Java code: Java needs , Scala needs its own .Here is a simplified example of the problem. There are classes , , . They call each other in a line: .In my situation, classes  and  are framework classes (I don\u2019t need to migrate them). Class  is business-logic and will benefit a lot from Scala cool features.I need to rewrite class  in Scala while preserving integrity with classes  and . The best rewrite would look like (doesn\u2019t work):Ideal behaviour: Lists inside  are native Scala Lists. All in/out  get boxed/unboxed automagically. But this doesn't work.Instead, this does work (thanks to  ()):But it looks ugly.How do I achieve transparent magic conversion of Lists and Maps between Java <-> Scala (without need to do toScala/toJava)?If it is not possible, what are the best practices for migrating Java -> Scala code that uses  and friends?Trust me; you don't  transparent conversion back and forth.  This is precisely what the  functions attempted to do.  In practice, it causes a lot of headaches.The root of the problem with this approach is Scala will automatically inject implicit conversions as necessary to make a method call work.  This can have some really unfortunate consequences.  For example:This code wouldn't be entirely out of character for someone who is new to the Scala collections framework or even just the concept of immutable collections.  Unfortunately, it is completely wrong.  The result of this function is the  map.  The call to  triggers an implicit conversion to , which happily accepts the new values and is promptly discarded.  The original  is unmodified (as it is, indeed, immutable).Jorge Ortiz puts it best when he says that you should only define implicit conversions for one of two purposes:Since  is obviously not a new type unrelated to anything in our hierarchy, we can't fall under the first proviso.  Thus, our only hope is for our conversion  to qualify for the second one.  However, it makes absolutely no sense for Scala's  to inherit from .  They are really completely orthogonal interfaces/traits.  As demonstrated above, attempting to ignore these guidelines will almost always result in weird and unexpected behavior.The truth is that the javautils  and  methods were designed to solve this exact problem.  There is an implicit conversion (a number of them actually) in javautils from .   is a brand new type defined by javautils, so its only purpose is to add members to .  In particular, it adds the  method, which returns a wrapper map which implements  and delegates to your original  instance.  This makes the process much more explicit and far less error prone.In other words, using  and   the best practice.  Having gone down both of these roads independently in a production application, I can tell you first-hand that the javautils approach is much safer and easier to work with.  Don't try to circumvent its protections merely for the sake of saving yourself 8 characters! Here are some quick examples using Jorge Ortiz's :the javautils project is available from the With Scala 2.8, it could be done like this:"},
{"body": "I know what the monads are and how to use them. What I don't understand is  makes, let's say,  a monad? In Haskell a monad  is a monad because it's instantiated from  class (which has at least 2 necessary functions  and  that makes class , indeed, a monad). But in Scala we've got this:Nothing related to a monad.If I create my own class in Scala, will it be a monad by default? Why not? is a concept, an abstract interface if you will, that simply defines a way of composing data. supports composition via , and that's pretty much everything that is needed to wear the \"monad badge\".From a theoretical point of view, it should also:but this is not strictly enforced by Scala.Monads in scala are a much looser concept that in Haskell, and the approach is more practical.\nThe only thing monads are relevant for, from a language perspective, is the ability of being used in a for-comprehension. is a basic requirement, and you can  provide ,  and .However, there's no such thing as strict conformance to a  typeclass, like in Haskell.Here's an example: let's define our own monad.As you see, we're only implementing  and  (well, and  as a commodity).\nCongratulations, we have a monad! Let's try it out:Nice! We are not doing any filtering, so we don't need to implement . Also since we're yielding a value, we don't need  either. Basically you implement whatever you wish to support, without strict requirements. If you try to filter in a for-comprehension and you haven't implemented , you'll simply get a compile-time error.Anything that (partially) implements, through duck-typing, the  trait is considered to be a monad in Scala. This is different than how monads are represented in Haskell, or the . However, in order to benefit of the  comprehension syntactic sugar in Scala, an object has to expose some of the methods defined in the  trait.Also, in Scala, the equivalent of the Haskell  function is the  keyword used for producing values out of a  comprehension. The desugaring of  is a call to the  method of the \"monad\".The way I'd put it is that there's an emerging distinction between monads as a  vs. a .  Haskell has the latter, in the form of the  type class.  But if you have a type that has (or can implement) the monadic operations and obeys the laws, that's a monad as well.These days you can see monads as a design pattern in Java 8's libraries.  The  and  types in Java 8 come with a static  method that corresponds to Haskell , and a  method.  There is however no  type.Somewhere in between you also have the \"duck-typed\" approach, as Ionu\u021b G. Stan's answer calls out.  C# has this as well\u2014LINQ syntax isn't tied to a specific type, but rather it can be used with any class that implements certain methods."},
{"body": "Are there any special concurrency operators, or is functional style programming good for concurrency? And why?At the moment, Scala already supports two major strategies for concurrency -  concurrency (derived from Java) and  concurrency (inspired by Erlang). In the nearest future (Scala 2.9), there will be two big additions to that: Actors syntax (concurrency operators) is heavily influenced with Erlang (with some important additions) - with regards to the library you use (standard actors, Akka, Lift, scalaz), there will be different combinations of question and exclamation marks:  (in the most cases for sending a message one-way), , , etc. Besides that, first-class functions make your life way easier even when you work with old Java concurrency frameworks: ExecutorService, Fork-Join Framework, etc. Above all of that stands immutability that simplifies concurrency a lot, making code more predictable and reliable.There are a number of language features that make Scala good for concurrency.  For example:Further reading:Well, there is the hype and there is the reality. Scala got a fame for being good for concurrency because it is a functional language and because of its actors library. Functional languages are good for concurrency because they focus on immutability, which helps concurrent algorithms. Actors got their reputation because they are the base to Erlang's track record of massively concurrent systems.So, in a sense, Scala's reputation is due to being a \"me too\" of successful techniques. Yet, there is something that Scala does bring to the table, which is its ability to support such additions to the language through libraries, which makes it able to adapt and adopt new techniques as they are devised.Actors are not native to Scala, and yet there are already there different libraries in wide use that all seem to be. Neither is transactional memory, but, again, there are already libraries that look like they are.They, these libraries, are even available for java, but there they are clunky to use. So the secret is not so much what it can do, but that it makes it look easy.The big keyword here is immutability. See . Since any variable can be defined as mutable or immutable, this is a big win for concurrency, since if an object cannot be changed, it is thread safe and therefore writing concurrent programs is easier.Just to rain on everyone's parade a bit, .I've dual-coded a fair few multithreaded things in Scala (mainly using Futures, a bit with Actors too) and C++ (using TBB) since then (mostly Project Euler problems).  General picture seems to be that Scala needs ~1/3 the number of lines of code of the C++ solution (and is quicker to write), but the C++ solution will be ~x10 faster runtime (unless you go to some efforts to avoid any \"object churn\", as shown in the fast version in the answer referenced above, but at that point you're losing much of the elegance of Scala).  I'm still on 2.7.7 mind you; haven't tried Scala 2.8 yet.Using Scala was my first real encounter with a language which strongly emphasised immutability and I'm impressed by its power to hugely simplify the mental model you have to maintain of object \"state machines\" while programming (and this in turn makes coding  for concurrency easier).  It's certainly influenced how I approach C++ coding."},
{"body": "Does Scala have a native way to count all occurrences of a character in a string? If so, how do I do it? If not, do I need to use Java? If so, how do I do that? Thanks!i don't use Scala or even java but google search for \"Scala string\" brought me to which contains :Seems pretty straight forward but i dont use Scala so don't know the syntax of calling a member function.  May be more overhead than needed this way because it looks like it can search for a sequence of characters. read on a different result page a string can be changed into a sequence of characters and you can probably easily loop through them and increase a counter. "},
{"body": "I have 50,000 tasks and want to execute them with 10 threads.\nIn Java I should create Executers.threadPool(10) and pass runnable to is then wait to process all. Scala as I understand especially useful for that task, but I can't find solution in docs. THe simplest approach is to use the  class and associated infrastructure. The  method asynchronously evaluates the block passed to it and immediately returns a  representing the asynchronous computation. Futures can be manipulated in a number of non-blocking ways, including mapping, flatMapping, filtering, recovering errors, etc.For example, here's a sample that creates 10 tasks, where each tasks sleeps an arbitrary amount of time and then returns the square of the value passed to it.In this example, we first create a sequence of individual asynchronous tasks that, when complete, provide an int. We then use  to combine those async tasks in to a single async task -- swapping the position of the  and the  in the type. Finally, we block the current thread for up to 15 seconds while waiting for the result. In the example, we use the global execution context, which is backed by a fork/join thread pool. For non-trivial examples, you probably would want to use an application specific .Generally, blocking should be avoided when at all possible. There are other combinators available on the  class that can help program in an asynchronous style, including , , and .Also, consider investigating the  library, which provides actor-based concurrency for Scala and Java, and interoperates with .This simplest approach is to use Scala's Future class, which is a sub-component of the Actors framework.  The scala.actors.Futures.future method creates a Future for the block passed to it.  You can then use scala.actors.Futures.awaitAll to wait for all tasks to complete.For example, here's a sample that creates 10 tasks, where each tasks sleeps an arbitrary amount of time and then returns the square of the value passed to it.You want to look at either the Scala actors library or Akka. Akka has cleaner syntax, but either will do the trick.So it sounds like you need to create a pool of actors that know how to process your tasks. An Actor can basically be any class with a receive method - from the Akka tutorial ():You'll want to create a pool of actor instances and fire your tasks to them as messages. Here's a post on Akka actor pooling that may be helpful: In your case, one actor per task may be appropriate (actors are extremely lightweight compared to threads so you can have a LOT of them in a single VM), or you might need some more sophisticated load balancing between them.EDIT:\nUsing the example actor above, sending it a message is as easy as this:The actor will then output \"received test\" to standard output.Messages can be of any type, and when combined with Scala's pattern matching, you have a powerful pattern for building flexible concurrent applications.In general Akka actors will \"do the right thing\" in terms of thread sharing, and for the OP's needs, I imagine the defaults are fine. But if you need to, you can set the dispatcher the actor should use to one of several types:It's trivial to set a dispatcher for an actor:See In this way, you could limit the thread pool size, but again, the original use case could probably be satisfied with 50K Akka actor instances using default dispatchers and it would parallelize nicely.This really only scratches the surface of what Akka can do. It brings a lot of what Erlang offers to the Scala language. Actors can monitor other actors and restart them, creating self-healing applications. Akka also provides Software Transactional Memory and many other features. It's arguably the \"killer app\" or \"killer framework\" for Scala.If you want to \"execute them with 10 threads\", then use threads. Scala's actor model, which is usually what people is speaking of when they say Scala is good for concurrency,  such details so you won't see them.Using actors, or futures with all you have are simple computations, you'd just create 50000 of them and let them run. You might be faced with issues, but they are of a different nature.Here's another answer similar to mpilquist's response but without deprecated API and including the thread settings via a custom ExecutionContext:"},
{"body": "I'm trying to search a scala collection for an item in a list that matches some predicate. I don't necessarily need the return value, just testing if the list contains it.In Java, I might do something like:In Groovy, I can do something like:What's the idiomatic way to do this in Scala? I could of course convert the Java loop style to Scala, but I feel like there's a more functional way to do this.Use filter: But if all you need is to check use :If you're just interested in testing if a value exists, you can do it with.... Other methods (some based on comments):Count elements that satisfy predicate (and check if count  > 0)Find the first element that satisfies predicate (as suggested by Tomer Gabel and Luigi Plinge this should be more efficient because it returns as soon as it finds one element that satisfies the predicate, rather than traversing the whole List anyway)For the simple case where we're actually only checking if one specific element is in the list"},
{"body": "While learning functional programming, I keep coming across the term \"reason about\" especially in the context of pure functions and/or referential transparency. Can somebody explain what exactly does this mean?Typically when writing a program, your job doesn't end with merely writing the code, but you would also want to know some properties your code exhibits. You can arrive at these properties by two means: either by logical analysis or by empirical observation.Examples of such properties include:When you measure these properties empirically, you get results with limited precision. Therefore mathematically proving these properties is far superior, however it is not always easy to do. Functional languages typically have as one of their design goals making mathematical proofs of their properties more tractable. This is what is typically meant by reasoning about programs.In terms of functions or lesser units, the above applies, but also sometimes the author simply means thinking about the algorithm or designing the algorithm. It depends on the particular usage.As an aside, some examples of how one might reason about some of these things and how empirical observation can be made: We can prove that code is correct, if we can show equationally that it does what it is supposed to do. So for a sort function if we can show that any list we give it will have the property of being sorted, we know our code is correct. Empirically we can create a unit test suite where we give our code examples of input and check that the code has the desired output. We can analyze our code and prove performance bounds of the algorithm so that we know how does the time it takes depend on the size of the input. Empirically we can benchmark our code and see how quickly it actually runs on a particular machine. We can perform load testing and seeing how much actual input can our machine/algorithm take before it folds/becomes impractical.Reasoning about code, in , means thinking about your code and what it  does (not what you think it should do.) It meansamong other things. For me, the reasoning part plays the biggest role when I'm debugging or refactoring.To use an example you mentioned: Referential transparency helps me a lot when I'm trying to figure out what's wrong with a function. The referential transparency guarantees that as I'm poking around with the function, giving it different arguments, I know that the function will react the same way inside my program. It doesn't depend on anything other than its arguments. That makes the function easier to reason about \u2013 as opposed to an imperative language where the function may depend on some external variable that can change under my nose.Another way of looking at it (and this is more helpful when refactoring) is that the more you know your code satisfies certain properties, the easier it becomes to reason about. I know, for example, thatThis is a useful property I can just apply directly when I'm refactoring. The fact that I can state such properties of Haskell code makes it easier to reason about. I could try to state this property in a Python program, but I would be much, much less confident of it, because if I'm unlucky in my choices of  and  the results may vary wildly.Usually when people say \"reasoning\" they mean \"equational reasoning\", which means proving properties about your code without ever running it.These properties can be very simple.  For example, given the following definition of  and :... we might then want to prove that:This is easy to prove because:Notice how I've proven this for  .  This means that I know that this property is always true no matter what, therefore I no longer need to test this property in some sort of unit test suite because I know that it can never fail.Informally it means, \"Being able to tell what a program will do just by looking at the code.\"  This can be surprisingly difficult in most languages, due to side-effects, casting, implicit conversions, overloaded functions and operators, etc.  I.e., what you can't reason about code using just your brain, you have to run it to see what it will do for a given input.\"Reasoning about a program\" is just \"analyzing a program to see what it does\".The idea is that purity simplifies understanding, both by a human changing a program and by a machine compiling a program or analyzing it for broken corner cases."},
{"body": "I'm learning scala, and I'm looking to update a nested node in some xml. I've got something working but i'm wondering if its the most elegant way.I have some xml: And i want to update the  node in , but not the one in .Here is my function: Is there a more succint way of writing this function?I think the original logic is good. \nThis is the same code with (shall I dare to say?) a more Scala-ish flavor:It looks more compact (but is actually the same :) )if you want, you can get rid of the updateElements too. You want to apply the updateVersion to all the elements of the sequence. That's the . With that, you can rewrite the linewithAs update version takes only 1 parameter I'm 99% sure you can omit it and write:And end with:What do you think?All this time, and no one actually gave the most appropriate answer! Now that I have learned of it, though, here's my new take on it:Now, for a few explanations. The class  is abstract. It defines two methods, both called . One of them takes a single , the other a  of . It's an abstract class, so we can't instantiate it directly. By adding a definition, in this case override one of the methods, we are creating an anonymous subclass of it. Each RewriteRule needs concern itself with a single task, though it can do many.Next, class  takes as parameters a variable number of . It's transform method takes a  and return a  of , by applying each and every  used to instantiate it.Both classes derive from , which defines a few methods with which one need not concern oneself at a higher level. It's  method calls , though, so both  and  can use the syntactic sugar associated with it. In the example, the former does and the later does not.Here we use two levels of , as the first applies a filter to higher level nodes, and the second apply the change to whatever passes the filter.The extractor  is also used, so that there is no need to concern oneself with details such as namespace or whether there are attributes or not. Not that the content of the element  is completely discarded and replaced with . It can be matched against too, if needed.Note also that the last parameter of the extractor is , and not . That means these elements can have multiple children. If you forget the , the match may fail. In the example, the match would not fail if there were no whitespaces. Because whitespaces are translated into  elements, a single whitespace under  would case the match to fail.This code is bigger than the other suggestions presented, but it has the advantage of having much less knowledge of the structure of the XML than the others. It changes any element called  that is below -- no matter how many levels -- an element called , no matter namespaces, attributes, etc.Furthermore... well, if you have many transformations to do, recursive pattern matching becomes quickly unyielding. Using  and , you can effectively replace  files with Scala code.You can use Lift's CSS Selector Transforms and write:See I have since learned more and presented what I deem to be a superior solution in another answer. I have also fixed this one, as I noticed I was failing to account for the  restriction.Thanks for the question! I just learned some cool stuff when dealing with XML. Here is what you want:Now, explanation. First and last case statements should be obvious. The last one exists to catch those parts of an XML which are not elements. Or, in other words, text. Note in the first statement, though, the test against the flag to indicate whether  may be changed or not.The second and third case statements will use a pattern matcher against the object Elem. This will break an element into  its component parts. The last parameter, \"children @ _*\", will match children to a list of anything. Or, more specifically, a Seq[Node]. Then we reconstruct the element, with the parts we extracted, but pass the Seq[Node] to updateNodes, doing the recursion step. If we are matching against the element , then we change the flag mayChange to , enabling the change of the version.In the last line, we use node.theSeq to generate a Seq[Node] from Node, and (0) to get the first element of the Seq[Node] returned as result. Since updateNodes is essentially a map function (for ... yield is translated into map), we know the result will only have one element. We pass a  flag to ensure that no  will be changed unless a  element is an ancestor.There is a slightly different way of doing it, that's more powerful but a bit more verbose and obscure:This version allows you to change any \"version\" tag, whatever it's prefix, attribs and scope. provides tools for \"in place\" edits.  Of course its all immutable but here's the solution in Scales:The XPath like syntax is a Scales signature feature, the  after the string specifies it should have no namespace (local name only). iterates over the resulting elements and transforms them, joining the results back together.One approach would be lenses (e.g. scalaz's). See  for a very clear presentation.I really don't know how this could be done elegantly. FWIW, I would go for a different approach: use a custom model class for the info you're handling, and have conversion to and from Xml for it. You're probably going to find it's a better way to handle the data, and it's even more succint.However there is a nice way to do it with Xml directly, I'd like to see it."},
{"body": "I would like to check constructor arguments and refuse to construct throwing  in case the arguments set is not valid (the values don't fit in expected constraints). How to code this in Scala?In Scala, the whole body of the class is your primary constructor, so you can add your validation logic there.Scala provides a utility method  that lets you write the same thing more concisely as follows:A better approach might be to provide a factory method that gives a  instead of throwing an exception. (Note: requires Scalaz)"},
{"body": "In the file Parsers.scala (Scala 2.9.1) from the parser combinators library I seem to have come across a lesser known Scala feature called \"lazy arguments\". Here's an example:Apparently, there's something going on here with the assignment of the call-by-name argument  to the lazy val .So far I have not been able to work out what this does and why it's useful. Can anyone help?Call-by-name arguments are called .  Lazy vals are called  and then the value is stored.  If you ask for it again, you'll get the stored value.Thus, a pattern likeis the ultimate put-off-work-as-long-as-possible-and-only-do-it-once pattern.  If your code path never takes you to need  at all, then it will never get evaluated.  If you need it multiple times, it'll only be evaluated once and stored for future use.  So you do the expensive call either zero (if possible) or one (if not) times, guaranteed.The wikipedia article for  even answers what the  keyword does:Additionally, what you have in this code sample with  is a call-by-name parameter. A parameter declared this way remains unevaluated, until you explicitly evaluate it somewhere in your method.Here is an example from the scala REPL on how the call-by-name parameters work:As you can see, the  does not get evaluated at all in the second call. Combining the lazy value with a call-by-name parameter like above results in the following meaning: the parameter  is not evaluated immediately when calling the method. Instead it is assigned to the lazy value , which is also not evaluated immediately. Only lateron, when  is used this leads to the evaluation of . But, as  is a  the parameter  will only be evaluated  and the result is stored in  for later reuse in the loop.You can easily see in the repl, that the multiple evaluation can happen otherwise:"},
{"body": "I did all the preference work, installed scala, everything works except this, i just can not test a scala class in the console window. In tutorials I see they just type in something behind the scala> but it just won't work, i can type some random stuff there below which doesn't do anything when I press enter.Can you guys help me out? I would be very thankful.\nCheers Once you type on the Scala console in IntelliJ Community Edition. Hit + instead of enter to execute.took me a while to get around it :)If you want the console to execute when you hit enter, go to , and right-click . Select  and hit enter in the first box. Then click  and then .\nYou have to press \u2318-Enter on Mac OS X when you're done typing.I guess it's better to change the message from \n\"Type in expressions to have them evaluated.\"\nto \n\"Type in expressions and press Ctrl + Enter to have them evaluated.\""},
{"body": "Still the newbie in Scala and I'm now looking for a way to implement the following code on it:What would be the best way to implement the same funcionality that RetryableService implements but in Scala?It basically calls the  method N times, if all of them fail the exception is then raised, if they succeed it moves on. This one does not return anything but then I have another version that allows for returning a value (so, i have two classes in Java) and I believe I could do with a single class/function in Scala.Any ideas?Current implementation in java is as follows:Recursion +  by-name parameters == awesome.Usage is like this:: slight variation inspired by 's answer.  One fewer line of code :-): The recursion bothered me in that it added several calls to the stack trace.  For some reason, the compiler couldn't optimize tail recursion in the catch handler.  Tail recursion not in the catch handler, though, optimizes just fine :-): Apparently I'm going to make it a hobby to keep coming back and adding alternatives to this answer. Here's a tail-recursive version that's a bit more straightforward than using , but using  to short-circuit a function isn't idiomatic Scala.. As is my hobby, I revisit this answer occasionally. Scala 2.10 as introduced , which provides a clean way of implementing retry in a tail-recursive way.Here is one possible implementation:You can use it like this: also returns  if body was processed successfully and  if body was only  throwing exceptions.If you want to bobble up last exception, then you can take very similar approach but use  instead of :Also, as you can see, at the end, instead of having only last exception, I have them all. So you can also wrap them in some  if you want and then throw it. (for simplicity, I just throw last exception)There is a method in : Given a , you can create a new  which will retry a certain number of times, where the delay between retries is defined by the  parameter. e.g.:I'd suggest this - It does: It can probably be improved to be more idiomatic Scala, but I am not a big fan of one-liners that require the reader to know the entire standard library by heart anyways.You can express the idea in functional style using :As we can see, tail recursion can be used here.This approach gives you the additional benefit that you can parametrize the catch container, so you can only retry a certain subset of exceptions, add finalizers etc. So the final version of  might look like:With this, you can do complex stuff like:I like the accepted solution, but suggest checking the exception is NonFatal:You don't want to retry a control flow exception, and usually not for thread interrupts...There is an existing library that can help with that, called , and there is a Java library too, called .Here are some examples of using :If you want control of which exceptions you retry, you can use methods in :(As written, it will also retry on null; that's the  part.  If you want nulls to be returned, use  inside the iterator fill instead.)Let's try this out withDoes it work?Looks good!I ended up adapting a previous answer to allow filtering on which exceptions to retry on:You can call in two ways:or with partial functions (also showing version where don't care about return value)This project seems to provide some nice implementations for different retry mechanisms\nThis solution is not optimized by compiler to tail recursion for some reason (who knows why?), but in case of rare retries would be an option:Usage:Usage:Usage:A reusable object/method with a pause between attempts:Code:"},
{"body": "What makes Scala such a wonderful language,  than the type system?  Almost everything I read about the language brings out 'strong typing' as a big reason to use Scala, but there has to be more than that.  What are some of the other compelling and/or cool language features that make Scala a really useful tool?Here are some of the things that made me favour Scala (over, say, usual Java):a) Type inference. The Java way of doing it:.. is rather verbose compared to Scala. The compiler should be able to figure it out if you give one of these lists.b) First-order functions. Again, this functionality can be emulated with classes, but it's ugly.c) Collections that have  and . These two tie in with (b), and also these two are something I wish for every time I have to write Java.d) Pattern matching and case classes.e) Variances, which mean that if ,  then  as well.Throw in some static types goodness as well, and I was sold on the language quite fast.It's a mash up of the best bits from a bunch of languages, what's not to love:Scala is often paraded for having closures and implicits.  Not surprising really, as lack of closures and explicit typing are perhaps the two biggest sources of Java boilerplate!But once you examine it a little deeper, it goes far beyond Java-without-the-annoying bits,  Perhaps the greatest strength of Scala is not one specific named feature, but how successful it is in unifying all of the features mentioned in other answers.The union of object orientation and functional programming for example:  Because functions are objects, Scala was able to make Maps implement the Function interface, so when you use a map to look up a value, it's no different syntactically from using a function to calculate a value.  In unifying these paradigms so well, Scala truly is a post-functional language.Or operator overloading, which is achieved by not actually having operators, they're just methods used in infix notation.  So  is just calling the  method on an integer.  If the method was named  instead then you'd use it as  which is no different from .  This is made possible because of another combination of features; everything in Scala is an object, there are no primitives, so integers can have methods.Type classes were also mentioned, achieved by a combination of higher-kinded types, singleton objects, and implicits.Other features that work well together are case classes and pattern matching, allowing you to easily build and deconstruct algebraic data types, without having to manually write all the tedious equality, hashcode, constructor and getter/setter logic that Java demands.Specifying immutability by default, offering lazy values, and providing first class functions all combine to give you a language that's very suited to building efficient functional data structures.The list goes on, but I've been using Scala for over 3 years now, and I'm still amazed almost daily at how well everything just works together.Scala is also a small language, with a spec that (surprisingly!) only needs to be around 1/3 the size of Java's.  This is partly because Java has a lot of special cases in the spec that Scala simplifies away, partly because of removing features such as primitives and operators, and partly because a lot of functionality has been moved from the language and into the libraries.As a benefit of this, all the techniques available to the Scala library authors are also available to any Scala user, which makes it a great language for defining your own control-flow constructs and for building DSLs.  This has been used to great effect in projects like Akka - a 3rd-party Actor framework.Finally, it scales the full range of programming styles.The runtime interpreter (known as the REPL) allows you to very quickly explore ideas in an interactive session, and Scala files can also be run as scripts without needing explicit compilation.  When coupled with type inference, this gives Scala the feel of a dynamic language such as Ruby, Perl, or a bash script.At the other end of the spectrum, traits, classes, objects and self-types allow you to build a full-scale enterprise system based on distinct components and using dependency injection without the need of 3rd-party tools.  Scala also integrates with Java libraries at a level almost on-par with native Java, and by running on the JVM can take advantage of all the speed benefits offered on that platform, as well as being perfectly usable in containers such as tomcat, or with OSGi.I'm new to Scala, but my impression is:Really good JVM integration will be the driving factor. JRuby can call java and java can call JRuby code, but it's explicitly calling into another language, not the clean integration of Scala-Java. So you can use Java libraries, and even mix and match in the same project.I started looking at scala when I had a realization that the thing which will drive the next great language is easy concurrency. The JVM has good concurrency from a performance standpoint. I'm sure someone will say that Erlang is better, but Scala is actually usable by normal programmers.Where Java falls down is that it's just so painfully verbose. It takes way too many characters to create and pass a Functor. Scala allows passing functions as arguments.It isn't possible in Java to create a union type, or to apply an interface to an existing class. These are both easy in Scala.Static typing usually has a big penalty of verboseness. Scala eliminates this downside while still giving the upside of static typing, which is compile time type checking, and it makes code assist in editors easier.The ability to extend the language. This has been the thing that has kept Lisp going for decades, and that allowed Ruby on Rails.The type system really is Scala's most distinguishing feature. It also has a lot of syntactic conveniences over, say, Java.But for me, the most compelling features of Scala are:In effect, these features let you approximate (and in some ways surpass) Haskell's . Combined, they let you write exceptionally modular code.Just shortly:What I find especially attractive in all of its magnificient features, among others:Supposedly it's very easy to make Scala code run  on multiple processors.Expressiveness of control flow. For example, it's very common to have a collection of data which you need to process in some way. This might be a list of trades in which the processing involves grouping by some properties (the currencies of the investment instruments) and then doing a summation (to get totals-per-currency perhaps).In Java this involves separating out a piece of code to do the grouping (a few lines of for-loop) and then another piece of code to do the summation (another for loop). In Scala, this type of thing is typically achievable in one line of code using functional programming and then folding, which reads very expressively l-to-r.Of course, this is just an argument for a functional language over Java.The great features of Scala has already been mentioned. One thing that shines through past all features though, is how tastefully everything is integrated.Scala manages to be one of the most powerful language around without having a feeling of having bolted on features in haste. Neither are the language an academic exercise in proving a point. Innovation and really advanced concepts are brought in to the language with uncanny practicality and elegance.In short: Martin Odersky is a pure design genius. That is what's so great about Scala!I want to add the multi-paradigm (OO and FP) nature gives Scala an edge over other languagesEvery day you code Java you will become more and more miserable, every day you code Scala you will become happier.Here's a few fairly in depth explanations for the appeal of functional languages.If we abandon feature discussion and will talk about style, i would say it's pipe-line style of coding. You start from some object or collection, types dot and property or dot and transformation and do it until you form desired result. This way it's easy to write a chain of transoformations that will be easy to read them also. Traits to some extend will also allow you to apply the same approach to constructing types."},
{"body": "Just wondering if anyone knows of a web-scraping library that takes advantage of Scala's succinct syntax. So far, I've found , but this seems poorly-documented and maintained. I'm wondering if anyone out there has done scraping with Scala and has advice. (I'm trying to integrate into an existing Scala framework rather than use a scraper written in, say, Python.)First there is a plethora of HTML scraping libs in JVM all you need to do is .The four I have used are:I have used Selenium but never for scraping. .I would recommend pimping an existing Java library over some half baked Scala lib.I don't have a Scala-specific recommendation, but for the JVM in general I've had good success with:The Tagsoup route actually works quite well with Scala since Scala's built-in XML \"dsl\" is pretty concise (if you can forgive its perf issues and occasional API weirdness).  Also, Tagsoup will handle nearly any garbage document you give it.  It also has niceties like built-in understanding of many HTML entities that other SAXParsers will choke on as being undeclared. - JSoup + CSS selectors if possible, otherwise Tagsoup + scala XML. If slow is ok, tagsoup first, then jsoup the result.I'd recommend Goose: It's not as general-use as you might need but if you are scraping article content from popular sites, it may work out of the box. It also provides a framework for you to work from if you want to extend their code to cover other sites."},
{"body": "I would like to be able to encrypt files on disk and/or data in memory using GnuPG from a Java application.  If possible I'd like to avoid having to make system calls out to the GPG command line tools.Is there a recommended library, or can you recommend the best approach to GPG encrypting from Java (or Scala)?I'm developing and intend to run the application in a Linux environment, although a cross-platform solution would be preferred.You can try to call the JAVA API of .Its  mentions:You have here .There might be some  though, since BouncyCastle does not use GnuPG, but rather implements OpenPGP (RFC2440) in Java.I recently had to work on GPG encryption-decryption and did find BountyCastle's PGP library does the trick. The steps were1) Add the version in pom.xml properties2) Add the following dependencies3) In the implementation class added the provider with Java Security4) The rest of the code was just simple Java implementationI hope this helps.There is  which is based on gpgme, and works on top of GnuPG 1.4.  We're updating it for GnuPG 2.x and are using it in our Android app.  You can get the code to those here:"},
{"body": "When do you choose to type a given function's return type as  vs  vs  (or alternatively even deeper within 's hierarchy)?How do you make that decision? We have a lot of code that returns s by default (usually starting from results of a DB query and successive transformations). I tend to want to make the return types  by default and  when specifically expecting a given order. But I don't have a strong justification for doing so.I am perfectly familiar with the definition of each trait, so please don't answer with defining the terms.This is a good question. You have to balance two concerns:Where (1) asks you to be as little specific about the type (e.g.  over ), and (2) asks you the opposite.Even if the return type is just , you can still return let's say a , so if the caller wishes to gain extra power, it can just call  or  on it, and that operation is cheap for a .As a measure of the balance, I would add a third point:Here are my rules of thumb:Of course, this is my personal choice, but I hope it sounds sane.Make your method's return type as specific as possible. Then if the caller wants to keep it as a  or type it as a , they can. This is why the compiler infers the most specific type by default.These are the \"common-sense\" guidelines.  They are simple, practical, and work well in practice while balancing principles and performance.  The principles are: satisfies both principles.  As described in :90% of the time, your data is a Seq.Other notes:Drawbacks:There are some slight downsides to these \"common-sense\" conventions.Footnotes:A rule of thumb I follow is, depending on implementation, to make the return types as specific as possible and the types of arguments as general as possible. It's an easy to follow rule and it provides you with consistent guarantees on the type properties with maximum freedom.Say, if you have a function implementation which just traverses a data structure with methods like ,  or  - those that are implemented in the trait , you can expect it to perform equally on any type of input collection - be it a , ,  or even a , so your input argument should be specified as . The choice of output type of the function should only depend on its implementation: in this case it should be  too. If however in your function you force this data structure to some more specific type with methods like ,  or , you should specify the appropriate type. Notice the consistency between the implementation and the return type?If your function accesses the elements of input by index, the input should be specified as , as it is the most general type that provides you with guarantees on effective implementation of method . In case of abstract members the same rule applies with the only difference that you should specify the return types based on how you plan to use them instead of implementation, thus most often they will be more general than in implementation. The categorical choices ,  or  are the most expected.Following this rule you protect yourself from very common cases of bottleneck when, for instance, items get appended to  or  gets called on a  instead of a , yet your program remains a nice degree of freedom and is consistent in sense of choice of types."},
{"body": "I have seen many people in the Scala community advise on avoiding subtyping \"like a plague\". What are the various reasons against the use of subtyping? What are the alternatives?Types determine the granularity of composition, i.e. of extensibility.For example, an interface, e.g. Comparable, that combines (thus conflates) equality and relational operators. Thus it is impossible to compose on just one of the equality or relational interface.In general, the  of inheritance is undecidable. Russell's paradox implies that any set that is extensible (i.e. does not enumerate the type of every possible member or subtype), can include itself, i.e. is a subtype of itself. But in order to identify (decide) what is a subtype and not itself, the invariants of itself must be completely enumerated, thus it is no longer extensible. This is the paradox that subtyped extensibility makes inheritance undecidable. This paradox must exist, else knowledge would be static and thus .Function composition is the surjective substitution of subtyping, because the input of a function can be substituted for its output, i.e. any where the output type is expected, the input type can be substituted, by wrapping it in the function call. But composition does not make the bijective contract of subtyping-- accessing the interface of the output of a function, does not access the input instance of the function.Thus composition does not have to maintain the  (i.e. unbounded) invariants and thus can be both extensible and decidable. Subtyping can be  more powerful where it is provably decidable, because it maintains this bijective contract, e.g. a function that sorts a immutable list of the supertype, can operate on the immutable list of the subtype.So the conclusion is to enumerate all the invariants of each type (i.e. of its interfaces), make these types orthogonal (maximize granularity of composition), and then use function composition to accomplish extension where those invariants would not be orthogonal. Thus a subtype is appropriate only where it provably models the invariants of the supertype interface, and the additional interface(s) of the subtype are provably orthogonal to the invariants of the supertype interface. Thus the invariants of interfaces should be orthogonal.Category theory  of the invariants of each subtype, i.e. of Functor, Applicative, and Monad, which , i.e. see the aforementioned example of the power of subtyping for lists.One reason is that equals() is very hard to get right when sub-typing is involved. See . Specifically \"Pitfall #4: Failing to define equals as an equivalence relation\". In essence: to get equality right under sub-typing, you need a double dispatch. I think the general context is for the lanaguage to be as \"pure\" as possible (ie using as much as possible ), and comes from the comparison with Haskell.\nFrom \"\"As mentioned in this :But the actual recommendation would be to use the best solution adapted to the program you are currently developing.Focusing on subtyping, ignoring the issues related to classes, inheritance, OOP, etc.. We have the idea subtyping represents a isa relation between types. For example, types A and B have different operations but if A isa B we then can use any of B's operations on an A.OTOH, using another traditional relation, if C hasa B then we can reuse any of B's operations on a C. Usually languages let you write one with a nicer syntax, a.opOnB instead of a.super.opOnB as it would be in the case of composition, c.b.opOnBThe problem is that in many cases there's more than one way to relate two types. For example Real can be embedded in Complex assuming 0 on the imaginary part, but Complex can be embedded in Real by ignoring the imaginary part, so both can be seen as subtypes of the other and subtyping forces one relation to be viewed as preferred. Also, there are more possible relations (e.g. view Complex as a Real using theta component of polar representation).In formal terminology we usually say morphism to such relations between types and there are special kinds of morphisms for relations with different properties (e.g. isomorphism, homomorphism).In a language with subtyping usually there's much more sugar on isa relations and given many possible embeddings we tend to see unnecessary friction whenever we're using the unpreferred relation. If we bring inheritance, classes and OOP to the mix the problem becomes much more visible and messy.My answer does not answer why it is avoided but tries to give another hint at why it  be avoided. Using \"type classes\" you can add an abstraction over existing types/classes without modifying them. Inheritance is used to express that some classes are specializations of a more abstract class. But with type classes you can take any existing classes and express that they all share a common property, for example they are . And as long as you are not concerned with them being  you don't even notice it. The classes don't inherit any methods from some abstract  type as long as you don't use them. It's a bit like programming in dynamic languages.Further reads:I don't know Scala, but I think the mantra 'prefer composition over inheritance' applies for Scala exactly the way it does for every other OO programming language (and subtyping is often used with the same meaning as 'inheritance'). Here you will find some more information.I think lots of Scala programmers are former Java programmers. They are used to think in term of Object Oriented subtyping and they should be able to easily find OO-like solution for most problems. But Functional Programing is a new paradigm to discover, so people ask for a different kind of solutions. is the best paper I have found on the subject. A motivating quote from the paper \u2013"},
{"body": "The path from SBT to Scala-IDE is well described in many places:What is the reverse for this? If I start a new project in Scala-IDE, can I just add a build.sbt file and somehow tell Eclipse to use this when running the application?Apologies if this seems obvious to some, but I've recently moved from ItelliJ Idea to Scala-IDE and I'm not certain about setting up Scala-IDE to use SBT and my sbt config files.No, you cannot do this. The way to do it is as you described. Then, whenever you make changes to build.sbt (e.g., new jar dependency), rerun the  command from sbt and refresh the project in Eclipse so that the newly generated files are reloaded.As a seasoned Eclipse user, I wondered the same thing. Amazed that no one seemed to be going that direction, I decided to roll my own project.Install plugin, create or modify build.sbt and dependencies are fetched, project files updated.It works for me, I use it every day. But it's not perfect or finished, and I'm not putting in a lot of effort in it at the moment. But do feel free to contribute!"},
{"body": "Play framework 2.0 is a full-stack standalone framework for creating web applications. Probably, many people need to integrate it into their build management, nevertheless. Unfortunately, I did not find much information about his.Here is my use case: I want to create a new project, which uses Scala and Play 2.0. I do NOT want to use sbt. I want to use Gradle, and dependency management should be done via Maven repositories.I have only found this play module:  which supports dependency management via Maven. I am looking for something like these examples in Grails:\n or Of course, I could write scripts / tasks which call \"play console commands\". Though, I do not like this solution. Is there a better way to use Gradle / Maven for build management? \nIf this is the only solution, then I would use Gradle, which then calls Play commands (i.e. sbt internally), right? Does this even work, or will there emerge other problems?Thanks in advance...Best regards,\nKaiThis is a very tricky business. SBT in Play is used for fetching dependencies, compiling source and templates, and for the SBT incremental compilation + auto-reloading feature. I have written a  script to resolve all Play 2.0 dependencies and set-up Eclipse or IntelliJ IDEA classpaths, and made it .I will try to implement the compilation later when I have time, but that would require some research. \nOf course, you can add compile and run tasks that just delegate to SBT but that would require describing all your project dependencies in both SBT and Gradle, which will become difficult to manage.I have updated the  file. I have added  and  tasks that should help in a CI environment. The  task does the following:You can use  and  to remove the output of the above commands, respectively.Note that there appears to be a strange problem (bug?) on Windows, which means that even if  fails, gradle will tell you it succeeded.I think you are simply missingin your file. Check .Good news, as of Gradle 2.7 there is an official play plugin: . They are rolling this out in 3 milestones:  It's available via the . On the above linked blog post, they encourage people to give it a try.I designed a simple build script in Gradle for Play Framework 1.2.x which you could also use for Play 2.x"},
{"body": "As per Scala 2.10, what are the advantages (if any) of emitting bytecode for the JVM 1.7, when compared to the default of emitting for the 1.6?Previous Scala versions emitted version 49.0 of the byte code, which corresponded to Java 5. With Scala 2.10 version , corresponding to Java 6 and which has the principal advantage of activating the faster verifier introduced with that version, so it should lead to (slightly) better run time performance.As you note, with 2.10 it became possible to emit version 51.0 byte code, which corresponds to Java 7. There are a few differences between version 50.0 and version 51.0:  the biggest is the inclusion of the  instruction, with the plumbing that goes with it (see  for the gory details).As far as Scala usage of the 51.0 byte code goes, even though the technical parts are in place it is my understanding that work is still at an experimental stage in using this feature. See  and  which shows that the team is working on getting the performance benefits of method handles without having to introduce a dependency on Java 7.Scala 2.11 maintained the default of emitting version 50.0 bytecode, but the  is now to jump straight to Java 8 bytecode with Scala 2.12. In the meantime, there is a  available for Scala 2.11 that will let you try out  that are being prototyped for Scala 2.12, and which will become the default back-end with Scala 2.12.Anyway, the long-awaited proposed benefits all come from using the  bytecode (and its associated MethodHandle structures). They include:(Spoiler: Using  to implement closures in the experimental backend is currently  than the present optimised closure creation!)"},
{"body": "I know about the parallel collections in Scala.  They are handy!  However, I would like to iterate over the lines of a file that is too large for memory in parallel.  I could create threads and set up a lock over a Scanner, for example, but it would be great if I could run code such as:Unfortunately, howeverWhat is the easiest way to accomplish some parallelism here?  For now, I will read in somes lines and handle them in parallel.You could use grouping to easily slice the iterator into chunks you can load into memory and then process in parallel.In my opinion, something like this is the simplest way to do it.I'll put this as a separate answer since it's fundamentally different from my last one (and it actually works)Here's an outline for a solution using actors, which is basically what Kim Stebel's comment describes.  There are two actor classes, a single FileReader actor that reads individual lines from the file on demand, and several Worker actors.  The workers all send requests for lines to the reader, and process lines in parallel as they are read from the file.I'm using Akka actors here but using another implementation is basically the same idea.This way, no more than 4 (or however many workers you have) unprocessed lines are in memory at a time.The comments on Dan Simon's answer got me thinking.  Why don't we try wrapping the Source in a Stream:Then you could consume it in parallel like this:I tried this out, and it compiles and runs at any rate.  I'm not honestly sure if it's loading the whole file into memory or not, but I don't  it is.I realize this is an old question, but you may find the  implementation in the  to be a useful no-assembly-required implementation of this:Below helped me to achieveWe ended up writing  at our company so we would understand the parallelism exactly."},
{"body": "How can I change Scala version in a sbt project?I would like SBT to check whether the system's Scala version is correct and if it is not the case then download it.Change  in  to whatever Scala version your project should be using - see .As mentioned in , you can:But the  is more suited for what you want, as it shows in action how to change the  property.So you should be able toand each time trigger a compilation with a different Scala version."},
{"body": "I'm using IntelliJ IDEA with the Scala plugin. If I reference HashMap in code, and then use Alt-Enter to add the import, the package gets imported as:What's the  part of this? It seems to work with and without it.It has to do scala imports being relative -  gives you a way to specify an absolute package name.  See the You would only need it if inside your current package you had a nested package scala.collection.immutable containing HashMap. This would be preferred by a relative import without the _root_ part. That was not quite right, the problems start already if you have a scala package either as an ancestor or nested in the current package.The Scala language specification has this to say about  in section 9.4 See the following PDF for the full language reference: "},
{"body": "Some time ago Oracle decided that adding Closures to Java 8 would be an good idea. I wonder how design problems are solved there in comparison to Scala, which had closures since day one.Citing the  from :As far as I understand 2., 6. and 7. aren't a problem in Scala, because Scala doesn't use Checked Exceptions as some sort of \"Shadow type-system\" like Java.What about the rest?Scala targets JDK 5 and 6 which don't have method handles, so it hasn't tried to deal with that issue yet.Scala doesn't have checked exceptions.Scala doesn't have loop index variables.  Still, the same idea can be expressed with a certain kind of while loop .  Scala's semantics are pretty standard here.  Symbols bindings are captured and if the symbol happens to map to a mutable reference cell then on your own head be it.Scala users tend to use functions (or implicit functions) to coerce functions of the right type to an interface. e.g.Scala's standard library includes FuncitonN traits for 0 <= N <= 22 and the spec says that function literals create instances of those traitsSince Scala doesn't have checked exceptions it can punt on this whole issueSame deal, no checked exceptions.Type erasure is type erasure.  The above literals produce scala.lang.Function1 regardless of the choice for A and B. If you prefer, you can write Scala arbitrarily limits the number of arguments to be at most 22 so that it doesn't have to generate the FunctionN classes dynamically.The Scala specification does not say that it must.  But as of 2.8.1 the the compiler does not optimizes the case where a lambda does not capture anything from its environment.  I haven't tested with 2.9.0 yet.I'll address only number 4 here.One of the things that distinguishes Java \"closures\" from closures found in other languages is that they can be used in place of interface that does not describe a function -- for example, . This is what is meant by SAM, Single Abstract Method.Java does this because these interfaces abound in Java library, and they abound in Java library  Java was created without function types or closures. In their absence, every code that needed inversion of control had to resort to using a SAM interface. For example,  takes a  object that will perform comparison between members of the array to be sorted. By contrast, Scala can sort a  by receiving a function , which is easily passed through a closure. See note 1 at the end, however.So, because Scala's library was created for a language with function types and closures, there isn't need to support such a thing as SAM closures in Scala.Of course, there's a question of Scala/Java interoperability -- while Scala's library might not need something like SAM,  library does. There are two ways that can be solved. First, because Scala supports closures and function types, it is very easy to create helper methods. For example:Actually, this particular example can be made even shorter by use of Scala's by-name parameters, but that's beside the point. Anyway, this is something that, arguably, Java could have done instead of what it is going to do. Given the prevalence of SAM interfaces, it is not all that surprising.The other way Scala handles this is through implicit conversions. By just prepending  to the  method above, one creates a method that gets automatically (note 2) applied whenever a  is required but a function  is provided.Implicits are very unique, however, and still controversial to some extent.: Actually, this particular example was choose with some malice...  has  abstract methods instead of one, which is the whole problem with it. Since one of its methods can be implemented in terms of the other, I think they'll just \"subtract\" defender methods from the abstract list.And, on the Scala side, even though there's a sort method that uses , not , the standard sorting method calls for a  object, which is quite similar to Java's ! In Scala's case, though,  performs the role of a .: Implicits are automatically applied, ."},
{"body": "In Scala, the syntax for selecting a type from a class is different from that of selecting anything else from a class. In that the former uses a hash as the selection operator instead of a dot. Why is that?Example: If we have a class like so...Why do we select the type from the class like this...instead of like this? is called a  and will match any type  of any enclosing instance of type . If you write a type , the compiler will look for the  (and not ) called  and will refer to its enclosing  type only. This is often used in the context of singleton objects.For instance:If Scala used  for type projections, this would lead to confusion because the preceding identifier could be interpreted either as a type or as a value\u2026 Hence the . Note that, as @kassens writes, Java  has type projections in that respect."},
{"body": "I am working my way through the AngularJS tutorial. Angular uses it's own JS routing mechanism to allow for single page apps. A sample routing file for Angular looks like this:I am trying to come up with a good place to store my partials (Angular specific HTML files). Ideally i WOULD like the ability to template them from within Play (i.e. have them as *.scala.html files). I can accomplish this using a a Play routes file like so:I basically partials/ to a controller action like this:The solution I am looking for is a combination of two ideals:Any ideas? When I was trying something similar I came to the conclusion that it's better to break it on 2 parts:I know it doesn't sound great and really doesn't answer your question on how to do it, but trying to link both frameworks may be problematic due to the way templates and their urls are mapped in Angular, and the benefit will be very small as any change will imply a lot of work, thus removing the arguably main benefit of both Play and Angular, rapid development. This also allows you to separate concerns better, which if your project grows may be important as you can just take the AngularJS code away as a standalone app connecting to a backend, and it will work fine.You can see some sample code of what I said (based on the TODO tutorial of AngularJS) in this . I warn you, the code is not too nice, but should give you an idea and as a bonus shows you how to integrate Jasmine into Play, for AngularJS unit testing.The eventual seed () is yet another way to build a Play + AngularJS app. The code is a sweetheart and well-documented.This won't answer your question directly, but I found this to be the best way to build Play + Angular apps: Yes, it's possible to create server-side meta-templates of client-side templates. This offers some unique abilities, as the two methods don't overlap completely. There's also plenty of room for confusion so be sure you know why you're writing a Play block instead of an Angular directive.Whether or not you should do it remains an open question; it really depends on whether you actually need access to server information in your templates. An example of where I think it would be necessary and appropriate would be for implementing access control in your views.Now to answer your question. The problem is solved by inlining the partials instead of trying to provide a route for them to be loaded on demand. See .Here's what the template looks like:And the app:Just include this helper:And make sure to use it within the root scope of your angular app.For question #1 you could introduce a route like this:Then in your controller you would need to map from view name to actual view:You might want to render the templates lazy or require the request to be present, then you should probably do something like this:Your action would look something like this:If you want a specific controller method for a certain route (question #2) you simple add a route  the dynamic one:I really think it's not a very good idea even it comes from a respectfully mind indeed.I think it's a very good practice to leave every think as defaulted (convention over configuration principle) which means for me that we've probably more interest to keep each Paradigms (Play and AngularJS) separated as one or both could evolve in the near or the far future which will have its cost for code maintenance.The second very important point is testability, if you mix both technos you'll end up with a mix to come up with real good coverage of tests in both side of your application.\nCheers This might not answer the question exactly but you can try follow this project as it seems good example to build Play/scala/Angular apps:"},
{"body": "Having a traithow do I implement a method that accepts an instance of any case class and returns its copy with the trait mixed in?The signature of the method looks like:This can be done with macros (that are officially a part of Scala since 2.10.0-M3). .1) My macro generates a local class that inherits from the provided case class and Persisted, much like  would do. Then it caches its argument (to prevent multiple evaluations) and creates an instance of the created class.2) How did I know what trees to generate? I have a simple app, parse.exe that prints the AST that results from parsing input code. So I just invoked , noted the output and reproduced it in my macro. parse.exe is a wrapper for . There are different ways to explore ASTs, read more in .3) Macro expansions can be sanity-checked if you provide  as an argument to scalac. In that case all expansions will be printed out, and you'll be able to detect codegen errors faster.edit. Updated the example for 2.10.0-M7It is not possible to achieve what you want using vanilla scala. The problem is that the mixins like the following:create a  mixed in, but it is not done at runtime. The compiler simply generates a new anonymous class:See  for more info.What you are trying to do is known as record concatenation, something that Scala's type system does not support. (Fwiw, there exist type systems - such as  and  - that provide this feature.)I think type classes might fit your use case, but I cannot tell for sure as the question doesn't provide sufficient information on what problem you are trying to solve.You can find an up to date working solution, which utilizes a Toolboxes API of Scala 2.10.0-RC1 as part of  project.The following solution is based on the Scala 2.10.0-M3 reflection API and Scala Interpreter. It dynamically creates and caches classes inheriting from the original case classes with the trait mixed in. Thanks to caching at maximum this solution should dynamically create only one class for each original case class and reuse it later. The following code was tested with Scala 2.10.0-M3.The trait to be mixed in. The actual worker objectThe test appWhile it's not possible to compose an object AFTER it's creation, you can have very wide tests to determine if the object is of a specific composition using type aliases and definition structs:Any object with a  will qualify as Persisted.Achieving what I THINK you are trying to do is possible with implicit conversions:"},
{"body": "Soooo...Semigroups, Monoids, Monads, Functors, Lenses, Catamorphisms, Anamorphisms, Arrows... These all sound good, and after an exercise or two (or ten), you can grasp their essence. And with , you get them for free...However, in terms of real-world programming, I find myself struggling to find usages to these notions. Yes, of course I always find someone on the web using Monads for IO or Lenses in Scala, but... still... What I am trying to find is something along the \"prescriptive\" lines of a pattern. Something like: \"here, you are trying to solves , and one good way to solve it is by using lenses !\"Suggestions?Update: Something along these lines, with a book or two, would be great (thanks Paul): I gave  focused on the practical application of monoids and applicative functors/monads via . I gave another version of the same talk , where the emphasis was more on the validation. I would watch the first talk until I start on validations and then skip to the second talk (27 minutes in).There's also  which shows how you might use  in a \"practical\" application. That is, if you are designing software for nightclub bouncers.The key to functional programming is abstraction, and composability of abstractions. Monads, Arrows, Lenses, these are all abstractions which have proven themselves useful, mostly because they are composable. You've asked for a \"prescriptive\" answer, but I'm going to say no. Perhaps you're not convinced that ?I'm sure plenty of people on StackOverflow would be more than happy to try and help you solve a  problem the FP way. Have a list of stuff and you want to traverse the list and build up some result? Use a fold. Want to parse XML?  uses arrows for that. And monads? Well, tons of data types turn out to be Monads, so learn about them and you'll discover a wealth of ways you can manipulate these data types. But its kind of hard to just pull examples out of thin air and say \"lenses are the Right Way to do this\", \"monoids are the best way to do that\", etc. How would you explain to a newbie what the use of a for loop is? If you want to [blank], then use a for loop [in this way]. It's so general; there are tons of ways to use a for loop. The same goes for these FP abstractions.If you have many years of OOP experience, then don't forget you were once a newbie at OOP. It takes time to learn the FP way, and even more time to unlearn some OOP tendencies. Give it time and you will find plenty of uses for a Functional approach.I think you can take the reverse approach and instead when writing a small piece of functionality, ask yourself whether any of those would apply: Semigroups, Monoids, Monads, Functors, Lenses, Catamorphisms, Anamorphisms, Arrows... A lots of those concepts can be used in a local way.Once you start down that route, you may see usage everywhere. For me, I sort of get Semigroups, Monoids, Monads, Functors. So take the example of answering this question . It's a real usage for the person asking the question (a self described noob). I am trying to answer in a simple way but I have to  myself from scratching the itch \"there are monoids in here\". Scratching it now: using  and the fact that Int and List are monoids and that the monoid property is preserved when dealing with tuple, maps and options:But I don't come to that result by thinking . It comes more naturally by thinking  combined with the fact that scalaz has utility methods like . Interestingly when looking at the resulting code it's not obvious that I'm totally thinking in terms of monoid.You might like  by Chris Marshall. He covers a couple of Scalaz goodies - namely Monoid and Validation - with many practical examples. Ittay Dror has written a very accessible  on how Functor, Applicative Functor, and Monad can be useful in practice.  and 's blogs also have a bunch of posts covering use cases for categorical constructs.This answer just lists a few links instead of providing some real substance here. (Too lazy to write.) Hope you find it helpful anyway.I understand your situation, but you will find that to learn functional programming you will need to adjust your point of view to the documentation you find, instead of the other way around. Luckily in Scala you have the possibility of becoming a functional programmer gradually.To answer your questions and explain the point-of-view difference, I need to distinguish between \"type classes\" (monoids, functors, arrows), mathematically called \"structures\", and generic operations or algorithms (catamorphisms or folds, anamorphisms or unfolds, etc.). These two often interact, since many generic operations are defined for specific classes of data types.You look for prescriptive answers similar to design patterns: when does this concept apply? The truth is that you have surely seen the prescriptive answers, and they are simply the definitions of the different concepts. The problem (for you) is that those answers are intrinsically different from design patterns, but it is so for good reasons.On the one hand, generic algorithms are not design patterns, which suggest a structure for the code you write; they are abstractions defined in the language which you can directly apply. They are general descriptions for common algorithms which you already implement today, but by hand. For instance, whenever you are computing the maximum element of a list by scanning it, you are hardcoding a fold; when you sum elements, you are doing the same; and so on. When you recognize that, you can declare the essence of the operation you are performing by calling the appropriate fold function. This way, you save code and bugs (no opportunity for off-by-one errors), and you save the reader the effort to read all the needed code.On the other hand, structures concern not the goal you have in mind but properties of the entities you are modeling. They are more useful for bottom-up software construction, rather than top-down: when defining your data, you can declare that it is a e.g. a monoid. Later, when processing your data, you have the opportunity to use operations on e.g. monoids to implement your processing. In some cases it is useful to strive to express your algorithm in terms of the predefined ones. For instance, very often if you need to reduce a tree to a single value, a fold can do most or all of what you need. Of course, you can also declare that your data type is a monoid when you need a generic algorithm on monoids; but the earlier you notice that, the earlier you can start reusing generic algorithms for monoids.Last advice is that probably most of the documentation you will find about these concepts concerns Haskell, because this language has been around for much more time and supports them in a quite elegant way. Quite recommended here are , a Haskell course for beginners, where among others chapters 11 to 14 focus on some type classes, and  (which contains links to various articles with specific examples). EDIT: Finally, an example of applications of Monoids, taken from Typeclassopedia, is here: . I'm not saying there is little documentation for Scala, just that there is more in Haskell, and Haskell is where the application of these concepts to programming was born."},
{"body": "Suppose I have a , and I want to call  on each element, and get back the result as a .What are the various ways to do this in Scala? Is there a solution with a minimal amount of explicit typing? \u2014 i.e., I want to specify that I want a  rather than a , but I'd like the  argument to be inferred from the filter function.Or should I explicitly pass a  instance? Where do I get these from \u2014 for s, s and s?Use  as the  and let the typer know what you want your result type to be (unfortunately you need to specify String here)Scala 2.10.0 introduced an easy way to convert a collection to another collection:Alternatively ask for an  explicitly:If you want to do this without creating the intermediate , then:You could do it (awkwardly, but more generally) using Now provide a typeclass instance from the List ~> Vector transformation:Define a wrapper and an implicit conversion:Then:"},
{"body": "I've created a play project with play 2.3.7In the project directory, I ran activator and ran the  command to generate eclipse project files.When I go to eclipse (I'm using the Scala IDE from typesafe Build id: 4.0.0-vfinal-20150119-1023-Typesafe , there is an error in my Application.scala file:Is there something amiss with my setup? The app runs fine when I execute  at the activation console prompt.Thanks!EDIT: Added codeThe error is on the 'Ok..' line.There is a file in views called index.scala.html, and the app runs file when I run it from activator..Occasionally after adding a view in Play 2.4.x, IntelliJ IDEA sometimes gets confused and absolutely refuses to build. Even rebuild Project fails:This still happens from time-to-time in IDEA 15. And when it does, the command line provides the quickest, most-reliable fix:That's it! IDEA will now compile the project as expected.Update:In the rare case that  completed successfully on the command line, but IntelliJ IDEA 15 still gives the same \"object x is not a member\" error, then this has solved IDEA's confusion:File Menu:The other solutions did not work for me. Some would give me different errors, some would clear the Problems tab but leave me with a red squiggle under  and auto-complete would not work with the scala.html templates.What finally worked was to open the project's properties, go to , and add both of the following directories:If you only do  then you'll miss out on the class files generated from the  directory.In  you can find the solution (in brief: adding  folder to the compilation path), give it a try.I had the same problem. I added  and  to my Java build path and Eclipse stopped complaining.Adding  which is having  package to source fixed for me.I had the same issue running Play 2.4.0-RC1 using default SBT layout () and solved it by adding to :@brent-foust 's answer worked for me but only initially.  Every time I rebuilt the project from within IDEA I would then get \"not found: routes\" errors from within  until I performed Brent's workaround again.I eventually discovered the solution to that was changing a line in the .iml file fromtoI don't know what the long term implications of doing this are but it has fixed this problem.  Some of the other similar problems mentioned might also be fixed by applying the same change to some of the other folders listed in the .iml.For me when importing the project into intellij making sure I \"checked\" the \"auto import\" checkbox did the trick.1) Add the following line to your sbt.build file:2) Add the follwing line to your plugins.sbt file under the project folder:3) Run the \"eclipse\" command from within sbtas explained in the documentation of the play framework:"},
{"body": "I get notified about \"unchecked\" warnings when I compile my Scala project with SBT 0.11, but I can't see the warnings themselves.What I see isWhat exactly am I to \"re-run with -unchecked\" and how to do that? Neither , nor , nor  seem to work.I've found the answer .The solution is to addto the project's  file.It can also take \"-feature\" as an option and works with sbt 0.13"},
{"body": "How would you sort a scala.collection.Map[java.lang.String, Int] by its values (so on the Int)? What is a short and elegant way to do that?Depending on what the expected output collection type is (s are sorted on the keys), you could use something like this:Result would be the list of key/value pairs sorted by the value:There is a Map type that retains the original order, , if you apply this, you have a map again:Then you have:(Scala 2.8)"},
{"body": "Say I have a . It can thus have the following values:\n\nI want to reduce it so that the first becomes  while the two others become . Obviously there are many ways to accomplish this, but I'm looking for a simple, perhaps built-in, less-than-one-liner.Flatten does exist now.As before,(in addition to the other answers) will also do the same thing.You could use   to do this, as this is one of the  operations:Here it is in the REPL:It's available to anything with a typeclass instance for a Monad.followed by a bunch of characters to get me up to the minimum that stackoverflow allowsI think the conversion to the Iterable is just fine.  Use these steps to go from  to a single (which returns )You might use flatMap like the following:This will map the  to a .\nIf you just have an Option you can use it as following:This will flatten your  to .Well, I actually don't understand how come it could be just None (the third case). If it can really be also just None, then I would vote for Rex Kerr's answer, otherwise just .get would be enough:"},
{"body": "In Scala one might define methods inside other methods. This limits their scope of use to inside of definition block. I use them to improve readability of code that uses several higher-order functions. In contrast to anonymous function literals, this allows me to give them meaningful names before passing them on.For example:Is there any runtime cost because of the nested method definition I should be aware of?Does the answer differ for closures?During compilaton, the nested functions are  and  are moved out to the same level as . They get compiler synthesized names:  and .In addition, when you refer to one of these methods without an argument list, it is lifted into a function. So  is the same as writing . This anonymous function is compiled to a separate class , which does nothing more than forward to .As a side note, the process of lifting the method to a function is called Eta Expansion. It is triggered if you omit the argument list in a context where a function type is expected (as in your example), or if you use  in place of the entire argument list, or in place of each argument (.If the code was directly in the closure, you would have one fewer method call in your stack, and one fewer synthetic method generated. The performance impact of this is likely to be zero in most cases.I find it to be a fantastically useful tool to structure code, and consider your example very idiomatic Scala.Another useful tool is using small blocks to initialize a val:You can use  to see exactly how Scala code is translated into a form ready for the JVM. Heres the output from your program: There is a small runtime cost.  You can observe it here (apologies for the long code):There are four different methods for summing integers:And we see the results on my machine in terms of nanoseconds taken in the inner loop:So, bottom line is: nesting functions really doesn't hurt you at all in simple cases--the JVM will figure out that the call can be inlined (thus  and  give the same times).  If you take a more functional approach, the function call can't be  neglected, but the time taken is vanishingly small (approximately 0.4 ns extra per call).  If you use a lot of closures, then closing them gives an overhead of something like 1 ns per call at least in this case where a single mutable variable is written to.You can modify the code above to find answers to other questions, but the bottom line is that it is all very fast, ranging between \"no penalty whatsoever\" through to \"only worry about in the very tightest inner loops that otherwise have minimal work to do\".(P.S. For comparison, the creation of a single small object takes ~4 ns on my machine.) The current benchmark is ~3 years old and Hotspot and the compiler have significantly evolved. I am also using  to perform the benchmarks.What is very surprising is that closure test was completed about 4ns faster than the others. This seems to be an idiosyncrasy of Hotspot instead of the execution environment, multiple runs have returned the same trend.Using a closure that performs boxing is a huge performance hit, it takes approx 3.579ns to perform one unboxing and reboxing, enough to do a lot of primitive math operations. In this specific position, things might get better with the work being done on a . In the general case, boxing might be alleviated by .\nThe new optimizer doesn't really help here, it makes  0.1 ns slower and  0.1 ns faster: Performed with 2.11.0-20131209-220002-9587726041 from magarciaEPFL/scala"},
{"body": "I'm looking at alternatives to  or  as a way of figuring out what the compiler is doing in Scala. With the new reflection/macros library,  seems a good candidate for that, as shown in retronym's 's . It even shows how one used to do that, pre-M4.So the question is, what's the shortest/easiest thing I can type on Scala's REPL to get the AST for an expression, post-Scala 2.10.0-M4?A lot of things previously defined in package  have moved to :Furthermore it is possible to check if a string containing some Scala code is a valid Scala expression and - even better - do some evaluation:. In 2.10.0-RC1 some methods of  have been renamed.  is now just , and  is now called .The most complicated thing here is the raw tree representation of an expression. When one wants to use macros, the macros have to be defined the same way as shown by . But with some helper methods it is possible to define some not so ugly looking macro implementations:But now we come in problems with path-dependent-types - we have to write their paths explicitly if we want not import them.There is improve of new Scala version"},
{"body": "I want to make a scala function which returns a scala tuple.I can do a function like this:and this will work fine, but now I want to tell the compiler what I expect to be returned from the function instead of using the built in type inference (after all, I have no idea what a  is).The compiler will interpret the type  as a Also, you can create a type alias if you get tired of writing (Int,String,String)"},
{"body": "I'm running Scala 2.10.2. I want to create a list, then add some elements to the list and expect to see all the elements in the lists when I call the list's name. But I observed something quite weird (At least weird for me since I'm a newbie). Below is the what I tried to do in my First, I created the list  with 2 elements (1.0 and 5.5). I call  and get what I expect; the two elements. Now I tried to add another element to the list using  which returned a new list with a new list of elements I added (2.2 and 3.7) Sweet! I even checked someone else's code for help:  to use a new construct . So at this stage I'm all happy, but I call  and I get the unexpected: `res3: List[Double] = List(1.0, 5.5)'. Where are the elements I added? And how do I add these elements correctly so that when I call  I get a new list with all the stuff I added? You are using an immutable list. The operations on the List return a new List. The old List remains unchanged. This can be very useful if another class / method holds a reference to the original collection and is relying on it remaining unchanged. You can either use different named vals as inor use a var as inThis is equivalent syntax for:Or you could use one of the mutable collections such asNot to be confused with the following that does not modify the original mutable List, but returns a new value:However you should only use mutable collections in performance critical code. Immutable collections are much easier to reason about and use. One big advantage is that immutable List and  are Covariant. Don't worry if that doesn't mean anything to you yet. The advantage of it is you can use it without fully understanding it. Hence the collection you were using by default is actually  its just imported for you automatically.Note: personally I use Seq as my default collection. This uses the immutable Seq trait. List /Vector / Array is an implementation detail for performance not functionality. List is more efficient for small collections.Use  or similar if you really need mutation.I will try to explain the results of all the commands you tried.First of all,  is a type alias to  (defined in Predef.scala).Using the List companion object is more straightforward way to instantiate a . Ex:  returns a list resulting from the concatenation of the given list prefix and this listThe original List is NOT modified is a  Definitely not what you want. returns a new list consisting of all elements of this list followed by elem.The type is  because it is the common superclass between  and l is left unmodified because no method on  modified the List.Since you want to append elements to existing list, you can use var List[Int] and then keep on adding elements to the same list. \nNote -> You have to make sure that you insert an element into existing list as follows:-var l: List[int] = List()   // creates an empty listl = 3 :: l                 // adds 3 to the head of the listl = 4 :: l                // makes int 4 as the head of the list// Now when you will print l, you will see two elements in the list ( 4, 3)"},
{"body": "I have read that with Scala, it is generally advised to use Traits instead of Abstract classes to extend a base class.Is the following a good design pattern and layout? Is this how Traits were intended to replace Abstract?I don't know what your source is for the claim that you should prefer traits over abstract classes in Scala, but there are several reasons  to:The last reason is by far the most important in my view. At least a couple of the other issues  in future versions of Scala, but it will remain the case that defaulting to classes will constrain your programs in ways that are (at least arguably) consistent with good design. If you decide you actually really do want the power provided by traits, they'll still be there, but that'll be a decision you make, not something you just slip into.So no, in the absence of other information, I'd suggest using an abstract class (ideally a sealed one) and two concrete classes that provide implementations.OTOH, traits allow you to build and test the functionality of complex objects in a granular fashion, and to reuse core logic so as to provide different flavors. For example, a domain object might be deployed to a data server, which persists to a database, while a web server might employ read-only versions of the same object that are updated from the data server.Nothing is suitable for every scenario. Use the right construct for the task at hand. Sometimes the reality of an implementation brings to light issues for specific use cases which were unknown at design time. Re-implementing using different assumptions and constructs can yield surprising results."},
{"body": "I'm trying to use the negation of a boolean function in Scala, such as:But I get the error:How can I refer to the negation of p? The negation of  is a function that applies  to its argument and negates the result.If you want to be able to write  or  you can use , which pimps functions that return a bool with various logical operators.Shortest negation of p: When you apply the predicate  as an argument to another function: For example, using a Set of integers (try it on a Scala worksheet):Another way to solve it without the use of an anonym function is to define a concrete function for this task.You can also define ! yourselfbut this only seems to works for functions of type Int=>Boolean, and not (Int)=>Boolean. The not(even) solution works with both."},
{"body": "I just noticed this construct somewhere on web:What does  mean? Is this a syntax sugar for some method call? What constraints should my custom class satisfy so that it can take advantage of this syntax sugar?Generally, the  notation is used for type ascription, forcing the compiler to see a value as some particular type.  This is not  the same as casting.In this case, you're ascribing the special  type.  This mirrors the asterisk notation used for declaring a varargs parameter and can be used on a variable of any type that subclasses :That's scala syntax for exploding an array. Some functions take a variable number of arguments and to pass in an array you need to append  to the array argument.I'm psychic, I predict you'll see that wildcard a  in scala:"},
{"body": "I've studied scala for several months, but still at the beginner level. Now I found I have huge trouble with functional programming and scala's type system. I tried finding some documents and blogs, but I can't understand most of them(especially type-system ones). So I'm looking for some small and good scala projects that I can read the sources to learn.Could you please recommend me some?There's a popular set of \"Scala Problems\" out there, called Problems come in all difficulties, and many flavors, basic functional programming included.If you follow the community there are the common suspects(those writing the scalaz book):If you through this I have some more :)\nIf you want an suggested order I word start with debasishs blog , then the simple exercises from tonys blog then read through runars blog(even the java ones!) then Erics after that Pauls and have a look at lucs ideas.There is also scala labs: You can try the \"\"  by the designer of Scala, Martin Odersky. You only need to register at Coursera (click on the \"Join for Free\" button on the right sidebar) and then you will have access to the class which consists of lecture , between 6 and 15 minutes in length, that are accompanied by ,  and  per video.The course also provides learning resources, discussion forums, instructions to setup tools, sbt tutorial, Eclipse tutorial, IntelliJ IDEA tutorial, Scala tutorial, Scala style guide and a Scala cheatsheet.I also came across a series of very nicely written blog posts on Scala by Joel Abrahamsson:I hope the resources that I provided will prove to be helpful.If you are looking for some step-by-step tutorial than you should test  Try to complete  from sample chapters  &  to see if it match your expectations. has tons of problems that you can code up in multiple functional programming languages including Scala. The best part is that they have pre-written tests like an online judge. "},
{"body": "How does  supercede the answers given in Stack Overflow question  (it doesn't work because the \"jcl\" package is gone) and in  (it doesn't work for me in a complicated test which I'll try to boil down and post here later).The latter is actually a Scala Map question, but I think I need to know both answers in order to iterate over a .In 2.8, you import  and use as a Scala map.  Here's an example (in 2.8.0.RC1):If you specifically want a Scala iterator, use  (after the conversions import)."},
{"body": "What are the advantages and dis\u00adadvantages of frameworks Lift, Play and Wicket? What characteristics are best or only supported by each?Thanks:Lightweight Java-based framework, with Scala support available as an extra.very good for rapid prototyping, fast-feedback-loop kind of work.  Embeds the compiler, so you just edit source code in place and pages get immediately updated.  Learning curve is shallow.:Stateful Java-based framework, with Scala support available as an extra.Shallower learning curve into Scala, especially if you already have wicket experience.\nGood separation of concerns, POJO-based model.  Arguably one of the best Java web frameworks currently available.:Stateful native-Scala framework.\nDeep Scala integration, so no need to generate bean setter/getter methods or worry about interop between Java/Scala collections.  Fully embraces functional-programming concepts, such as immutability and closures.Also the steepest learning-curve of the three.  One common piece of advice is therefore to learn the Scala language before getting started with Lift, especially if you come from a Java background.:There are also other Scala-based frameworks available (such as Scalatra and Pinky) for web development, though not as well-known as Lift.  It wouldn't hurt to check these out as well!For more information, see this question: There are many threads that compares these web frameworks for Scala. SeeLift / Wicket here: Talking about the advantages of Lift, one should mention  where Lift really excels. In short:Just visit the linked page for more details - these features really make Lift unique among competitors.See also: - Stateful Java-based framework for desktop-like applications (GWT based, but server-side, no javascript, no html). - stateless Java-based framework for light web applications.Both have excellent documentation and are easy to learn. "},
{"body": "I am reading through the Scala Cookbook ()There is an example related to Future use that involves for comprehension.So far my understanding about for comprehension is when use with a collection it will produce another collection with the same type. For example, if each   is of type , the following should also be of type :Could someone explain me what exactly happening when use  in this code?\nI know if it was a generator it will fetch each element by looping.First about for comprehension. It was answered on SO many many times, that it's an abstraction over a couple of monadic operations: , , . When you use , scalac desugars this lines into monadic : into it looks like an imperative computation (what a monad is all about), you bind a computation result to the . And  part is desugared into  call. Result type depends on the type of 's. trait has a  and  functions, so we can use for comprehension with it. In your example can be desugared into the following code:It goes without saying that if execution of  depends on  then you can't escape sequential execution, but if the future computations are independent, you have two choices. You can enforce sequential execution, or  for parallel execution. You can't  the latter, as the execution context will handle this.will always run sequentailly. It can be easily explained by the desugaring, after which the subsequent  calls are only invoked inside of the flatMaps.is able to run in parallel and the for comprehension aggregates the results.It allows , ,  to run in parallel, if possible. It may not be possible, depending things like how many threads are available to execute Future computations, but by using this syntax you are telling the compiler to run these computations in parallel if possible, then execute the  when all have completed."},
{"body": "I always thought that  was merely a shortcut for , but apparently I am mistaken, since it doesn't seem to use  at all. Why is that so? And how can I do the \"redirecting\" of  below in Scala? is shortcut for  and you can use  or  for redirecting. Also,  only affects the current thread while System.setOut\naffects the whole JVM. Additionally Scala 2.9  evaluates each line in its own thread, so  is not usable there."},
{"body": "String interpolation is  starting Scala 2.10 This is the basic example I was wondering if there is a way to do dynamic interpolation, e.g. the following (doesn't compile, just for illustration purposes) is actually a method on  (or something which can be implicitly converted from ).  When you writethe compiler desugars it intoBy default,  gives you , , and * methods.As you can see, the compiler itself picks out the name and gives it to the method.  Since this happens at compile time, you can't sensibly do it dynamically--the compiler doesn't have information about variable names at runtime.You can use vars, however, so you can swap in values that you want.  And the default  method just calls  (as you'd expect) so you can play games like(0 was already called by the REPL in this example).That's about the best you can do.*Here is a possible  solution to #1 in the context of the original question based on Rex's excellent answerNote that  can be a user-defined interpolator introduced through an implicit class. The documentation gives an example for a  interpolator,This is inherently impossible in the current implementation: local variable names are not available at execution time -- may be kept around as debug symbols, but can also have been stripped. (Member variable names are, but that's not what you're describing here)."},
{"body": "Browsing Shapeless code, I came across this seemingly extraneous   and :I almost ignored it as a typo since it does nothing but apparently it does something. See this commit: I have no idea what it does. Can someone explain?Any type can be followed by a  enclosed sequence of type and abstract non-type member definitions. This is known as a \"refinement\" and is used to provide additional precision over the base type that is being refined. In practice refinements are most commonly used to express constraints on abstract type members of the type being refined.It's a little known fact that this sequence is allowed to be empty, and in the form that you can see in the shapeless source code,  is the type  with an empty refinement. Any empty refinement is ... empty ... so doesn't add any additional constraints to the refined type and hence the types  and  are equivalent. We can get the Scala compiler to verify that for us like so,So why would I do such an apparently pointless thing in shapeless? It's because of the interaction between the presence of refinements and type inference. If you look in  of the Scala Language Specification you will see that the type inference algorithm attempts to avoid inferring singleton types in at least some circumstances. Here is an example of it doing just that,As you can see from the definition of  the Scala compiler knows that the value  has the more precise type  (ie. the singleton type of ), however, unless explicitly requested it won't infer that more precise type. Instead it infers the non-singleton (ie. widened) type  as you can see in the case of .But in the case of  in shapeless I explicitly  the singleton type to be inferred for uses of the  member (the whole point of  is enable us to pass between the type and value levels via singleton types), so is there any way the Scala compiler can be persuaded to do that?It turns out that an empty refinement does exactly that,As you can see in the above REPL transcript, in the first case  has been typed as the widened type  whereas  has been assigned the singleton type .Is this mandated by the SLS? No, but it is consistent with it, and it makes some sort of sense. Much of Scala's type inference mechanics are implementation defined rather than spec'ed and this is probably one of the least surprising and problematic instances of that."},
{"body": "In Java, it's a common best practice to do string concatenation with StringBuilder due to the poor performance of appending strings using the + operator. Is the same practice recommended for Scala or has the language improved on how java performs its string concatenation?Scala uses Java strings (), so its string concatenation is the same as Java's: the same thing is taking place in both. (The runtime is the same, after all.) There is a special  class in Scala, that \"provides an API compatible with \"; see .But in terms of \"best practices\", I think most people would generally consider it better to write simple, clear code than maximally efficient code, except when there's an actual performance problem or a good reason to expect one. The  operator doesn't really have \"poor performance\", it's just that  is equivalent to  (i.e. it creates a new  object), which means that, if you're doing a lot of concatenations to (what looks like) \"a single string\", you can avoid creating unnecessary objects \u2014 and repeatedly recopying earlier portions from one string to another \u2014 by using a  instead of a . Usually the difference is not important. (Of course, \"simple, clear code\" is slightly contradictory: using  is simpler, using  is clearer. But still, the decision should usually be based on code-writing considerations rather than minor performance considerations.)Scalas String concatenation works the same way as Javas does.is translated toStringBuilder is . That's the reason why the value appended to the StringBuilder is boxed by the compiler.You can check the behavior by decompile the bytecode with javap.Scala uses  as the type for strings, so it is subject to the same characteristics.I want to add: if you have a sequence of strings, then there is already a method to create a new string out of them (all items, concatenated). It's called .Example: ()"},
{"body": "I'm doing a bit of Scala gymnastics where I have  in which I try to find the \"smallest\" element. This is what I do right now:It works fine, but I'm not quite satisfied - it's a bit long for such a simple thing, and . Using  would be much more elegant:... but  and  throw exceptions when the sequence is empty. Is there an idiomatic, more elegant way of finding the smallest element of a possibly empty list as an ? does what you want?Edit: Here's an example incorporating your :or, as generic method:which you could invoke with A safe, compact and  version with Scalaz:Hardly an option for any larger list due to  complexity:Scala allows us to capture the error with . Let's write a function that makes use of it:Now let's test that:How about this?Or, more verbose, if you don't want to swallow other exceptions:Alternatively, you can pimp all collections with something like this:And then just write .In Haskell you'd wrap the  call as"},
{"body": "For a monad , Is it possible to turn  into ?I've tried following the types to no avail, which makes me think it's not possible, but I thought I'd ask anyway. Also, searching Hoogle for  didn't return anything, so I'm not holding out much luck.No, it can not be done, at least not in a meaningful way.Consider this Haskell codeThis takes  first, prints it (IO performed here), then reads a line from the user.Assume we had an hypothetical . Then as a mental experiment, consider:The above has to do all the IO in advance, before knowing , and then return a pure function. This can not be equivalent to the code above.To stress the point, consider this nonsense code below:Magically,  should know in advance what the user is going to type next! A session would look asThis requires a time machine, so it can not be done.No, it can not be done. The argument is similar to .Assume by contradiction  exists.\nSpecialize  to the continuation monad   (I omit the newtype wrapper).Specialize :Apply:By the Curry-Howard isomorphism, the following is an intuitionistic tautologybut this is Peirce's Law, which is not provable in intuitionistic logic. Contradiction.The other replies have nicely illustrated that in general it is not possible to implement a function from  to  for any monad . However, there are specific monads where it is quite possible to implement this function. One example is the reader monad:No.For example,  is a monad, but the function  has no meaningful implementation:What do you put instead of ? ?  of what then? Or ?"},
{"body": "I started working my way through the , which is organized around a suite of unit tests with blanks that one needs to fill in.  (This idea was modeled after a similar Ruby Koans project.)   You start the sbt tool running a test, and it admonishes:  ...and so you go look at this unit test and it says:...and, after meditation, you realize that you should fill in the blank like this:...and then it moves on to the next unit test.   My question, though, is what is this  operator? I can't seem to find it anywhere.  Is this a DSL operator defined in the Scala Koans project itself?  Or is it part of the ScalaTest framework?  Or in Scala proper?This is the triple-equals operator from . Have a look at this page: . It says:"},
{"body": "is there a way to extend a case class without constantly picking up new vals along the way? For example this doesn't work\"a\" conflicts with \"a\" , so I'm forced to rename to a1. But I don't want all kinds of extra public copies of \"a\" so I made it private. This just doesn't seem clean to me.. am I missing something?As the previous commenter mentioned: case class extension should be avoided but you could convert your Edge class into a trait. If you want to avoid the private statements you can also mark the variables as override Don't forget to prefer  over  in traitsThis solution offers some advantages over the previous solutions:In this way:Case classes can't be extended via subclassing. Or rather, the sub-class of a case class cannot be a case class itself."},
{"body": "I almost always have a Scala REPL session or two open, which makes it very easy to give Java or Scala classes a quick test.  But if I change a class and recompile it, the REPL continues with the old one loaded.  Is there a way to get it to reload the class, rather than having to restart the REPL?Just to give a concrete example, suppose we have the file Test.scala:We compile it and start the REPL:Then we change the source file tobut we can't use it:Class reloading is not an easy problem.  In fact, it's something that the JVM makes very difficult.  You do have a couple options though:Unfortunately, both of these are limited by the Scala REPL's implementation details.  I use JRebel, and it usually does the trick, but there are still cases where the REPL will not reflect the reloaded class(es).  Still, it's better than nothing.There is an alternative to reloading the class if the goal is to not have to repeat previous commands.  The REPL has the commandwhich restarts the REPL environment and plays back all previous valid commands.  (The invalid ones are skipped, so if it was wrong before, it won't suddenly work.)  When the REPL is reset, it does reload classes, so new commands can use the contents of recompiled classes (in fact, the old commands will also use those recompiled classes).This is not a general solution, but is a useful shortcut to extend an individual session with re-computable state.There is an command meet you requirementwhich will reload the scala source file and recompiled to classes , then you can replay you code This works for me....If your new source file  looks something like this...You first have to load the new changes into Scala console (REPL).Then re-import the package so you can reference the new code in Scala console.Now enjoy your new code without restarting your session :)If the .scala file is in the directory where you start the REPL you can ommit the full path, just put , and then import."},
{"body": "How do I create an array of multiple dimensions? For example, I want an integer or double matrix, something like  in Java.I know for a fact that arrays changed in Scala 2.8 and that the old arrays are deprecated, but are there multiple ways to do it now and if yes, which is best?Like so:It's deprecated. Companion object exports factory methods :"},
{"body": "I have a DataFrame generated as follow:The results look like:As you can see, the DataFrame is ordered by  in an increasing order, then by  in a descending order.I would like to select the top row of each group, i.e.So the desired output would be:It might be handy to be able to select the top N rows of each group as well.Any help is highly appreciated.:Something like this should do the trick:This method will be inefficient in case of significant data skew.:Alternatively you can join with aggregated data frame:It will keep duplicate values (if there is more than one category per hour with the same total value). You can remove these as follows::Neat, although not very well tested, trick which doesn't require joins or window functions: (Spark 1.6+, 2.0+):::The last two methods can leverage map side combine and don't require full shuffle so most of the time should exhibit a better performance compared to window functions and joins.To get only the first row of each group, you can try this.For Spark 2.0.2 with grouping by multiple columns:If the dataframe has to be grouped by multiple columns, this can helpHope this helps someone with similar problemWe can use the rank() window function (where you would choose the rank = 1)\nrank just adds a number for every row of a group (in this case it would be the hour)here's an example. ( from  )"},
{"body": "(Yes I know I can call Java code from Scala; but that is pointless; I want to DELETE the Java code, not keep it around and have to look at it and maintain it forever!)Are there any utilities out there to convert Java source to Scala source?I believe theoretically it should be possible to accomplish with minimal lossage.I have found this but it seems inactive, probably buggy/incomplete...\nAny alternatives?IntelliJ kinda, sorta, does this. You need to open a project with your Java sources. You can then copy/paste expressions, methods, or entire classes in to a .scala file. This converts to equivalent Scala code.The fidelity of conversion isn't perfect, and, for this reason, it doesn't support a bulk conversion yet.I recommend using the latest version of  and the . The Community Edition is free.Aside from this, Paul Phillips once started the  project to translate code from Java to Scala (or, potentially, ), and even improve it in the process! He explains the concept in this . However this effort was stalled, presumably because he turned his attention to directly contributing to the Scala compiler and standard library.In IntelliJ IDEA theres is a refactoring called \"Convert to Scala\". It's under Refactor menu (after you install Scala plugin) Ctrl+Shift+G.\nSo you don't need to play with copy/paste.Just make sure you have a java file opened, there is no this option if it's a scala or other file.I just published the following tool : . I tried Jatran as well, but was frustrated with some bugs and difficult integration. Scalagen is extensible and comes with a Maven plugin.You can use online  which uses  library.(I haven't used this)noticed this, Mar 2012: I don't think it's possible to automatically convert Java to Scala in the general case.  Many of the lower-level constructs in Java don't exist in Scala (e.g. fields and static members), Scala places limitations on constructors that don't exist in Java, and Scala doesn't have raw types like Java (generics without the generic parameters specified)."},
{"body": "In scala, we cannot extend :gives an error In my case someone has defined some functionality in an object and I need to extend it (basically add another method). What would be the easiest way to extend this object? As so often the correct answer depends on the actual business requirement. Extending from an object would in some sense defy the purpose of that object since it wouldn't be a singleton any longer.What might be a solution is to extract the behavior into an abstract trait. And create objects extending that trait like so:If you want use methods and values from another object you can use import.You can't actually extend an object, because that would create two of it, and an object by definition exists only once (edit: well, that's not quite true, because the object definition can be in a class or method).For your purposes, try this:It doesn't actually extend, but it does allow you to call new methods on it than were originally defined.The only way to share code between two objects is by having one or more common superclass/trait.You can convert parent into class + companion object, and then have child extend class E.g.in Parent.scalaAnd then in Child.scalaYes, it's more a hack than a solution."},
{"body": "Why using , ,  etc. are considered better than using  for Scala Options? If I use I can call  safely. Well, it kind of comes back to \"tell, don't ask\". Consider these two lines:In the first case, you are looking inside  and then reacting depending on what you see. In the second case you are just telling  what you want done, and let it deal with it.The first case knows too much about , replicates logic internal to it, is fragile and prone to errors (it can result in run-time errors, instead of compile-time errors, if written incorrectly).Add to that, it is not composable. If you have three options, a single for comprehension takes care of them:With , things start to get messy fast.One nice reason to use  is parsing something with nested options. If you have something likeThe console prints . If you extend this to a case where you have a class that optionally stores a reference to something, which in turn stores another reference, for comprehensions allow you to avoid a giant \"pyramid\" of None/Some checking.There are already excellent answers to the actual question, but for more -foo you should definitely check out .The reason it's more useful to apply things like , , and  directly to the  instead of using  and then performing the function is that it works on either    and you don't have to do special checks to make sure the value is there. The result for  here is an , which is desirable since if  is optional, then  might be undetermined as well.  Since  doesn't work on , you'd have to do a bunch of extra work to make sure you didn't get any errors; extra work that is done for you by .Trying to perform  our Operations with    is more imperative style where u need to tel  . In other words , we are dictating things and  digging more into the  internals. Where as    are more functional way of doing things where we are say . Put simply:"},
{"body": "For example, there is a string val . How do you separate it into ?You can use  as follows:If you want an array, you can use Do you need characters?Do you need bytes?Do you need strings?Do you need UTF-32 code points?  Okay, that's a tougher one.Additionally, it should be noted that if what you actually want isn't an actual list object, but simply to do something which each character, then Strings can be used as iterable collections of characters in ScalaActually you don't need to do anything special. There is already implicit conversion  in  to  and  extends  so you have all goodies that available in it, like: has  conversion that has higher priority than  in . So String end up being , that is also  of chars. "},
{"body": "Here is a little Scala session that defines and tries out some functions:works nicely.oops.    works well!Now, I've seen the  syntax when folding (, etc). So as I understand it  basically means \"an argument\". So  basically means a function with an argument, which is given to \". But why isn't that  the same as just ? Why is there a difference if I append a ?So I kept exploring...Here it works without ! What's the difference between a ed function, and a ed function?There's no difference between a def'ed function and a val'ed function:See? All of these are functions, which is indicated by the  type they have.Do you see an  type? If you do, go see an ophthalmologist, because there's none. The type here is , commonly used to denote a .Actually, , ,  and  are all methods, which return functions.  is a method which returns a . Also,  through  do not take parameters (only  could, anyway), while  does.So, the difference is pretty simple. In the first case, you tried to assign a method to a val, but did not fill in the parameters the method take. So it failed, until you added a trailing underscore, which meant .In the second example you had a function, so you didn't need to do anything else.A method is not a function, and vice versa. A function is an object of one of the  classes. A method is a handle to some piece of code associated with an object.See various questions about methods vs functions on Stack Overflow.The  declares a method within a surrounding object/class/trait, similar to the way you define methods in Java. You can only use s within other objects/classes/traits. In the REPL, you cannot see the surrounding object because it's \"hidden\", but it does exist.You cannot assign a  to a value, because the  is not a value - it's a method in the object.The  declares and instantiates a , which exists at runtime. Function objects are instances of anonymous classes which extend  traits.  traits come with an  method. The name  is special, because it can be omitted. Expression  is desugared into .The bottomline is - since function objects are runtime values which exist on the heap, you can assign them to values, variables and parameters, or return them from methods as return values.To solve the issue of assigning methods to values (which can be useful), Scala allows you to use the placeholder character to create a function object from a method. Expression  in your example above actually creates a wrapper function around the method  - it is equivalent to .The underscore means different things in different contexts. But it can always be thought of as .When applied in place of parameters, the effect is to lift the method to a function.Note, the method has become a function of type (String) => String. The distinction between a method and a function in Scala is that methods are akin to traditional Java methods. You can't pass them around as values. However functions are values in their own right and can be used as input parameters and return values.The lifting can go further:Lifting this function results in another function. This time of type () => (String) => (String)From what I can tell, this syntax is equivalent to substituting all of the parameters with an underscore explicitly. For example:"},
{"body": "How do you update multiple columns using Slick Lifted Embedding ?  doesn't say much.I expected it to be something like thisI figured it out. It should be like this, why your documentation is so bad ? I have to Google pretty much every silly thing or dig through unit-tests for hours. Please improve it. Thanks.With Slick 2.x and 3.x, this way of writing it works: "},
{"body": "scala-2.11 folder appeared after recent update of IDEA and Scala plugin.\nWhat should it be used for?  Usually such directories are used for binary version-dependent code. For example, macros in 2.10 are not source-compatible with macros in 2.11, so if you're building your project for different binary versions and you're using macros, it makes sense to put code which is only valid for the specific version in different source roots. SBT then will use the appropriate directory when compiling for 2.10 or 2.11.If you're using SBT, though, you would need to set such thing up manually in the build definition. If you're not using SBT, then probably IDEA plugin was updated to handle such things by itself."},
{"body": "What I'd like to achieve is having a proper implementation forI may know what B is, but don't know what A is (but if B has a self type then I could add some constraints on A).\nThe scala compiler is happy with the above signature, but I could not yet figure out how the implementation would look like - if it is possible at all. Some options that came to my mind:Do you have any other ideas that might work? Which way would you recommend? What kind of \"challenges\" to expect?\nOr should I forget it, because it is not possible with the current Scala constraints?Intention behind my problem:\nSay I have a business workflow, but it's not too strict. Some steps have fixed order, but others do not, but at the end all of them has to be done (or some of them required for further processing).\nA bit more concrete example: I have an A, I can add B and C to it. I don't care which is done first, but at the end I'll need an A with B with C.Comment: I don't know too much about Groovy but SO popped up  and I guess it's more or less the same as what I'd like, at least conceptional.I believe this is impossible to do strictly at runtime, because traits are mixed in at compile-time into new Java classes.  If you mix a trait with an existing class anonymously you can see, looking at the classfiles and using javap, that an anonymous, name-mangled class is created by scalac: returnsWhile  returnsAs you can see, scalac creates a new anonymous class that is loaded at runtime; presumably the method  in this anonymous class creates an instance of  and calls  on it, but I'm not completely sure., we can do a pretty hacky trick here:Since you  to use the Scala compiler, AFAIK, this is probably close to the cleanest solution you could do to get this.  It's quite slow, but memoization would probably help greatly.This approach is pretty ridiculous, hacky, and goes against the grain of the language.  I imagine all sorts of weirdo bugs could creep in; people who have used Java longer than me warn of the insanity that comes with messing around with classloaders.I wanted to be able to construct Scala beans in my Spring application context, but I also wanted to be able to specify the mixins to be included in the constructed bean:The difficulty is that Class.forName function does not allow me to specify the mixins. In the end, I extended the above hacky solution to Scala 2.9.1. So, here it is in its full gory; including bits of Spring.The code cannot yet deal with constructors with parameters and does not copy annotations from the parent class\u2019s constructor (should it do that?). However, it gives us a good starting point that is usable in the scala Spring namespace. Of course, don\u2019t just take my word for it, verify it in a Specs2 specification:"},
{"body": "I need to get a simple JSON serialization solution with minimum ceremony. So I was quite happy finding . This works perfectly with plain case classes, e.g.But the following fails:Or would you recommend any other standalone library that handles my case more or less fully automatically? I don't care whether that is with macros at compile time or reflection at runtime, as long as it works out of the box.Here is a manual implementation of the  companion object:Verification:Alternatively the direct format definition:Now ideally I would like to automatically generate the  and  methods. It seems I will need to use either reflection or dive into macros.The library  includes the  strategy, but also the [play-json-extensions] strategy (flat string for case objects mixed with objects for case classes no extra $variant or $type unless needed). It also provides serializers and deserializers for   based enums.\nThere is now a library called  which allows you to write :This will generate the corresponding formats automatically, it will also handle disambiguation of the following case by adding a $variant attribute (the equivalent of 0__ 's  attribute)would generate "},
{"body": "I'd like to estimate the big-oh performance of some methods in a library through benchmarks. I don't need precision -- it suffices to show that something is O(1), O(logn), O(n), O(nlogn), O(n^2) or worse than that. Since big-oh means upper-bound, estimating O(logn) for something that is O(log logn) is not a problem.Right now, I'm thinking of finding the constant multiplier k that best fits data for each big-oh (but will top all results), and then choosing the big-oh with the best fit.Given the comments so far, I need to make a few things clear:Here's one example of the kind of stuff I want to measure. I have a method with this signature:Given an , it will return the nth element of a sequence. This method can have O(1), O(logn) or O(n) given the existing implementations, and small changes can get it to use a suboptimal implementation by mistake. Or, more easily, could get some other method that  on it to use a suboptimal version of it.In order to get started, you have to make a couple of assumptions.In particular, (3) is difficult to achieve in concert with (1).  So you may get something with an exponential worst case, but never run into that worst case, and thus think your algorithm is much better than it is on average.With that said, all you need is any standard curve fitting library.   has a fully adequate one.  You then either create a function with all the common terms that you want to test (e.g. constant, log n, n, n log n, nn*n, e^n), or you take the log of your data and fit the exponent, and then if you get an exponent not close to an integer, see if throwing in a log n gives a better fit.(In more detail, if you fit  for  and , or more easily , you can get the exponent ; in the all-common-terms-at-once scheme, you'll get weights for each term, so if you have  where  is large, you'll pick up that term also.)You'll want to vary the size by enough so that you can tell the different cases apart (might be hard with log terms, if you care about those), and safely more different sizes than you have parameters (probably 3x excess would start being okay, as long as you do at least a dozen or so runs total).Edit: Here is Scala code that does all this for you.  Rather than explain each little piece, I'll leave it to you to investigate; it implements the scheme above using the C*x^a fit, and returns ((a,C),(lower bound for a, upper bound for a)).  The bounds are quite conservative, as you can see from running the thing a few times.  The units of  are seconds ( is unitless), but don't trust that  much as there is some looping overhead (and also some noise).Note that the  method is expected to take about sqrt(2)m*time to run, assuming that static initialization data is used and is relatively cheap compared to whatever you're running.  Here are some examples with parameters chosen to take ~15s to run:Anyway, for the stated use case--where you are checking to make sure the order doesn't change--this is probably adequate, since you can play with the values a bit when setting up the test to make sure they give something sensible.  One could also create heuristics that search for stability, but that's probably overkill.(Incidentally, there is no explicit warmup step here; the robust fitting of the Theil-Sen estimator should make it unnecessary for sensibly large benchmarks.  This also is why I don't use any other benching framework; any statistics that it does just loses power from this test.)Edit again: if you replace the  method with the following:then you can get an estimate of the exponent when there's a log term also--error estimates exist to pick whether the log term or not is the correct way to go, but it's up to you to make the call (i.e. I'm assuming you'll be supervising this initially and reading the numbers that come off):(Edit: fixed the RMS computation so it's actually the mean, plus demonstrated that you only need to do timings once and can then try both fits.)I don't think your approach will work in general.The problem is that \"big O\" complexity is based on a limit as some scaling variable tends to infinity.  For smaller values of that variable, the performance behavior can appear to fit a different curve entirely.The problem is that with an empirical approach you can never know if the scaling variable is large enough for the limit to be apparent in the results.Another problem is that if you implement this in Java / Scala, you have to go to considerable lengths to eliminate distortions and \"noise\" in your timings due to things like JVM warmup (e.g. class loading, JIT compilation, heap resizing) and garbage collection.Finally, nobody is going to place much trust in empirical estimates of complexity.  Or at least, they wouldn't if they understood the mathematics of complexity analysis.In response to this comment:This is true, though my point is that you (Daniel) haven't factored this in.For simple cases, yes.For complicated cases and real world cases, that is a dubious assumption.  For example:If you are happy to estimate this empirically, you can measure how long it takes to do exponentially increasing numbers of operations.  Using the ratio you can get which function you estimate it to be.e.g. if the ratio of 1000 operations to 10000 operations (10x) is (test the longer one first)  You need to do a realistic number of operations to see what the order is for the range you have.Its is just an estimate as time complexity is intended for an ideal machine and something should can be mathematically proven rather than measures.e.g. Many people tried to prove empirically that PI is a fraction. When they measured the ratio of circumference to diameter for circles they had made it was always a fraction. Eventually, it was generally accepted that PI is not a fraction.What you are looking to achieve is impossible in general. Even the fact that an algorithm will ever stop cannot be proven in general case (see ). And even if it does stop on your data you still cannot deduce the complexity by running it. For instance, bubble sort has complexity O(n^2), while on already sorted data it performs as if it was O(n). There is no way to select \"appropriate\" data for an unknow algorithm to estimate its worst case.We have lately implemented a tool that does  for JVM code. You do not even have to have access to the sources. It is not published yet (still ironing out some usability flaws) but will be soon, I hope.It is based on . In short, byte code is augmented with cost counters. The target algorithm is then run (distributed, if you want) on a bunch of inputs whose distribution you control. The aggregated counters are extrapolated to functions using involved heuristics (method of least squares on crack, sort of). From those, more science leads to an estimate for the average runtime asymptotics (, for instance). For example, the method is able to reproduce rigorous classic analyses done by Knuth and Sedgewick with high precision.The big advantage of this method compared to what others post is that you are , that is in particular independent of machine, virtual machine and even programming language. You really get information about your algorithm, without all the noise.And---probably the killer feature---it comes with a complete GUI that guides you through the whole process.You can find a preliminary website (including a beta version of the tool and the papers published) .(Note that average runtime can be estimated that way while worst case runtime can never be, except in case you  the worst case. If you do, you can use the average case for worst case analysis; just feed the tool only worst case instances. In general, runtime bounds , though.)You should consider changing a critical aspects of your task.  Change the terminology that you are using to: \"estimate the runtime of the algorithm\" or \"setup performance regression testing\"Can you estimate the runtime of the algorithm?  Well you propose to try different input sizes and measure either some critical operation or the time it takes.  Then for the series of input sizes you plan to programmaticly estimate if the algorithm's runtime has no growth, constant growth, exponential growth etc.So you have two problems, running the tests, and programmatically estimating the growth rate as you input set grows.  This sounds like a reasonable task.  I'm not sure I get 100% what you want. But I understand that you test your own code, so you can modify it, e.g. inject observing statements. Otherwise you could use some form of aspect weaving?How about adding resetable counters to your data structures and then increase them each time a particular sub-function is invoked? You could make those counting  so they will be gone in the deployed library.Then for a given method, say , you would test that with all sorts of automatically generated data sets, trying to give them some skew, etc., and gather the counts. While as Igor points out you cannot verify that the data structure won't ever violate a big-O bound, you will at least be able to assert that in the actual experiment a given limit count is never exceeded (e.g. going down a node in a tree is never done more than  times) -- so you can detect some mistakes.Of course, you would need certain assumptions, e.g. that calling a method is O(1) in your computer model.This requirement is key. You want to detect outliers with minimal data (because testing should be , dammit), and in my experience fitting curves to numerical evaluations of complex recurrences, linear regression and the like will overfit. I think your initial idea is a good one.What I would do to implement it is prepare a list of expected complexity functions g1, g2, ..., and for data f, test how close to constant f/gi + gi/f is for each i. With a least squares cost function, this is just computing the  of that quantity for each i and reporting the smallest. Eyeball the variances at the end and manually inspect unusually poor fits.For an empiric analysis of the complexity of the program, what you would do is run (and time) the algorithm given 10, 50, 100, 500, 1000, etc input elements. You can then graph the results and determine the best-fit function order from the most common basic types: constant, logarithmic, linear, nlogn, quadratic, cubic, higher-polynomial, exponential. This is a normal part of load testing, which makes sure that the algorithm is first behaving as theorized, and second that it meets real-world performance expectations despite its theoretical complexity (a logarithmic-time algorithm in which each step takes 5 minutes is going to lose all but the absolute highest-cardinality tests to a quadratic-complexity algorithm in which each step is a few millis).EDIT: Breaking it down, the algorithm is very simple:Define a list, N, of various cardinalities for which you want to evaluate performance (10,100,1000,10000 etc)For each element X in N: Create a suitable set of test data that has X elements.Start a stopwatch, or determine and store the current system time.Run the algorithm over the X-element test set.Stop the stopwatch, or determine the system time again.The difference between start and stop times is your algorithm's run time over X elements.Repeat for each X in N.Plot the results; given X elements (x-axis), the algorithm takes T time (y-axis). The closest basic function governing the increase in T as X increases is your Big-Oh approximation. As was stated by Raphael, this approximation is exactly that, and will not get you very fine distinctions such as coefficients of N, that could make the difference between a N^2 algorithm and a 2N^2 algorithm (both are technically O(N^2) but given the same number of elements one will perform twice as fast)."},
{"body": "Is there a  method in Groovy? I want to do something like I do with the following Scala snippet:there is such a method in groovy, it is called collect:\n"},
{"body": "I've found myself stuck on a very trivial thing :-]I've got an enum:In a code I have to convert it conditionally to a number (varianr-number correspondence differs on context). I write:And this gives me an \"unreachable code\" compiler error for every branch but whatever is the first (\"case FOO => 4\" in this case). What am I doing wrong?I suspect the code you are actually using is not , but , lowercase, which will cause Scala to just assign the value to , instead of comparing the value to it.In other words:The following code works fine for me: it produces 6Could you say how this differs from your problem please?"},
{"body": "Where I can learn how to construct the AST's that Scala's macros generate?The Scaladoc isn't as helpful as I'd like. For example:But how do I figure out what an Apply node is? Where can I find a list of the node types in AST's, and how they fit together?There isn't a lot of documentation for the internals of the compiler available, but the things that are available should be enough to get started., has written his . In Appendix D (p. 95) he describes the architecture of the AST. It includes also a graphical overview:Another way to find information about the AST is to look directly into the sources of , which contains the AST.If one needs to find out how a specific source code snippet is represented internally there is :You could take a look at the scaladoc () or at the slides (, the \"Learn to learn\" part).Here's what I usually do. I wrote a simple script called , which takes Scala code as an argument and then compiles it with  ( uses another helper script: .  to see its sources as well). The advantage this approach has over  is that it doesn't require the code to typecheck. You could write a small snippet of code, which refers to non-existent variables or classes, and it still will successfully run and show you the AST. Here's an example of output:There's also a script called , which does the same, but stops after . That's sometimes useful to understand how exactly the typechecker transforms the parser trees. However, both toolboxes and macros work with parser trees, so I use  for tree construction purposes very rarely."},
{"body": " says:I'd like to know exactly how and where Akka is used in Play, and what are the consequences of having Play build on top of Akka.In Play 2.0, Play delegated all requests to go through an actor.  It heavily depended on Akka's future API and other parts.In Play 2.1, with the move of Akka's future API into Scala 2.10, Play started depending less directly on Akka.  It gets all it's execution contexts from Akka, and provides integration with Akka, but that's about the extent of it.In Play 2.3, we're adding new features to aid Akka integration, particularly around WebSockets.In Play 2.4, Play will be ported to the new akka-http (formerly known as spray), at which point, Play will be as built on Akka as you can get.What are the consequences?  Akka provides a paradigm for programming that makes concurrency simple to deal with.  It also provides great abstractions for distributed programming - the hardest thing about distributed programming is dealing with failures (which happen all the time) appropriately.  Most tools try to address this by trying to hide failures from you, but unfortunately hiding something doesn't make it go away, and actually really makes things harder because when you try to address specific types of failures, the fact that they are hidden away from you gets in your way.  Akka pushes failures in your face, so that when you're coding, you are forced to think about how your application will respond to failures.  Consequently you're forced to design/code your application in such a way that it is tolerant to failures.  It also gives you the tools to deal with them in a hierarchical fashion, allowing you to specify at what level you want to handle what type of failure, and how the failure should be responded to (die, retry up to n times, etc).So how does this help Play?  The better question is how does it help a Play user?  Akka helps me to implement Play itself, but it's possible to implement it without Akka (in fact Netty does most of the heavy lifting now, that will change in Play 2.4).  The important thing is that Play seamlessly integrates with Akka, making it easy to handle HTTP requests with actors, handle failures etc, and this helps Play users because it allows them to design their application in such a way that it is scalable and resilient.UPDATE: The above was written 3 years ago, a lot has changed since then. Play 2.4 did provide experimental support for akka-http, but Play still by default uses Netty.In Play 2.5, we deprecated the iteratees API and switched to Akka streams. This meant that now all asynchronous IO was going through Akka streams. Soon (not sure if that will be Play 2.6 or later), Play will flick the switch to make akka-http the default backing implementation of the server (though not yet the WS client)."},
{"body": "As you know, SBT is compatible with Maven in some way -- SBT recognizes simple Maven POMs and can use dependencies and repositories specified in them. However,  says that, if inline dependency is specified in SBT project definition, POM will be ignored (so using both in this case is impossible):Does anyone know, if any kind of converter from Maven POM to SBT project definition exists (translating POM's XML into project definition Scala code)? I'm considering writing such script (that will help to migrate my old Scala/Maven projects to SBT), but want to know first, if this functionality already exists.Converter is far too strong a term for this hack, but I wrote a script to take a block of  and output SBT style deps: All the tips above had the issue for me that properties were not resolved, and as we make heavy use of the dependencyManagement from parent poms, I was hoping for something that actually could fully understand maven.  I whipped together a simplistic scriptlet that outputs the dependencies from maven and just takes the top level items and then does a simple regex for the group, artifact, version, and scope (the artifact type is ignored)I piped this directly to project/build.sbt.  The sample output is (remember to keep empty spaces between sbt lines)I didn't manage to find an undocumented capability in SBT that allows to make such conversions (POM -> project definition), and have came up with writing  that creates  with repos/dependencies from .In case you just need to convert Maven/XML dependencies into SBT/Scala, you can use  provided by Not a converter, but a step-by-step guide for moving a multimodule project from Maven to SBT can be found .Quite nice for understanding what actually happens and ensuring you have a fair amount of control over the proces..I wrote  project for convertion java maven project to sbt project.Take a look a CodaHale's Maven-SBT project over at Git-Hub. Basically, CodaHale has swapped out IVY from SBT and replaced it with Maven, so POM related tasks  be be more compatible/flexible.I wrote yet another hack to convert between  and .  It's useful for converting the bulk of what I'm interested in.I just had the same problem and created a solution in javascript that you can access here:\n "},
{"body": "I am building an app with SBT (0.11.0) using a Scala build definition like so:I'm packaging a .jar at the end of the process.My question is a simple one: is there a way of accessing the application's name (\"my-app-name\") and version (\"0.1\") programmatically from my Scala code? I don't want to repeat them in two places if I can help it.Any guidance greatly appreciated!I just wrote .\nAfter installing the plugin:Edit: The above snippet has been updated to reflect more recent version of sbt-buildinfo.It generates  object with any setting you want by customizing . (I wrote it) but here's a quick script to generate a file:You can get your version as .Name and version are inserted into manifest. You can access them using java reflection from  class."},
{"body": "Scala seems to define 3 kinds of assertions: ,  and .As far as I can understand, the difference (compared to a generic assertion) of  is that it is specifically meant for checking inputs (arguments, incoming messages etc). And what's the meaning of  then?If you look at the code in  you'll see that all three do very similar job:There are also versions which take extra arguments for reporting purposes (see ).The difference is in the exception type they throw and error message they generate.However, static checkers could treat all three differently. The intention is for  to specify a condition that a static check should attempt to prove,  is to be used for a condition that the checker may assume to hold, while  specifies a condition that the caller must ensure. If a static checker finds a violation of  it considers it an error in the code, while when  is violated it assumes the caller is at fault.The difference between assert() and assume() is that The intended consumer / context of assert() is testing, such as a Scala JUnit test, while that of assume() is \"as a means of design-by-contract style specification of pre- and post-conditions on functions, with the intention that these specifications could be consumed by a static analysis tool\" (excerpted from the ).In the context of static analysis, as Adam Zalcman has pointed out, assert() is an all-execution-paths assertion, to check a global invariant, while assume() works locally to pare down the amount of checking that the analyzer needs to do. assume() is used in the context of assume-guarantee reasoning, which is a divide and conquer mechanism to help model checkers assume something about the method so as to tackle the state explosion problem that arises when one attempts to check all paths that the program may take. For example, if you knew that in the design of your program, a function f1(n1 : Int, n2:Int) is NEVER passed n2 < n1, then stating this assumption explicitly would help the analyzer not have to check a LOT of combinations of n1 and n2.In practice, since such whole-program model checkers are still mostly theory, let's look at what the scala compiler and interpreter does:More from the excellent scaladoc on this topic:A set of  functions are provided for use as a way to document and dynamically check invariants in code. assert statements can be elided at runtime by providing the command line argument  to the  command.Variants of  intended for use with static analysis tools are also provided: ,  and .  and ensuring are intended for use as a means of design-by-contract style specification of pre- and post-conditions on functions, with the intention that these specifications could be consumed by a static analysis tool. For instance,The declaration of addNaturals states that the list of integers passed should only contain natural numbers (i.e. non-negative), and that the result returned will also be natural. require is distinct from assert in that if the condition fails, then the caller of the function is to blame rather than a logical error having been made within addNaturals itself. ensures is a form of assert that declares the guarantee the function is providing with regards to it's return value.\n)I second Adams answer, here are just some small additions:When  is violated, the verification tool silently prunes the path, i.e. does not follow the path any deeper. Hence  is often used to formulate pre-conditions,  to formulate post-conditions.These concepts are used by many tools, e.g. the concolic testing tool , software bounded model checking tools like  and , and partly also by static code analysis tools based on abstract interpretation. The article  introduces these concepts and tries to standardize them. "},
{"body": "I'm trying to get started with Scala and cannot get out of the starting gate.A file consisting of the linegives meRegardless of what x is and regardless of where I put the file (I had a theory that I had to place the file in a directory hierarchy to match the package definition, but no).  I get the same error with the example code from the web site and with the REPL.It looks like you're trying to declare the  membership in a Scala script (run using the  command) or in the REPL.Only files defining just classes and objects which are compiled with  may be defined as belonging to a package.When you run code in a script or a REPL session, behind the scenes it is actually compiled inside a method of an object, in which scope a package declaration wouldn't be legal.Since Scala 2.11.0-M7 you can use  (fix for issue ). This option allows defining packages in the REPL:I don't get this error. How are you compiling this? And, by the way, what web site? As for REPL, it doesn't accept packages. Packages are only for compiled code."},
{"body": "Here is the only way I know to ask it at the moment.  As Understand it Scala uses the Java Virtual Machine.  I thought Jruby did also.  Twitter switched its middleware to Scala.   Could they have started with Jruby to start with and not had their scaling problems that caused them to move from Ruby to Scala in the first place?  Do I not understand what Jruby is?  I'm assuming that because Jruby can use Java it would have scaled where Ruby would not.Does it all boil down to the static versus dynamic types, in this case?Scala is \"scalable\" in the sense that the  can be improved upon by libraries in a way that makes the extensions look like they are part of the language. That's why actors looks like part of the language, or why BigInt looks like part of the language.This also applies to most other JVM languages. It does not apply to Java, because it has special treatment for it's basic types (Int, Boolean, etc), for operators, cumbersome syntax that makes clear what is built in the language and what is library, etc.Now, Scala is more performatic than dynamic languages  because the JVM has no support for them. Dynamic languages on JVM have to resort t reflection, which is very slow.No, not really.  It's not that the JVM is somehow magic and makes things scale by its magic powers; it's that Scala, as a language, is architected to help people write scalable systems.  That it's on top of the JVM is almost incidental.I don't really think that the language is the biggest problem here. Twitter grew insanely fast, which always leads to a code mess. And if you do a rewrite, it is a good idea to go for a different language - that bars you from building your own mistakes again and/or to \"reuse some parts\". Also, Ruby is not really meant for that kind of heavy data handling that the twitter backend does.\nThe frontend remains Ruby, so they still use it.You have to separate out different meanings of scaling:Scala helps on the first point because it compiles to Java bytecode that's really similar to Java, and therefore usually has the same performance as Java.  I say \"usually,\" because Scala there are some cases where idiomatic Scala causes large amount of boxing to take place where idiomatic Java would not (this is slated to change in Scala 2.8).Performance is of course different than scaling.  Equivalent code written in JRuby would scale just as well, but the slope of the line would be steeper - you'd need more hardware to handle the same number of requests, but the shape of the line would be the same.  But from a more practical perspective the performance helps because you rarely can scale in a perfectly linear fashion with respect to adding core or especially servers and having better performance slows the rate at which you have to add capacity.Scala helps with the second point because it has an expressive, compile-time enforced type system and it provides a lot of other means for managing the complexity of your code, such as mixins.  You can write spaghetti code in any language, but the Scala compiler will tell you when some of the noodles are broken while with JRuby you'll have to rely solely on tests.  I've personally found that for me Python breaks down at about 1000 closely related LOCs, and which point I have to refactor to either substantially reduce of the LOCs or make the structure more modular.  Of course this refactoring would be a good idea regardless of what your language, but occasionally the complexity is inherent.  Dealing with a large number of tightly couple LOCs isn't easy in any language, but it is much easier in Scala than it is in Python, and I think the analogy extends to Ruby/JRuby as well.Scala is a statically typed language. JRuby is dynamically typed. That is why Scala is faster than JRuby, even though both run on the JVM. JRuby has to do a lot of work at runtime (method resolution, etc.) that Scala does at compile-time. For what it's worth, though, JRuby is a very fast Ruby implementation.Scalability is not an inherit language capability. You are talking about speed.A better question to ask would be \"Why is Scala faster than other JVM languages (or is it)?\". As others have pointed out, it's a static vs. dynamic language thing.There's an interesting discussion from the Twitter developers themselves in the comments of . \nThey've evaluated the different options and decided to implement the back-end in Scala because: it ran faster than the Ruby/JRuby alternatives and they felt they could benefit from static typing."},
{"body": "Is there an easy way to convert a    to a? In Scala 2.8 this became much much easier, and there are two ways to achieve it.  One that's sort of explicit (although it  implicits): Since I wrote this, the Scala community has arrived at a broad consensus that  is good, and  is bad, because of the potential for spooky-action-at-a-distance.  So don't use  at all!Yes use  conversions:Which can then be easily implemented:"},
{"body": "Is it possible and what would be the most efficient neat method to add a column to Data Frame? More specifically, column may serve as Row IDs for the existing Data Frame.In a simplified case, reading from file and not tokenizing it, I can think of something as below (in Scala), but it completes with errors (at line 3), and anyways doesn't look like the best route possible: It's been a while since I posted the question and it seems that some other people would like to get an answer as well. Below is what I found.So the original task was to append a column with row identificators (basically, a sequence ) to any given data frame, so the rows order/presence can be tracked (e.g. when you sample). This can be achieved by something along these lines:The \"closest\" to this functionality in Spark API are  and . According to , the former . In my opinion, this is a bit confusing and incomplete definition. Both of these functions can operate on  data frame only, i.e. given two data frames  and  with column :So unless you can manage to transform a column in an existing dataframe to the shape you need, you can't use  or  for appending arbitrary columns (standalone or other data frames).As it was commented above, the workaround solution may be to use a  - this would be pretty messy, although possible - attaching the unique keys like above with  to both data frames or columns might work. Although efficiency is ...It's clear that appending a column to the data frame is not an easy functionality for distributed environment and there may not be very efficient, neat method for that at all. But I think that it's still very important to have this core functionality available, even with performance warnings.not sure if it works in spark 1.3 but in spark 1.5 I use withColumn:I use this when I need to use a value that is not related to existing columns of the dataframeThis is similar to @NehaM's answer but simplerI took help from above answer. However, I find it incomplete if we want to change a  and current APIs are little different in . \n returns a  of  which contains each row and corresponding index. We can use it to create new  according to our need. I hope this will be helpful."},
{"body": "I'm kinda new to Scala trying it out while reading Beggining Scala by David Pollack.\nHe defines a simple recursive function that loads all strings from the file:It's elegant and awesome except that it had thrown a StackOverflow exception when I tried to load a huge dictionary file.Now as far as I understand Scala supports tail recursion, so that function call couldn't possibly overflow the stack, probably compiler doesn't recognize it? So after some googling I tried @tailrec annotation to help the compiler out, but it saidAm I understanding tail recursion wrong? How do I fix this code?Scala can only optimise this if the last call is a call to the method itself.Well, the last call is not to , it's actually to the  (cons) method.A way to make this tail recursive is to add an accumulator parameter, for example:To prevent the accumulator leaking into the API, you can define the tail recursive method as a nested method:It's not tail recursive (and can't ever be) because the final operation is not a recursive call to , it's a call to the  method.The safest way to resolve this is with a nested method that uses an accumulator:In this particular case, you could also lift the accumulator to a parameter on , give it a default value of , and avoid the need for an inner method.  But that's not always possible, and it can't be called nicely from Java code if you're concerned about interop."},
{"body": "Specifically I'm looking at Problem 1 hereThe code as listed is as followsI can follow everything except for \"view\".  In fact if I take out view the code still compiles and produces exactly the same answer.View produces a lazy collection, so that calls to e.g.  do not evaluate every element of the collection. Elements are only evaluated once they are explicitly accessed. Now  does access all elements, but with  the call to  doesn't create a full Vector. (See comment by Steve)A good example of the use of view would be:Here Scala tries to create a collection with  elements to then access the first 10. But with view:I don't know much about Scala, but perhaps  might help...So it sounds as if the code will still work without , but might in theory be doing some extra work constructing all the elements of your collection in  rather than  fashion. "},
{"body": "When I try to compile any class in my project I get the error below:I've seen how to set the output path in IDEA and I've done it. But as the error claims that it is shared between the same module I couldn't solve it.Obs.: Using Maven and IntelliJ IDEA.Please, can anyone help?all you need to do is:EnjoySet up the output paths for your modules to different directories, as explained here: I resolved this by selecting \"Inherit project compile output path\" in the Project Structure settings window.I had this happen with the root module in a multi-module project. Since the root module was just a placeholder, it didn't actually contain any code, but IDEA still complained that it was sharing an output path () between test and production.The fix was to addat the top of the root  file (applying the otherwise unnecessary Java plugin) and reimporting the project. This allowed IDEA to pick up the Java-default  and  output directories.Open up the module settings and look for errors. Fix said errors and everything should work.Problem solved!I have reset backward and forward my git repository many times and close-reopen idea after some of it. When finaly IDEA shows a red message at the right top while it was starting. It says something like: the project has already a eval module, and if I want to delete eval module. Yes, it was quite confusing, but I click on delete and my problem disappear. I guess that for some reason I become with 2 eval modules and it delete one of it solving output path error."},
{"body": " is used to preserve insertion order in the map, but this only works for mutable maps. Which is the immutable  implementation that preserves insertion order? implements an immutable map using a list-based data structure, and thus preserves insertion order.The following extension method -  can be quite useful when working with s.While  will preserve insertion order, it is not very efficient - e.g. lookup time is linear. I suggest you create a new collection class which wraps both the  and the . The immutable map should be parametrized as , where the  in the tuple gives you the pointer to the corresponding entry in the . You then keep an entry counter on the side. This tree map will sort the entries according to the insertion order.You implement insertion and lookup in the straightforward way - increment the counter, insert into the hash map and insert to the the counter-key pair into the treemap. You use the hash map for the lookup.You implement iteration by using the tree map.To implement remove, you have to remove the key-value pair from the hash map and use the index from the tuple to remove the corresponding entry from the tree map."},
{"body": "I am trying to use IntelliJ with a play framework 2.11 application.I installed the Play Framework 2 plugin and the Scala plugin for IntelliJ.I created a Play application.  I have been struggling writing and running Specs 2 tests in IntelliJ.  My run config says to run \"make\" first when running the Specs 2 test, however it doesn't look like my test classes are being generated.  Keeps on telling me that it could not find the specification.  When I look on the file system, there is no code in target/test-classes, the directory is empty.  Further, it seems to take a LONG time to do the build, at least compared to running the Play console.I wanted to see how people are using Play with IntelliJ.  Do you just use IntelliJ as an editor, and run everything through the Play console?Is there a way whereby you can run your Application tests in IntelliJ (getting your test classes to run)?I have never had any problem running the Play console and running ~test-only test=xxx.Spec.  It has typically been rather fast.Here is the exception I am getting in IntelliJ when I try to run my Specs2 tests:: In newer versions if IntelliJ IDEA, it is no longer necessary to create the module from play/activator. IntelliJ IDEA has now a really good support for SBT projects. If exists, delete all the idea related directories inside your project. Then in IntelliJ IDEA click File -> Open and choose your build.sbt file. That's all.IntelliJ IDEA has a good integration for the Play Framework 2. Sometimes it jams, but most of the time it runs. I use it to run(single, all) tests, start or debug the play application and edit my code (o; And this all from within the IDE and without the sbt console.Here is a short tutorial with the most important steps. Currently I use IntelliJ IDEA 12.1 with the newest Play Framework 2 and Scala plugins.Start the play console:Create the module:Select the  under the test directory and click  from the context menu. You should get an error that the compiled template could not be found. This is because the IDE doesn't compile the templates, but this can be done by run the application once. Also follow point 5 and then run the test again.Select a controller and click  from context menu. This should start the application on address: .If you update your application dependencies then you must tell the IDE about this changes. Also after running the  command you must close the IDE and remove some files from project directory. If you execute the  command before removing the files, you get double dependencies in your play project.Execute the following steps to update your dependencies:Play console includes a fork of a sbt plugin named . The play's fork got a little lagged behind the original plugin, and has some problems in IntelliJ when you run . You can use the original plugin, which doesn't have any issues. In order to use this plugin in your play project, you need to..1.Add the following lines to  file: (the blank line in the middle is required)2.Run  from the play console.I usually used IntellijIDEA (version 12.0.4) only for Play Framework code editor because of: And I usually run and debug the apps with Play SBT console. It's reasonable fast. But, sometimes when executing  command on console, I found that the  task take too long time (nearly 10 minutes). I don't know why this sometimes happen, but overall the use of Play SBT console is my choice.I'm doing that because there is usually  intepreted as  like following :The first time when I start learning Play Framework, I was facing such problem. So, at the end, I choose to use Play SBT console to run and debugging app then.And sorry I cannot answer for the question number 2. Until now I only tried running and debugging play application. For testing purpose I've never tried before for Play 2.x. "},
{"body": "I want to do the following, but the self-type line just doesn't compile. Do I have this syntax wrong or is this just impossible?You can have a single self-type which is a compound type.Try this:With  you can do it with :Reflection used.But, firstly you should not overuse self-types because of . Methods from question can be added simply by extending other tait:or type both methods explicitly:Self-types are usually used with classes. This is a good way for trait to require being a subclass of a desired class.There is also one good use of self-type with triats: when you want to manipulate order of class initialization with multi-inheritance."},
{"body": "I just started exploring Scala in my free time. I have to say that so far I'm very impressed. Scala sits on top of the JVM, seamlessly integrates with existing Java code and has many features that Java doesn't. Beyond learning a new language, what's the downside to switching over to Scala?Well, the downside is that you have to be prepared for Scala to be a bit rough around the edges: You also have to take some risk that Scala as a language will fizzle out.That said, ! My experiences are positive overall; the IDE's are useable, you get used to what the cryptic compiler errors mean and, whilst your Scala codebase is small, a backwards-compatibility break is not a major hassle.It's worth it for , the  functionality of the collections, , the  model, extractors, covariant types etc. It's an awesome language.It's also of great  benefit to be able to approach problems from a different angle, something that the above constructs allow and encourage.Some of the downsides of Scala are not related at all to the relative youth of the language. After all, Scala, has about 5 years of age, and Java was very different 5 years into its own lifespan.In particular, because Scala does not have the backing of an enterprise which considers it a strategic priority, the support resources for it are rather lacking. For example:Another important difference is due to how Sun saw Java and  sees Scala. Sun saw Java as a product to get enterprise customers. EPFL sees Scala as a language intended to be a better language than existing ones, in some particular respects (OOxFunctional integration, and type system design, mostly).As a consequence, where Sun made JVM glacially-stable, and Java fully backward compatible, with very slow deprecation and removal of features (actually, removal?), JAR files generated with one version of Scala won't work at all with other versions (a serious problem for third party libraries), and the language is constantly getting new features as well as actually removing deprecated ones, and so is Scala's library. The revision history for Scala 2.x, which I think is barely 3 years old, is impressive.Finally, because of all of the above, third party support for Scala is . But it's important to note, though, that , which makes  out of selling the  IDE, has supported Scala for quite some time, and keeps improving its support. That means, to me, that there is demand for third party support, and support is bound to increase.I point to the book situation. One year ago there was no Scala book on the market. Right now there are two or three introductory Scala books , about the same number of books should be out before the end of the year, and there is a book about a very important web framework based on Scala, .I bet we'll see a book about  not too far in the future, as well as books about Scala and concurrency. The publishing market has apparently reached the tipping point. Once that happens, enterprises will follow.I was unshackled from the J2EE leash last year wanted to do something new after 12 years of Java in the enterprise building very large system for some of the worlds biggest companies.I had tried Ruby on Rails in the past. After building a few sample apps I did not like the feel of it or the fact that I would have to write a ton of unit tests to cover stuff that is normally done by a compiler. Groovy on Grails was my next port of call. I have to say I do like this but it suffers from the same dynamic typing problems as ROR. Don't get me wrong I am not putting Grails down as it is an excellent framework and I will still use it. Each has its own place IMO. I then jumped on Scala and have now built a hybrid application based on Scala and Spring MVC. At first working with Scala is difficult but it gets easier and more productive the more time you put into it. I've reached a tipping point where I now want to invest time in Lift as well. The combination of \"Programming in Scala\" and David Pollak's \"Beginning Scala\" books is good for learning the language, the latter with a less academic bent. Scala is still young and has some way to go. I think it has a bright future and I see momentum is already picking up. Recently one of the creators of the Groovy language said in a blog post he would never have bothered designing Groovy if Scala had been around at the time.  I think some more work on better Java API integration/wrapping will give Scala the boost it needs to win more followers. The basic integration is there already but I think its could be polished a bit more. Yes IDE support is there but it is basic at the moment. The powerfully refactoring support of Intellij is not there yet and I miss that a lot. The compiler + IDE support with a mix of of other plugins is not mature yet. I sometimes get very weird internal compiler errors  caused by how Scala sits with JDO enhancement for the Goggle app engine. However these are little things that can be easily fixed. Early adaptation of new technologies and languages always comes with a little pain. But this bit of pain can produce great pleasure in the future.If I look at the capabilities of Scala compared to early Java its miles ahead. When I moved from C++ to Java the JVM was not ready yet regarding scalability. There used to be lots of weird crash and burn JVM core dumps on various OSes. All of this has now been fixed in Java and the JVM is rock solid. Scals runs in the JVM so it has been given a massive head start on native platform integration. Its standing on the shoulders of giants! After years of building and supporting enterprise applications my vote is for a language where a compiler can catch most of the non functional bugs before even unit tests are built. I love the type checking mixed with the power of functional programming. I like the fact that I am doing OO++. I think the development community will decide if Scala is the future or not. The downside of  adopting Scala now would be if it did not pick up momentum and adaptation. It would be very difficult to maintain an Scala code base with very few Scala developers around.  However I watched Java come from the skunk works into the enterprise to replace C++ and it was all pushed from the bottom up by the developer community. Time will tell for Scala but currently it has my vote. I'll tell you my little personal experience, and how I found that it wasn't so easy to integrate Scala with existing Java libraries:I wanted to get started with something easy, and as I thought that Scala was very well suited for scientific computation I wanted to do a little wrapper around JAMA (Java Matrix library)... My initial approach was to extend the Matrix type with a Scala class and then overload the arithmetic operators and call the Java native methods, but:I think I could have used an empty primary constructor that initialized the superclass with some default values (for example, a [[0]]), or just make an adapter class that used the Jama.Matrix as a delegate, but if a language is supposed to be elegant and seamless integrated with another, that kind of things shouldn't happen.Those are my two cents.I don't think there are any downsides.  Actually learning new language is very helpful for broadening your programming knowledge. You might gain from Scala such things as generic classes, variance annotations, upper and lower type bounds, inner classes and abstract types as object members, compound types, explicitly typed self references, views, and polymorphic methods. It consistently breaks backwards compatibilty.\nCommunity size is small.\nIDE support isn't there yet.Otherwise its fine.\nIt is just a young language, it will get there eventually.\nGreat for hobbyists, not ready for enterprise.The two, by which I mean four, biggest downsides I'm seeing are:"},
{"body": "I want to iterate over a list of values using a beautiful one-liner in Scala.For example, this one works well:But if I use the placeholder , it gives me an error:Why is that? Can't compiler infer type here?This:is equivalent to this:There's no indication as to what might be the type of , and, to be honest, it doesn't make any sense to print a function.You obviously (to me) meant to print , where  would the the parameter passed by , instead. But, let's consider this...  takes, as a parameter, a , where  is the type parameter of the list. What you are passing to  instead is , which is an expression that returns .If you wrote, instead , you'd be passing a completely different thing. You'd be passing the function(*) , which takes  and returns , fitting, therefore, the requirements of .This gets slightly confused because of the rules of expansion of . It expands to the innermost expression delimiter (parenthesis or curly braces), except if they are in place of a parameter, in which case it means a different thing: partial function application.To explain this better, look at these examples:Here, we applies the second and third arguments to , and returned a function requiring just the remaining argument. Note that it only worked as is because I indicated the type of , otherwise I'd have to indicate the type of the argument I was not applying. Let's continue:Let discuss  in more detail, because this is a very important point. Recall that  is a function , right? So, if I were to type , would that make any sense? That's what was done in .What confuses people is that what they really wanted was:In other words, they want the  replacing  to jump to  the parenthesis, and to the proper place. The problem here is that, while it may seem obvious to them what the proper place is, it is not obvious to the compiler. Consider this example, for instance:Now, if we were to expand this to outside the parenthesis, we would get this:Which is definitely not what we want. Now, back to the confusion between anonymous function and partial application, look here:The first line doesn't work because of how operation notation works. Scala just sees that  returns , which is not what expects.The second line works because the parenthesis let Scala evaluate  as a whole. It is a partial function application, so it returns , which is acceptable.The third line doesn't work because  is anonymous function, which you are passing as a parameter to . You are  making  part of an anonymous function, which is what you wanted.Finally, what few people expect:This works. Why it does is left as an exercise to the reader. :-)(*) Actually,  is a method. When you write , you are not passing a method, because methods can't be passed. Instead, Scala creates a closure and passes it. It expands like this: The underscore is a bit tricky.  According to the spec, the phrase:is equivalent toTryingyields:If you add some types in:The problem is that you are passing an anonymous function to  and it's not able to deal with it.  What you really want to do (if you are trying to print the successor to each item in the list) is:There is a strange limitation in Scala for the nesting depth of expressions with underscore. It's well seen on the following example:Looks like a bug for me."},
{"body": "In , the author said:Why is a  object more object-oriented? What's the good of not using static members, but singleton objects?Trying for the \"big picture\";  most of this has been covered in other answers, but there doesn't seem to be a single comprehensive reply that puts it all together and joins the dots.  So here goes...Static methods on a class are not methods on an object, this means that:The whole point of objects is that they can inherit from parent objects, implement interfaces, and be passed as arguments - static members have none of these properties, so they aren't truly object-oriented, they're little more than a namespace.Singleton objects, on the other hand, are fully-fledged members of the object community.Another very useful property of singletons is that they can easily be changed at some later point in time to not be singletons, this is a particularly painful refactoring if you start from static methods.Imagine you designed a program for printing addresses and represented interactions with the printer via static methods on some class, then later you want to be able to add a second printer and allow the user to chose which one they'll use...  It wouldn't be a fun experience!Singleton objects behave like classes in that they can extend/implement other types.Can't do that in Java with just static classes -- it's  over the  with a  that allows (at least) nicer namespaces/stable identifiers and hides the distinction.Hint: it's called -oriented programming.Seriously.Maybe I am missing something fundamentally important, but I don't see what the fuss is all about: objects are more object-oriented than non-objects because they are objects. Does that really need an explanation?Note: Although it sure sounds that way, I am really  trying to sound smug here. I have looked at all the other answers and I found them terribly confusing. To me, it's kind of obvious that objects and methods are more object-oriented than namespaces and procedures (which is what static \"methods\" really are) by the very  of \"object-oriented\".An alternative to having singleton objects would be to make classes themselves objects, as e.g. Ruby, Python, Smalltalk, Newspeak do.For static members, there is no . The class really just is a namespace.In a singleton, there is always at least  object.In all honesty, it's splitting hairs.It's more object oriented in the sense that given a Scala class, every method call is a method call on that object. In Java, the static methods don't interact with the object state.In fact, given an object  of a class  with the static method , it's considered bad practice to call . Instead it's recommended to call  (I believe Eclipse will give you a warning). Java static methods can't be overridden, they can just be hidden by another method:What will  print? In Scala, you would stick the static methods in companion objects A and B and the intent would be clearer as you would refer explicitly to the companion A or B.Adding the same example in Scala:There is some difference that may be important in some scenarios. In Java you \n so if you had class with static methods you would not be able to customize and override part of its behavior. If you used singleton object, you could just plug singleton created from subclass.It's a marketing thing, really.  Consider two examples:Now, what are the observable differences here?It reminds me of the religious wars 20 years ago over whether C++ or Java were \"really\" Object Oriented, since after all both exposed primitive types that aren't \"really\" objects -- so, for example you can't inherit from  but can from ."},
{"body": "In java exceptions have at least these four constructors:If you want to define your own extensions, you just have to declare a descendent exceptions and implement each desired constructor calling the corresponden super constructorHow can you achieve the same thing in scala?so far now I saw  and this , but I suspect there must be an easier way to achieve such a common thingDefault value for  is null. And for  it is either  or null:So you can just use default values:well, this is the best I've found so farthis way you may use the \"new MissingConfigurationException\" or the apply method from the companion objectAnyway, I'm still surprised that there isn't a simpler way to achieve itYou can use .To me, it appears there are three different needs which have a dynamic tension with each other:If one doesn't care about number 3, then  (a peer to this one) seems pretty succinct.However, if one values number 3 while trying to get as close to number 1 and 2 as possible, the solution below effectively encapsulates the Java  leak into your Scala API.And if you would like to eliminate having to use  where  is actually used, add this companion object (which just forwards all of the apply calls to the existing \"master\" class constructor):Personally, I prefer to actually suppress the use of the  operator in as much code as possible so as to ease possible future refactorings. It is especially helpful if said refactoring happens to strongly to the Factory pattern. My final result, while more verbose, should be quite nice for clients to use.  It's only a small leap from the original question to desire to create an ecosystem of specialized s for a package or API. The idea is to define a \"root\"  from which a new ecosystem of specific descendant exceptions can be created. For me, it was important to make using  and  much easier to exploit for specific types of errors.For example, I have a  method defined which verifies a set of conditions prior to allowing a case class to be created. Each condition that fails generates a  instance. And then the List of s are returned by the method. This gives the client the ability to decide how they would like to handle the response;  the list holding exception, scan the list for something specific and  that or just push the entire thing up the call chain without engaging the very expensive JVM  command.This particular problem space has three different descendants of , one abstract () and two concrete ( and ).The first, , is a direct descendant to , very similar to , and is abstract (to prevent direct instantiations).  has a \"companion object trait\",  which acts as the instantiation factory (suppressing the  operator).The second, , is an indirect  descendant and a direct concrete implementation of . It defines both a companion object and a class. The companion object extends the trait . And the class simply extends the abstract class  and marks it  to prevent any further extensions.The third, , is a direct descendant to  which wraps a  of s and then dynamically manages the emitting of the exception message.And then bringing all of that together as a whole and tidy things up, I place both  and  within object . And this is what the final result looks like:And this is what it would look like for a client to use the above code to create their own exception derivation called :And then the use of this exception looks like this:I went well beyond answering the original question because I found it so difficult to locate anything close to being both specific and comprehensive in Scala-ifying  in specific or extending into the more general \"exception ecosystem\" with which I grew so comfortable when in Java.I'd love to hear feedback (other than variations on, \"Wow! That's way too verbose for me.\") on my solution set. And I would love any additional optimizations or ways to reduce the verbosity WITHOUT LOSING any of the value or terseness I have generated for the clients of this pattern.Scala pattern matching in try/catch blocks works on interfaces. My solution is to use an interface for the exception name then use separate class instances.Instantiating MyClass will output \"oops\".Here is a similar approach to the one of @roman-borisov but more typesafe.\nThen, you can create Exceptions in the Java manner:"},
{"body": "I'm following the tutorial  on Scala  and  methods. There's such an example:When I try to use it I get an error:However, this works:or even What's wrong with the code in the tutorial? Isn't the latter expression the same as the first one without parenthesis? is a method.  method is defined on functions.  converts  from method to function, so  can be called on it.  expects a function as it's argument. You are giving it a method  by converting  into a function with  (The underscore can be left out because the compiler can automatically convert a method into a function when it knows that a function is expected anyway). So your code:is the same asbut notPS\nI didn't look at the link you provided.Well, this:is an eta expansion. It converts methods into functions. On the other hand, this:is an anonymous function. In fact, it is a partial function application, in that this parameter is not applied, and the whole thing converted into a function. It expands to:The exact rules for expansion are a bit difficult to explain, but, basically, the function will \"start\" at the innermost expression delimiter. The exception is partial function applications, where the \"x\" is moved outside the function -- if  is used in place of a parameter.Anyway, this is how it expands:Alas, the type inferencer doesn't know the type of x or y. If you wish, you can see exactly what it tried using the parameter .From  documentation: so you should writeto treat  and  as partially applied functions (i.e )I believe the tutorial was written for an earlier version of Scala (probably 2.7.7 or earlier). There have been some changes in the compiler since then, namely, extensions to the type system, which now cause the type inferencing to fail on the:The lifting to a function still works with that syntax if you just write:"},
{"body": "What are the key differences between the approaches taken by Scala and F# to unify OO and FP paradigms?What are the relative merits and demerits of each approach? If, in spite of the support for subtyping, F# can infer the types of function arguments then why can't Scala?I have looked at F#, doing low level tutorials, so my knowledge of it is very limited. However, it was apparent to me that its style was essentially functional, with OO being more like an add on -- much more of an ADT + module system than true OO. The feeling I get can be best described as if all methods in it were static (as in Java static).See, for instance, any code using the pipe operator (). Take this snippet from the :The function  is not a method of the list instance. Instead, it works like a static method on a  module which takes a list instance as one of its parameters.Scala, on the other hand, is fully OO. Let's start, first, with the Scala equivalent of that code:Here,  is a method on the instance of . Static-like methods, such as  on  or  on , are much more uncommon. Then there are functions, such as  above. Here,  is not a method on , but neither it is a static method. Instead, it is an  -- the second main difference I see between F# and Scala.Let's consider the F# implementation from the Wikipedia, and an equivalent Scala implementation:The above Scala implementation is a method, but Scala converts that into a function to be able to pass it to . I'll modify it below so that it becomes a method that returns a function instead, to show how functions work in Scala.So, in Scala, all functions are objects implementing the trait , which defines a method called . As shown here and in the list creation above,  can be omitted, which makes function calls look just like method calls.In the end, everything in Scala is an object -- and instance of a class -- and every such object does belong to a class, and all code belong to a method, which gets executed somehow. Even  in the example above  a method, but has been converted into a keyword to avoid some problems quite a while ago.So, how about the functional part of it? F# belongs to one of the most traditional families of functional languages. While it doesn't have some features some people think are important for functional languages, the fact is that F# is function by , so to speak.Scala, on the other hand, was created with the intent of  functional and OO models, instead of just providing them as separate parts of the language. The extent to which it was succesful depends on what you deem to be functional programming. Here are some of the things that were focused on by Martin Odersky:In my opinion, Scala is unparalled in combining FP and OO. It comes from the OO side of the spectrum towards the FP side, which is unusual. Mostly, I see FP languages with OO tackled on it -- and it  tackled on it to me. I guess FP on Scala probably feels the same way for functional languages programmers.Reading some other answers I realized there was another important topic: type inference. Lisp was a dynamically typed language, and that pretty much set the expectations for functional languages. The modern statically typed functional languages all have strong type inference systems, most often the  algorithm, which makes type declarations essentially optional.Scala can't use the Hindley-Milner algorithm because of Scala's support for . So Scala has to adopt a much less powerful type inference algorithm -- in fact, type inference in Scala is intentionally undefined in the specification, and subject of on-going improvements (it's improvement is one of the biggest features of the upcoming 2.8 version of Scala, for instance).In the end, however, Scala requires all parameters to have their types declared when defining methods. In some situations, such as recursion, return types for methods also have to be declared.Functions in Scala can often have their types  instead of declared, though. For instance, no type declaration is necessary here: , where  is actually an anonymous function of type . Likewise, type declaration of variables is often unnecessary, but inheritance may require it. For instance,  and  have a common superclass , but actually belong to different subclases. So one would usually declare  to make sure the correct type is assigned.This limited form of type inference is much better than statically typed OO languages usually offer, which gives Scala a sense of lightness, and much worse than statically typed FP languages usually offer, which gives Scala a sense of heavyness. :-)Notes: is functional - It  OO pretty well, but the design and philosophy is functional nevertheless. Examples:It feels relatively clumsy to use F# in a mainly object-oriented way, so one could describe the main goal as to . is multi-paradigm with focus on flexibility. You can choose between authentic FP, OOP and procedural style depending on what currently fits best. It's really about .There are quite a few points that you can use for comparing the two (or three). First, here are some notable points that I can think of:The key difference is that Scala tries to blend the paradigms by making sacrifices (usually on the FP side) whereas F# (and OCaml) generally draw a line between the paradigms and let the programmer choose between them for each task.Scala had to make sacrifices in order to unify the paradigms. For example:Another consequence of this is that F# is based upon tried and tested ideas whereas Scala is pioneering in this respect. This is ideal for the motivations behind the projects: F# is a commercial product and Scala is programming language research.As an aside, Scala also sacrificed other core features of FP such as tail-call optimization for pragmatic reasons due to limitations of their VM of choice (the JVM). This also makes Scala much more OOP than FP. Note that there is a project to bring Scala to .NET that will use the CLR to do genuine TCO.Type inference is at odds with OO-centric features like overloading and subtypes. F# chose type inference over consistency with respect to overloading. Scala chose ubiquitous overloading and subtypes over type inference. This makes F# more like OCaml and Scala more like C#. In particular, Scala is no more a functional programming language than C# is.Which is better is entirely subjective, of course, but I personally much prefer the tremendous brevity and clarity that comes from powerful type inference in the general case. OCaml is a wonderful language but one pain point was the lack of operator overloading that required programmers to use  for ints,  for floats,  for rationals and so on. Once again, F# chooses pragmatism over obsession by sacrificing type inference for overloading specifically in the context of numerics, not only on arithmetic operators but also on arithmetic functions such as . Every corner of the F# language is the result of carefully chosen pragmatic trade-offs like this. Despite the resulting inconsistencies, I believe this makes F# far more useful.From  article on Programming Languages:From the above excerpt it is clear that Scala's approach to unify OO and FP paradigms is far more superior to that of OCaml or F#.HTH.Regards,\nEric.The syntax of F# was taken from OCaml but the object model of F# was taken from .NET. This gives F# a light and terse syntax that is characteristic of functional programming languages and at the same time allows F# to interoperate with the existing .NET languages and .NET libraries very smoothly through its object model.Scala does a similar job on the JVM that F# does on the CLR. However Scala has chosen to adopt a more Java-like syntax. This may assist in its adoption by object-oriented programmers but to a functional programmer it can feel a bit heavy. Its object model is similar to Java's allowing for seamless interoperation with Java but has some interesting differences such as support for traits.If functional programming means programming with , then Scala bends that a bit.  In Scala, if I understand correctly, you're programming with  instead of functions.When the class (and the object of that class) behind the method don't matter, Scala will let you  it's just a function.  Perhaps a Scala language lawyer can elaborate on this distinction (if it even is a distinction), and any consequences."},
{"body": "I'm very new to Scala. I have downloaded it, got it working in Eclipse where I'll be developing it; but I can't make it work in Terminal.All sites and books say to just type  - this doesn't work.The  infuriatingly says:I'm very new to this, and using Jargon or assuming too much knowledge of frameworks around Scala will ruin a good response; please keep it simple.Thank youFor OS X, I highly recommend .The  is incredibly easy.  Once installed, you just need to run  and scala will be installed and ready to go.  Homebrew also has tons of other goodies just a  away.If you don't already have Java installed, you can install that with . or  may have something similar, but I prefer Homebrew.Not a big fan of cluttering up my  variable. I just symlink all my programs to /usr/local/bin, which is in the classpath. For example, if you downloaded scala and uncompress it in /opt/scala-2.9.0-1, run the following in the terminal.Now just type  in the terminal and you're all set.\nThis way you don't have to set your  or change it when you have a new version of scala you want to try out. If you download a new version, you can uncompress it in any location and symlink the new version. Say you download version 2.9.1 and uncompress it in /opt/scala-2.9.1. You can type the following in the terminalNow, to use scala 2.9.1 you just run  at the terminal.\nWhen you are ready to switch to 2.9.1 fulltime, just update the symlink.You could also add scaladoc, scalac, scalap and others in the same wayYou need to add  to your  environment variable.On Mac OS X the path to Scala bin is usually:  and on Windows usually: .On Mac OS X use the Terminal and write (using your username):then close the Terminal and start it again.Scala recommends using Homebrew to install the Typesafe stack for Scala 2.9.2.Homebrew will install soft links in  for , , , , ,  and . Follow the soft links to its final referent in order to determine where $SCALA_HOME needs to be. $SCALA_HOME should contain  and .Typesafe recommends using  instead of  to get the interpreter going, because  will also manage library dependencies to libraries such as Akka. That said, if you want to use , , ,  and  directly, you may need to run a  on the referents of the soft links.If you don't want to add more directories to your path, try these steps:My way of making Scala run every time you type \"scala\" in Terminal was to add the path not to the .bashrc file but to the /etc/paths one.I hope this helps.If you downloaded scala with macports try typing scala-2.9 (or whatever is the filename of the contents of folder /opt/local/bin/scala/)(At least this works for OSX mountain lion)"},
{"body": "I would like to convert a  to a  or a .  Is there an easy way to do it?Assuming you're using immutable mapsThe  companion object's apply method takes repeated map entry parameters (which are instances of  of the appropriate parameter types).  produces an  (in this particular case). The  tells the compiler that the array's contents are to be treated as repeated parameters.An alternative to using  as described by sblundy is to append the existing map to an empty Here's a general way to convert between various Scala collections.See also this question describing : Is it possible to adjust the implicits such that this works? Or would this only be possible if  were added to ?Here's a way you can do it with a Scala implicit class:Since Map[A,B] has an implicit path to a TraversableOnce[Tuple2[A, B]], the following works:It will even work on a list of Tuple2s, similar to toMap:Since internal data structures in implementations are completely different, you'll have to add elements one-by-one anyway. So, do it explicitly:"},
{"body": "After years of working OK, I'm suddenly getting this message when trying to start the JVM:I tried uninstalling, and got a message saying a DLL was missing (unspecified)\nTried re-installing, all to no avail.At the same time, when trying to start Scala I get:Checked  and  - both OKCan anyone help?Might be a slightly different cause, but that second issue occurs for me in scala 2.9.0.1 on Win7 (x64), though scala-2.9.1.final has already resolved this issue mentioned here:My  set to a path like this: Note the space and the parentheses.If you change line 24 in  from:toIt works fine. Note the quotes around the set command parameters, this will properly enclose any spaces and 'special' characters (eg: spaces and parentheses) in the variable's value.Hope this helps someone else searching for an answer.I checked my environment variables - JAVA_HOME & PATH and they all refer to c:\\java. So this was bit frustrating. After sometime I found that the default installation also copied java.exe, javaw.exe and javaws.exe to C:\\Windows\\System32 (i.e. uninstall of JRE didnt went well). I just removed them and voila, I'm back on track. That annoying error is no longer popping.This works for meput  %JAVA_HOME%\\bin   at the begin of PATH.I had the same problem: I have a 64 bit Windows and when I typed \"java -version\" in CMD-Console i received the same Error message.\nTry to start a 64bit-cmd(C:\\Windows\\SysWOW64\\cmd.exe) and you will see, it works there ;)If this was working before, it means the  isn't correct anymore.That can happen when the  becomes too long and gets truncated.\nAll posts (like ) suggest , which you can test first in a separate DOS session, by setting a minimal path and see if java works again there.Finally the  concludes:scary ;)I thought I will share how I resolved the same issue \"Error Could not open lib\\amd64\\jvm.cfg\". I found the Java run time Jre7 is missing amd64 folder under lib. However, I have 1.7.0_25 JDK which is having jre folder and also having amd64.I moved the original contents of jre7 folder to a backup file and copied everything from 1.7.0_25\\jre.Now I am not getting this error anymore and able to proceed with scene builder.The Java 7 install on my work PC broke after a patch was forced out to us, giving this error any time you tried to run a Java program.  Somehow the entire 'lib' subdirectory of the Java 7 install vanished!  Might have been related to having both Java 6 and Java 7 installed -- the 'jre6' directory still had everything there.In any case, I fixed it by uninstalling both Java 6 and Java 7 and reinstalling just Java 7.  But if the file it's complaining about is actually there, then you're likely having a path issue as described in some of the other answers here.Had suddenly the same Problem, from one day to another eclipse saidafter trying to run java on the consolenow i just deleted the whole directory and everything worked again... i don't know there this jre came from, i hope it was not a virusI had a similar problem (trying to start a Jenkins slave agent on Windows) on Windows 2008R2, Java 1.7.0_15.\nI had two situations that contributed to the problem and that changing both of them fixed it:\n1) Installing Java in a unix-compatible path (changing from c:\\Program Files... to c:\\Software...); I don't think this directly affected the problem described in this thread, but noting the change;\n2) Running Java not through a shortcut. It originally failed with a shortcut, but re-running from the direct executable (C:\\Software\\Java...\\bin\\java) worked.Reinstalling java didn't help me. But the trick to put the JAVA_HOME variable at the beginning of the env-vars. The problem occoured after an upgrade from jdk1.7.0_11 to jdk1.7.0_13Another workaround is using shortpath in windows:This should work in windows 64 environment as it worked for me in win7 64bit version.I have changed the java installation path from  to another folder like  and updated the  and path values accordingly,it worked.exampleI had the same problem in  and I fixed it by changing the  from 64 bit to :Window > Preferences > Java > Installed JREs > Add... > Next > Directory > select \"C:\\Program Files (x86)\\Java\\jre1.8.0_65\" instead of \"C:\\Program Files\\Java\\jre1.8.0_60\"I had this problem after updating your java. The best way to solve this problem is just go to your  folder. There you will find two jre folders one is as jre.your version and other with exactly like jdk folder. Try to remove jre.1.your version folder. There you go your problem is solved. Hope this might help. It's worked for me.Typically it because of upgrading JRE.It changes symlinks into C:\\ProgramData\\Oracle\\Java\\javapath\\Intall JDK - it will fix this.C:\\ProgramData\\Oracle\\Java\\javapath worked for me .. , I have took back up of the files and removed the files in it.Opened new cmd prompt and then .. tested and works like charmIt wasn't in the path. Finally fixed by uninstalling java, removing all references to it from the registry, and then re-installing. None the wiser, but back working again. Thanks all @Highland Mark- Can you tell me the process to removing references from registry. I tried all possible way people mentioned here, nothing worked."},
{"body": "I wonder why  doesn't have a method  like this defined:equivalent toIs there no better than using  + ?You can do:or  But what you really want is Scalaz\u2019s   (basically a  which not only handles the  value but also maps the  part, which is what you described)defined as (where  is the pimped option value)which is equivalent to .Although I should remark that you should only use cata when it is the \u2018more natural\u2019 way of expressing it. There are many cases where a simple \u2013 suffices, especially when it involves potentially chaining lots of s. (Though you could also chain the s with function composition, of course \u2013 it depends on whether you want to focus on the function composition or the value transformation.)I personally find methods like  that take two closures as arguments are often overdoing it. Do you really gain in readability over  + ? Think of a newcomer to your code: What will they make of Do you really think this is clearer thanIn fact I would argue that neither is preferable over the good oldAs always, there's a limit where additional abstraction does not give you benefits and turns counter-productive.It was finally added , with the signature .As mentioned by Debilski, you can use Scalaz's  or . As Jason commented, named parameters make this look nice:Now, if the value you want in the  case is  for some  and you have a function  for the  case, you can do this:So,becomesPersonally, I think  should have an  method which would be the catamorphism. That way you could just do this:orIn fact, this would be nice to have for all algebraic data structures."},
{"body": "I'm replacing some code generation components in a Java program with Scala macros, and am running into the Java Virtual Machine's limit on the size of the generated byte code for individual methods (64 kilobytes).For example, suppose we have a large-ish XML file that represents a mapping from integers to integers that we want to use in our program. We want to avoid parsing this file at run time, so we'll write a macro that will do the parsing at compile time and use the contents of the file to create the body of our method:In this case the compiled method would be just over the size limit, but instead of a nice error saying that, we're given a giant stack trace with a lot of calls to  and are told that we've slain the compiler.I have  that involves splitting the cases into fixed-sized groups, creating a separate method for each group, and adding a top-level match that dispatches the input value to the appropriate group's method. It works, but it's unpleasant, and I'd prefer not to have to use this approach every time I write a macro where the size of the generated code depends on some external resource.Is there a cleaner way to tackle this problem? More importantly, is there a way to deal with this kind of compiler error more gracefully? I don't like the idea of a library user getting an unintelligible \"That entry seems to have slain the compiler\" error message just because some XML file that's being processed by a macro has crossed some (fairly low) size threshhold.Since somebody has to say something, I followed the instructions at  to try to compile the tree before returning it.If you give the compiler plenty of stack, it will correctly report the error.(It didn't seem to know what to do with the switch annotation, left as a future exercise.)as opposed towhere the client code is just that:My first time using this API, but not my last. Thanks for getting me kickstarted.Imo putting data into .class isn't really a good idea.\nThey are parsed as well, they're just binary. But storing them in JVM may have negative impact on performance of the garbagge collector and JIT compiler.In your situation, I would pre-compile the XML into a binary file of proper format and parse that. Elligible formats with existing tooling can be e.g.  or good old . Some implementations of the latter may also provide basic indexing which could even leave the parsing out - the app would just read from the respective offset."},
{"body": "Is there a simple, hassle-free approach to serialization in Scala/Java that's similar to Python's pickle?  Pickle is a dead-simple solution that's reasonably efficient in space and time (i.e. not abysmal) but doesn't care about cross-language accessibility, versioning, etc. and allows for optional customization.What I'm aware of:Kryo and protostuff are the closest solutions I've found, but I'm wondering if there's anything else out there (or if there's some way to use these that I should be aware of). Please include usage examples! Ideally also include benchmarks.Scala now has  which performs as good or better than Kyro depending on scenario - See slides 34-39 in  presentation.I actually think you'd be best off with kryo (I'm not aware of alternatives that offer less schema defining other than non-binary protocols). You mention that pickle is not susceptible to the slowdowns and bloat that kryo gets without registering classes, but kryo is still faster and less bloated than pickle even without registering classes. See the following micro-benchmark (obviously take it with a grain of salt, but this is what I could do easily):Outputs 174 bytes and 1.18-1.23 seconds for me (Python 2.7.1 on 64-bit Linux)Outputs 68 bytes for me and 30-40ms (Kryo 1.04, Scala 2.9.1, Java 1.6.0.26 hotspot JVM on 64-bit Linux). For comparison, it outputs 51 bytes and 18-25ms if I register the classes.Kryo uses about 40% of the space and 3% of the time as Python pickle when not registering classes, and about 30% of the space and 2% of the time when registering classes. And you can always write a custom serializer when you want more control.Twitter's  is just awesome. It uses Kryo for serialization but is ultra simple to use. Also nice: provides a MeatLocker[X] type which makes any X a Serializable.I would recommend . It uses implicits which are resolved at compile time, so it's very effective and typesafe. It comes with built-in support for many common Scala datatypes. You have to manually write the serialization code for your (case) classes, but it's easy to do.Another good option is the recent (2016) :For example:"},
{"body": "In Java I sometimes use class variables to assign a unique ID to each new instance. I do something likeHow can I do this in Scala?Variables on the companion object:To amplify on Thomas' answer:The  definition is usually put in the same file with the class, and  have the same name. This results in a single instance of an object having the name of the class, which contains whatever fields you define for it.A handy do-it-yourself Singleton construction kit, in other words.At the JVM level, the object definition actually results in the definition of a new class; I think it's the same name with a  appended, e.g. . Just in case you have to interoperate some of this stuff with Java."},
{"body": "Can anybody provide some details on  operator in scala. \nI think: Are there any other explanations? I see many definitions in the scala source file.  is  - it is an  and is therefore one of:In this case,  appears twice in the library, once in  as a class and once as a method on . For the method on , it checks whether the type represented by  manifest is a subtype of that represented by the manifest argument.For the type in , this is relatively new and I am also slightly confused about it because it seems to be part of a triumvirate of identical declarations!The  type is defined in  along with the related types  and  as follows:This uses the Scala feature that a generic type  can be written .  This can be used, as noted by aioobe, to provide an evidence parameter for methods that only apply to some instances of a generic type (the example given is the  method that can only be used on a  of ).  As noted in the comment, this generalizes a normal generic type constraint to allow it to refer to any in-scope abstract type/type parameter.  Using this () has the advantage over simply using an evidence parameter like () in that the latter can lead to unintended in-scope implicit values being used for the conversion.I'm sure I'd seen some discussion on this on one of the Scala mailing lists, but can't find it at the moment.I asked around, and this is the explanation I got: is typically used as an evidence parameter. For example in ,  is declared as . This expresses the constraint that the  method only works if the traversable contains 2-tuples.  is another example.  is used to express the constraint that you can only flatten a traversable of traversables.Actually, it checks if the class  by the  apple is a subclass of the class represented by the manifest fruit.For instance:Copy from scala.Predef.scala:To better understand the .I tried to devise a simpler implementation. The following did not work.At least because it won't type check in all  cases.Hmm... I can't seem to find \"<:<\" anywhere as well, but \"<:\" denotes subtyping.\nFrom  : "},
{"body": "Why should I choose Scala over another language for a new project?  In what areas does it excel in?There were a few nice answers given, sadly I could only mark one as the accepted answer.  However, overall it looks like Scala's appeal comes from two primary things:First read this: Concurrency is becoming ever-more important.  Already we have 1000 core processors being demonstrated, and what computer nowadays doesn't already come with a GPU capable of highly-parallel operations?In order to take advantage of future machines, your software has to be able to make use of all this concurrency, and Java's current threading primitives just aren't going to help here.  Let's face it, concurrent programming under current mainstream paradigms is crazy-hard.Scala is a functional language.  As such it fully embraces concepts such as immutability, for-comprehensions, closures, etc.  It also has 4 native actor libraries available (at last count).  All of this is perfectly suited to the next generation of concurrent programming.  You can't bury your head in the sand and pretend that none of this exists -  - imperative loops simply aren't going to cut it any more!Scala 2.9, due to be released early 2011, will support parallel operations such as:On top of all that, Scala can do everything that Java does, typically with a much cleaner syntax.  It's also far more object-oriented than Java (with its primitives and static methods), a fact which is often overlooked in the false belief that object orientation and functional programming are mutually exclusive.In what area does Scala excel?  I guess that area would have to be \"the future\"...My own comment on Landei's answer suggested a powerful reason to use Scala: if you want your project/team/company to appeal to forward-looking, talented engineers, instead of guys who managed to pick up some Java from a CS elective at Reed or somewhere and have been writing JSP pages for Visa International's internal HR systems for the last eight years, then you'll need a more interesting language choice.Scala runs on JVM and is fully compatible with Java. In fact, it's nearly identical to Java in terms of what you can do and how it works, so you could use Scala for anything Java would be a good choice for as well (notably: enterprise environments and cross-platform applications). According to the  it's ideal for web-services.As for what makes it better/worse than Java for any given task, well, I'll let  do the talking.Scala is clearly the best all-purpose weapon on the JVM. Scala can do everything Java can do, and almost always Scala can do it better (with very good interoperability to Java), so IMHO there is no good reason to consider Java at all for a new project.There are a few cases where dynamic typing (JRuby, Jython, Groovy) or meta-programming features (Clojure) can give you an initial edge, but there is nothing else that scales as good as Scala, and which gives you the freedom to mix paradigms as you need it, so as bigger as a project gets, as bigger gets the advantage of using Scala.[Update]\nIn response to the questions:Scala does a couple of things well:Good use cases for Scala development? Anything that deals with XML, concurrent applications, or situations where functional programming is preferable (many cases).Why should you prefer Scala over other languages? You shouldn't necessarily. I think that it's a great choice for a project that would otherwise be implemented in Java, because it has some concrete advantages over Java.A guy from Twitter actually put together , and he makes a lot of the same points I have (with a few more).Scala is the best statically typed on the JVM.That is the result of Scala's advanced type system, the orthogonality of features, the way it was written so that it could be extended seamlessly through libraries, and its easy integration with existing Java code.I could make a list of these features, but it would be meaningless without knowing the language. I could say Scala has definition-site declaration of variance with correct enforcement, but how one would measure that without having experienced it?The outcome, however, is that code written in Scala is fast, compact, reliable and indicative of the domain of the problem. That's valid on all levels too, so Scala  are also like that.Let's discuss a bit about the alternatives here, to make the distinction clear.There are non-JVM languages, but the decision on whether to go for a JVM language or not is almost completely separate from the decision about the language itself. That's a technological decision that has to be made before choosing a language. Because of that, I'm only comparing Scala to other JVM languages.Most other JVM languages are dynamically typed. While there are factors to consider when choosing between dynamically typed and statically typed languages, the one that really makes a difference in the end is personal preference. Otherwise, just refer to one of many resources about dynamically typed and statically typed languages.As far as statically typed languages on the JVM goes, Scala has the most advanced type system. In fact, as far as I know it is almost alone in having a type system which is inspired by actual type system theory instead of being hacked together based on preferences and experience of the language implementor.Java, for instance, does not have safe variance. Other languages go the opposite way of Scala, by providing \"most common\" data structures in the language itself, and making it impossible for anyone to write other complex data structures as libraries.So, if you choose JVM and statically typed, and you have the flexibility of choosing a language, then Scala is the best language for that project, whatever that project may be.For my own private projects, I already adopted Scala as a natural successor for Java (for the reasons mentioned in other answers). But I'm very hesitant to suggest that Scala development tools (especially IDEs) are fit for large-scale commercial projects. Namely the IDEs refactoring support, which in my opinion is mandatory for agile development, is not up to Java standards, yet (using current IDEA 10, nightly build plugin). In addition, I think it's quite hard for companies to come up with Scrum-sized teams of decent Scala developers. See also the video 'Sneaking Scala Into Your Organization' by David Copeland: For the time being, I can imagine mixing-in smaller Scala modules into larger Java projects."},
{"body": "In a previous question, , it seems that people had used  to figure out how to access  from Java. I would like to know how they did that. FYI, the answer is:which can be shorted to:Given this program ():I ran  on it like this:This is a portion of the  output:I also ran javap on  and . How would one figure out from the  output that:?There are rules how Scala code is compiled to JVM-bytecode. Because of potential name clashes the generated code is not always intuitive to understand but if the rules are known it is possible to get access to the compiled Scala code within Java.There are no special rules here. The following Scala classcan be accessed like a normal Java class. It is compiled to a file named :Notice, that for methods without a parameterlist an empty parameterlist is created. Multiple parameterlists are merged to a single one.For a class  Getters and Setters are created. For a class  only a Getter is created:Notice, that in Java an identifier is not allowed to include special signs. Therefore scalac generates for each of these special signs a specific name. There is a class  which can encode/decode the ops:A class  is translated by the same schema as when the field is created in the constructor. Direct access to the variable  from Java is not possible, because it is private.There is no such thing as a Scala object in Java. Therefore scalac has to do some magic. For an object  two JVM-class files are generated:  and . The first one works like an interface, it includes static methods to access fields and methods of the Scala object. The latter is a singleton class which cannot be instantiated. It has a Field which holds the singleton instance of the class, named , which allows access to the singleton:The Scala compiler automatically generates an apply-method for a case class and Getters for fields. The case class  is easily accessed:A trait , which contains only abstract members, is compiled to an interface, which is placed in a class files named . Therefore it can easily implemented by a Java class:If the trait contains concrete members there is a class file named  generated, additionally to the normal interface. The traitcan also easily implemented within Java. The class file  contains the concrete members of the trait, but it seems that they are impossible to access from Java. Neither javac nor the eclipse-javac will compile an access to this class.Some more detail about how traits are compiled can be found .Function literals are compiled as anonymous instances of the classes FunctionN. A Scala objectis compiled to the normal class-files, as describes above. Furthermore each function literal gets its own class-file. So, for function values a class file named  is generated, where N is a continuous number. For function methods (methods, which return a function) a class file named  is generated. The parts of the function name are separated by dollar signs and in front of the  identifier there are also two dollar signs. For nested functions the name of the nested function is appended to the outer function, this means an inner function will get a class file like . When an inner function does not have a name, as seen in  it gets the name .This means in our case we get:To access them use their apply-method:You should know:javap says it has a constructor and an apply method. Furthermore it says the class is abstract. Thus only the apply-method can used:It has a constructor and an apply-method (because we know Option has one and Some extends Option). Use one of them and be happy:It has no constructor, no apply-method and doesn't extend Option. So, we will take a look to :Yeah! We found a  field and the apply-method of Option. Furthermore we found the private constructor: is abstract, thus we have to use . It has an apply-method which expects an . So to get a List we need first a Seq. But if we look to Seq there is no apply-method. Furthermore when we look at the super-classes of Seq and at  we can only find an apply-methods which expects a Seq. So, what to do?We have to take a look how scalac creates an instance of List or Seq. First create a Scala class:Compile it with scalac and look at the class file with javap:The constructor is interesting. It tells us, that an array of ints is created (l. 12) which is filled with 1, 2 and 3. (l. 14-25). After that this array is delivered to  (l. 26). This resulting  is again delivered to our List (l. 29). At the end, the List is stored in the field (l. 32).\nWhen we wanna create a List in Java, we have to do the same:This looks ugly, but it works. If you create a nice looking library which wraps the access to the Scala library it will be easy to use Scala from Java.I know there are some more rules how Scala code is compiled to bytecode. But I think with the information above it should be possible to find these rules by yourself.I'm not competing with the other answer, but since people seem often not to notice this, you can do this in the repl."},
{"body": "I am in the process of migrating from Slick to Slick 2, and in Slick 2 you are meant to use the  method when projecting onto a case class (as shown here )The problem is when the case class has a companion object, i.e. if you have something like thisAlong with a companion objectIn the same scope, the  method no longer works, because its trying to run  on the , instead of the .Is there a way to retrieve the  of  rather than the , so you can call  properly?You can also writeto avoid repeating the types.This is very similar to what Alexey Romanov said, but in order to avoid lifting  whenever you need , we just add it to our companion objects.Now you can call  just like you would have if it didn't have a companion object.One workaround is define a companion object as follows:See. To build on some of the other comments you could do the following as well since tuple is calling the generated default apply method for the case class."},
{"body": "I am trying to show the SBT dependency tree as described in the :But I get this error:What is wrong? Why doesn't SBT build the tree?, each argument sent to sbt is supposed to be a command,  so will:This obviously fails, since  needs an argument. This will do what you want:If you want to actually view the library dependencies (as you would with Maven) rather than the task dependencies (which is what  displays), then you'll want to use the  plugin.Add the following to your project/plugins.sbt (or the global plugins.sbt).Then you have access to the  command, and others.If you want to view , you can use the  plugin: Output example:\n\ntext (without colors): Note that the plugin has a completely different nature than printing trees. It's designed for fast and concurrent dependency downloads. But it's nice and can be added to almost any project, so I think it's worth mentioning."},
{"body": "I'm still fairly new to Scala, and I'm discovering new and interesting ways for doing things on an almost daily basis, but they're not always sensible, and sometimes already exist within the language as a construct and I just don't know about them.  So, with that preamble, I'm checking to see if a given string is comprised entirely of digits, so I'm doing:is this sensible or just needlessly silly?  It there a better way?  Is it better just to call x.toInt and catch the exception, or is that less idiomatic?  Is there a performance benefit/drawback to either?Try this: takes a function (in this case ) that takes an argument that is of the type of the elements of the collection and returns a ; it returns  if the function returns  for all elements in the collection, and  otherwise.Do you want to know if the string is an integer?  Then  it and catch the exception.  Do you instead want to know if the string is all digits?  Then ask one of:You also may consider something like this:You could simply use a regex for this.Or simplyAnd to improve this a little bit, you can use the pimp my library pattern to make it a method on your string:@Jesper's answer is spot on.Do  do what I'm suggesting below (explanation follows)Since you are checking if a given string is numeric (title states you want a decimal), the assumption is that you intend to make a conversion if the forall guard passes.A simple implicit in scope will save a whopping 9 key strokes ;-)  The compiler will now allow  since the implicit tries to convert whatever string you use to Double, with zero guarantee of success, yikes.implicit conversions then seem better suited to situations where an acceptable default value is supplied on conversion failure (which is not always the case; therefore implicit with caution) might not performance-wise be the optimal choice, but otherwise it's neat:"},
{"body": "What is the difference between = and := in Scala?I have googled extensively for \"scala colon-equals\", but was unable to find anything definitive. in scala is the actual assignment operator -- it does a handful of specific things that for the most part you don't have control over, such as is not a built-in operator -- anyone can overload it and define it to mean whatever they like. The reason people like to use  is because it looks very assignmenty and is used as an assignment operator in other languages.So, if you're trying to find out what  means in the particular library you're using... my advice is look through the Scaladocs (if they exist) for a method named .from Martin Odersky:from Scala allows for operator overloading, where you can define the behaviour of an operator just like you could write a method.As in other languages,  is an assignment operator.The is no standard operator I'm aware of called , but could define one with this name.  If you see an operator like this, you should check up the documentation of whatever you're looking at, or search for where that operator is defined.There is  you can do with Scala operators.  You can essentially make an operator out of virtually any characters you like. performs assignment.   is not defined in the standard library or the language specification.  It's a name that is free for other libraries or your code to use, if you wish."},
{"body": "In Scala variables are declared like:Where the type (Double) follows the identifier (stockPrice). Traditionally in imperative languages such as C, Java, C#, the type name precedes the identifier.Is it purely a matter of taste, or does having the type name in the end help the compiler in any way? Go also has the same style.As well as supporting type inference, this has an ergonomic benefit too.For any given variable name + type, chances are that the name is the more important piece of information.  Moving it to the left makes it more prominent, and the code more readable once you're accustomed to the style.Other ergonomic benefits:Kevin's got it right. The main observation is that the \"type name\" syntax works great as long as types are short keywords such as  or :For the price of one you get two pieces of information: \"A new definition starts here\", and \"here's the (result) type of the definition\". But we are way past the area of simple primitive types nowadays. If you writethe most important part of what you define (the name) is buried behind the type expression. Compare withIt says clearlyIt's afterwards so that it can be removed for type inference:However, it is  true that imperative languages traditionally have types first. For example, Pascal doesn't.Now, C does it, and C++, Java and C# are based on C's syntax, so naturally they do it that way too, but that has absolutely nothing to do with imperative languages.It should be noted that even C doesn't \"traditionally\" define the type before the variable name, but indeed allows the declarations to be interleaved.where the type for foo is declared both before and after it, lexically.Beyond that, I'm guessing this is a distinction without a difference.  The compiler developers certainly couldn't care one way or another."},
{"body": "There are more and more programming languages (Scala, Clojure,...) coming out that are made for the Java VM and are therefore compatible with the Java Byte-Code.I'm beginning to ask myself: Why the Java VM?That one is easy:The JVM is rock solid and works from the tiniest Java SmartCard (ok, a tiny VM ;) to the biggest clustered super-computers you can imagine.Because it is rock solid, there are entire countries where people have:I'm developping commercial software in Java: we're selling on Windows and on OS X but we're all developping on Linux. And it just works. And it works on Solaris too, etc.  And that's because we're targetting the JVM: goodbye portability issue. As long as the platform has a VM, the software shall work.Then I can't believe anyone mentioned it yet: it is a very good start from a security point of view.The Java VM is, by design, immune to buffer overrun/overflow. This is . This is actually .The only \"Java\" buffer overflow that I remember on Linux (that prompted me to upgrade due to security concern) was actually a buffer overflow leading to arbitrary code execution in... a C written lib (zlib if I remember correctly, back in the days Java on Linux was still defaulting to that lib).Sure, for webapps buffer overrun/overflow leading to arbitrary code execution are not the most important vector of attack anymore (now that XSS and SQL injection have stolen the show).  But in all the other cases buffer overrun/overflow are  source of most security issues.The JVM is immune to that.In addition to that thanks to its design it's easy to plug various tools on it like profilers and debuggers.It is a very solid and secure (when used correctly) technology. That is why it is so widely used by both tech-savvy (Java is  at Google, from GMail to their Android to GWT etc.) and tech-not-that-savvy companies.It is actually arguable that Java's success comes from the JVM and that \"Java the VM\" is much more important than \"Java the language\".The JVM is the biggest \"language\" success story of these last 20 years. And it is deserved. And it is here to stay :) Because somebody else has already taken the trouble to make sure it runs well on every major platform.If you write your own VM, you have to write one for Linux/Unix/BSD, one for Mac, and one for Windows, and you have to support the VM on all those platforms as well as supporting your language compiler and runtime libraries.  If you use the Java VM, the first part is taken care of for you by Oracle, IBM and Apple.Because with almost 20 years of effort the Java VM isI mean, if i would plan to release a new language why shouldn't I use the JVM? Ok, there are others vms (llvm, parrot) but the JVM is estensively supported, known, and tested.And it works also quite good! The lack of  will be solved with the relase of Java7 so better than this..It's free & open, mature (stable, complete), and the surrounding ecosystem is huge.Writing a VM as good as the JVM is hard. JVM's spec and its implementations have been refined for well over a decade by the finest minds in the industry.Second question first: Why reinvent the wheel? There are multiple implementations of the Java VM, including highly-optimized ones, and there's at least one available for darned near any platform out there.First question second: There is a  amount of third party library and tool support for code compiled to Java bytecode and run on the VM. When you create a new language, one big issue is that you have to create everything that people can use with it, or make it compatible with something else (like linking to existing static C libraries, or .Net, or COM, or the Java VM). The Java VM's open specification (erm, so far), deep history, and broad applicability make it a nice target for new languages.Obvious reasons:Because its not a Microsoft technology.  As others have mentioned, there are a lot of great reasons to use the JVM, but I think it would be slightly less popular if Microsoft's CLR was as free, as portable, and as open."},
{"body": "So I was just learning new Java 8, specially lambdas and date and time api. I was comparing it with scala. My basic idea was to find the execution time difference between imperative, Stream and parallel stream. So I decided to create a Library application and do some operations like searching, filtering, sorting etc. I created a Library class with a list field called books and populated it with 1000 books. Then created a functional interface for search and did some operation in all three styles. It all worked fine. My code is:Now I tried to re-create the same program in scala and see whether the execution is faster or not? Surprisingly scala didn't allow me to do parallel sorting (Or may be I'm being ignorant here). My scala library isHere the  gives a ParSeq. This doesn't have a sort method. Is there any way I could create a parallel sort with my books list in scala. So I could write something like:Your help is much appreciated. Thanks.One can implement a parallel sort in scala e.g.  . I don't know why ParSeq doesn't offer sorting in its API. Note that there are a number of alternatives to , so don't necessarily assume that your results from ParSeq are representative of Scala parallelism in general. (But benchmarks are valuable and I wish you well with yours).i am very much new to scala but as far as my understanding goes:  the sort itself will never be parallel, only the conversion to and from the Java array might.\nsorted is provided by scala.collection.SeqLike only \nTry this link:\nGitHub.Com/Scala/Scala/blob/master/src/library/scala/collection/\u2026"},
{"body": "Scala 2.11 is out and the 22 fields limit for case classes seems to be fixed (, ).This has been an issue for me for a while because I use case classes to model database entities that have more than 22 fields in Play + . My solution in Scala 2.10 was to break the models into multiple case classes, but I find this solution hard to maintain and extend, and I was hoping I could implement something as described below after switching to Play 2.3.0-RC1 + Scala 2.11.0:The code above fails to compile with the following message for both the \"Reads\" and the \"Writes\":Updating the \"Reads\" and \"Writes\" to:Also doesn't work:My understanding is that Scala 2.11 still has some limitations on functions and that something like what I described above is not possible yet. This seems weird to me as I don't see the benefit of lifting the restrictions on case classes if one it's major users cases is still not supported, so I'm wondering if I'm missing something.Pointers to issues or implementation details are more than welcome! Thanks!This is not possible, out of the box, for several reasons:However it is possible to bypass the second point by either:First, creating the missing :and then by providing your own  instance:Finally, regarding the first point, I have sent a  to try to simplify the current implementation.We were also breaking our models into multiple case classes, but this was quickly becoming unmanageable.  We use  as our object relational mapper, and Slick 2.0 comes with a  that we use to generate classes (which come with apply methods and copy constructors to mimic case classes) along with methods to instantiate models from Json (we do not automatically generate methods to convert models into Json because we have too many special cases to deal with).  Using the Slick code generator does not require you to use Slick as your object relational mapper.This is part of the input to the code generator - this method takes a JsObject and uses it to either instantiate a new model or update an existing model.For example, with our ActivityLog model this produces the following code.  If \"original\" is None then this is being called from a \"createFromJson\" method and we instantiate a new model; if \"original\" is Some(activityLog) then this is being called from an \"updateFromJson\" method and we update the existing model.  The \"condenseUnit\" method being called on the \"val errs = ...\" line takes a Seq[Try[Unit]] and produces a Try[Unit]; if the Seq has any errors then the Try[Unit] concatenates the exception messages.  The parseJsonField and parseField methods are not generated - they're just referenced from the generated code.You can use Jackson's Scala module. Play's json feature is built upon Jackson scala . I don't know why they put a 22 field limit here while jackson supports more than 22 fields. It may make sense that a function call can never use more than 22 parameters, but we can have hundreds of columns inside a DB entity, so this restriction here is ridiculous and makes Play a less productive toy.\ncheck this out:I'm making a library. please try this cases where case classes might not work; one of these cases is that the case classes cannot take more than 22 fields. Another case can be that you do not know about schema beforehand. In this approach, the data is loaded as an RDD of the row  objects. Schema is created separately using the StructType and StructField  objects, which represent a table and a field respectively. Schema is applied to the row RDD to create .I tried Shapeless \"Automatic Typeclass Derivation\" based solution proposed in another answer, and it didn't work for our models - was throwing StackOverflow exceptions (case class with ~30 fields and 4 nested collections of case classes with 4-10 fields).So, we've adopted  solution and it worked flawlessly. Confirmed that by writing ScalaCheck test. Notice, that it requires Play Json 2.4."},
{"body": "I hear .sbt files have been improved in various ways in 0.13, and that now I can specify multi-project builds in them. mentions that we can now define subprojects in a .sbt file. I also know that multiple .sbt files in the root will be aggregated into a single conceptual file.What I'd really like, though, is to not pollute my root with a dozen subproject .sbt files. Is there a way I can throw the subproject build.sbt files into their respective subdirectories, keep some common code between them somewhere shared, and then have a root build.sbt for the entire project that aggregates the subprojects? I have a similar setup in .scala files right now but would prefer to use .sbt files if possible.If that isn't possible, what is the \"correct\" way to construct large multi-project builds with .sbt files?It should already be the case in 0.12 that you can put  files in the base directory of a subproject and the settings there will be included in that project's scope.Code is reused between  files by creating a normal  file in .  The code in  will be available for use in the  files.  The definitions in one  are not visible to other  files, at least in 0.13.  This is mainly an implementation restriction and it is undetermined whether this will be lifted in future versions.The default root project will aggregate all subprojects, including those coming from projects defined in .The current difficulty is making it explicit.\nFor example, the following  in the root directory would define a subproject in .\nThis is a full definition, defining the ID, base directory, etc... for the project.However, it cannot reference anything defined in .  (The existence of  is not known until after  is compiled and evaluated.)\nSo, to explicitly define what  aggregates, you'd need something like:However, this duplicates the definition of .A possible solution going forward is to make the root definition just a reference, like:"},
{"body": " .I have made this question community wiki and invite others to keep it updated.  Is anyone aware of benchmarks of Clojure's performance?  I have done some of my own (although nothing too formal) and it didn't fair too well in comparison to other functional languages (tried Haskell and OCaml).  But how does it look compared to Java or another language on the JVM (e.g. Scala)?  And how does it compare to other Lisps?There was some  on the  forum about adding Clojure on there, but nothing has been done yet.   I'm going to continue to add to this as I find more:@igouy pointed out that the benchmark scripts for clojure  by jafingerhut on github.Two very relevant threads from the Clojure discussion group:And separately, these blog posts:And lastly, a related question on stackoverflow:Most of these discussions lead me to think that Clojure's performance is very favorable in comparison to other languages running on the JVM, although there is no doubt that it can be very difficult to come to a broad conclusion when comparing languages because their performance can vary dramatically dependent on the task.Lau Jensen just posted a great discussion about benchmarking with JVM languages on his blog: .See     the current clojure implementation has not been focussed on performance, but the next version supposedly will.This is an important question that just about everyone thinks about before considering clojure. Its also a hard question even for mature languages that are not adding things, like chunked sequences, that radically change the performance of some specific (though common) tasks. I found some good thoughs in . Many of the benchmarks you find will be related to previous versions of both java and clojure so its unlikely that anyone can find \"really good benchmarks\". I great question to ask your self here is . This is a precondition to clojure being fast enough. If you can convince your self that the answer to this question is yes then it is safe to proceed in Clojure and implement the parts that your profiling identifies as bottle necks in Java. Because you have a failback language with well known performance It will generally be safe to go with Clojure. For performance questions please refer to this blogpost:This shows a Clojure implementation of the WideFinder2 challenge which is faster than both Java, Scala and single threaded C. Compare with official times.Regarding Daniels remark that Clojure will never be faster, we see that its obviously incorrect based on the results above. Mutability is faster than immutability which is Clojures default, yet Clojure allows for local transients (ie. temporarily mutable data), so that one can achieve optimal speed.Refer to clj-me.cgrand.net for many optimization techniques.In conclusion: Clojure can be as fast as you would like it to be while still allowing you to maintain a simple elegant and robust codebase, almost a unique combination.You might also be interested in the  serie from Tim Bray. He discusses some performance issues.Clojure will never be able to match a Scala program that takes full advantage of mutability in an algorithm that's benefitted by it. There's also the fact that Clojure is a dynamic language, which, presently, isn't very well supported by JVM.On the other hand, Clojure excels at enabling parallel, asynchronous and distributed algorithms, and immutable algorithms generally speaking.So, if you want (mostly) immutability and multicore efficiency, Clojure will make those much easier to achieve. If your algorithms really, really need to heavily use mutability for efficiency, then Scala will make those easier.Anything in between, it likely won't matter either way."},
{"body": "I've searched for the use of  in the source code of the standard library of Scala 2.8.1. It looks like only a handful of traits and classes use this annotation: , , , , , , , , , .None of the collection classes are . Why not? Would this generate too many classes?This means that using collection classes with primitive types is very inefficient, because there will be a lot of unnecessary boxing and unboxing going on.What's the most efficient way to have an immutable list or sequence (with  characteristics) of s, avoiding boxing and unboxing?Specialization has a high cost on the size of classes, so it must be added with careful consideration. In the particular case of collections, I imagine the impact will be huge.Still, it is an on-going effort -- Scala library has barely started to be specialized.Specialized can be expensive (  ) in both size of classes and compile time. Its not just the size like the accepted answer says. Open your scala REPL and type this. Sorry :-). Its like a compiler bomb.Now, lets take a simple traitThe above will result in two compiled classes. Foo, the pure interface and Foo$1, the class implementation.Now, A specialized template parameter here gets expanded/rewritten for 9 different primitive types ( void, boolean, byte, char, int, long, short, double, float ). So, basically you end up with 20 classes instead of 2.Going back to the trait with 5 specialized template parameters, the classes get generated for every combination of possible primitive types. i.e its exponential in complexity.2 * 10 ^ If you are defining a class for a specific primitive type, you should be more explicit about it such asUnderstandably one has to be frugal using specialized when building general purpose libraries. is Paul Phillips ranting about it. Partial answer to my own question: I can wrap an array in an  like this:(Ofcourse you could still modify the contents if you have access to the underlying array, but I would make sure that the array isn't passed to other parts of my program)."},
{"body": "I'm using an external library written in Java (). One of the function calls has the signature , and I keep getting compiler errors when trying to call it from Scala, that is:Is there a workaround for this issue?"},
{"body": "Should my custom exception types be es? On the plus side, I get extractors. On the minus side, I get incorrect equality semantics. But I can avoid that by overriding . So does it make sense, at a conceptual level, to make them es?This is very subjective of course, but in my opinion it is good practice to have exception classes as case classes.\nThe main rationale being that when you catch an exception, you are doing pattern matching, and  case classes are much nicer to use in pattern matching.\nHere's an example that takes advantadge of the ability to use the full power of pattern matching in a catch block, when using a case class exception:In this example, we have only one exception, but it contains an  field for when you want to know the exact error type that occured (usually this is modelled through a hierarchy of exception, I'm not saying this is better or worse, the example is just illustrative). Because the  is a case class I can simply do  to catch the exception only for the error type . Without the extractors that we get for free with case classes, I would have to catch the exception everytime, and then retrow in case I'm actually not interested, which is definitly more verbose.You say that case classes give you incorrect equality semantics. I don't think so. , as the writer of the exception class gets to decides what equality semantics makes sense. After all when you catch an exception, the catch block is where you decide which exceptions to catch usually based on the type alone, but could be based on the value of its fields or whatever, as in my example. The point is that the equality semantics of the exception class has little to do with that.One common idiom you lose by making exceptions case classes is the pattern of creating a subclass hierarchy of exceptions with subclassing used to indicate greater specificity of the error condition.    Case classes can't be subclassed.I like the answer from R\u00e9gis Jean-Gilles. But if you have a good reason to not make a case class (see answer of Dave Griffith), you can archieve the same as in the sample above with a normal class and unapply:"},
{"body": "Let's say I store bank accounts information in an immutable : and I want to withdraw, say, $50 from Mark's account. I can do it as follows: But this code seems ugly to me. Is there better way to write this? There's no  in the  API, unfortunately. I've sometimes used a function like the following (modeled on Haskell's , with a different order of arguments):Now  does what you want. You could also use the  to get the more natural  syntax, if you really wanted something cleaner.(Note that the short version above throws an exception if  isn't in the map, which is different from the Haskell behavior and probably something you'd want to fix in real code.)This could be done with . The very idea of a lens is to be able to zoom in on a particular part of an immutable structure, and be able to 1) retrieve the smaller part from a larger structure, or 2) create a new larger structure with a modified smaller part. In this case, what you desire is #2.Firstly, a simple implementation of , stolen from , stolen from scalaz:Next, a smart constructor to create a lens from \"larger structure\"  to \"smaller part\" . We indicate which \"smaller part\" we want to look at by providing a particular key. (Inspired by what I remember from ):Now your code can be written:n.b. I actually changed  from the answer I stole it from so that it takes its inputs curried. This helps to avoid extra type annotations. Also notice , because remember, our lens is from  to . This means the map will be unchanged if it does not contain the key . Otherwise, this solution ends up being very similar to the  solution presented by Travis.An  proposes another alternative, using the  operator from scalazThe  operator will sum the values of an existing key, or insert the value under a new key."},
{"body": "Related questions: I have a  (more than 5 millions items) and I need to get  items from it. The most natural way to do it is to use heap/priority queue . There are several good implementations of priority queue for JVM (Scala/Java), namely: First 2 are nice, but they store all the items, which in my case gives critical memory overhead. Third (Lucene implementation) doesn't have such a drawback, but as I can see from documentation it also doesn't support custom comparator, which makes it useless for me. So, my question is: Is there a  with  and ? Finally I've created my own implementation, based on Peter's answer:(where  is taken from  question)You could use a SortedSet e.g. TreeSet with a custom comparator and remove the smallest when the size reachs N.How can you say Lucene's doesn't support a custom comparator?Its abstract and you must implement the abstract method Though an old question but it may be helpful to somebody else.\nYou can use  of Google's Java library guava. I can't think of a ready-to-use one, but you can check  of this collection with similar requirements.The difference is the comparator, but if you extend from  you'll have it. And on each addition check if you haven't reached the limit, and if you have - drop the last item. Below is the implementation I used before. Complies with Peter's suggestion.I would appreciate any feedback btw. It seems like using a  is not very efficient after all because the calls to  seem to take sublinear time. I changed the  to a . The modified  method looks like this:Exactly what I was looking for. However, the implementation contains a bug:Namely: if elementsLeft > 0 and e is already contained in the TreeSet.\nIn this case, elementsLeft is decreased, but the number of elements in the TreeSet stays the same. I would suggest to replace the corresponding lines in the add() method byTry this code:Here is one I put together if you have guava. I think it is is pretty complete. Let me know if I missed something. You can use the gauva ForwardingBlockingQueue so you don't have to map all the other methods. Create a PriorityQueue that has size limit. It stores N max numbers."},
{"body": "Given that it's impossible to see into the future, what factors related to Clojure, Scala or Haskell are likely to determine whether one of them catches on?Are there cultural or economic issues that could give one of these languages an advantage over the others?Or are none of these languages likely to gain traction because of their conceptual complexity?On the top of my head:Scala, for instance, is still evolving and moving \"too fast\" to be \"largely\" used even though some big projects have already adopted it.Edit: November 2009See  presentation, Slide 10 and 11:From the Haskell world, I see the main things to continue to push on as:Besides this, it is hard to say. Some random thoughts: Haskell's been around for 20 years, has a very large user base, and plenty of commercial support. Clojure is tiny in comparison. Scala and Clojure get \"free points\" by running on .NET or JVM. Does that matter? How much does the runtime matter? GHC has a very fast custom parallel runtime, because it isn't the JVM, but people like to use the JVM. Same for .NET. Does maturity/stability matter?And on top of all of this, who is doing the best outreach?Oh, and we have .I think that to break into the  (i.e. C, C#, C++, Java) they need  which do in-house development but . I'm think of large banks, insurance companies, service-companies, management consultancies etc.However, there is a big barrier to this acceptance these days; namely ,  and . Without a large company like Sun, IBM or Microsoft providing support it's going to be very difficult to persuade companies like these that any new language is a .Without persuading these companies, the market for developers familiar with the languages will be small. As long as there is a small base of users, the language can afford to make backwards-incompatible changes requested by the community. Hence a vicious cycle of non-mainstream-adoption.Some of these languages may easily gain acceptance first in non-commercial and open source environment. More or less what happened to Perl, Python, Ruby (and some other languages).Ease of deployment of the software (think apt-get) and freedom of the programmer promotes language diversity in the open source world. Once some of the software becomes sufficiently important, the language becomes immortal in terms of support. Once its immortal, it is a safe bet for everyone. Small developers will be first, and if they gain some advantages from using the language, big companies will follow.So, it's a matter of which community is more friendly to open source (teaching, documentation, infrastructure) and which language allows a programmer to be more productive.OK, I'll take a wild guess. I think the factor needed for success is \"can it do Java's job?\".The real thing that Java does is static typing. While this is annoying for small programs it allows the construction of large stable systems. It allows refactoring with confidence. Think about it: every language has kind of a maximum size program that it supports. To go over that size requires increasingly above-average design and implementation.As a secondary consideration, Java is quite fast and has a rather complete library.So, I'm guessing those are the criteria, and so Scala has a chance.The only negative is the difficulty of running cheap shared server web hosts with the JVM.Backing by a large software development company.  Look at what it took Java and C# to get where they are.  That is what it will take for others to get there as well.One cultural factor is how similar a language is to existing popular languages. For example, the evolution of C -> C++ -> Java -> C#. Haskell has the largest gap from the mainstream, with an unfamiliar syntax, runtime stack, programming paradigm and a community oriented much more towards academia than industry.In my humble opinion, the main factor regards the outcome of the research aiming at providing implicit parallelism in purely functional languages. If/when this will work, then Haskell is likely to become the mainstream.As for Scala and Clojure... Scala has the historically more acceptable syntax. That's an overkill. Some can argue that Clojure has a macro system, but macros are expressively equivalent to clojures, so this is not a real advantage.Anyway, I believe that statically-typed languages are not appropriate for all the areas, namely Web Development. And curiously nowadays there's a tendency to make everything web-based. I do not think that Ruby was a hype: It is maturing very fast.Haskell or Ruby will sweep evryone in the world. Chances are more for Ruby to be successful since Haskell is complex and difficult to learn.Ruby 2.x will become father of 'Scala and Clojure'. It will have new version with high speed, GIL removed and built-in modules for Parallel processing, functional programing, macros. Thinking further, more libraries are to be built in ruby for various tasks. This will be done once the version 2.X will come with all the above mentioned feechurs.I believe Matz and Antonio !!!!! C++, Java, .NET, Python, Scala will be seen in musuems. XXXXX \nC, Perl and PHP will still live for small apps/tasks.  \nMySQL, Oracle, SQLServer will also be seen in 'db section' of museum. XXXXX \nSQlite, PostgreSQL will be still alive outside.  Cheers,\nUr man"},
{"body": "In sbt 0.10.1, I frequently use  to narrow down the number of my tests.However, I want to narrow down such that I only run tests whose name/description matches a regular expression. Is there some syntax to achieve something like this?Full regular expressions are not supported by .  Wildcards are supported, however.Only the asterisk  is interpreted specially here and not the periods.  This will select all tests beginning with  and ending with .Or just all test s: and other testing information is documented here: You can match on test  by their name (instead of or in addition to suite class names) by using . ScalaTest  with the  argument:This runs all tests with \"insert\" in their name, then only the matching cases within suites ending in , as you would intuit. You can also use  and  to include or exclude, respectively, tags from ScalaTest's tagging support, and  to match an exact test name.Specs2  with an  argument: and  support Spec2's tagging features.See the inline links for full lists of arguments that the runners support. These appear to only work with the  sbt command and not ."},
{"body": "I have a small script that's written in Scala which is intended to load a MongoDB instance up with 100,000,000 sample records. The idea is to get the DB all loaded, and then do some performance testing (and tune/re-load if necessary).The problem is that the load-time per 100,000 records increases pretty linearly. At the beginning of my load process it took only 4 seconds to load those records. Now, at nearly 6,000,000 records, it's taking between 300 and 400 seconds to load the same amount (100,000)! That's two orders of magnitude slower! Queries are still snappy, but at this rate, I'll never be able to load the amount of data that I'd like.Will this work faster if I write a file out with all of my records (all 100,000,000!), and then use  to import the whole thing? Or are my expectations too high and I'm using the DB beyond what it's supposed to handle?Any thoughts? Thanks!Here's my script:Here's the output from my script:Some tips :I've had the same thing.  As far as I can tell, it comes down to the randomness of the index values.  Whenever a new document is inserted, it obviously also needs to update all the underlying indexes.  Because you're inserting random, as opposed to sequential, values into these indexes, you're constantly accessing the entire index to find where to place the new value.This is all fine to begin with when all the indexes are sitting happily in memory, but as soon as they grow too large you need to start hitting the disk to do index inserts, then the disk starts thrashing and write performance dies.As you're loading the data, try comparing  with the available memory, and you'll probably see this happen.Your best bet is to create the indexes  you've loaded the data.  However, this still doesn't solve the problem when it's the required _id index that contains a random value (GUID, hash, etc.), then your best approach might be to think about sharding or getting more RAM.What I did in my project was adding up a bit of multithreading (the project is in C# but I hope the code is self-explanatory). After playing with the necessary number of threads it turned out that setting the number of threads to the number of cores leads to a slightly better performance(10-20%) but I suppose this boost is hardware specific. Here is the code:Another alternative is to try . They use Fractal Indexes which means that .TokuMX is going to be included as a custom storage driver in an upcoming version of MongoDB.The current version of MongoDB runs under Linux. I was up and running on Windows quite quickly using ."},
{"body": "How can I code my bundle in Scala and then deploy it into OSGI container?Do I compile it into \"java\" first or can i deploy scala straight into OSGI and use some kind of bundles to recognize it?Any pointers would be great.\nCurrently I am using Apache Felix as my osgi-container, but just a simple explanation of generic concepts would suffice to get me started.: Code in scala, deploy to OSGi.A quick intro video by the author here OSGi does not care what language you write your code in: JVM bytecode is just JVM bytecode. So:You'll notice that your bundle has dependencies on the Scala library. Again there is nothing strange about this, it's just like having dependencies in you Java code. In order for those dependencies to resolve, you need to install the Scala library bundle from There is nothing special to it: write your code in Scala and wrap it up as an OSGi bundle by providing the necessary bundle meta data and service descriptors as you would do with Java.  can help you in the process. Have a look at the  (a Scala script engine) for a working example. The  in the  takes care of generating and including the bundle meta data in the final jar file. It refers to the  xml file which you need to provide."},
{"body": "Is there any equivalent in scala parallel collections to LINQ's  which sets the number of threads which will run a query? I want to run an operation in parallel which needs to have a set number of threads running.With the newest trunk, using the JVM 1.6 or newer, use the:This may be a subject to changes in the future, though. A more unified approach to configuring all Scala task parallel APIs is planned for the next releases.Note, however, that while this will determine the number of processors the query utilizes, this may not be the actual number of threads involved in running a query. Since parallel collections support nested parallelism, the actual thread pool implementation may allocate more threads to run the query if it detects this is necessary.EDIT:Starting from Scala 2.10, the preferred way to set the parallelism level is through setting the  field to a new  object, as in the following example:While instantiating the  object with a fork join pool, the parallelism level of the fork join pool must be set to the desired value ( in the example).Independently of the JVM version, with Scala 2.9+ (introduced parallel collections), you can also use a combination of the  and  functions to execute parallel jobs on small chunks, like this: creates chunks of length 2 or less,  makes sure the collection of chunks is not parallel (useless in this example), then the  function is executed on the small parallel chunks (created with ), thus insuring that at most 2 threads is executed in parallel.This might be however slightly less efficient than setting the worker pool parameter, I'm not sure about that."},
{"body": "Anyone know of a good scala library to do whitespace removal/compaction from XML?to:scala.xml.Utility.trim() should do what you want:For whatever it's worth, this is what I've got going on now in the \"roll my own\" strategy:"},
{"body": "Here are two solutions to exercise 4.9 in Cay Horstmann's Scala for the Impatient: \"Write a function lteqgt(values: Array[Int], v: Int) that returns a triple containing the counts of values less than v, equal to v, and greater than v.\" One uses tail recursion, the other uses a while loop. I thought that both would compile to similar bytecode but the while loop is slower than the tail recursion by a factor of almost 2. This suggests to me that my while method is badly written.Adjust the size of bigArray according to your heap size. Here is some sample output:Why is the while method so much slower than the tailrec? Naively the tailrec version looks to be at a slight disadvantage, as it must always perform 3 \"if\" checks for every iteration, whereas the while version will often only perform 1 or 2 tests due to the else construct. (NB reversing the order I perform the two methods does not affect the outcome). (after reducing array size to 20000000)Under Java  I get  for tail-recursion and while-loop respectively.Under Java  I get    So under Java 6 your while-loop is actually faster; both have improved in performance under Java 7, but the tail-recursive version has overtaken the loop.The performance difference is due to the fact that in your loop, you conditionally add 1 to the totals, while for recursion you always add either 1 or 0. So they are not equivalent. The equivalent while-loop to your recursive method is:and this gives exactly the same execution time as the recursive method (regardless of Java version).I'm not an expert on why the Java 7 VM (HotSpot) can optimize this better than your first version, but I'd guess it's because it's taking the same path through the code each time (rather than branching along the  /  paths), so the bytecode can be inlined more efficiently.But remember that this is not the case in Java 6. Why one while-loop outperforms the other is a question of JVM internals. Happily for the Scala programmer, the version produced from idiomatic tail-recursion is the faster one in the latest version of the JVM.The difference could also be occurring at the processor level. See , which explains how code slows down if it contains unpredictable branching.The two constructs are not identical.  In particular, in the first case you don't need any jumps (on x86, you can use cmp and setle and add, instead of having to use cmp and jb and (if you don't jump) add.  Not jumping is faster than jumping on pretty much every modern architecture.So, if you have code that looks likewhere you  add or you  jump instead, vs.(which only makes sense in C/C++ where 1 = true and 0 = false), the latter tends to be faster as it can be turned into more compact assembly code.  In Scala/Java, you can't do this, but you  dowhich a smart JVM should recognize is the same as x += (a < b), which has a jump-free machine code translation, which is usually faster than jumping.  An even smarter JVM would recognize thatis the same yet again (because adding zero doesn't do anything).C/C++ compilers routinely perform optimizations like this.  Being unable to apply any of these optimizations was not a mark in the JIT compiler's favor; apparently it can as of 1.7, but only partially (i.e. it doesn't recognize that adding zero is the same as a conditional adding one, but it does at least convert  into fast machine code).Now, none of this has anything to do with tail recursion or while loops per se.  With tail recursion it's more natural to write the  form, but you can do either; and with while loops you can also do either.  It just so happened that you picked one form for tail recursion and the other for the while loop, making it look like recursion vs. looping was the change instead of the two different ways to do the conditionals."},
{"body": "What is the Scala's way to write the following code:I.e. what is the idiomatic way of executing the same piece of code based on multiple case values? Doesn't quite seem do the trick, since Scala evaluates the match value based on the first matching case, i.e. if i=2 the code will return nothing.Thanks for help!According to  there is no fallthrough, but you can make use of .This should do the trick:Case statements can actually include additional logic guards using a standard if statement.  So you could do something like:The matching guards can be any boolean function or composition of functions, so it gives it a lot more power than the standard switch statement in Java.While not applicable here, for more complex problems you can 'fallthrough' in a sense using the andThen function on partial functions.If you are dealing with actual classes (instead of strings or ints), you need  before each class to make them into a pattern before joining them with .You get nice exhaustive checking too! Note that you cannot do case class destructuring when using alternative pattern matching (e.g.  will fail to compile."},
{"body": "I am teaching Scala for the first time and my students are finding the deliberate \"punning\" involved in companion objects very confusing.  Consider the following example:The confusion comes in when I use verbal phrases such as \"a stack object\" or \"stack objects\" or especially \"the stack object\".  My students have difficulty understanding whether I mean the singleton object Stack or objects of the Stack class.I'm looking for alternative ways to verbalize such things that beginners can understand more easily.  I've considered always referring to objects of the Stack class as \"Stack instances\" or \"instances of Stack\", but it seems crazy when trying to teach OO not to be able to call these things objects.  I've been trying to ALWAYS use the phrase \"singleton object\" or \"companion object\" when talking about the singleton object Stack, but the syntax of Scala is working against me there because it only uses the word \"object\".In this case, I could rename the singleton object StackFactory instead of Stack, but that's only an option for my own classes, not for the thousand and one companion objects already built into Scala.Edit: Sorry, I wasn't clear enough in my question.  The main confusion happens NOT when referring to the companion object.  In that case, as several people have noted, it is easy enough to use to a phrase such as \"the companion object\".  Instead, the main confusion happens when referring to ordinary instances.  Then, if I say \"a stack object\" (meaning some stack instance) or \"the stack object\" (meaning  particular instance), some fraction of the students will think I mean the companion object -- even though I did not use the word companion or singleton.And I can see perfectly well where the confusion comes from, because the word \"object\" appears  only with the companion object.This issue in Scala is certainly not without irony, but even in the Java space there can be naming issues regarding , its instances, its class and instances of its class.For Scala I propose the following terminology:In the end, I would say that the idea of OO includes objects (in the sense of modules and composition) as much as objects (in the sense of instances).Regardless of how you decide, consistency is key, especially for teaching.I think  (if it stands alone) or  (if it comes with a class) is the best way to name . The Scala Language Reference calls them modules. But modules have by now been too much associated with run-time entities (as in OSGI) to be comfortable with that.I think using the phrase \"companion object\" should be clear enough. I'd go forI think your solution sounds good. \"The  singleton\" very clearly denotes the fact that you're talking about a singleton object rather than a whole class of objects. As for the names working against you, explain that  declares a  object (singleton), whereas  is a template for a whole  of objects (class instances).One other thing you should probably make sure is that   . If the students don't understand that point then they'll probably have a harder time making the distinction between the singleton  and instances of .Note that if you rename the object then it's no longer a companion object. Classes and their companion objects have special permissions granted by the Scala compiler to access each other's private members. If you change the name of the singleton so that it doesn't match the class then there's no longer any indicator to the compiler that it's a companion object and it loses the companion privileges.If you're really good about being consistent you can also refer to the instances of  as \"Stack instances\", but that might require a bit much habit twisting. Other than that, \"the Stack companion object\" or \"object Stack\", never \"the Stack object\". Maybe my proposal is a bit naive, but I would just call(In fact, in Odersky's \"programming scala\" the term most often used is \"companion object\", which I think is pretty clear...)I would also stress the difference between a stack instance and the stack companion, it's a common source of confusion.and regarding the confusion of your students, I would empathize how important is for a university student to be careful and consistent with the use of terminology, you could also provide some quiz or exercises to tell the difference between each of these concepts.Sometimes terminology is better learnt by using it.If you want a singleton object to be a companion of a class or trait, it  have the same name (and package and source file).Companions can access each others private members, e.g. a factory in the companion object can still create instances of a (companion) class with a private constructor."},
{"body": "I have a Boolean and would like to avoid this pattern:What I'd like to do is Any suggestions with a code example would be much appreciated.Scalaz has a way to do it with .\nThat would allow you to write :If you don't want to add a Scalaz dependency, just add the following in your code : Option().collect() is a good pattern for this kind of thing.from the REPL:None of the other answers answer the question as stated!  To get the exact semantics you specified use:You can then writeeg:Here, the guard holds the condition and the value passed to  is the value returned if the guard evaluates to true.Another choice:Usage:If you don't mind  being evaluated no matter the value of  you can also useThis would give you:"},
{"body": "What is a difference between tell and forward, in case I will send the same message:andThe  will be different on the receiving end.\nMessage sends using  (also known as ):  message  to . \n  that message to . \n thinks the  of message  is .\nMessage sends using :  message  to . \n  that message to .\n thinks the  of message  is .\nWorth pointing out is, that you can achieve the same as  when explicitly setting the sender of a message using , however this is not typical Akka-style:\nFor more info refer to the .Tell sets the sender as the actor sending the message.Forward keeps the original sender of the message."},
{"body": "in normal Scala map and flatMap are different in that flatMap will return a iterable of the data flattened out into a list.\nHowever in the Akka documentation, map and flatMap seem to do something different?It says \"Normally this works quite well as it means there is very little overhead to running a quick function. If there is a possibility of the function taking a non-trivial amount of time to process it might be better to have this done concurrently, and for that we use flatMap:\"Can someone please explain what is the difference between map and flatMap here in Akka futures?In \"normal\" Scala (as you say), map and flatMap have nothing to do with Lists (check Option for example).Alexey gave you the correct answer. Now, if you want to know why we need both, it allows the nice  syntax when composing futures. Given something like:The compiler rewrites it as:If you follow the method signature, you should see that the expression will return something of type . Suppose now only map was used, the compiler could have done something like:However, the result whould have been of type . That's why you need to flatten it. To learn about the concept behind, here is one the best introduction I've read:The type, basically:I am pasting the implementation of the two methods here.\nthe difference in english terms is below\nFrom implementation the difference is that flatMap actually calls the function with result when the promise completes.For a great article read:http//danielwestheide.com/blog/2013/01/16/the-neophytes-guide-to-scala-part-9-promises-and-futures-in-practice.html"},
{"body": "I tried  but it sorted in ascending order.  also sorts in descending order. I looked on stackoverflow and the answers I found were all outdated or . I'd like to use the native dataframe in spark.You can also sort the column by importing the spark sql functionsOr Or It's in  for  method:Note  and  inside  for the column to sort the results by."},
{"body": "Having  I am trying to get the first and the last  of the set by means of  and  respectively but the compiler says . How to define this ordering (both implicit and explicit options are interesting)?To facilitate working with Joda DateTime in Scala, nscala-time was created:\nAfter including it in your project with you can just import . Either all at once:or a particular one:"},
{"body": "How do you replace an element by index with an immutable List.E.g.In addition to what has been said before, you can use  function that replaces sub-sequences of a sequence:If you want to replace index 2, thenIf you want to find every place where there's a 2 and put a 5 in instead,In both cases, you're not really \"replacing\", you're returning a new list that has a different element(s) at that (those) position(s).You can use  (which is a method on ).It's probably better to use a  for this purpose, becuase updates on  take (I think) constant time.If you do a lot of such replacements, it is better to use a muttable class or Array.You can use map to generate a new list , like this :"},
{"body": "I'd like to use  to declare multiple variable like this:But for whatever reason, it's a syntax error, so I ended up using either:orI personally find both options overly verbose and kind of ugly.Also, I know Scala is very well thought-out language, so why isn't the  syntax allowed?The trivial answer is to declare them as tuples:What might be interesting here is that this is based on pattern matching. What is actually happens is that you are constructing a tuple, and then, through pattern matching, assigning values to ,  and .Let's consider some other pattern matching examples to explore this a bit further:Just as a side note, the  example, in particular, may be written more simply, and without illustrating pattern matching, as shown below.Daniel's answer nicely sums up the correct way to do this, as well as why it works.  Since he already covered that angle, I'll attempt to answer your broader question (regarding language design)...Wherever possible, Scala strives to avoid adding language features in favor of handling things through existing mechanisms.  For example, Scala doesn't include a  statement.  However, it's almost trivial to roll one of your own as a library:This can be used in the following way:( this is included in the standard library for Scala 2.8)Multiple assignment syntaxes such as are allowed by languages like Ruby (e.g. ) fall into the category of \"redundant syntax\".  When Scala already has a feature which enables a particular pattern, it avoids adding a new feature just to handle a special case of that pattern.  In this case, Scala already has pattern matching (a general feature) which can be used to handle multiple assignment (by using the tuple trick outlined in other answers).  There is no need for it to handle that particular special case in a separate way.On a slightly different aside, it's worth noting that C's (and thus, Java's) multiple assignment syntax is also a special case of another, more general feature.  Consider:This exploits the fact that assignment returns the value assigned in C-derivative languages (as well as the fact that assignment is right-associative).  This is not the case in Scala.  In Scala, assignment returns .  While this does have some annoying drawbacks, it is more theoretically valid as it emphasizes the side-effecting nature of assignment directly in its type.I'll add one quirk here, because it hit myself and might help others.When using pattern matching, s.a. in declaring multiple variables, don't use Capital names for the variables. They are treated as names of classes in pattern matching, and it applies here as well.Error message does not really tell what's going on:I thought `-ticking would solve the issue but for some reason does not seem to (not sure, why not):Still the same errors even with that.Please comment if you know how to use tuple-initialization pattern with capital variable names.It seems to work if you declare them in a tupleAlthough I would probably go for individual statements. To me this looks clearer:especially if the variables are meaningfully named.If all your variables are of the same type and take same initial value, you could do this."},
{"body": "In a Scala Compiler Plugin, I'm trying to create a new class that implement a pre-existing trait. So far my code looks like this:But it doesn't seem to get added to the package. I suspect that is because actually the package got \"traversed\" before the trait that causes the generation, and/or because the \"override def transform(tree: Tree): Tree\" method of the TypingTransformer can only return  Tree, for every Tree that it receives, so it cannot actually produce a new Tree, but only modify one.So, how do you add a new Class to an existing package? Maybe it would work if I transformed the package when \"transform(Tree)\" gets it, but I that point I don't know the content of the package yet, so I cannot generate the new Class this early (or could I?). Or maybe it's related to the \"Position\" parameter of the Symbol?So far I found several examples where Trees are modified, but none where a completely new Class is created in a Compiler Plugin.The full source code is here: The trick is to store the newly created s and use them when creating a new . Note that you need to deal with both Symbols and trees: a package symbol is just a handle. In order to generate code, you need to generate an AST (just like for a class, where the symbol holds the class name and type, but the code is in the  trees).As you noted, package definitions are higher up the tree than classes, so you'd need to recurse first (assuming you'll generate the new class from an existing class). Then, once the subtrees are traversed, you can prepare a new PackageDef (every compilation unit has a package definition, which by default is the empty package) with the new classes. In the example, assuming the source code is the compiler wraps it intoand the plugin transforms it into"},
{"body": "I was reading  about using the  method in the REPL code for interactive debugging, but then I found  saying that  and  were removed from  in Scala 2.10. Unfortunately, that post doesn't explain  the code was removed. I'm assuming that these functions were removed because there's a better way of doing this. If that's the case, could someone please enlighten me?Perhaps the idea is that you should just work with the  directly? As far as I can tell, it shouldn't be much more complex than:Compared to the old  API, this approach gets rid of an additional level of indirection for both the  condition (which was wrapped into a ) and the / (which were temporary wrappers used only to fill in the  arguments).This approach also allows you to specify your  as needed. For example, ."},
{"body": "Now with announcement of  Google clarified the foreseeable future of Java in relation to Android. But what are the implications to Scala and other JVM-based languages developers. In particular:All these questions boil down to one: Can Scala be used for Android development in future without sacrificing the benefits of new Scala features and new Android tool chain?Related reading:Related questions: Related:Please vote for Jack tool feature request:EDIT:Below is a hypothetical flow of Jack build with some extra blocks that was added basing on my logic and what i have learned from available docs.Base assumption is that Dalvik supports up to Java 7 bytecode instructions. If that is correct Java 8 instructions can not be directly passed to Dalvik, they should be somehow transformed to Java 7. (May be something similar to that Scala compiler always does).Than the question is where is that transformation happens? Seems Jill can't process Java 8 bytecode as for now, so that possibly happens in block (3) of hypothetical flow. If that is correct than only Java source project files are subject to transformation and the answer to  is - No. Java 8 classes from libraries can not be utilized until Jill will be able to do it (if it's possible at all). That is we can not use Scala 12+.If all code optimization is performed in block (6)  than the answer to  is - Yes. Scala code being converted to library .jar can benefit from Jack optimizations. But preliminarily it should be transformed to .jayce (AST-like representation) that will increase build time.And finally Jack produces .dex Dalvik bytecode to maintain compatibility with older Dalvik runtimes (ART consumes Dalvik bytecode too). So the answer to  is: Yes, Java 8 features can be used. But only in project Java sources. App is still compatible with any runtime. But Java 8 advantages are dropped due to converting to Java 7 (Dalvik bytecode).It\u2019s important to understand that there is  introduced:So it sounds like there is  here:Joan is correct, but I think Jill will have support for Java 8 at some point to, otherwise it will be impossible to use Java 8 in android libraries that will be consumed by android apps, as they package their code in a jar files inside of aar and I don't see this format change happening anywhere soon. Anyway we can only guess, as Google is currently a blackbox in respect to those kinds of changes.Google just announced that the Jack toolchain will deprecate the Jack toolchain and Android adds \"support for Java 8 language features directly into the current javac and dx set of tools\"Source: and:"},
{"body": "By all accounts, Scala's  is a bit of a mess - everything I've read about it mentions resources left open, mysterious bugs...I was wondering whether that was still the case in recent versions of Scala and, if so, what are worthy alternatives?I've mostly heard of  and  (and, obviously standard Java IO primitives). Did I miss anything? If anyone has experience with these or other projects, what are their respective pros and cons?I'm inclined to go for , since I found the author's  to be a fairly high quality source of useful of information, but I'd love to know more about the alternatives and what other people use. might be worth trying.It provides some nice DSL for managing IO resources of various kinds.Using the package  in Java standard library may also be simple enough if you don't require advance features. For example, to read the lines of a file into memory:And to write a sequence of lines into a file:Check\n\nfor more information."},
{"body": "I have a fairly normal  currently being built using Maven. I would like to support both Scala 2.9.x and the forthcoming 2.10, which is not binary or source compatible. I am willing to entertain converting to SBT if necessary, but I have run into some challenges.My requirements for this project are:Things I have tried:I accept that there may not be a solution that does what I want for Scala, and/or I may need to write my own Maven or Scala plugins to accomplish the goal. But if I can I'd like to find an existing solution.I am close to accepting @Jon-Ander's excellent , but there is still one outstanding piece for me, which is a unified release process. The current state of my . (I'll reproduce it here in an answer later for posterity).The sbt-release plugin does not support multi-version builds (i.e.,  does not behave as one might like), which makes a sort of sense as the process of release tagging doesn't really need to happen across versions. But I would like two parts of the process to be multi-version: testing and publishing.What I'd like to have happen is something akin to two-stage maven-release-plugin process. The first stage would do the administrative work of updating Git and running the tests, which in this case would mean running  so that all versions are tested, tagging, updating to snapshot, and then pushing the result to upstream.The second stage would checkout the tagged version and , which will rerun the tests and push the tagged versions up to the Sonatype repository.I suspect that I could write  values that do each of these, but I'm not sure if I can support multiple  values in my . It probably can work with some additional scopes, but that part of SBT is still strange majick to me.What I currently have done is changed the  to not publish. I then have to checkout the tagged version by hand and run  after the fact, which is close to what I want but does compromise, especially since the tests are only run on the current scala version in the release process. I could live with a process that isn't two-stage like the maven plugin, but does implement multi-version test and publish.Any additional feedback that can get me across the last mile would be appreciated.Most of this is well supported in sbt within a single source treeVersion specific source directories are usually not need. Scala programs tends to be source compatible - so often in fact that \ncrossbuilding () has first class support in sbt.If you really need version specific code, you can add extra source folders.\nPutting this in your build.sbt file will add \"src/main/scala-[scalaVersion]\" as a source directory for each version as you crossbuild in addition to the regular \"src/main/scala\".\n(there is also a plugin available for generating shims between version, but I haven't tried it - )version specific source jars - see crossbuilding, works out of the boxintegrated deployment -  (has awesome git integration too)Maven end-users - Shaded - I have used this one  which have worked fine for my needs.\nYour problem with the assembly plugin can be solved by rewriting the generated pom.\nHere is an example ripping out joda-time.Complete build.sbt for for referenceI've done something similar to this with SBT as an example:\nIt's made possible by SBT allowing all the settings to be determined based on other settings, which means that most other things should \"just work\".As far as the pom.xml aspect I can only recommend asking the question in the SBT mailing list, I would be surprised if you couldn't do that however.My blog post  contains an example of a slightly more finegrained solution for attaching different source directories; one per major S. Also it explains how to create scala-version-specific code that can be used by not-specific code.: Sbt now supports this out of the box: "},
{"body": "While trying to find a solution to another question () I came across a diverging implicit expansion error. I'm looking for an explanation about what this meansHere's the use case:If you run this in scala with the  argument passed, you get more information:This is mostly speculation, but would seem to make some sense. I will try to investigate further:This seems to suggest that there are three implicits that are being considered here. Ultimately, the signature of  requires it to find something of type . So it's trying to construct your implicit function . Firstly, it's trying to fill in  by finding an implicit of type , where it's searching in Predef - which seems like barking up the wrong tree. It's then trying to find an implicit for  in the same place, since  takes an implicit parameter of type , where  is  by virtue of . So it can't construct .It then tries to use  in math.Ordering, but this also doesn't fit. However, I think this is what's giving the somewhat confusing 'diverging implicits' message. The problem isn't that they're diverging, it's that there isn't a suitable one in scope, but it's being confused by the fact that there are two paths to go down. If one tries to define  without the implicit ordered function, then it fails with just a nice message saying that it can't find a suitable implicit."},
{"body": " has 2 methods that are specified to prepend an element to an (immutable) list: technically has a more general type signature\u2014\u2014but ignoring the implicit, which according to the doc message merely requires  to be , the signatures are equivalent. If they are in fact identical, I assume  would be preferred to avoid depending on the concrete implementation . But why was another public method defined, and when would client code call it?There is also an extractor for  in pattern matching, but I'm wondering about these particular methods.See also: The best way to determine the difference between both methods is to look it the source code.The  of :The  of :As you can see, for , both methods do one and the same (the compiler will choose  for the  argument).So, which method to use? Normally one would choose the interface () than the implementation () but because  is a general data structure in functional languages it has its own methods which are widely used. Many algorithms are build up the way how  works. For example you will find a lot of methods which prepend single elements to  or call the convenient  or  methods because all these operations are . Therefore, if you work locally with a  (inside of single methods or classes), there is no problem to choose the -specific methods. But if you want to communicate between classes, i.e. you want to write some interfaces, you should choose the more general  interface. is more generic, since it allows the result type to be different from the type of the object it is called on. For example:"},
{"body": "Is it lack of time, some technical problem or is there a reason why it should not exist?It's just a missing case that will presumably eventually be filled in.  There is no reason not to do it, and in certain cases it would be considerably faster than the immutable tree (since modifications require log(n) object creations with an immutable tree and only 1 with a mutable tree).Edit: and in fact it was filled in in 2.12..(There is a corresponding  also.)Meanwhile you can use the Java TreeMap, which is exactly what you need.I assume that the reason is that having a mutable variant doesn't bring a big benefit. There are some cases mentioned in the other answers when a mutable map could be a bit more efficient, for example when replacing an already existing value: A mutable variant would save creation of new nodes, but the complexity would be still .If you want to keep a shared reference to the map, you can use  which wraps any immutable map into a mutable structure.You'll also notice that  doesn't have a mutable equivalent either. It's because they share the common base class , and the underlying data structure that keeps the  ordered by elements or keys is a . I don't know too much about this data structure, but it's pretty complex (insertion and removal are pretty expensive compared to other Maps), so I assume that had something to do with a mutable variant not being included.Basically, it's probably because the underlying data structure isn't readily mutable so  isn't. So, to answer your question, it's a technical problem. It can definitely be done though, there's just not much of a use case for it.There may be performance reasons for a mutable , but usually you can use an immutable map in the same way as you would a mutable one. You just have to assign it to a  rather than a . It would be the same as for , which has mutable and immutable variants:"},
{"body": "I wonder if there are any game engine written in Scala or easily accesible from Scala?All the Java gaming engines are easily accessible due to easy Java integration. There are several (not sorted in any way):A good presentation how to start coding a game in Java is  that applies as well.As  the ointerop should be easy. As mcherm pointed out, for . To have the nice Scala feel you could add some . I am currently working on a 3d engine in Scala: It still has a long way to go before it's a competitor to the major Java engines out there, but good progress is being made and more help is always appreciated. :) is an example of someone using a library like LWJGL from Scala. That seems like your best plan -- integrate with a good Java library. The integration betweeen Scala and Java is quite strong: in order to use such a library you need to integrate in \"both directions\": call Java code from your Scala (to do things like drawing to the screen) and also implement Java interfaces from your Scala code (to do things like responding to events). Fortunately, Scala makes it quite easy to do both.There is such engine in the works. Right now I am building a solid base with an easy to use math library to be optimized via compiler plugin. The math part is finished, including all the vectors, matrices, quaternions, rotations and projections for setting up the camera and ready to be plugged into opengl pretty much out of the box. A simple renderer will be done in a near feature. The compiler plugin will take time.Link: It's probably not a full fledged game engine though. (I dunno I have never written large games).There is a 2D game engine : For java, there is also Slick2d, which is pretty far developed and still active. Integration in Scala is easy as always."},
{"body": "I've been working with Scala for a while now and have written a 10,000+ line program with it, but I'm still confused by some of the inner workings. I came to Scala from Python after already having intimate familiarity with Java, C and Lisp, but even so it's been slow going, and a huge problem is the frustrating difficulty I've often found when trying to investigate the inner workings of objects/types/classes/etc. using the Scala REPL as compared with Python. In Python you can investigate any object  (type, object in a global variable, built-in function, etc.) using  to see what the thing evaluates to,  to show its type,  to tell you the methods you can call on it, and  to get the built-in documentation.  You can even do things like  to find out the documentation on the package named  (which holds regular-expression objects and methods), even though there isn't an object associated with it.In Scala, you can try and read the documentation online, go look up the source code to the library, etc., but this can often be very difficult for things where you don't know where or even what they are (and it's often a big chunk to bite off, given the voluminous type hierarchy) -- stuff is floating around in various places (package , , various implicit conversions, symbols like  that are nearly impossible to Google). The REPL should be the way to explore directly, but in reality, things are far more mysterious.  Say that I've seen a reference to  somewhere, but I have no idea what it is.  There's apparently no such thing as a \"guide to systematically investigating Scala thingies with the REPL\", but the following is what I've pieced together after a great deal of trial and error:Example of failure using :Example of enlightening error message:OK, now let's try a simple example.Simple enough ...Now, let's try some real cases, where it's not so obvious:What does this mean?  Why is the type of  simply , whereas the class is ? I gather that the $ is the way that companion objects are shoehorned into Java ...  but Scala docs on Google tell me that  is  -- how can I deduce this from the REPL?  And how can I look into what's in it?OK, let's try another confusing thing:OK, this left me hopelessly confused, and eventually I had to go read the source code to make sense of this all.So, my questions are:Thanks for any enlightenment.You mentioned an important point which Scala lacks a bit: the documentation.The REPL is a fantastic tool, but it is not as fantastic at it can be. There are too much missing features and features which can be improved - some of them are mentioned in your post. Scaladoc is a nice tool, too, but is far away to be perfect. Furthermore lots of code in the API is not yet or too less documented and code examples are often missing. The IDEs are full ob bugs and compared to the possibilities Java IDEs show us they look like some kindergarten toys.Nevertheless there is a gigantic difference of Scalas current tools compared to the tools available as I started to learn Scala 2-3 years ago. At that time IDEs compiled permanently some trash in the background, the compiler crashed every few minutes and some documentation was absolutely nonexistent. Frequently I got rage attacks and wished death and corruption to Scala authors.And now? I do not have any of these rage attacks any more. Because the tools we currently have are great although the are not perfect!There is , which summarizes a lot of great documentation. There are Tutorials, Cheat-sheets, Glossaries, Guides and a lot of more great stuff. Another great tools is , which can find even the weirdest operator one can think of. It is Scalas  and even though it is not yet as good as his great ideal, it is very useful.Great improvements are coming with Scala2.10 in form of Scalas own Reflection library:Documentation for the new Reflection library is still missing, but in progress. It allows one to use scalac in an easy way inside of the REPL:This is even greater when we want to know how Scala code is translated internally. Formerly wen need to type  to get the internally representation. Furthermore some small improvements found there way to the new release, for example:Scaladoc will gain some  (click on type-hierarchy).With Macros it is possible now, to improve error messages in a great way. There is a library called , which does this:There is a tool which allows one to find libraries hosted on GitHub, called .The IDEs now have some semantic highlighting, to show if a member is a object/type/method/whatever. The semantic highlighting feature of .The javap feature of the REPL is only a call to the native javap, therefore it is not a very featue-rich tool. You have to fully qualify the name of a module:Some time ago I have written a , which offers a lot of things to know.And the best: This is all done in the last few months!So, how to use all of these things inside of the REPL? Well, it is not possible ... not yet. ;)But I can tell you that one day we will have such a REPL. A REPL which shows us documentation if we want to see it. A REPL which let us communicate with it (maybe like ). A REPL which let us do cool things we still cannot imagine. I don't know when this will be the case, but I know that a lot of stuff was done in the last years and I know even greater stuff will be done in the next years.Javap works, but you are pointing it to , which is a , not a . Point it instead to .Now, for the most part just entering a value and seeing what the result's type is is enough. Using  can be helpful sometimes. I find that use  is a really bad way of going about it, though.Also, you are sometimes mixing types and values. For example, here you refer to the object :And here you refer to the class :Objects and classes are not the same thing, and, in fact, there's a common pattern of objects and classes by the same name, with a specific name for their relationship: companions.Instead of , just use tab completion:If you enter the power mode, you'll get way more information, but that's hardly for beginners. The above shows types, methods, and method signatures. Javap will decompile stuff, though that requires you to have a good handle on bytecode.There's other stuff in there -- be sure to look up , and see what's available.Docs are only available through the scaladoc API. Keep it open on the browser, and use its  capability to quickly find classes and methods. Also, note that, as opposed to Java, you don't need to navigate through the inheritance list to get the description of the method.And they do search perfectly fine for symbols. I suspect you haven't spent much time on scaladoc because other doc tools out there just aren't up to it. Javadoc comes to mind -- it's awful browsing through packages and classes.If you have specific questions Stack Overflow style, use  to search with symbols.Use the  Scaladocs: they'll diverge from whatever version you are using, but they'll always be the most complete. Besides, right now they are far better in many respects: you can use TAB to alternate between frames, with auto-focus on the search boxes, you can use arrows to navigate on the left frame after filtering, and ENTER to have the selected element appear on the right frame. They have the list of implicit methods, and have class diagrams.I've made do with a far less powerful REPL, and a far poorer Scaladoc -- they do work, together. Granted, I skipped to trunk (now HEAD) just to get my hands on tab-completion.Note that  can facilitate the type exploration/discovery.It now includes:You need to pass fully qualified class name to .First take it using :Then use  (doesn't work from repl for me: \":javap unavailable on this platform.\") so example is from a command line, in repl, I believe, you don't need to specify classpath:But I doubt this will help you. Probably you're trying to use techniques you used to use in dynamic languages. I extremely rarely use repl in scala (while use it often in javascript). An IDE and sources are my all."},
{"body": " However, superficially, it seems that it can easily be encoded in an object-oriented language that supports abstract type members. For example, we can encode the elements of SML module system in Scala as follows:Are there any significant features such an encoding would miss? Anything that can be expressed in SML modules that encoding can't express? Any guarantees that SML makes that this encoding would not be able to make?There are a few fundamental differences that you cannot overcome easily:The differences become even more interesting when you move beyond SML, to higher-order, first-class, or recursive modules. We briefly discuss a few issues in Section 10.1 of our ."},
{"body": "I am thinking about migrating to play 2.0 after play 1.2. One thing that bothers me is that people say Scala is more \"preferred\" for a play 2.0 application. I know the differences over 1.2 and 2.0, but I'm unsure if there are differences between Play 2.0 with Java and Play 2.0 with ScalaSo there are questions in my mind: I just finished a prototype using Play 2.0 with Java and now am considering to learn Scala just so I can switch to it for further development.It's not just the usual  discussion - the problem as I see it with the Play framework is that it forces Scala idioms onto Java. An example from the documentation about :It works but doesn't look like conventional Java. Those parentheses look really scary. Even Eclipse gets confused and never knows what generics I need or want to use - I always have to choose them manually.Also note that in the documentation they made this look pretty by removing  annotations, using just two spaces for indentation and overall choosing a very simple example without validation or error recovery so they don't use too many lines. I'm not even sure you could configure a code formatter to output it like this without messing up other code completely. In practice I ended up with an unreadable block of a horrible Java-Scala-abomination just for getting some data from another service.Unfortunately I can't find any example of combining responses in Scala. At least calling single  looks much shorter and easier to read.AFAIK, it is better to go with Scala, because it offers more functionnality.For instance (correct me if I'm wrong), you can directly using the  with Scala (aka reactive programming), but I don't know how to achieve that with Java.Also, the Java API of Play is an upper layer in the Play architecture, the core is written with Scala; so if you want to lean what's happening inside, go with Scala.And also note that you can mix Java and Scala in the same Play project, so you can move smoothly to Scala.I was new to Play and just wrote a small-mid sized Project with the Java side of it.It works and is powerful, but a lot of Features just seem to be very Scala centric and the Java support is sub-par.\nThe Java API Doc is virtually non existent, which takes away some comfort in the IDE (IntelliJ - I recommend it).\nThe integration in the Java world is not very \"natural\" (SBT..), It does not build as a war or maven module (but there seem to be plugins to do this). You kind of loose the benefits of the existing Java tooling.The concepts which differentiate Play from the other Frameworks almost all seem to be scala native and Java second.I stumbled onto some bugs and quirks and I always would end up in Scala code I had to read and understand to fix or work around it. (Actions, RequestBody stuff, Async, routes, template implicits and so on)So the Java side of things just feels very second tier to me, although they are clearly making an effort for them to be equally good.Now, after the Project, I would say it is very usable and would even use it again (in Java).As djangofan commented, when you google for Play help, you get examples in Scala.However, I haven't yet (still a beginner) found an equivalent of Ebean for Scala.  The Scala examples I've found all use direct SQL for persistence.  Where's the database abastraction?Since I haven't found a database abstraction for Play with Scala, I'm continuing with Java..."},
{"body": "I am frequently getting an  from SBT.Sometimes it also exits abruptly with:Any solutions?This sometimes happens if you compile huge codebases - a lot of classes get loaded into the VM running sbt.You need to increase the  space for sbt - use the flag , where  you can change with the desired size of the permanent generation.Run:to locate you sbt startup script. Then edit it to include the flag with the  command that runs the sbt launcher in the similar way as it is  for modifying  and .Adding the  flag should also enable sbt to unload the classloaders with classes from the previous compilation runs that are no longer being used.EDIT:Alternatively, you can set these options in the  environment variable if you are using the ."},
{"body": "I'm relatively new to Scala and am trying to define a generic object method.  However, when I refer to the parameterized type within the method I am getting \"No ClassTag available for T\".  Here is a contrived example that illustrates the problem.Thanks in advance for help in understanding what is wrong here and how to make this contrived example work.To instantiate an array in a generic context (instantiating an array of  where  is a type parameter), scala needs at runtime to have information about , in the form of an implicit value of type .\nConcretely, you just need to require from the caller of your method to (implicitly) pass this  value, which can conveniently be done using a :For a (torough) description of this situation, see this document:(To put it shortly, ClassTags are the reworked implementation of ClassManifests, so the rationale remains)"},
{"body": "I have an immutable Set of a class, Set[MyClass], and I want to use the Set methods intersect and diff, but I want them to test for equality using my custom equals method, rather than default object equality testI have tried overriding the == operator, but it isn't being used.Thanks in advance.Edit:The intersect method is a concrete value member of GenSetLikespec: \nsrc: so the intersection is done using the filter method.Yet another Edit:filter is defined in TraversableLikespec: src: What's unclear to me is what it uses when invoked without a predicate, p. That's not an implicit parameter.equals and hashCode are provided automatically in case class only if you do not define them.  If what you want is reference equality, still write equals and hashCode, to prevent automatic generation, and call the version from AnyRefWith that: You cannot override the  from AnyRef, which is sealed and always calls equals. If you tried defining a new (overloaded) , it is not the one that  calls, so it is useless here and quite dangerous in general. As for the call to , the reason it works is that  is a . And yes,  is used, you will see that function implementation () is a synonymous for , and  most implementations of  use  in contains ( uses the  instead).  And  calls .Note: the implementation of my first  is quick and dirty and probably bad if MyClass is to be subclassed . If so, you should at the very least check type equality () or better define a  method (you may read  by Daniel Sobral)You'll need to override  as well. This is almost always the case when you override , as  is often used as a cheaper pre-check for ; any two objects which are equal  have identical hash codes. I'm guessing you're using objects whose default  does not respect this property with respect to your custom equality, and  the Set implementation is making assumptions based on the hash codes (and so never even calling your equality operation).See the Scala docs for  and : . It could be made immutable by replacing the internal store with a  and returning a modified copy of itself upon each operation\"It is not possible to override == directly, as it is defined as a final method in class Any. That is, Scala treats == as if were defined as follows in class Any:\" from Programming In Scala, Second Edition"},
{"body": "Have somebody use both Buildr and Gradle and can make comparison of this build tools. From first look they are very similar. But what to choose. And also it is good to hear about Scala support and various IDE integration(IDEA, NetBeans, Eclipse).Thanks.I have tried using, both and I'd definitely recommend Gradle. While both of them have pretty much the same expressiveness, I found Gradle to be much more stable (since version 1.0), less effort to install, and better documented. Although Buildr is undoubtedly a great effort, at the moment Gradle is much more professionally made.The only problem I faced with Gradle is lack of native support for reusing ivy.xml (and ivysettings.xml), despite the fact that Gradle actually uses Ivy as its dependency engine. This feature has been promised for sometime in the future soon. Buildr, on the other hand has a ready-to-use plugin for that (it's not very configurable though).If you need to migrate from Maven, I'd do some investigation on both tools and their support for the features you need. For new projects I'd recommend Gradle.Neither have good support for Eclipse, but Gradle have a work-in-progress Eclipse plugin that works very much like the Ant plugin. Also, Gradle's generation of Eclipse settings files is highly customizable, so that's another way to integrate it with Eclipse.See if this helps (probably outdated): If compared to BuildR, I'd recommend Gradle:\n1. Groovy is much more \"native\" to the JVM ecosystem than Ruby. Even if you use JRuby, you'd still need to install a lot of gems\n2. BuildR's development is crawling. They have not added new features in a long timeHaving said that, I'd question using either one of these: With the power to script any task comes the risk that the build will have a lot of logic that people need to learn before being able to maintain it. Since this is not the main logic of the product, you have the risk of people that do not want to maintain the build or worse, doing it in copy&paste way that is not right (e.g., performance wise). With Maven, there's no logic in the pom. Also, using Gradle/BuildR means that to build the product, one needs to install Gradle/BuildR. Maven is much more standard here. I looked at both gradle and buildr and went for buildr in the end because of one huge disadvantage with gradle: start-up time of the build system. buildr is much snappier and gave me much faster incremental builds. Performance is something I found almost unbearable with gradle. Just consider how many times a build system is kicked off and add up the constant overhead.BTW I found the build server with both build systems (buildr and gradle) to be not very useful  and error prone (that is: I spent more time with troubleshooting than it actually sped up things)."},
{"body": "I'm working on a Scala API (for Twilio, by the way) where operations have a pretty large amount of parameters and many of these have sensible default values. To reduce typing and increase usability, I've decided to use case classes with named and default arguments. For instance for the TwiML Gather verb:The parameter of interest here is . It is the only parameter which is  optional in the sense that if no value is supplied, no value will be applied (which is perfectly legal). I've declared it as an option in order to do the monadic map routine with it on the implementation side of the API, but this puts some extra burden on the API user:As far as I can make out, there are two options (no pun intended) to resolve this. Either make the API client import an implicit conversion into scope:Or I can redeclare the case class with a null default and convert it to an option on the implementation side:My questions are as follows:Here's another solution, partly inspired by .  It also involves a wrapper, but the wrapper is transparent, you only have to define it once, and the user of the API doesn't need to import any conversions:To address the larger design issue, I don't think that the interaction between Options and named default parameters is as much oil-and-water as it might seem at first glance.  There's a definite distinction between an optional field and one with a default value. An optional field (i.e. one of type ) might  have a value. A field with a default value, on the other hand, simply does not require its value to be supplied as an argument to the constructor. These two notions are thus orthogonal, and it's no surprise that a field may be optional and have a default value.That said, I think a reasonable argument can be made for using  rather than  for such fields, beyond just saving the client some typing.  Doing so makes the API more flexible, in the sense that you can replace a  argument with an  argument (or vice-versa) without breaking callers of the constructor[1].As for using a  default value for a public field, I think this is a bad idea. \"You\" may know that you expect a , but clients that access the field may not. Even if the field is private, using a  is asking for trouble down the road when other developers have to maintain your code. All the usual arguments about  values come into play here -- I don't think this use case is any special exception.[1] Provided that you remove the option2opt conversion so that callers must pass a  whenever an  is required.Don't auto-convert anything to an Option. Using , I think you can do this nicely but in a typesafe way.Then you can call it as you wanted to - obviously adding your  methods to  and  as appropriate. The main negative here is that it is a ton of boilerplatePersonally, I think using 'null' as default value is perfectly OK here. Using Option instead of null is when you want to convey to your clients that something may not be defined. So a return value may be declared Option[...], or a method arguments for abstract methods. This saves the client from reading documentation or, more likely, get NPEs because of not realizing something may be null.In your case, you are aware that a null may be there. And if you like Option's methods just do  at the start of the method.However, this approach only works for types of AnyRef. If you want to use the same technique for any kind of argument (without resulting to Integer.MAX_VALUE as replacement for null), then I guess you should go with one of the other answersI think as long as no language support in Scala for a real kind of void (explanation below) \u2018type\u2019, using  is probably the cleaner solution in the long run. Maybe even for all default parameters.The problem is, that people who use your API know that some of your arguments are defaulted might as well handle them as optional. So, they\u2019re declaring them asIt\u2019s all nice and clean and they can just wait and see if they ever get any information to fill this Option.When finally calling your API with a defaulted argument, they\u2019ll face a problem.I think it would be much easier then to do thiswhere the  would just be left out in case of . But this is not possible.So you really should think about who is going to use your API and how they are likely to use it. It may well be that your users already use  to store all variables for the very reason that they know they are optional in the end\u2026Defaulted parameters are nice but they also complicate things; especially when there is already an  type around. I think there is some truth in your second question.Might I just argue in favor of your existing approach, ? It's all of 6 more characters for the API user to type, shows them that the parameter is optional, and presumably makes the implementation easier for you.I think you should bite the bullet and go ahead with . I have faced this problem before, and it usually went away after some refactoring. Sometimes it didn't, and I lived with it. But the  is that a default parameter is not an \"optional\" parameter -- it's just one that has a default value.I'm pretty much in favor of  .I was also surprised by this.  Why not generalize to:Any reason why that couldn't just be part of Predef?"},
{"body": "I use scala sbt console to test my methods. (commands :  then ) But the code changes done in eclipse or other external editor, are not getting reflected in the sbt console. Every time, I have to quit the console (using Crt + D) and again start it using  command to see the changes. Any one facing this problem? \nIs there any ways to reload the code from console? I am using Ubuntu 64-Bit, Not without using something like JRebel, mostly because class definitions could break in such a way as to make instances already loaded unusable. The only suggestion I have is to run the console with  so that if changes have been made they will be recompiled and the console re-entered.Also if you're regularly running a set of commands the initialCommands sbt setting configures commands to be run immediately after starting the console.One option is to use  in the console - this will reload it and replay all the commands you've entered so far.For a better solution you might want to read my  on incremental development with JRebel & Scala. You should modify the sbt startup script like this:When you start the REPL from inside SBT, for instance with the command:changes to the imported classes will be reflected automatically without the need to do a  or  the REPL - something reminiscent of the interactive Lisp programming.this will be rebuilding the app each time source code changes"},
{"body": "I want to change  of a Spark job in dev/prod environment. It seems to me that the easiest way to accomplish this is to pass  to the job. Then Typesafe config library will do the job for me.Is there way to pass that option directly to the job? Or maybe there is better way to change job config at runtime?Change  command line adding three options:I Had a lot of problems with passing -D parameters to spark executors and the driver, I've added a quote from my blog post about it:\n\"\nThe right way to pass the parameter is through the property: \n\u201c\u201d and \u201c\u201d:\nI\u2019ve passed both the log4J configurations property and the parameter that I needed for the configurations. (To the Driver I was able to pass only the log4j configuration).\nFor example (was written in properties file passed in spark-submit with \u201c\u2014properties-file\u201d):\n\u201c\u201cYou can read  about overall configurations of spark.\nI'm am running on Yarn as well.Here is my spark program run with addition java optionas you can see \nthe custom config file\n the executor java options\n the driver java options.\n Hope it can helps.I am starting my Spark application via a spark-submit command launched from within another Scala application. So I have an Array likewhere  is:It is a bit tricky to understand where (not) to escape quotes and spaces, though. You can check the Spark web interface for system property values.if you write in this way, the later  will overwrite the previous one, you can verify this by looking at sparkUI after job started under  tab.so the correct way is to put the options under same line like this:\n\nif you do this, you can find all your settings will be shown under sparkUI."},
{"body": "I'm hoping there is a way to define a type for a function in Scala.For example, say I want a function that takes two Ints and returns a Boolean, I could define a function that uses that like this:Is there a way to define the type of f? Then I could do something like:or "},
{"body": "Consider the following Scala case class:Pattern matching allows me to extract one field and discard others, like so:What I would like to do, and what's more relevant when a case class has ~20 odd fields, is to extract only a few values in a way that does not involve typing out .I was hoping that named args could help here, although the following syntax doesn't work:Is there any hope here, or am I stuck typing out many, many ?: I understand that I can do , yet I'm still wondering whether there's even terser syntax that does what I need without having to introduce an extra .I don't know if this is appropriate, but you can also build an object just to match that field, or that set of fields (untested code):or evenYou can always fall back to guards. It's not really nice, but better than normal pattern matching for you monster case classes :-PThat said I think you should rethink your design. Probably you can logically group some parameters together using case classes, tuples, lists, sets or maps. Scala  support nested pattern matching:"},
{"body": "I'm getting compile errors when running the  task as the sources reference new classes in  package that only appeared in Java 7.I have the following in :In sbt:It compiles and runs fine in Eclipse. How can I set up sbt to use Java 7 for compilation?The most reliable (perhaps only) way to do this at the moment it to start SBT with  in the JDK7 folder.Modify your  launcher script; or use  that allows you to specify Java Home (and so much more!) as command line options.Simply setting  changes the Java version used to compile and fork processes, but does not change the version of the Java standard library on the classpath, nor the version used to run tests, which are always run the the same JVM as SBT.If you use Linux or Mac, another possibility is to look at , a command line Java manager.It allows you to choose per project which JDK to use.I use , which is a tool from the Python ecosystem. In a nutshell, it is a shell script which allows you to change your PATH variable easily and get back to what it was before, if you need to.I'm assuming you want to change whatever you have set in JAVA_HOME by default, which you can do when invoking sbt:This works for me on OSX with sbt 0.13.8change javacOption to 1.7?  I don't think setting the javaHome is necessary."},
{"body": "I couldn't find the answer to this in any other question. Suppose that I have an abstract superclass Abstract0 with two subclasses, Concrete1 and Concrete1. I want to be able to define in Abstract0 something likewhere Self would be the concrete subtype. This would allow chaining calls to setOption like this:and still get Concrete1 as the inferred type of obj.What I don't want is to define this:because it makes it harder for clients to handle this type. I tried various possibilities including an abstract type:but then it is impossible to implement setOption, because  in Abstract0 does not have type Self. And using  also doesn't work in Abstract0.What solutions are there to this issue?This is what  is for:As you can see setOption returns the actual type used, not Abstract0. If Concrete0 had  then  would work UPDATE: To answer JPP's followup question in the comment (how to return new instances:\nThe general approach described in the question is the right one (using abstract types). However, the creation of the new instances needs to be explicit for each subclass. One approach is:Another one is to follow the builder pattern used in Scala's collection library. That is, setOption receives an implicit builder parameter. This has the advantages that building the new instance can be done with more methods than just 'copy' and that complex builds can be done. E.g. a setSpecialOption can specify that the return instance must be SpecialConcrete.Here's an illustration of the solution:UPDATE 2: \nReplying to JPP's second comment. In case of several levels of nesting, use a type parameter instead of type member and make Abstract0 into a trait:This is the exact use case of . It would be like:"},
{"body": "What's the difference between:andCan I declare parameters as s, and change their values later? For instance,For the first part the answer is scope: If you prefix parameters with ,  they will be visible from outside of class, otherwise, they will be private, as you can see in code above.And yes, you can change value of the var, just like usually.Thismakes the fields available externally to users of the class e.g. you can later doIf you specify the args as , then the scoping is the same as for , but the fields are mutable.If you are familiar with Java, you can get the idea from this example:is similiar toWhile is similar toThe intuition is that without var/val, the variable is only accessible inside the constructor. If var/val is added, the class will have the member variables with the same name.You could use a  and in that case the  class would have those variables available outside of the class. \n. Then the following code would work as expected. "},
{"body": "In , is there a way to restrict messages to actors to be of a specific static type other than using the \"Typed Actor\" APIs that use an RPC style programming model?Can I use the message passing style with Akka without throwing away static type safety at the actor boundaries?For example, I'd like to use code like this:Perhaps one would have to extend some base trait or have a construct like  to allow for system level messages (, etc.).Then you'd have to encode the message type into the Actor ref, which would drastically decrease the value of something like the ActorRegistry.Also, with powerful mechanics like \"become\" (which is fundamental to the actor model) typing the messages is less valuable.Since Akka doesn't leak memory when a message is not matched to the current behavior, there is not the same risk of sending the \"wrong\" messages to the \"wrong\" actor.Also, Actors are by nature dynamic, so if you want to make them static, use TypedActor (which is not RPC, it's just as RPC as regular actors, void methods are ! calls, Future return type is !!! and other return types are based on !!)The common practice is to declare what messages an Actor can receive in the companion object of the Actor, which makes it very much easier to know what it can receive.Does that help?In Scala stdlib there was  for making basic actors untyped (which is not applicable to Akka, because it doesn't support nested receives, as I remember). Lift, in its turn, supports typed actors out-of-the-box.However, using channels, it's still possible to create strongly typed actors with stdlib:Actually restricting an Actor to have only single type as input is not very useful. What is more useful to my mind is to list possible inputs in a strictly typed manner.There is an approach for strictly typed inputs of actors ():In your case the interface consists of a single input contact:   Within SynapseGrid framework handling of signals is defined with Builder:Obviously one cannot construct Signal with incompatible types. Thus we have compile time checking. In SynapseGrid there is a DSL for working with signals and contacts. For instance, to send Foo or Bar from outside:Of course one may simply send the message:It sounds like Akka's  was to have addressed this, but (according to  ).In the documentation for Akka 2.2.3, there a good  section that discusses the difficulties in support typed message sends and responses.  There's also a good NEScala talk by Roland Kuhn,  ( / ), that discusses the implementation of typed channels."},
{"body": "I am not sure what the difference is between specifying Unit as the return type of my scala method or leaving out the return type altogether. What is the difference?Can anyone please advise?Implicit  return type:Explicit  return type:Return type inferred from the last method expression, still  because this is the type of , but confusing:All the methods above are equivalent. I would prefer  because the lack of  operator after method signature alone is enough for me. Use explicit  when you want to extra document the method. The last form is confusing and actually treated as a warning in .The  operator is crucial. If it is present it means: \"\" in method body. Obviously you cannot use this syntax for  methods. If it is not,  is assumed.The special syntax for procedures (methods returning ) was a mistake. Don't use it. It is confusing and dangerous for beginners with a Java/C(++) background. And it is a unnecessary special treatment. Always use the equal sign, with and without type inference (the latter should only be used for private members):The Scala community is divided about it. On the one hand, not using explicit return types means you can easily forget the  sign, which results in errors that are often annoying to track. On the other hand, -returning methods having a different syntax puts them in a separate category, which pleases some people.Personally, I'd rather this syntax would go away -- the errors resulting from missing   annoying to track. But it's not even deprecated, so I'd rather take advantage of it than just suffer the problems of its existence.So, use whatever you wish. There's going to be people criticizing your choice either way, and people praising it either way."},
{"body": "I have been toying with Scala and I was wondering if anyone had had any experience with using hibernate and mysql as a persistent store for scala objects? Does it work out of the box or is there a lot to do?Most of the time, Scala + Hibernate works quite well, with minor bumps which could be overcome easily. For exmaple, when dealing with collections, Hibernate requires the use of java.util interfaces. But you could import scala.collection.jcl.Conversions._ if you want to tap on Scala's more powerful library.You may want to check out ' post for more information.It is definitely not a lot of work. A simple hibernate + scala example can be defined in a few tens of lines. Scala and Java can be mixed up in the same project. In particular, the hibernate-scala combination makes possible to combine the JPA framework, and a very flexible orm layer with the elegance of immutable structures and functional programming as provided by scala. The easiest way to experiment with hibernate and scala is by using an in-memory hsqldb database via hibernate/jpa. First off, let\u2019s define the domain model. In this case, a scala class annotated according to hibernate style, about my buddies.Note how the scala class, is much more compact than the java class, since we don\u2019t need the typical getter/setter boilerplate code. Now let\u2019s make sure, that the jpa modules and the database model is loaded. According to the hibernate specification, let\u2019s add the well known hibernate configuration file: resources/META-INF/persistence.xml:After defining the persistency configuration, let\u2019s move on the main scala file:The code is quite straightforward. Once the JPA EntityManager is created via the factory, the data model is available for insertion, deletion, query, using the methods defined in the documentation of hibernate and jpa. This example has been set up using sbt. After retrieving the necessary packages, and compiling the source, running the tutorial will produce the following log:Scala Query is not Hibernate but may be interesting.There are issues.  Because some features of JPA leverage nested annotations, e.g. collections, you're in trouble because Scala does not yet support nested annotations.  That'll go away when 2.8 comes out.See  for more on this topic plus other incompatibilities.Note that Scala 2.8, now in RC5 and expected to release shortly, supports nested annotations.  The release has many other cool features as well.I have not used Hibernate with scala directly, but I am using JPA. Hibernate provides a JPA implementation, and the way you define JPA persistent classes or Hibernate ones is not much different, so I think using Hibernate without the JPA layer is possibleI am using hibernate with Scala. The real problem that I had to solve was how to persist Enumerations in hibernate. I've put my working solution on githubBasically one needs to define own UserTypeHave a look at  where there is full JPA adaptation for Scala."},
{"body": "I tried Google search and could not find a decent  example. What does it do? Why does it take a boolean function?Please point me to a reference (except the Scaladoc).The  method takes a function  that returns a Boolean.  The semantics of  says: return  if for every  in the collection,  is true.So:means:  if 1, 2, and 3 are less than 3,  otherwise.  In this case, it will evaluate to  since it is not the case all elements are less than 3: 3 is not less than 3.There is a similar method  that returns  if there is  element  in the collection such that  is true.So: means:  if  1, 2, and 3 is less than 3,  otherwise.  In this case, it will evaluate to  since it is the case some element is less than 3: e.g. 1 is less than 3.A quick example of how you can play with this function using a  script.create a  file with and call it with "},
{"body": "I'm using the following:I'm frequently running into this error:Currently I'm running into this when simply trying to call a  on my MongoDB collection.Upon Googling, it seems like it may be caused by dependency issues. The thing is, I'm using Scalatra just to serve an API and actually don't require any of the scalate stuff. I commented out all references to it, but I still get this. Could it be a dependency issue between the libraries I'm using?In my company we had this problem with CharSequence. This is due to Scala 2.10.x does not work with Java 8. This was described in Scalas . Switch back to Java 7.Switching to Scala 2.10.4 solved the problem for me.I have a similar problem on Java 1.8.0 working with Scala 2.10.4 and sbt 0.12.4. But when upgrading to sbt 0.13.7, the problem is solved.I am seeing the same issue with software requiring Scala 2.9.2+ and Java 1.7 running on Scala 2.10.4 and Java 1.8. Changed to Java 7 via sudo update-alternatives --config javaI solved this by using:My environment is:I hope a stable version of Scala will help you, Try 2.10.x with Java 1.7.x versions for better performance. I mean it worked for me also in the same.I had the same problem. However by switching to Java 7 my problem is resolved. \nSo now this configuration works for me:"},
{"body": "Can command  run a compiled scala code? If so, why do we have an exclusive command ?You can run byte code generated by Scala if you include all necessary runtime libs for Scala (scala-library.jar, scala-swing.jar ...) in the classpath. The scala command does this automatically, and supports Scala specific command line arguments.Yes, it can. Scala is compiled down to Java bytecode. But remember that it depends on the Scala runtime classes, so you need to still have Scala's jar files on the classpath.Convenience wrapper.As long as you have installed the scala runtime you should be fine: compile classes with  and run them with .Just want to add my own answer as additional value for the future readers:those two can't be done using java"},
{"body": " says should import all classes from package , except  and . But it gives me this error: Is there still a way to do this?The   has to be put at the end - not at the beginning:See the detailed info in , page 50, paragraph 4.7"},
{"body": "I have started work on FP recently after reading a lot of blogs and posts about advantages of FP for concurrent execution and performance. My need for FP has been largely influenced by the application that I am developing, My application is a state based data injector into another subsystem where timing is very crucial (close to a 2 million transactions per sec). I have a couple of such subsystems which needs to be tested. \nI am seriously considering using FP for its parallelism and want to take the correct approach, many posts on SO talk about disadvantages and advantages of Scala, Haskell and Clojure wrt language constructs, libraries and JVM support. From a language point of view I am ok to learn any language as long as it will help me achieve the result.Certain posts favor Haskell for pattern matching and simplicity of language, JVM based FP lang have a big advantage with respect to using existing java libraries. JaneStreet is a big OCAML supporter but I am really not sure about developer support and help forums for OCAML.If anybody has worked with handling such large data, please share your experience.Do you want  or do you want ?If you want fast, you should use C++, even if you're using FP principles to aid in correctness.  Since timing is crucial, the support for soft (and hard, if need be) real-time programming will be important.  You can decide exactly how and when you have time to recover memory, and spend only as much time as you have on that task.The three languages you've stated all are , and then only when used in a rather traditional imperative way.  They all use garbage collection, which will introduce uncontrolled random delays in your transactions.Now, that said, it's a  of work to get this running in bulletproof fashion with C++.  Applying FP principles requires considerably more boilerplate (even in C++11), and most libraries are mutable by default.  (Edit: Rust is becoming a good alternative, but it is beyond the scope of this answer to describe Rust in sufficient detail.)Maybe you don't have the time and can afford to scale back on other specifications.  If it is not  but  that is crucial, for example, then you  want Scala over Clojure (see the , where Scala wins every benchmark as of this writing  has lower code size in almost every case (Edit: CLBG is not helpful in this regard any more, though you may find archives supporting these statements on the Web Archive)); OCaml and Haskell should be chosen for other reasons (similar benchmark scores, but they have different syntax and interoperability and so on).As far as which system has the best concurrency support, Haskell, Clojure and Scala are all just fine while OCaml is a bit lacking.This pretty much narrows it down to Haskell and Scala.  Do you need to use Java libraries?  Scala.  Do you need to use C libraries?  Probably Haskell.  Do you need neither?  Then you can choose either on the basis of which one you prefer stylistically without having to worry overly much that you've made your life vastly harder by choosing the wrong one.I've done this with Clojure, which proved pretty effective for the following reasons:I can't speak with quite so much experience of the other languages, but my impression from some practical experience of Haskell and Scala is:Overall, I think you could be happy with any of these. It will probably come down to how much you care about the JVM and your view on type systems."},
{"body": "I wonder how to get a class object for an object type in Scala. Ok, that is a mouth full because of the double meaning for object. So here an example which will fail:If  was class it works perfectly. Any ideas? The reason why  doesn't work is because  is not a .Classes and traits define types, objects do not.Since Main is an object, for your example to work, simply replace your assignment line with;"},
{"body": "In Scala, I can make a caseclass, , and then put it in a list like so:Now, nothing strange here. The following is strange to me. The operator  is a function on a list, right? With any function with one argument in Scala, I can call it with infix notation.\nAn example is  is a function  on the object . The class  I just defined does not have the  operator, so how is the following possible?In Scala 2.8 RC1, I get the following output from the interactive prompt:I can go on and use it, but what is the explanation?From the Spec:You can always see how these rules are applied in Scala by printing the program after it has been through the 'typer' phase of the compiler:It ends with a . And that is the sign, that this function is defined in the class to the right (in  class here).So, it's , not  in your example.One aspect missing in the answers given is that to support  in pattern matching expressions: :so  would produce the same result. The expression  works because the default extractor  is defined for the case class and it can be used infix.If the method name ends with a colon () the method is invoked on the , which is the case here. If the method name doesn't end with colon, the method is invoked on the left operand. For example, ,  is invoked on .So, in your example,  is a method on its right operand, which is a ."},
{"body": "there is an aspect of futures that I do not exactly understand from the official tutorial ref. Do futures in scala have a built in time-out mechanism of some kind? Let's say the example below was a 5 gigabyte text file... does the implied scope of \"Implicits.global\" eventually cause onFailure to fire in a non-blocking way or can that be defined? And without a default time-out of some kind, wouldn't that imply it's possible neither success nor failure would ever fire? You only get timeout behavior when you use blocking to get the results of the .  If you want to use the non-blocking callbacks ,  or , then you would have to roll your own timeout handling.  Akka has built in timeout handling for request/response () messaging between actors, but not sure if you want to start using Akka.  FWIW, in Akka, for timeout handling, they compose two  together via , one which represents the actual async task and one that represents the timeout.  If the timeout timer (via a ) pops first, you get a failure on the async callback.A very simplified example of rolling your own might go something like this.  First, an object for scheduling timeouts:Then a function to take a Future and add timeout behavior to it:Note that the  I am using here is from Netty.I've just created a  class for a coworker:You can specify the timeout when you wait on the future:For , the  method lets you specify a timeout.For ,  lets you specify a timeout.I do not think there is a timeout built-in the execution of a Future.Play framework contains Promise.timeout so you can write code like followingI'm quite surprise this is not standard in Scala. My versions is short and has no dependenciesUsage exampleNobody's mentioned , yet. The flows have an easy  method, and applying that on a single-source stream works like a Future.But, akka-streams also does cancellation so it can actually end the source from running, i.e. it signals the timeout to the source.If you want the writer (promise holder) to be the one who controls the timeout logic, use , in the following way:This way, if your promise completion logic never takes place, your caller's future will still be completed at some point with a failure."},
{"body": "I like Scala's propose of operator precedence but in some rare cases, unmodified rules may be inconvenient, because you have restrictions in naming your methods. Are there ways to define another rules for a class/file, etc. in Scala? If not, would it be resolved in the future?Operator precedence is fixed in the  by the first character in the operator. Listed in increasing order of precedence:And it's not very probable that it will change. It will probably create more problems than it fixes. If you're used the normal operator precedence changing it for one class will be quite confusing.There is no such ability and there is little likelihood of it being added in the forseeable future.See Chapter 6 of the book  for \"Rational\" class example."},
{"body": "I am trying to  convert all the Headers/ColumnNames of a DataFrame in Spark-scala. as of now i come up with following code which only replaces a single column name. Please help on this.If structure is flat:the simplest thing you can do is to use  method:If you want to rename individual columns you can use either  with :which can be easily generalized to multiple columns:or :which use with  to rename multiple columns:With nested structures () one possible option is renaming by selecting a whole structure:Note that it may affect  metadata. Another possibility is to rename by casting:or:For those of you interested in PySpark version:"},
{"body": "For example, there is a Scala array . How to choose a random element from this array?A better answer that does not involve reshuffling the array at all would be this:This also works generically "},
{"body": "Okay, fair warning: this is a follow-up to my  from last week.  Although I think this question isn't as ridiculous.  Anyway, here goes:Assume I have some base trait  with subclasses ,  and , I can declare a collection  for example, that can contain values of type ,  and . Making the subtyping more explicit, let's use the  type bound syntax.Now instead assume I have a typeclass  with members ,  and  (where \"member\" means the compiler can find some , etc. in implicit scope). Similar to above, I want to declare a collection of type , using context bound syntax.This isn't legal Scala, and attempting to emulate may make you feel like a .  Remember that context bound syntax (when used correctly!) desugars into an implicit parameter list for the class or method being defined, which doesn't make any sense here.So let's assume that typeclass instances (i.e. implicit values) are out of the question, and instead we need to use implicit conversions in this case.  I have some type  (the \"v\" is supposed to stand for \"view,\" fwiw), and implicit conversions in scope ,  and .  Now I can populate a , despite ,  and  being otherwise unrelated.But what if I want a collection of things that are implicitly convertible both to views  and ?  I can't say  because my implicit conversions don't magically aggregate that way.I solved my problem like this:Now I can write  like a boss.  For example:The implicit conversions from  and  to type  occur when the sequence is populated, and then the implicit conversions from  to  and  occur when calling  and ..  Not terrible, but there are two things I dislike there:I can give a partial answer for the point 4. This can be obtained by applying a technique such as :"},
{"body": "In  he mentioned in section (5.1 Future Work) that one of the interesting areas of research would be to extend the framework with ambient references and he cited Van Cutsen's paper.Excerpt: And citated paper is: Is this what Akka did? If not, do you think it is still relevant to research this area given the fact that Akka exists today?Yes, this is possible with Akka.There are 2 ways to achieve this as far as I know:"},
{"body": "I was wondering if scala had an equivalent to java's  that can be applied to a function or whatever to ignore any deprecation warnings[1] that function emits?1: Relevant warning in my case is:  I am aware of the problems with stop however there are still some cases where due to legacy code we have to use it.No, and an enhancement request [1] for such a feature was closed as . I agree it would be useful. I expect that the Scala core team aren't against the idea, but they have finite resources and many higher priorities.[1] There is a simple compiler plugin for this:  (a bit shameless plug)"},
{"body": "I believe I understand the basics of inline functions: instead of a function call resulting in parameters being placed on the stack and an invoke operation occurring, the definition of the function is copied at compile time to where the invocation was made, saving the invocation overhead at runtime.So I want to know:Never  anything whose implementation might reasonably change and which is going to be a  part of a .When I say \"implementation change\" I mean that the logic actually might change. For example:Let's say the \"natural comparison\" then changed to be based on an atomic counter. You may find that an application ends up with 2 components, each built and inlined against different versions of the comparison code.Personally, I use @inline for alias:Now, I couldn't find a way to know if it does anything (I tried to jad the generated .class, but couldn't conclude anything). My goal is to explicit what I want the compiler to do. But let it decide what's best, or simply do what it's capable of. If it doesn't do it, maybe later compiler version will."},
{"body": "I'm trying to write a CSV parser using Scala parser combinators. The grammar is based on . I came up with the following code. It almost works, but I cannot get it to correctly separate different records. What did I miss? The default RegexParsers ignore whitespaces including space, tab, carriage return, and line breaks using the regular expression . The problem of the parser above unable to separate records is due to this. We need to disable skipWhitespace mode. Replacing whiteSpace definition to just  does not solve the problem because it will ignore all spaces within fields (thus \"foo bar\" in the CSV becomes \"foobar\"), which is undesired. The updated source of the parser is thus What you missed is whitespace.  I threw in a couple bonus improvements.With Scala Parser Combinators library out of the Scala standard library starting from 2.11 there is no good reason not to use the much more performant Parboiled2 library.\nHere is a version of the CSV parser in Parboiled2's DSL:The default whitespace for  parsers is , which includes new lines. So ,  and  never get a chance to be processed, as it is automatically skipped by the parser. "},
{"body": "How do you test reference equality in Scala?The function you are looking for is , which is a member of :"},
{"body": "I have spent a while learning the topic of Scala execution contexts, underlying threading models and concurrency. Can you explain in what ways does   and  as described in the ?In , it is presented as a means to await api that doesn't implement Awaitable. (Perhaps also just long running computation should be wrapped?).What is it that it actually does? doesn't easily betray its secrets.  is meant to act as a hint to the  that the contained code is blocking and could lead to thread starvation. This will give the thread pool a chance to spawn new threads in order to prevent starvation. This is what is meant by . It's not magic though, and won't work with every .Consider this example:This is using the default global . Running the code as-is, you will notice that the 100 s are all executed immediately, but if you remove , they only execute a few at a time. The default  will react to blocking calls (marked as such) by spawning new threads, and thus doesn't get overloaded with running s.Now look at this example with a fixed pool of 4 threads:This  isn't built to handle spawning new threads, and so even with my blocking code surrounded with , you can see that it will still only execute at most 4 s at a time. And so that's why we say it --it's not guaranteed. As we see in the latter , it's not guaranteed at all.How does it work? As linked,  executes this code: retrieves the  from the current thread, seen . A  is usually just a  with the  trait mixed in. As seen in the source, it is either stored in a , or if it's not found there, it is pattern matched out of the current thread. If the current thread is not a , then the  is used instead.Next,  is called on the current .  is an abstract method in , so it's implementation is dependent on how the  handles it. If we look at the  (when the current thread is not a ), we see that  actually does nothing there. So using  in a non- means that nothing special is done at all, and the code is run as-is, with no side-effects.What about threads that are s? For instance, in the  context, seen ,  does quite a bit more. Digging deeper, you can see that it's using a  under the hood, with the  defined in the same snippet being used for spawning new threads in the . Without the implementation of  from the  (thread), the  doesn't know you're blocking, and won't try to spawn more threads in response.Scala's  too, uses  for its implementation."},
{"body": "In the REPL, I define a function. Note the return type.And if I specify the return type as StringWhy the difference? I can also specify the return type as List[Any], so I guess String is just a wrapper supertype to java.lang.String. Will this have any practical implications or can I safely not specify the return type?This is a very good question! First, let me assure you that you can safely specify the return type.Now, let's look into it... yes, when left to inference, Scala infers , instead of just . So, if you look up \"String\" in the , you won't find anything, which seems to indicate it is not a Scala class either. Well, it has to come from someplace, though.Let's consider what Scala imports by default. You can find it by yourself on the REPL:The first two are packages -- and, indeed,  can be found on ! Is that it, then? Let's check by instantiating something else from that package:So, that doesn't seem to be it. Now, it can't be inside the  package, or it would have been found when looking up on the ScalaDoc. So let's look inside , and there it is!That means  is an  for  (which was imported previously). That looks like a cyclic reference though, but if you check the , you'll see it is defined with the full path:Next, you might want to ask  Well, I don't have any idea, but I suspect it is to make such an important class a little less dependent on the JVM."},
{"body": "If I understand correctly, traits are the closest thing to Java interfaces and class constructors automatically set the variables.But what if I have a class that extends a trait and has a constructor which sets a variable from the trait, so something like:Where I want the  string of the trait been set when I make a  object.The compiler seems to give me errors about this. What is the correct way to achieve this? must define the abstract  in  (would be the same for a ). This can be done in the constructor(of course, it could be done in the body of  too). By default, constructor parameters will be turned to private  if need be, that is if they are used outside the initiailization code, in methods. But you can force the behavior by marking them  or , and possibly control the visibility as inHere you need a public  to implement . The trait declares an uninitialized var; the class then sets it equal to the input parameter.Alternatively,declares the getter/setter pair corresponding to a foo, which are set by the class."},
{"body": "Are there any languages that target the LLVM that:Scala is all of these, but only targets the JVM. F# (and to some extent C#) is most if not all of these, but only targets .NET. What similar language targets the LLVM?There's a  targeting the LLVM.You could also try using F# through .Also, the  project is implementing both the JVM and the .NET CLI on top of LLVM; it's still in its early stages but once it matures you could use it with F#, or any JVM-targeting functional languages (Scala, Clojure, etc.)I'm not sure how far these have progressed, but they may be worth adding to the list:Scala for LLVM - \nTimber for LLVM - \nMono for LLVM - Yes... . C++ has everything on your list except for list comprehensions. It is also the flagship LLVM language.\"Are statically typed\"Yup\"Use type inference\"\"Are functional (i.e. lambda expressions, closures, list primitives, list comprehensions, etc.)\"Lambdas in c++ look like . You can either capture closed over variables by reference (similar to lisp) like so: [&]. Alternatively you can capture by value like so: [=].In C++ map, zip, and reduce are called std::transform and std::accumulate.You can also rig up list comprehensions using a macro and and a wrapper around std::transform if you really want...\"Have first class object-oriented features (inheritance, polymorphism, mixins, etc.)\"Of course. C++ allows virtual dispatch + multiple inheritance + implementation inheritance. Note: mixins are just implementation inheritance. You only need a special \"mixin\" mechanism if your language prohibits multiple inheritance.\"Have a sophisticated type system (generics, covariance and contravariance, etc.)\"C++ templates are the most powerful generics system in any language as far as I know."},
{"body": "What benefits and limitations has  language comparing to , especially from the web applications developer point of view? Comparison charts and proof-links are strongly welcome.P. S. This is not a holy war question (-:I've been working with Ruby and Rails for over 2 years and I'm just about a month into learning Scala and Lift, so my opinion is probably biased, but here it is.Ruby makes you feel amazing. Every new thing I discovered about the language made me giggle like a little schoolgirl. Working with gems is really simple and with  it is probably the best library management system I've came across.There are loads of gems for everything, from API wrappers, to complex .Ruby also has amazing community that will make learning the language really really easy. Take  for example. If you watch them, .For me, Ruby is probably the best thing I've discovered in my whole programming life, because the way the community works made me change my whole approach to programming.On the other hand, I don't see this in the Scala world. I don't want to raise a flame war, but this is just my impression after just starting to learn the language. In Ruby community, everything felt so engaging and made me wanna learn more and more. But with Scala, I tend to run into a lot of obstacles and not as much documentation and tutorials, that would help me overcome them.There are tons of books devoted to very specific topis on Ruby, be it , , , and bunch of other stuff (yes Pragmatic Programmers rule). The best thing is, that the whole community feels like a family, it's not the same as in the Java world, where everything is distributed and nobody talks to eachother, at least that's my impression. So what this comes down to, at least for me, is how easy and engaging it is to get into the world of the language and start doing something, and Ruby is definitely a winner here, at least for me.On the performance side, Scala is faster, no doubt about it. That's one of the reasons I'm getting into Scala, as it has really great concurrency model and allows you to do things that Ruby just can't do. But for most web applications, you won't need to do this.Ruby is slow, that's the only downside there is. It is getting faster and faster, and for 98% of the cases you won't see it as a problem. But Scala is faster.Ruby community also embraces testing and beautiful code, which naturally leads to better apps. "},
{"body": "thenI saw the book writes that when use  it also results . My Scala version is 2.9. prepends a single item whereas  prepends a complete list. So, if you put a  in front of  it is taken as one item, which results in a nested structure."},
{"body": "What is the proper way to restart node in elasticsearch? (preferably via the REST API, java API might be ok too)The correct way to restart a node is to shut it down, using either the  or sending a  signal to the process (eg with ).Once shut down, you can start a new node using whatever you use to run elasticsearch, eg the service wrapper, or just starting it from the command line.If you are using the service wrapper, you can restart a node by passing it the  command: eg  but that is just a convenience wrapper for the above.The  API has been disabled since version 0.11 as it was problematic.There is a restart API analogous to the . Just replace \"shutdown\" with \"restart\". See also the .Every time a node goes down and/or and new node comes up, the cluster redistributes the shards, which may not be desired when you just need to restart a node, therefore you can make use of Rolling restart:More on this: "},
{"body": "Is there an easy way to convert a case class into a tuple?I can, of course, easily write boilerplate code to do this, but I mean without the boilerplate.What I'm really after is a way to easily make a case class lexicographically Ordered. I can achieve the goal for tuples by importing scala.math.Ordering.Implicits._, and voila, my tuples have an Ordering defined for them. But the implicits in scala.math.Ordering don't work for case classes in general.How about calling  in the companion object?You might try extending the  trait, for N=1-22, which  extends. It will give you  a lot of Tuple semantics, like the , , etc. methods. Depending on you how you use your types, this might be sufficient without creating an actual Tuple. Came across this old thread while attempting to do this same thing. I eventually settled on this solution:"},
{"body": "In python I can do this:Here the passed in tuple is being decomposed while its being passed to .Right now in scala right now I am doing this:Is there something I can do here so that the decomposition happens while the arguments are passed in?  Just curious.Thanks. You can create a function and match its input with pattern matching:Or match the input of the method with the  keyword:Another way is to use a function with two arguments and to \"tuple\" it:How about:The _1, _2 etc. access the different elements of the tuple.prints:"},
{"body": "I know you can   with companion objects but I am wondering why as a language design were statics dropped out of class definitions.The O in OO stands for \"Object\", not class.  Being object-oriented is all about the objects, or the instances (if you prefer)Statics don't belong to an object, they can't be inherited, they don't take part in polymorphism.  Simply put, statics aren't object-oriented.Scala, on the other hand,  object oriented.  Far more so than Java, which tried particularly hard to behave like C++, in order to attract developers from that language.They are a hack, invented by C++, which was seeking to bridge the worlds of procedural and OO programming, and which needed to be backwardly compatible with C. It also admitted primitives for similar reasons.Scala drops statics, and primitives, because they're a relic from a time when ex-procedural developers needed to be placated.  These things have no place in any well-designed language that wishes to describe itself as object-oriented.Concerning  it's important to by truly OO, I'm going to shamelessly copy and paste this snippet from Bill Venners on the mailing list:Couldn't have put it better myself!So if you want to create just one of something, then both statics and singletons can do the job.  But if you want that one thing to inherit behaviour from somewhere, then statics won't help you.In my experience, you tend to use that ability far more than you'd have originally thought, especially after you've used Scala for a while.I also posted this question on scala users google group and Bill Venners one of the authors of \"Programming in scala\" reply had some insights.Take a look at this:  and Here is an excerpt:Also take a look at this interview with Martin Odersky (scroll down to Object-oriented innovations in Scala section) Here is an excerpt:From a functional programming perspective static members are generally considered bad (see this  by Gilad Bracha - the father of java generics. It mainly has to do with side effects because of global state). But scala had to find a way to be interoperable with Java (so it had to support statics) and to minimize (although not totally avoid) global states that is created because of statics, scala decided to isolate them into companion objects.Companion objects also have the benefit of being extensible, ie. take advantage of inheritance and mixin composition (separate from emulating static functionality for interop). These are the things that pop into my head when I think about how statics could complicate things: Inheritance as well as polymorphism would require special rules. Here is an example:If you are 100% sure about what gets printed out, good for you. The rest of us can safely rely on mighty tools like  annotation, which is of course optional and the friendly  warning. This leads us to The \"static way\" of accessing stuff is a further special rule, which complicates things. Static members cannot be abstract. I guess you can't have everything, right?And again, these are just things which came to my mind after I gave the matter some thought for a couple of minutes. I bet there are a bunch of other reasons, why statics just don't fit into the OO paradigm.It's true, static member don't exists, BUT, it's possible to associate a singleton    object to each class:to obtain similar resultsObject oriented programming is all about objects and its states(Not touching state full and stateless object). I\u2019m trying to stress  so it\u2019s rational to pull off from objects. "},
{"body": "I have this issue that I have to work around every time. I can't map over something that is contained within a Future using a for comprehension.Example:This gives me the error:But if I do this it works fine: Should i not be able to do this by using a for comprehension, is the reason it works in my other example that I do not flatmap? I'm using Scala 2.10.0.Well, when you have multiple generators in a single for comprehension, you are  the resulting type. That is, instead of getting a , you get a :Now, how would you flatten a ? It can't be a , because you'll be getting multiple , and a  (as opposed to a ) can only store one of them. The same problem happens with , by the way:The easiest way around it is to simply nest multiple for comprehensions:In retrospect, this limitation should have been pretty obvious. The problem is that pretty much all examples use collections, and all collections are just , so they can be mixed freely. Add to that, the  mechanism for which Scala has been much criticized makes it possible to mix in arbitrary collections and get specific types back, instead of .And, to make things even more blurry,  can be converted into an , which makes it possible to combine options with collections as long as the option doesn't come first.But the main source of confusion, in my opinion, is that no one ever mentions this limitation when teaching for comprehensions.Hmm, I think I got it. I need to wrap within a future as the for comprehension adds a flatmap.This works:When I added above I did not see any answers yet. However to expand on this it is possible to do work within one for-comprehension. Not sure if it is worth the Future overhead though (edit: by using successful there should be no overhead).Another way to solve this is to simply use assignment  instead of  when you have another type than the initial map return type. When using assignment that line does not get flat-mapped. You are now free to do an explicit map (or other transformation) that returns a different type. This is useful if you have several transformation where one step that does not have the same return type as the other steps, but you still want to use the for-comprehension syntax because it makes you code more readable.Your original version doesn't compile because  and  are different monads. To see why this is a problem, consider what it desugars to:Clearly  is a list of  pairs, so the argument to our  is a function that maps lists of strings to lists of these pairs. But we need something that maps lists of strings to a  of some kind. So this doesn't compile.The version in your answer does compile, but it doesn't do what you want. It desugars to this:The types line up this time, but we're not doing anything interesting\u2014we're just pulling the value out of the , putting it back into a , and mapping over the result. So we end up with something of type , when we wanted a .What you're doing is a kind of double mapping operation with two (different) nested monads, and that's not something a -comprehension is going to help you with. Fortunately  does exactly what you want and is clear and concise.I find this form more readable than either the serial map or the serial yield:at the expense of the tupling map:or more precisely:"},
{"body": "When creating a  in scala, I call , and I get:This is because the signature for  is: ,\nwhich requires a varargs style argument.Is there a way to convert the  so that it can be accepted via ?Try this: Explicitly typing it as a varargs using  seems to work.Or this should work too:"},
{"body": "I have two strings in scala and I want to find out, if the bigger string () contains a smaller string ().What I found is doing it with regexps and matches like this ():which is (1) grossly overcomplicated for such a simple problem, but more importantly, (2) doesn't work for me, becausereturnsIf you want to do it with maximum efficiency, you may have to write it yourself (or find a good substring searching algorithm somewhere).  If you just want it to work at all, then in Scala:These are  regex searches.  You aren't using the regex match correctly either (edit: because that code asks for an exact match to the whole string, not to find a matching substring), but that's a different issue.  If you want a count of the number of matches, you can do something likeAlthough answered I thought I would also offer this regex style"},
{"body": "I found myself writing something like this quite often:Is there a shorter way to check if some value matches a pattern? I mean, in this case I could just write , but what if the pattern is more complex? Like when matching against a list or any pattern of arbitrary complexity. I'd like to be able to write something like this:I'm relatively new to Scala, so please pardon, if I'm missing something big :)This is exactly why I wrote these functions, which are apparently impressively obscure since nobody has mentioned them.The  operator in Scala is most powerful when used in functional style. This means, rather than \"doing something\" in the  statements, you would return a useful value. Here is an example for an imperative style:It is very understandable that the \"do nothing\" above hurts a little, because it seems superflous. However, this is due to the fact that the above is written in imperative style. While constructs like these may sometimes be necessary, in many cases you can refactor your code to functional style:In this case, you use the whole  statement as a value that you can, for example, assign to a variable. And it is also much more obvious that the  statement must return a value in any case; if the last case would be missing, the compiler could not just make something up.It is a question of taste, but some developers consider this style to be more transparent and easier to handle in more real-world examples. I would bet that the inventors of the Scala programming language had a more functional use in mind for , and indeed the  statement makes more sense if you only need to decide whether or not a certain action needs to be taken. (On the other hand, you can also use  in the functional way, because it also has a return value...)This might help:Now, some explanation on the general nature of the problem. There are three places where pattern matching might happen: ,  and . The rules for them are:There is, however, another situation where  might appear, which is function and partial function literals. For example:Both functions and partial functions will throw an exception if called with an argument that doesn't match any of the case statements. However, partial functions also provide a method called  which can test whether a match can be made or not, as well as a method called , which will turn a  into a , which means non-matching values will result in  instead of throwing an exception.A match is a combination of many different tests:Now, extractors are the methods  or , the first returning  or , and the second returning , where  means no match is made, and  will try to match  as described above.So there are all kinds of syntactic alternatives here, which just aren't possible without the use of one of the three constructions where pattern matches may happen. You may able to emulate some of the features, like value equality and extractors, but not all of them.Patterns can also be used in for expressions. Your code samplecan then be expressed asThe trick is to wrap a to make it a valid enumerator. E.g. List(a) would also work, but I think Some(a) is closest to your intended meaning.The best I can come up with is this:This won't win you any style points though. can be \u201cimproved\u201d to better match your requirement:then:I wouldn't do it, however. The  sequence is really ugly here, and the whole code looks much less clear than a normal match. Plus, you get the compile-time overhead of looking up the implicit conversion, and the run-time overhead of wrapping the match in a  (not counting the conflicts you could get with other, already defined  methods, like the one in ).To look a little bit better (and be less verbose), you could add this def to :and use it like this:which saves you your  line, but requires double braces if you want a block instead of a single statement... Not so nice.Note that this construct is not really in the spirit of functional programming, as it can only be used to execute something that has side effects. We can't easily use it to return a value (therefore the  return value), as the function is partial \u2014 we'd need a default value, or we could return an  instance. But here again, we would probably unwrap it with a match, so we'd gain nothing.Frankly, you're better off getting used to seeing and using those  frequently, and moving away from this kind of imperative-style constructs (following )."},
{"body": "What is the simplest method to remove the last character from the end of a String in Scala?I find Rubys String class has some very useful methods like . I would have used \"oddoneoutz\".headOption in Scala, but it is depreciated. I don't want to get into the overly complex:Please someone tell me there is a nice simple method like chop for something this common.How about using dropRight, which works in 2.8:-Which produces \"abc\" That's basically chop, right? If you're longing for a chop method, why not write your own  library and include it in your projects until you find a suitable, more generic replacement?Hey, look, it's in commons.."},
{"body": "I've read that Scala'a  construct automatically generates a fitting  and  implementation. What does exactly the generated code look like?As my professor used to say, only the code tells the truth! So just take a look at the code that is generated for:We can instruct the Scala compiler to show us the generated code after the different phases, here after the typechecker:So you can see that the calculation of the hash code is delegated to  and the equality depends on the equality of the case class' members.The generated   just calls , which is defined as:So what you get is , where  is the arity of your case class and  are the members of that case class.As of scala 2.9  for case classes uses : .MurmurHash ."},
{"body": "I've got a list of objects  which are all instantiated from the same class. This class has a field which must be unique . What is the cleanest way to iterate the list of objects and remove all objects(but the first) with the same property? Explanation: The groupBy method accepts a function that converts an element to a key for grouping.  is just shorthand for  (the compiler generates a unique name, something like ). So now we have a map . A  extends . So it can be traversed like a list, but elements are a tuple. This is similar to Java's . The map method creates a new collection by iterating each element and applying a function to it. In this case the function is  which is shorthand for .  is just a method of Tuple that returns the second element. The second element is List[Object] and  returns the first elementTo get the result to be a type you want:To explain briefly,  actually expects two arguments, a function and an object that is used to construct the result. In the first code snippet you don't see the second value because it is marked as implicit and so provided by the compiler from a list of predefined values in scope. The result is usually obtained from the mapped container. This is usually a good thing. map on List will return List, map on Array will return Array etc. In this case however, we want to express the container we want as result. This is where the breakOut method is used. It constructs a builder (the thing that builds results) by only looking at the desired result type. It is a generic method and the compiler infers its generic types because we explicitly typed l2 to be  or, to preserve order (assuming  is of type ): is a method that accepts an initial result and a function that accepts an element and returns an updated result. The method iterates each element, updating the result according to applying the function to each element and returning the final result. We go from right to left (rather than left to right with ) because we are prepending to  - this is O(1), but appending is O(N). Also observe the good styling here, we are using a pattern match to extract the elements.In this case, the initial result is a pair (tuple) of an empty list and a set. The list is the result we're interested in and the set is used to keep track of what properties we already encountered. In each iteration we check if the set  already contains the property (in Scala,  is translated to . In , the method  is . That is, accepts an element and returns true / false if it exists or not). If the property exists (already encountered), the result is returned as-is. Otherwise the result is updated to contain the object () and the property is recorded ()Update: @andreypopp wanted a generic method:to use: Also note that this is pretty efficient as we are using a builder. If you have really large lists, you may want to use a mutable HashSet instead of a regular set and benchmark the performance.Here is a little bit sneaky but fast solution that preserves order:Although it uses internally a var, I think it is easier to understand and to read than the foldLeft-solution.One more solutionI found a way to make it work with groupBy, with one intermediary step:Use it like this:Similar to IttayD's first solution, but it filters the original collection based on the set of unique values. If my expectations are correct, this does three traversals: one for , one for  and one for . It maintains the ordering of the original collection, but does not necessarily take the first value for each property. For example, it could have returned  instead.Of course, IttayD's solution is still faster since it does only one traversal. My solution also has the disadvantage that, if the collection has s that are actually the same, both will be in the output list. This could be fixed by removing  and returning  directly, with type . However, it seems like  does not exist... suggestions are welcome!With preserve order:I don't know which version of Scala you are using, but 2.8.2 definitely has "},
{"body": "I am not sure what is the difference between  and  in Scala.The question  has an answer that talks about ordering. That is understandable. But I still don't understand why this works (from REPL):but this does not:What does this error message mean?This line from the documentation is also confusing to me.Why will it be added ? I thought folding works differently.As defined by Scala,  is a linear operation while  is allowed to be a tree operation.  For example:In order to allow arbitrary tree decompositions of a sequential list, you must have a zero that doesn't do anything (so you can add it wherever you need it in the tree) and you must create the same sort of thing that you take as your binary arguments so the types don't change on you depending on how you decompose the tree.(Being able to evaluate as a tree is nice for parallelization.  If you want to be able to transform your output time as you go, you need both a combination operator  a standard start-value-transforms-sequence-element-to-desired-type function just like  has.  Scala has this and calls it , but in some ways this is more like  than  is.)I am not familiar with Scala, but Scala's collection library has a similar design to Haskell's.  This answer is based on Haskell and is probably accurate for Scala as well.Because  processes its inputs from left to right, it can have different input and output types.  On the other hand,  can process its inputs in various orders and so the inputs and output must all have the same type.  This is easiest to see by expanding out the fold expressions.   operates in a specific order:Note that array elements are never used as the first parameter to the combining function.  They always appear on the right of the . does not guarantee a specific order.  It could do various things, such as:Array elements can appear in either parameter of the combining function.  But your combining function expects its first argument to be an int.  If you don't respect that constraint, you end up adding strings to ints!  This error is caught by the type system.The neutral element may be introduced multiple times because, generally, a parallel fold is implemented by splitting up the input and executing multiple sequential folds.  A sequential fold introduces the neutral element once.  Imagine one particular execution of  where the array is split into two separate arrays, and these are folded sequentially in two threads.  (Of course, the real  function does not spit the array into multiple arrays.)  One thread executes , computing .  The other thread executes , computing .  Finally, the partial sums from the two threads are added together.  Note that the neutral element, , appears twice.NOTE: I could be completely wrong here.  My scala is less than perfect.I think that the difference is in the signature of the methods:vsIn short, fold is defined as operating on some type A1 which is a supertype of the array's type, which for your string array the compiler defines as \"Any\" (probably because it needs a type that can store your String  an int- notice that the combiner method passed to fold Fold takes two parameters of the same type?)  That's also what the documentation means when it talks about z- the implementation of Fold could be such that it combines your inputs in parallel, for instance:On the other hand, foldLeft operates on type B (unconstrained) and only asks that you provide a combiner method that takes a parameter of type B and another of your array's type (String, in your case), and produces a B. You are getting a compile-time error because the signature of  only allows folding values of type which is the supertype of the type of the values in the collection, and the only supertype of  (your collection type) and  (the type of your provided zero element) is . So, the type of the fold result is inferred to be  - and  does not have a method .Note that the two versions of  have different signatures:Why do they have different signatures? This is because  could be implemented in parallel, as is the case with parallel collections. When multiple processors fold over the values in the collections, each one of the processors takes a subset of elements of type  and produces the folded value of type , by consecutively applying . The results produced by those processors must be combined together into a final folding value - this is done using the  function, which does exactly that.Now, note that this cannot be done using the  in , because each of the processors  produces a folded value of type . Several values of type  cannot be combined using , because  only combines value  with another value of type  - there is no correspondance between types  and . In your example, assume the 1st processor takes elements  and the 2nd takes element . The first one will produce the folded value , and the second will produce another folded value . Now they have to combine their results to get the final folded value - this is impossible, because the closure  only knows how to combine an  and , and not 2  values.For situations where these types differ, use , in which you have to define how to combine two values of type :The  above defines how to do the final step when the fold result and the elements in the collection have different types. As described above, multiple processors may fold over subsets of elements in the collection. Each one of them will start its folded value by adding the neutral element.In the following example:always returns .However,  should not be used with , as it is not a neutral element:The above may return  or . If you don't use the neutral element for the , the results are nondeterministic. In the same way, you can use the  as a neutral element, but not a non-empty list.As another answer points out, the  method is primarily there to support a parallel fold. You can see this as follows. First we can define a kind of wrapper for integers that allows us to keep track of the operations that have been performed on its instances.Next we can create a parallel collection of these things and an identity element:First we'll try :So our zero value is only used once, as we'd expect, since  performs a sequential fold. Next we can clear the log and try :So we can see that the fold has been parallelized in such a way that the zero value is used multiple times. If we ran this again we'd be likely to see different values in the log.Here are the prototypes of the methodsSo, for fold the result is of type instead of any . Moreover, as specified in the doc, for  the order is not When typing  you assume that , an  is a subtype of . This is why the compiler throws an error. Here we have to see the  of  to understand what happens. Here is what we get: So basically,  is an implementation of  with a constraint on the output type. \nWe can now see that  will in practice be used the same way as in . So we can just conclude that this comment was made because nothing assures that behavior in future implementations. We can already see it now, with : "},
{"body": "Besides integration with dynamic languages on the JVM, what are the other powerful uses of a  in a statically typed language like Scala?I guess a dynamic type could be used to implement several of the features found in JRuby, Groovy or other dynamic JVM languages, like dynamic metaprogramming and method_missing.For example creating a dynamic query similar to Active Record in Rails, where a method name with parameters is translated to an SQL query in the background. This is using the method_missing functionality in Ruby. Something like this (in theory - have not tried to implement anything like this):Allowing usage like this, where you can call methods 'name' and 'findByName' without having them explicitly defined in the Person class:If dynamic metaprogramming was to be added, the Dynamic type would be needed to allow invoking methods that have been added during runtime..Odersky says the primary motivation was integration with dynamic languages: [edit] Martin further confirms this You might also use it for syntactic sugar on maps:To be honest this only saves you a couple of keystrokes from:"},
{"body": "I am about to start a project for a web application that should run on a Tomcat server. I have decided to go for Scala - the other alternative where I work being Groovy - essentially for type safety. I am now faced with the task of choosing the right tools for the job.The project I will need to develop will be accessed only through a JSON API. It will interact both with its own database and with two external services, which expose respectively a JSON and XML API. I will also need to be able to schedule periodic jobs where my application will execute various synchronization tasks with these external services.For the database, I would like to be able to define my models in Scala and automatically generate the schema. If the need arises to change my models, I would like to have migrations to handle it.For this application, I am trying to evaluate Lift, Play! 2 and Scalatra.What do you think would be the best choice for my task?If you are coming from a MVC background then Play will feel the most familiar.   Lift is not MVC and it takes some time to wrap your head around it.There is no reason that you can't use Scala with a Java-based framework like Jersey, Spring MVC, , Restlet, RESTEasy, etc.   Or you could use Scala with Grails just like you can use Java with Grails.  Note - Groovy has type safety starting with version 2.0 so that's something to consider. If you are thinking about Scalatra then don't forget , , ...  Be sure to check out Matt Raible's .  And these other SO questions:\n,  Although you already gave some candidates I recommend to you this talk by Tim Perrett (author of Lift in Action, in which he compares scala webframeworks :\n "},
{"body": "Is there already any collection of best practices for languages like Scala?I've found a work on design patterns for functional languages, . There's  design patterns for OO languages. But are there any patterns for functional-OO hybrids? All I've seen is  list. What is known?Two patterns from Bill Venners; I think both are heavily used in ScalaTest: (similar in structure to the decorator pattern, except it involves decoration for the purpose of class composition instead of object composition). (allows library designers to provide services that their clients can access either through mixins or imports). - just like the \"Scalable Component Abstraction\", it's not a pattern catalog, but it also deals with similar problems (e.g. the Visitor pattern) - an alternative to the Observer.We can also consider the Scala emulation of  type classes a design pattern. The first description (that I could find at least) is in\n. Quite some blog entries are also available with this topic.And I think I'm not completely wrong if I also mention the various monads. You can find a lot of resources dealing with them.While not directly a design pattern catalog itself, the paper \"\" (Martin Odersky; Matthias Zenger) examines three building blocks for reusable components:And it revisits several design pattern (publish/subscribe, subject/observer, Context/Component) to illustrate and understand what language constructs are essential to achieve systems of scalable and dynamic components.  One frequently observed pattern, which badly needs a name, is creating control abstractions with curried parameter lists and by-name parameters. yieldingIn as much as any Object-Functional language is quickly going to acquire an actor library,  a large number of actor-based patterns probably qualify for this question.  Almost any of the patterns in Bob Martin's  is recastable in terms of actors, with patterns like Load Balancer, Message Filter, Content-Based Router, and Content Enricher being especially common in systems architected around coarse-grained actors.Closely related, you might want to explore data structures as defined in  (or hybrid functional) languages. For one, the ability to treat functions as first-class values make some patterns (like ,  or ) unnecessary in some (not all) contexts. Secondly, data structures (and the algorithms that operate on them) are either the plumbing for design patterns, or present certain problems that design patterns attempt to address, see Wikipedia article . Better yet, I'd refer you to ."},
{"body": "I upgraded from IntelliJ IDEA from 12 CE to 13 CE few days ago and it has been hogging up CPU. Every few minutes it'll peak to 450-500% and then come down to 100-200%. Also, I've upgraded my Scala plugin to 0.30.380. Not sure what's causing the issue!?I'm posting this comment by K P as an answer, because K P does not have enough reputation.Go to your home folder, then navigate to Edit the file  by changing the   and  as follows:It decreased the CPU usage to 50%.Try clean up cache and restart. In Intellij 14 go to menu File -> Invalidate Caches / Restart...If anyone's wondering the \"right\" way to edit the idea.vmoptions or idea64.vmoptions file, here it is: On *NIX, you want to copyto:and edit that file to increase the heap memory for IntelliJ IDEA(OP's example settings: Xms = 512m and Xmx = 2048)As other said,increasing memory heap of IDEA is work, my IDEA version is 2016.3.4, following is setting way"},
{"body": "In Scala, the  class is derived from type  (see Scala Reference, 12.3.3). However, this seems counterintuitive to me, since a  (which needs to be defined for all ) has more stringent requirements than a , which can be undefined at some places.The problem I've came accross was that when I have a partial function, I cannot use a  to extend the partial function. Eg. I cannot do:(Hope the syntax is at least remotely right)Why is this subtyping done reversely? Are there any reasons that I've overlooked, like the fact that the  types are built-in?BTW, it would be also nice if  so I needn't have the dummy argument in the example above :-)The difference between the two approaches can be emphasized by looking at two examples. Which of them is right?One:Two:Because in Scala (as in any Turing complete language) there is no guarantee that a Function is total. That function is not defined at 0. A PartialFunction is just a Function that promises to tell you where it's not defined.  Still, Scala makes it easy enough to do what you wantIf you prefer, you can make func2Partial implicit. has methods which  does not, therefore it is the subtype.  Those methods are  and .Your real problem is that s are not inferred sometimes when you'd really like them to be.  I'm hopeful that will be addressed at some future date.  For instance this doesn't work:But this does:Of course you could always do this:And now it's more like you want:"},
{"body": "I have an Akka Actor that makes a call to MyObject.foo().  MyObject is not an Actor.  How do I setup Logging in it?  With an Actor it's simple, because I can just mixin ActorLogging.  In MyObject, I don't have access to context.system.  Do I create an akka.event.Logging with AkkaSystem() and then what for the LogSource implicit?Actually I would redirect Akka logging to  and use this API directly in all unrelated classes. First add this to your configuration:Then choose some SLF4J implementation, I suggest . In your actors continue using  trait. In other classes simply rely on SLF4J API - or even better - try out  facade around SLF4J.Tip: try out the following logging pattern in Logback:The  will print actor path when available (just like standard logging).Using Akka 2.2.1, I was able to put this into my App to get logging outside of an actor:This seems like a simpler solution for unifying an application's logging.I've now settled on simply passing my central logging system around through DI constructor injection (Guice). And in my classes that do logging regularly (where asynchronicity is important), I take the injected ActorSystem and call the in the classes constructor.As has been mentioned, you're spoiled for options for non-actor logging within an actor system. I am going to attempt to provide a set of heuristics to help you determine how you should route logging for your work.You're welcome to mix and match the above behaviors as necessary to meet your requirements. For example, you might choose to bind to SLF4J for libraries and use Akka logging for everything else. Just note that mixing blocking and non-blocking logging could cause race conditions where causes (logged async via an actor) are logged after their effects (logged sync directly)."},
{"body": "I am a big fan of Scala aesthetically, and of a lot of the conceptual work put into things like its typing system and libraries.However, as I have begun tinkering with Scala (and seen some of my coworkers tinker with it) i find myself having to dig for more and more Java knowledge (especially in the way of libraries).This presents me with a few problems:At this point though, i'm not sure whether i should bite the bullet and try and find the quickest and most comprehensive tour through Java to catch myself up on 20 years of Java developments, or whether its reasonable to continue trying to incrementally patch my knowledge as i wander around scala.Any wisdom that scala heads amongst us could offer would be greatly appreciated.P.S.\nI have no doubt in my ability to familiarize myself with Scala syntax, and i'm perfectly comfortable and happy with functional programming and the paradigms in the scala community.  But a programmer's competence is not just based on one's ability to teach oneself, but also one's ability to learn from, and adopt tools and skills from other people.You should take a  approach to learning Java. Learn it when you need it.In my opinion, much of the old Java knowledge is out of date, much of the new tutorials are redundant. You certainly don't want to bother yourself with Java's antiquated , for example. Many Java-based frameworks can be safely ignored. And the heavyweight JavaEE stack can be safely bypassed until you were forced to use a part of it.Many common patterns in Java are much simpler in Scala, with the former being burdened with much boilerplate code. Core logic should always be implemented in Scala. I believe you can do most of your work directly in Scala and only need to dip down into Java when building things like Swing or integrating with Spring, etc.In regard to choosing and using Java libraries, my personal guidelines are:That's a bit tounge-in-cheek, but is the impression I get about the maturity and stability of third party libraries after having done Java for the last 12 years.If you want to learn Spanish, start to learning Spanish, not Latin. Same for programming languages. There are two things from Java that are good to know: The first thing are APIs. But you need only a general overview about what exists. Even long time Java programmers don't know all the details. And finding the right API or lib for a problem is usually easy, as Java is so common, and even with weak Google Fu you shouldn't have any problems. The second thing you need to know are some basic principles and limitations of Java and the JVM (including how to build and run), that help you to understand some of Scala's problems and design decisions. One typical example would be \"type erasure\": If you don't understand this limitation of Java's generics, you'll run in problems when using generics in Scala.As you can see, the things you really need to know is limited. Everything else can be picked up on the way. It depends entirely on your definition of \"Competent Java Programmer\".A good understanding of the Java memory model and garbage collection strategies will help.  As will experience with a wide range of 3rd part libraries.On the other hand... if you're deeply indoctrinated into getter/setter dependency injection used by libraries like Spring, then you'll have to unlearn a lot of bad habits before you can properly deal with immutability  -  In this case, prior Java exposure is probably going to hinder you in learning Scala.There are lots of folk coming to scala and clojure from ruby/python, lisp/scheme, C#, who need to pick up on:primitives, autoboxingJVM startup options, how hotspot works, 32 vs. 64 bit, use openJDK?benchmarks, profiling, how to read stacktraces;zillions of test libs for every conceivable need. java.util.concurrentSwing API calls;  Classpath;  Maven, ant;  Hudson, Interfacesnamespaces/packages/directory layout, and all the other things automated by:intelliJ, netBean, or eclipse+UseConcMarkSweepGC -XX:+CMSIncrementalMode -XX:, EscapeAnalysis etc.\ncompressed oops, \n"},
{"body": "Given the following constructs for defining a function in Scala, can you explain what the difference is, and what the implications will be?vs.Thanks for the quick responses.  These are great.  The only question that remains for me is:If I omit the parenthesis, is there still a way to pass the function around?  This is what I get in the repl:Here are some similar questions I noticed:If you include the parentheses in the definition you can optionally omit them when you call the method. If you omit them in the definition you can't use them when you call the method.Additionally, you can do something similar with your higher order functions:Here  will only take  and not . What use this is, I don't know. But it does show that the types are distinct.Let me copy my answer I posted on :A Scala method of 0-arity can be defined with or without parentheses . This is used to signal the user that the method has some kind of side-effect (like printing out to std out or destroying data), as opposed to the one without, which can later be implemented as .See :To answer your second question, just add an :"},
{"body": "I'm developing on the standard Lift platform (maven and jetty). I'm repeatedly (once every couple of days) getting this:This is in my dev environment. It's not a problem because I can keep restarting the server. In deployment I'm not having these problems so it's not a real issue. I'm just curious.I don't know too much about the JVM. I think I'm correct in thinking that permanent generation memory is for things like classes and interned strings? What I remember is a bit mixed up with the .NET memory model...Any reason why this is happening? Are the defaults just crazily low? Is it to do with all the auxiliary objects that Scala has to create for Function objects and similar FP things? Every time I restart Jetty with newly written code (every few minutes) I imagine it re-loads classes etc. But even so, it cant' be that many can it? And shouldn't the JVM be able to deal with a large number of classes?CheersJoeFrom :May be this could help.Edit July 2012 (almost 3 years later): comments (and I have updated the answer above):See the full  for mroe.If you see this when running  ,\nset the . For Linux:For Windows:Should be fine now. If not, increase .You can also put these permanently to your environment.This is because of the reloading of classes as you suggested. If you are using lots of libraries etc. the sum of classes will rapidly grow for each restart. Try monitoring your jetty instance with VisualVM to get an overview of memory consumption when reloading.The mailing list () is the official support forum for Lift, and where you'll be able to get a better answer. I don't know the particulars of your dev setup (you don't go into much detail), but I assume you're reloading your war in Jetty without actually restarting it. Lift doesn't perform dynamic class generation (as suggested by VonC above), but Scala compiles each closure as a separate class. If you're adding and removing closures to your code over the course of several days, it's possible that too many classes are being loaded and never unloaded and taking up perm space. I'd suggest you enable the options JVM options mentioned by VonC above and see if they help.The permanent generation is where the JVM puts stuff that will probably not be (garbage) collected like custom classloaders.Depending on what you are deploying, the perm gen setting can be low. Some applications and/or containers combination do contain some memory leaks, so when an app gets undeployed sometimes some stuff like class loaders are not collected, resulting in filling the Perm Space thus generating the error you are having.Unfortunately, currently the best option in this case is to max up the perm space with the following jvm flag (example for 192m perm size):The other option is to make sure that either the container or the framework do not leak memory."},
{"body": "When running the Scala interpreter in Ubuntu 14.04, I get the following message printed as the first line:Followed by the familiar \"Welcome to Scala\" message.I'm worried because I haven't seen that when running Scala before - what does it mean, is it dangerous, etc?Apparently the environment variable  is set to  - I didn't set that, but what did and why? Can I safely unset it?Additional info:You can disable jayatana just for the current shell session by unsetting  like so:That way it will still be enabled for applications needing it.This occurs if you have installed jayatana which allows the hidden global menu in eclipse to work with Unity:that puts JARs in the  folder which is echoed by the JVM when starting up.  If you remove that software, you will not see the message:You may have to delete:and restart your session so that the environment variable  is not set.This is because of Jayatana, which is added to Ubuntu 15.04 in order to enable global menu for Java Swing applications. You can safely remove this message by entering the following command in a terminal. This command will remove the auto-start configuration of Jayatana.However, removing this option will disable the global menu support of Java Swing applications like Netbeans, IntelliJ IDEA. I have shared my workaround to suppress the message without losing the global menu in this article: On Windows,\nset JAVA_TOOL_OPTIONS=Setting to null will remove it from the environment. If you don't want to uninstall anything you can create a key store from a terminal:(you should create it at project's forlder)And then in Android Studio click Build->Generate Signed APK and choose created keystore.If you are using Windows OS, goto environment variables and delete or set to null the varible JAVA_TOOL_OPTIONS. "},
{"body": "I have played around with Scala for a while now, and I know that traits can act as the Scala equivalent of both interfaces and abstract classes.  How exactly are traits compiled into Java bytecode?I found some short explanations that stated traits are compiled exactly like Java interfaces when possible, and interfaces with an additional class otherwise. I still don't understand, however, how Scala achieves class linearization, a feature not available in Java.Is there a good source explaining how traits compile to Java bytecode?I'm not an expert, but here is my understanding:Traits are compiled into an interface and corresponding class.becomes the equivalent of...Which leaves the question: How does the static bar method in Foo$class get called?  This magic is done by the compiler in the class that the Foo trait is mixed into.becomes something like...Class linearization just implements the appropriate version of the method (calling the static method in the Xxxx$class class) according to the linearization rules defined in the language specification.For the sake of discussion, let's look the following Scala example using multiple traits with both abstract and concrete methods:At the moment (i.e. as of Scala 2.11), a single trait is encoded as:The primary advantage of this encoding is that a trait without concrete members (which is isomorphic to an interface) actually  compiled to an interface.However, Scala 2.12 requires Java 8, and thus is able to use default methods and static methods in interfaces, and the result looks more like this:As you can see, the old design with the static methods and forwarders has been retained, they are just folded into the interface. The trait's concrete methods have now been moved into the interface itself as  methods, the forwarder methods aren't synthesized in every class but defined once as  methods, and the static  method (which represents the code in the trait body) has been moved into the interface as well, making the companion static class unnecessary.It could probably be simplified like this:I'm not sure why this wasn't done. At first glance, the current encoding might give us a bit of forwards-compatibility: you can use traits compiled with a new compiler with classes compiled by an old compiler, those old classes will simply override the  forwarder methods they inherit from the interface with identical ones. Except, the forwarder methods will try to call the static methods on  and  which no longer exist, so that hypothetic forwards-compatibility doesn't actually work.A very good explanation of this is in:Quote:In the context of Scala 12 and Java 8, you can  see another explanation in :"},
{"body": "Isn't toList a method that converts something into a List?If yes so why can't I use parenthesis with it? I must be missing something more fundamental here.Here is the example:If a method is defined asthen it must be called aswith no extra parentheses.  We say that this method has .We could also define a parameter list but put nothing in it:Now, we could call either ofsince Scala allows the shortcut of omitting parentheses on method calls.As far as the JVM is concerned, there is no difference between the first definition (\"zero parameter lists\") and the second (\"one empty parameter list\").  But Scala maintains a distinction.  Whether this is a good idea or not is debatable, but the motivation might be clearer when you realize that we can alsowhich is known as , and then call any ofand now, if we were to convert this into a function, we would type it aswhile the second definition would bewhich is clearly different conceptually, even if practically you use it the same way (put in nothing and sooner or later get out a ).Anyway,  doesn't need any parameters, and the Scala standard is to leave off the parens unless the method changes the object itself (rather than just returning something), so it's  without any parens afterwards.  And thus you can only call it as .Your second line is actually interpreted as since  is \"magic\" syntax for .That's why the complier complains about \"not enough arguments\", as the apply method on lists takes one argument.Thus you can write e.g.:Let me answer from Scala coding style perspective.Scala  says...Omit empty parenthesis, only be used when the method in question has no side-effects (purely-functional). In other words, it would be acceptable to omit parentheses when calling queue.size, but not when calling println(). Religiously observing this convention will dramatically improve code readability and will make it much easier to understand at a glance the most basic operation of any given method. Resist the urge to omit parentheses simply to save two characters!"},
{"body": "In Scala, what's the best way to dynamically instantiate an object and invoke a method using reflection?I would like to do Scala-equivalent of the following Java code:In the above code, both the class name and the method name are passed in dynamically. The above Java mechanism could probably be used for  and , but the Scala types don't match one-to-one with that of Java. For example, a class may be declared implicitly for a singleton object. Also Scala method allows all sorts of symbols to be its name. Both are resolved by name mangling. See .Another issue seems to be the matching of parameters by resolving overloads and autoboxing, described in .  There is an easier way to invoke method reflectively without resorting to calling Java reflection methods: use Structural Typing.Just cast the object reference to a Structural Type which has the necessary method signature then call the method: no reflection necessary (of course, Scala is doing reflection underneath but we don't need to do it).The answers by  and  are quite good, so I'll just complement with one Scala 2.8 Experimental feature. In fact, I won't even bother to dress it up, I'll just copy the scaladoc.The instanciation part  use the : see this So the true type safe solution would be using a Factory.Note: as stated in , Manifest is here to stay, but is for now \"only use is to give access to the erasure of the type as a Class instance.\"You can then get a method through reflection:and invoke itWorking up from @nedim's answer, here is a  for a full answer, \nmain difference being here below we instantiate naive classes. This code does not handle the case of multiple constructors, and is by no means a full answer.Here is a  that would compile it:In case you need to invoke a method of a Scala 2.10 object (not class) and you have the names of the method and object as s, you can do it like this:This prints  to standard output.\nFurther details in ."},
{"body": "I'm wondering how can I use multiple type pattern matching. I have:So I'd like to write something like:I saw similar construction in some tutorial, but it gives me error:So is there a way to define few different types in on case clause? I think it would make code prettier. As if I will have 5 of them, I will write same code 5 times (calling doSomething()).Thanks in advance!You are missing the parenthesis for your case classes.\nCase classes without parameter lists are deprecated.Try this:If you have too many params for your case classes and don't like having to write long  patterns, then maybe:Or just:But perhaps you just wanted singleton case objects?"},
{"body": "I'm wondering if there is any ideomatic way to chain multiple InputStreams into one continual InputStream in Java (or Scala).What I need it for is to parse flat files that I load over the network from an FTP-Server. What I want to do is to take file[1..N], open up streams and then combine them into one stream. So when file1 comes to an end, I want to start reading from file2 and so on, until I reach the end of fileN. I need to read these files in a specific order, data comes from a legacy system that produces files in barches so data in one depends on data in another file, but I would like to handle them as one continual stream to simplify my domain logic interface. I searched around and found PipedInputStream, but I'm not positive that is what I need. An example would be helpful.It's right there in JDK! Quoting :You want to concatenate arbitrary number of s while  accepts only two. But since  is also an  you can apply it recursively (nest them):...you get the idea.This is done using SequencedInputStream, which is straightforward in Java, as Tomasz Nurkiewicz's answer shows.  I had to do this repeatedly in a project recently, so I added some Scala-y goodness via the \"pimp my library\" pattern.With that, I can do stream sequencing as followsor evenAnother solution: first create a list of input stream and then create the sequence of input streams:Here is a more elegant solution using Vector, this is for Android specifically but use vector for any Java"},
{"body": "What's the purpose of Symbol and why does it deserve some special  syntax e. g. ?Symbols are used where you have a closed set of identifiers that you want to be able to compare quickly. When you have two  instances they are not guaranteed to be interned[1], so to compare them you must often check their contents by comparing lengths and even checking character-by-character whether they are the same. With  instances, comparisons are a simple  check (i.e.  in Java), so they are constant time (i.e. O(1)) to look up.This sort of structure tends to be used more in dynamic languages (notably Ruby and Lisp code tends to make a lot of use of symbols) since in statically-typed languages one usually wants to restrict the set of items by type.Having said that, if you have a key/value store where there are a restricted set of keys, where it is going to be unwieldy to use a static typed object, a -style structure might well be good for you.: Java s are interned in some cases anyway; in particular string literals are automatically interned, and you can call the  method on a  instance to return an interned copy. Not all s are interned, though, which means that the runtime still has to do the full check unless they are the same instance; interning makes comparing two equal interned strings faster, but does not improve the runtime of comparing different strings. s benefit from being  to be interned, so in this case a single reference equality check is both sufficient to prove equality or inequality.[1] Interning is a process whereby when you create an object, you check whether an equal one already exists, and use that one if it does. It means that if you have two objects which are equal, they are precisely the same object (i.e. they are reference equal). The downsides to this are that it can be costly to look up which object you need to be using, and allowing objects to be garbage collected can require complex implementation.s are interned.The purpose is that  are more efficient than s and s with the same name are refered to the same  object (so having the same hashcode).Have a look at this read about Ruby symbols: You can only get the name of a :It's not very useful in Scala and thus not widely used. In general, you can use a symbol where you'd like to designate an identifier.For example, the reflection invocation feature which was planned for 2.8.0 used the syntax  where 'o' was a method added to Any and Symbol was added the method apply(Any*) (both with 'pimp my library'). Another example could be if you want to create an easier way to create HTML documents, then instead of using \"div\" to designate an element you'd write 'div. Then one can imagine adding operators to Symbol to make syntactic sugar for creating elements"},
{"body": "Suppose I have the traits Student, Worker, Underpaid, and Young.How could I declare a class (), CollegeStudent, with all these traits? I am aware of the simplests cases, such as CollegeStudent with one or two Traits:It is easy, when declaring a class you just use the \"with\" keyword as often as you wantthe order of the traits can be important if a trait is changing the behavior of the class, it all depends on traits you are using.Also if you don't want to have a class which always uses the same traits you can use them later:I think that it is very important to explain not only the syntax, but . I found the explanation in Jason Swartz's  (page 177) quite enlightning.And since it determines the shape of the inheritance tree, the linearization order is indeed one very important question to regard. As an example,  (where A is a class and B\nand C are traits) would become . The following few lines, also from the book, illustrate that perfectly:A call to  would have the REPL print the following:Which perfectly reflects the structure of the linearized inheritance graph."},
{"body": "How do you declare and initialize a variable to be used locally in a Play2 Scala template?I have this:declared at the top of the template, but it gives me this error:basically, you have to wrap the block in which you are going to use itActually, @c4k 's solution is working (and quite convenient) as long as you don't try to change the variable's value afterwards, isn't it?You simply place this at the top of your template:or, if it's a more complicated expression, you do this:You can even work with things like lists like that:For the given example, this would beIn the comments you said, that it gets typed to HTML type. However, that is only relevant if you try to overwrite  again, isn't it?scala template supports this, you can define variable in template   if you want to change its in template value you can byexamplevirtualeyes' solution is the proper one, but there is also other possibility, you can just declare a view's param as usually with default value, in such case you'll have it available for whole template + you'll keep possibility for changing it from the :controller:Note that Java controller doesn't recognise templates' default values, so you need to add them each time:If you don't want to wrap all your content with @defining, you can do this :The @defining directive is really unreadable in a template...There is one obvious solution which looks quite clean and may be preferred sometimes: define a scope around the template, define your variable inside of it, and let the scope produce the html code you need, like this:This has some drawbacks:The generated code:"},
{"body": "I am a newbie scala programmer and came across a weird behavior.Above basically I want to return true if  and . Otherwise, I want to return false. Now above I have read that there is no need to add a return statement in scala. So I have omitted  above. But it doesn't return the boolean. If I add a return statement as . it works perfectly. Why is it so?Also, why is it considered a bad practice to have return statements in scalaIt's not as simple as just omitting the  keyword.  In Scala, if there is no  then the last expression is taken to be the return value.  So, if the last expression is what you want to return, then you can omit the  keyword.  But if what you want to return is  the last expression, then Scala .An example:Here the last expression of the function  is an if/else expression that evaluates to a String.  Since there is no explicit  marked, Scala will infer that you wanted to return the result of this if/else expression: a String.Now, if we add something  the if/else expression:Now the last expression is an if/else expression that evaluates to an Int.  So the return type of  will be Int.  If we really wanted it to return the String, then we're in trouble because Scala has  that that's what we intended.  Thus, we have to fix it by either storing the String to a variable and returning it after the second if/else expression, or by changing the order so that the String part happens last.Finally, we can avoid the  keyword even with a nested if-else expression like yours:}This topic is actually a little more complicated as described in the answers so far. This  explains it in more detail and gives examples on when using return will actually break your code (or at least have non-obvious effects).At this point let me just quote the essence of the post. The most important statement is right in the beginning. Print this as a poster and put it to your wall :-)It gives one example, where it actually breaks something, when you inline a functionbecauseThis is only one of the examples given in the linked post and it's the easiest to understand. There're more and I highly encourage you, to go there, read and understand.When you come from imperative languages like Java, this might seem odd at first, but once you get used to this style it will make sense. Let me close with another quote:I don't program Scala, but I use another language with implicit returns (Ruby). You have code after your  block -- the last line of code is what's returned, which is why you're not getting what you're expecting.EDIT: Here's a simpler way to write your function too. Just use the boolean value of isEmpty and count to return true or false automatically:By default the last statement of a function will be returned.\nIn your example there is another statement after the point, where you want your return value.\nIf you want to return anything prior to your last statement, you still have to use .You could modify your example like this, to return a  from the first partDon't write  statements without a corresponding . Once you add the  to your fragment you'll see that your  and  are in fact the last expressions of the function."},
{"body": "Let's say I have this code:I expected  to only return , but instead, it returned . I know I could use  to extract only that part, but I'd have to have a pattern for the entire string, something like:Is there another way of achieving this, without using the classes from  directly, and without using unapply? Here's an example of how you can access  of each match:This prints  ().Depending on the complexity of the pattern, you can also use lookarounds to  match the portion you want. It'll look something like this: The above also prints  ().You want to look at , you're currently looking at , which is \"the entire matched string\".See ."},
{"body": "I find myself writing code like this when I want to repeat some execution n times:I'm looking for a shorter syntax like this:Does something like this exist in Scala already?I thought about using Range's foreach() method, but then the block needs to take a parameter which it never uses.You could easily define one using Pimp My Library pattern.The Range class has a foreach method on it that I think is just what you need. For example, this:produced With :With :And that works as you would expect with most types (more precisely, for ):You could also say  which only works if your  is a monadic value. It has better type-safety, since you can do this:but not this:Let me see you pull that off in Ruby.I'm not aware of anything in the library. You can define a utility implicit conversion and class that you can import as needed.Rahul just posted a similar answer while I was writing this...It can be as simple as this:"},
{"body": "I've seen \"<:\" and \">:\" and \"<%\" etc, can someone give (or locate) a good description of these? What are the possible constraints, what do they do, and whats an example of when to use them? means that  is a subtype of . This is also called an . Similarly,  means that  is a supertype of , a . is a view bound, and expresses that  must come equipped with a  that maps its values into values of type .It's confusing for me too, and I have a Masters in programming languages from Berkeley.There are two different beasts here, but they're all know as \"bounds\" and not \"constraints\"...First are the type bounds:These are essentially the same as  and  in java, and will actually be encoded as such in the generated bytecode, which is good for interop :)Then comes the syntactic sugar:These are NOT encoded in a way that Java could possibly understand (although they are represented in the , an annotation that scala adds to all classes to help the compiler, and which would ultimately be the base of an Scala reflection library)Both of these are converted to implicit parameters:For this reason, you can't combine your own implicits with either view bounds or context bounds, as Scala only permits one block labelled as implicit for any function or constructor.If you do need to use your own implicits then you must first manually convert any such bounds to the unsugared version and add this to the implicit block."},
{"body": "In Scala, for many (all?) types of collections you can create views.What exactly is a view and for which purposes are views useful?Views are non-strict versions of collections. This means that the elements are calculated at access and not eagerly as in normal collections.As an example take the following code:Just this will not print anything but every access to the list will perform the calculation and print the value, i.e. every call to  will result in  being printed. If you want to get a strict version of the collection again you can call  on it. In this case you will see all numbers printed out.One use for views is when you need to traverse a collection of values which are expensive to compute and you only need one value at a time. Also views let you build lazy sequences by calling  on them that will also cache the evaluated elements.See  from .One use case is when you need to collect first result of elements transformation:Prints: While: Prints: "},
{"body": "There are claims that Scala's type system is Turing complete. My questions are:I guess this applies to languages and type systems in general.There is a blog post somewhere with a type-level implementation of the SKI combinator calculus, which is known to be Turing-complete.Turing-complete type systems have basically the same benefits and drawbacks that Turing-complete languages have: you can do anything, but you can prove very little. In particular, you cannot prove that you will actually eventually do something.One example of type-level computation are the new type-preserving collection transformers in Scala 2.8. In Scala 2.8, methods like ,  and so on are guaranteed to return a collection of the same type that they were called on. So, if you  a , you get back a  and if you  a  you get back a .Now, as you can see,  can actually transform the element type. So, what happens if the new element type cannot be represented with the original collection type? Example: a  can only contain fixed-width integers. So, what happens if you have a  and you map each number to its string representation?The result  be a , but that's impossible. So, Scala chooses the most derived supertype of , which can hold a , which in this case is a .All of this computation is going on during , or more precisely during , using type-level functions. Thus, it is statically guaranteed to be type-safe, even though the types are actually computed and thus not known at design time.My  on encoding the SKI calculus in the Scala type system shows Turing completeness.For some simple type level computations there are also some examples on how to encode the natural numbers and addition/. Finally there is a great  on type level programming over on Apocalisp's blog."},
{"body": "Suppose I have a Java class with multiple constructors:How can I extend it in Scala and still provide access to all three of Base's constructors? In Scala, a subclass can only call one of it's superclass's constructors. How can I work around this rule?Assume the Java class is legacy code that I can't change.It's easy to forget that a trait may extend a class. If you use a trait, you can postpone the decision of which constructor to call, like this:Traits may not themselves have constructor parameters, but you can work around that by using abstract members instead. - Assuming that each of your constructors ultimately create the   of the object, create a companion object with \"static\" methods to create this stateThen create a private constructor taking the state and (public) overloaded constructors which defer to the primary constructorThis is a silly answer that would probably work somewhat but might be too much effort if the Java class has way too many constructors, but:Write a subclass in Java that implements a constructor that takes all the inputs the various other constructors would and calls the proper constructor of its superclass based on the presence or absence of inputs (via usage of \"null\" or some sort of sentinel values), then subclass that Java class in Scala and assign the sentinel values as default parameters.I would pick the most generic one (in this case, String) and do the internal conversion yourself if it meets the other criteria.Although I admit this is not the best solution and something strikes me as wrong about it. :-("},
{"body": "Can please someone explain the differences between Functor and Monad in the Scala context?Scala itself really does not emphasize the  and  terms that much. I guess using  is the functor side, using  is the Monad side.For me looking and playing around with  has been so far the best avenue to get a sense of those functional concepts in the scala context (versus the haskell context). Two years ago when I started scala, the scalaz code was gibberish to me, then a few months ago I started looking again and I realized that it's really a clean implementation of that particular style of functional programming.For instance the  implementation shows that a monad is a pointed  because it extends the  trait (as well as the  trait). I invite you to go look at the code. It has linking in the source itself and it's really easy to follow the links.So functors are more general. Monads provide additional features. To get a sense of what you can do when you have a functor or when you have a monad, you can look at You'll see utility methods that need an implicit functor (in particular applicative functors) such as  and sometime methods that needs a full monad such as . Taking  as the reference point, a type  (that is, a type F which is parameterized by some single type) is a functor if a function can be lifted into it. What does this mean:That is, if I have a function , a functor , then I now have a function . This is really just the reverse-way of looking at scala's  method, which (ignoring the  stuff) is basically:What I find interesting about this is that you might immediately jump to the (incorrect) conclusion that a Functor is a \"container\" of s. This is an unnecesssary restriction. For example, think about a function . If I have a function  and a function  then clearly, by composition, I have a function . But now, look at it this way:So the type X => A for some fixed X is also a functor. In , functor is designed as a trait as follows:hence the  method above is implementedA couple of functor instances:In , a monad is designed like this:It is not particularly obvious what the usefulness of this might be. It turns out that the answer is \"very\". I found Daniel Spiewak's  extremely clear in describing why this might be and also Tony Morris's stuff on , a good practical example of what might be meant by .The best article laying out in details those two notions is \"\" from .It is the basic layer from which you define:All three elements are used to define a . A while ago I wrote about that:  (I'm no expert though) The first thing to understand is the type ' T[X] ' : It's a kind of \"context\" (is useful to encode things in types and with this you're \"composing\" them) But see the other answers :)Ok, now you have your types inside a context, say M[A] (A \"inside\" M), and you have a plain function f:A=>B ... you can't just go ahead and apply it, because the function expects A and you have M[A]. You need some way to \"unpack\" the content of M, apply the function and \"pack\" it again. If you have \"intimate\" knowledge of the internals of M you can do it, if you generalize it in a trait you end with And that's exactly what a functor is. It transforms a T[A] into a T[B] by applying the function f.A Monad is a mythical creature with elusive understanding and multiple metaphors, but I found it pretty easy to understand once you get the applicative functor:Functor allow us to apply functions to things in a context. But what if the functions we want to apply are already in a context? (And is pretty easy to end in that situation if you have functions that take more than one parameter).Now we need something like a Functor but that also takes functions already in the context and applies them to elements in the context. And that's what the applicative functor is. Here is the signature:So far so good.\nNow comes the monads: what if now you have a function that puts things in the context? It's signature will be g:X=>M[X] ... you can't use a functor because it expects X=>Y so we'll end with M[M[X]], you can't use the applicative functor because is expecting the function already in the context M[X=>Y] .So we use a monad, that takes a function X=>M[X] and something already in the context M[A] and applies the function to what's inside the context, packing the result in only one context. The signature is:It can be pretty abstract, but if you think on how to work with \"Option\" it shows you how to compose functions X=>Option[X]EDIT: Forgot the important thing to tie it: the >>= symbol is called  and is  in Scala. (Also, as a side note, there are some laws that functors, applicatives, and monads have to follow to work properly).I think this great blog post will help you first for . "},
{"body": "I am starting to use  build my Scala code (and handle dependencies). As far as I know if I useon the command line this will run the main class of the main project.Is it possible to 'run' within any other project from the command line, i.e. not in the interactive session mode? (I'm thinking about something that might look like  or whatever...)What I would do in interactive mode is this:This seems to be straightforward enough, but I can not find any documentation describing this behavior. Hints would be much appreciated...You simply have to quote each command (as in the second example ), so in your case it would be: also workThis works:Worked for me:Also this may be of some help:"},
{"body": "I'm using SBT (within IntelliJ IDEA) to build a simple Scala project.I would like to know what is the  to build an  file (aka Fat JAR, Super JAR).I'm currently using SBT but when I'm submiting my JAR file to  I get the following error:Or this error during compilation time:It  it is because some of my dependencies include signature files (META-INF) which needs to be removed in the final Uber JAR file.I tried to use the  plugin like that:/project/assembly.sbt/project/plugins.sbt/build.sbtWhen I click \"\" in IntelliJ IDEA I get a JAR file. But I end up with the same error...I'm new to SBT and not very experimented with IntelliJ IDE.Thanks.Finally I totally skip using IntelliJ IDEA to avoid generating noise in my global understanding :)I started reading the .I created my project with the following file structure :Added the   in my  file. Allowing me to build a fat JAR :My minimal  looks like :: The  means not to include the dependency in the final fat JAR (those libraries are already included in my workers): META-INF discarding .: Meaning of  and Now I can build my fat JAR using SBT () by running the following command in my  root folder:My fat JAR is now located in the new generated  folder :Hope that helps someone else.For those who wants to embeed SBT within IntelliJ IDE: 3 Step Process For Building Uber JAR/Fat JAR in IntelliJ Idea: : JAR file having all external libraray dependencies in it.\nVoila!!! Uber JAR is built. JAR will be in \nAdd the following line to your project/plugins.sbtAdd the following to your build.sbtThe Assembly merge strategy is used to resolve conflicts occurred when creating fat jar."},
{"body": "I want to get the type of a variable at runtime.\nHow do I do this?So, strictly speaking, the \"type of a variable\" is always present, and can be passed around as a type parameter. For example:But depending on , that won't help you. For instance, may want not to know what is the type of the variable, but to know if the type of the  is some specific type, such as this:Here it doesn't matter what is the type of the variable, . What matters, what is checked is the type of , the value. In fact,  is useless -- you might as well have written it  instead. Also, this uses either  or a value's , which are explained below, and cannot check the type parameters of a type: you can check whether something is a  ( of something), but not whether it is, for example, a  or .Another possibility is that you want to  the type of the variable. That is, you want to convert the type into a value, so you can store it, pass it around, etc. This involves reflection, and you'll be using either  or a . For example:A  will also let you use type parameters you received on . This won't work:But this will:Here I'm using the  syntax, , which works just like the implicit parameter in the previous  example, but uses an anonymous variable.One can also get a  from a value's , like this:A  is limited in that it only covers the base class, but not its type parameters. That is, the  for  and  is the same, . If you need type parameters, then you must use a  instead. A  however, cannot be obtained from a value, nor can it be used on a pattern match, due to JVM's .Examples with  can get quite complex -- not even comparing two type tags is not exactly simple, as can be seen below:Of course, there are ways to make that comparison return true, but it would require a few book chapters to really cover , so I'll stop here.Finally, maybe you don't care about the type of the variable at all. Maybe you just want to know what is the class of a value, in which case the answer is rather simple:It would be better, however, to be more specific about what you want to accomplish, so that the answer can be more to the point.I think the question is incomplete. if you meant that you wish to get the type information of some typeclass then below: If you wish to print as you have specified then:If you are in repl mode then Or if you just wish to know what the class type then as @monkjack explains  might solve the purposeIf by  you mean the runtime class of the object that the variable points to, then you can get this through the class reference that all objects have.If you however mean the type that the variable was declared as, then you cannot get that. Eg, if you say then you will still get a  back from the above code."},
{"body": "I seem to have a problem with performance of \"sbt test\" after upgrading to macOS Sierra. On a previous version of OS X it took about 40-50 seconds to finish. macOS Sierra times are much higher than that. Last run I did was around 15 minutes. Compile times are about the same as on 'El Capitan'.I'm the only one from my team to try this new macOS so I can't tell if it's only happening on my mac or is it a universal issue.My colleague had a similar issue on Ubuntu and it was related with random number generation slowing down the tests - Unfortunately, that didn't work for me. Originally I tried that on JDK 8u54 and then tried updating to JDK 8u102 and that didn't help as well.P.S. I'm running Macbook Pro Mid-2015 2.8GHz i7, 16GB ram, 1TB SSD.I had the same problem. Tomcat went from 15 seconds to 6 minutes to initialise spring context after the upgrade... disabling csrutils didn't solve the issue for me.I  the problem by adding my Mac hostname (i.e. Macbook.local, or whatever your Mac is called) on the  file mapped to the  address as well as the  like this:If you're interested you can find some details on the issue and solution here:\nOn the post I also link to a  to help troubleshooting the issue and validating the solution.The problem is related (I believe) on how the localhost name resolution works and how the java.net.InetAddr class is retrieving the addresses. I verified with few colleagues and apparently it doesn't happen to everyone who upgraded to Sierra, but I'm still investigating the roots of this change.The solution anyway was the same that  implemented and worked immediately.Correct answer: For the lazy people:I have the same problem. My spring-boot application take 60 seconds to start on Sierra against 25 seconds on Yosemite.While debugging, I realized that the problem comes from InetAddress.getLocalHost().\nI changed my host file to add my hostname for 127.0.0.1 and :: 1 and now the application starts as fast as before.I think it is a general problem with the new OS. I have a similar problem: I have a web application which is deployed to tomcat. On El Capitan it started up in 10 secs, now it takes 95 secs and the client (a Swing based desktop app) cannot connect to it (or at least it took lot of time). I think it is something around network communication, because a simple test console app runs well.Enabling e.g. System Preferences > Sharing > Remote Login, results in the hostname being automatically assigned an IP address.As people are seeing issues after the upgrade, it makes sense to assume that 10.12 changed how the hostname is resolved, i.e. at least with 10.11 the hostname is always resolved, while with 10.12 it is resolved only if a service is enabled in System Preferences > Sharing (someone with 10.11 could confirm this).It could be because of some bugs in the way Mac OS Sierra was set up. Try doing what is called a SMC reset or PRAM reset to see if it will fix your Mac. These are software resets and will not harm your user data. They might require you to just reset system time after a restart. But they can potentially fix your problems. You can also look at some of the tips mentioned about Mac OS Sierra troubleshooting here - "},
{"body": "I sense that the Scala community has a little big obsession with writing \"concise\", \"cool\", , \"one-liner\" -if possible- code. This is immediately followed by a comparison to Java/imperative/ugly code.While this (sometimes) leads to easy to understand code, it also leads to inefficient code for 99% of developers. And this is where Java/C++ is not easy to beat.Consider this simple problem:  Ordering does not need to be preserved.Here is my version of the solution (It may not be the greatest, but it's what the average non-rockstar developer would do).It's Scala idiomatic, concise, and uses a few nice list functions. It's also very inefficient. It traverses the list at least 3 or 4 times.Here is my totally uncool, Java-like solution. It's also what a reasonable Java developer (or Scala novice) would write.Totally non-Scala idiomatic, non-functional, non-concise, but it's very efficient. It traverses the list only once!So, if 99% of Java developers write more efficient code than 99% of Scala developers, this is a huge \nobstacle to cross for greater Scala adoption. Is there a way out of this trap?I am looking for practical advice to avoid such \"inefficiency traps\" while keeping implementation clear ans concise. This question comes from a real-life scenario: I had to write a complex algorithm. First I wrote it in Scala, then I \"had to\" rewrite it in Java. The Java implementation was twice as long, and not that clear, but at the same time it was twice as fast. Rewriting the Scala code to be efficient would probably take some time and a somewhat deeper understanding of scala internal efficiencies (for vs. map vs. fold, etc)Let's discuss a fallacy in the question:This is presumed, with absolutely no evidence backing it up. If false, the question is moot.Is there evidence to the contrary? Well, let's consider the question itself -- it doesn't prove anything, but shows things are not that clear.Of the four claims in the first sentence, the first three are true, and the fourth, as shown by , is false! And why it is false? Because, contrary to what the second sentence states, it traverses the list more than once.The code calls the following methods on it:andLet's consider first .So let's consider this. When you resize, Java first clears the memory by storing 0 in each element, then Scala copies each element of the previous array over to the new array. Since size doubles each time, this happens log(n) times, with the number of elements being copied increasing each time it happens.Take for example n = 16. It does this four times, copying 1, 2, 4 and 8 elements respectively. Since Java has to clear each of these arrays, and each element must be read  written, each element copied represents 4 traversals of an element. Adding all we have (n - 1) * 4, or, roughly, 4 traversals of the complete list. If you count read and write as a single pass, as people often erroneously do, then it's still three traversals.One can improve on this by initializing the  with an initial size equal to the list that will be read, minus one, since we'll be discarding one element. To get this size, we need to traverse the list once, though.Now let's consider . To put it simply, it traverses the whole list to create a new list.So, we have 1 traversal for the algorithm, 3 or 4 traversals for resize, and 1 additional traversal for . That's 4 or 5 traversals.The original algorithm is a bit difficult to analyse, because ,  and  traverse a variable number of elements. Adding all together, however, it does the equivalent of 3 traversals. If  was used, it would be reduced to 2 traversals. With 2 more traversals to get the maximum, we get 5 traversals -- the same number as the non-functional, non-concise algorithm!So, let's consider improvements.On the imperative algorithm, if one uses  and , then all methods are constant-time, which reduces it to a single traversal.On the functional algorithm, it could be rewritten as:That reduces it to a worst case of three traversals. Of course, there are other alternatives presented, based on recursion or fold, that solve it in one traversal.And, most interesting of all, all of these algorithms are , and the only one which almost incurred (accidentally) in worst complexity was the imperative one (because of array copying). On the other hand, the  characteristics of the imperative one might well make it faster, because the data is contiguous in memory. That, however, is unrelated to either big-Oh or functional vs imperative, and it is just a matter of the data structures that were chosen.So, if we actually go to the trouble of benchmarking, analyzing the results, considering performance of methods, and looking into ways of optimizing it, then we can find faster ways to do this in an imperative manner than in a functional manner.But all this effort is very different from saying the average Java programmer code will be faster than the average Scala programmer code -- if the question is an example, that is simply false. And even discounting the question, we have seen no evidence that the fundamental premise of the question is true.First, let me restate my point, because it seems I wasn't clear. My point is that the code the average Java programmer writes may seem to be more efficient, but actually isn't. Or, put another way, traditional Java style doesn't gain you performance -- only hard work does, be it Java or Scala. Next, I have a benchmark and results too, including almost all solutions suggested. Two interesting points about it:The benchmark code is , and the results are .Here is a recursive method, which only iterates once. If you need performance, you have to think about it, if not, not. You can make it tail-recursive in the standard way: giving an extra parameter , which is per default the empty List, and collects the result while iterating. That is, of course, a bit longer, but if you need performance, you have to pay for it:I don't know what the chances are, that later compilers will improve slower map-calls to be as fast as while-loops. However: You rarely need high speed solutions, but if you need them often, you will learn them fast. Do you know how big your collection has to be, to use a whole second for your solution on your machine?As oneliner, similar to Daniel C. Sobrals solution: but that is hard to read, and I didn't measure the effective performance. The normal pattern is (x /: xs) ((a, b) => /* something */). Here, x and a are pairs of List-so-far and max-so-far, which solves the problem to bring everything into one line of code, but isn't very readable. However, you can earn reputation on CodeGolf this way, and maybe someone likes to make a performance measurement. An updated timing-method, to get the garbage collection out of the way, and have the hotspot-compiler warm up, a main, and many methods from this thread, together in an Object named I renamed all the methods, and had to modify uu2 a bit, to fit to the common method declaration (List [Int] => List [Int]). From the long result, i only provide the output for 1M invocations: The numbers aren't completly stable, depending on the sample size, and a bit varying from run to run. For example, for 100k to 1M runs, in steps of 100k, the timing for splitAt was as follows:The initial solution is already pretty fast.  is a modification from Daniel, often faster, but not always.The measurement was done on a single core 2Ghz Centrino, running xUbuntu Linux, Scala-2.8 with Sun-Java-1.6 (desktop). The two lessons for me are: First of all, the behavior of the methods you presented is not the same. The first one keeps the element ordering, while the second one doesn't.Second, among all the possible solution which could be qualified as \"idiomatic\", some are more efficient than others. Staying very close to your example, you can for instance use tail-recursion to eliminate variables and manual state management:or fold the list:If you want to keep the original insertion order, you can (at the expense of having two passes, rather than one) without any effort write something like:which is more clear than your first example.The biggest inefficiency when you're writing a program is worrying about the wrong things. This is usually the wrong thing to worry about. Why?The example you gave is not very functional, actually. Here's what you are doing:Mind you, it is not , but you know when functional code is at its best when it describes what you want, instead of how you want it. As a minor criticism, if you used  instead of  and  you could improve it slightly.Another way of doing it is this:Now, let's discuss the main issue. Is this code slower than the Java one? Most certainly! Is the Java code slower than a C equivalent? You can bet it is, JIT or no JIT. And if you write it directly in assembler, you can make it even faster!But the cost of that speed is that you get more bugs, you spend more time trying to understand the code to debug it, and you have less visibility of what the overall program is doing as opposed to what a little piece of code is doing -- which might result in performance problems of its own.So my answer is simple: if you think the speed penalty of programming in Scala is not worth the gains it brings, you should program in assembler. If you think I'm being radical, then I counter that you just chose the familiar as being the \"ideal\" trade off.Do I think performance doesn't matter? Not at all! I think one of the main advantages of Scala is leveraging gains often found in dynamically typed languages with the performance of a statically typed language! Performance matters, algorithm complexity matters a lot, and constant costs matters too.But, whenever there is a  between performance and readability and maintainability, the latter is preferable. Sure, if performance  be improved, then there isn't a choice: you have to sacrifice something to it. And if there's no lost in readability/maintainability -- such as Scala vs dynamically typed languages -- sure, go for performance.Lastly, to gain performance out of functional programming you have to know functional algorithms and data structures. Sure, 99% of Java programmers with 5-10 years experience will beat the performance of 99% of Scala programmers with 6 months experience. The same was true for imperative programming vs object oriented programming a couple of decades ago, and history shows it didn't matter.As a side note, your \"fast\" algorithm suffer from a serious problem: you use . That collection does not have constant time append, and has linear time . If you use  instead, you get constant time append  .For reference, here's how  is defined in  in the Scala standard library,It's not unlike your example code of what a Java programmer might come up with.I like Scala because, where performance matters, mutability is a reasonable way to go. The collections library is a great example; especially how it hides this mutability behind a functional interface. Where performance isn't as important, such as some application code, the higher order functions in Scala's library allow great expressivity and programmer efficiency.Out of curiosity, I picked an arbitrary large file in the Scala compiler () and counted something like 37 for loops, 11 while loops, 6 concatenations (), and 1 fold (it happens to be a ).What about this?A bit more ugly, but faster:Try this:Idiomatic, functional, traverses only once. Maybe somewhat cryptic if you are not used to functional-programming idioms.Let's try to explain what is happening here. I will try to make it as simple as possible, lacking some rigor.A  is an operation on a  (that is, a list that contains elements of type ) that will take an initial state  (that is, an instance of a type ) and a function  (that is, a function that takes the current state and an element from the list, and gives the next state, ie, it updates the state according to the next element).The operation will then iterate over the elements of the list, using each one to update the state according to the given function. In Java, it would be something like:For example, if you want to add all the elements of a , the state would be the partial sum, that would have to be initialized to 0, and the new state produced by a function would simply add the current state to the current element being processed:Try to write a fold to multiply the elements of a list, then another one to find extreme values (max, min).Now, the fold presented above is a bit more complex, since the state is composed of the new list being created along with the maximum element found so far. The function that updates the state is more or less straightforward once you grasp these concepts. It simply puts into the new list the minimum between the current maximum and the current element, while the other value goes to the current maximum of the updated state.What is a bit more complex than to understand this (if you have no FP background) is to come up with this solution. However, this is only to show you that it exists, can be done. It's just a completely different mindset.EDIT: As you see, the first and second  in the solution I proposed are used to  the fold. It is equivalent to what you see in other answers when they do . Note that the solutions proposed until now using  don't cover the case in which  is , and will throw an exception. The solution above will return  instead. Since you didn't specify the behavior of the function on empty lists, both are valid.Another contender.  This uses a ListBuffer, like Daniel's second offering, but shares the post-max tail of the original list, avoiding copying it."},
{"body": "So I've been trying to puzzle through the various ways you can define stuff in Scala, complicated by my lack of understanding of the way  blocks are treated:Presumably some of these are equivalent, some of these are syntactic sugar for others, and some are things I should not use, but I can't for the life of me figure it out. My specific questions are:To answer your questions in order:As \"user unknown\" indicates, the braces are only important for scoping purposes and do not make any difference in your use case here.is sytactic sugar forSo if you omit the \"=\" the method will always return an object of type Unit.\nIn Scala, methods and expressions always return something.If you write def f() = () => 10, it's the same as writingSo that means f is returning a function object.\nYou could however writeWhen you call that with f() it returns 10\nFunction objects and methods can be used interchangingly in most cases, but there are a few syntactic differences.\ne.g.\nWhen you writeyou get \"10\", but when you writeyou get On the other hand when you have thisBoth println will print the same thingWhen you use the name of a method at a place where a function object is expected and the method signature matches the signature of the expected function object it is automatically converted.\nSo  gets automatically converted by the scala compiler intoThe second form does not use an assignment. Therefore you can think of it as an Procedure. It is not meant to return something, and returns therefore Unit, even if the last statement could be used to return something specific (but it could be an if-statement, which would only have something specific in one branch).You don't need braces here. You need them, for instance, if you define a val, so the scope of this val is restricted to the enclosing block. You need the curly braces to define val sqr here. If the val is declared in an inner branch, the curly braces don't need to be at the top-level of the method:For further investigation when two methods return the same result, you can compile them and compare the bytecode. Two binary identical methods will be identic."},
{"body": "Following watching  on deriving , I got to looking at this example, which is just awesome:I was trying to understand what the  method was doing, here is the source code:OK, that is fairly confusing (!) - but it references the  method, which is declared thus:So I have a few questions: is a type parameter to one of Scalaz's main pimps, , that represents the Type Constructor (aka Higher Kinded Type) of the pimped value. This type constructor is used to look up the appropriate instances of  and , which are implicit requirements to the method .From the Scala Language Reference: is a type constructor. You can apply the type  to get a Value Type, , which can classify a value. Other type constructors take more than one parameter.The trait  requires that it's first type parameter must be a type constructor that takes a single type to return a value type, with the syntax . The type parameter definition describes the shape of the type constructor, which is referred to as its Kind.  is said to have the kind '.But how can  wrap a values of type ? The type  has a kind , and could only be passed as a type argument to a type parameter declared like . This implicit conversion in  converts a value of type  to a :Which in turn uses a trick with a type alias in  to partially apply the type constructor , fixing the type of the errors, but leaving the type of the success unapplied. would be better written as . I recently proposed to add such a syntax to Scala, it might happen in 2.9.Think of this as a type level equivalent of this:This lets you combine  with a , because the both share the type constructor . demands the the type constructor  must have associated instances of  and . This constitutes an Applicative Functor, which, like a Monad, is a way to structure a computation through some effect. In this case the effect is that the sub-computations can fail (and when they do, we accumulate the failures).The container  can wrap a pure value of type  in this 'effect'. The  operator takes two effectful values, and a pure function, and combines them with the Applicative Functor instance for that container.Here's how it works for the  applicative functor. The 'effect' here is the possibility of failure.In both cases, there is a pure function of type , being applied to effectful arguments. Notice that the result is wrapped in the same effect (or container, if you like), as the arguments.You can use the same pattern across a multitude of containers that have an associated Applicative Functor. All Monads are automatically Applicative Functors, but there are even more, like . and  are both Monads, so you could also used  (aka flatMap): is really similar to , but it provides the pure function for you to simply build a Tuple2 from the results.  is a shorthand for Here's our bundled examples for  and . I used a slightly different syntax to use the Applicative Functor, If any of the sub-computations fails, the provided function is never run. It only is run if all sub-computations (in this case, the two arguments passed to ) are successful. If so, it combines these into a . Where is this logic? This defines the  instance for . We require that the type X must have a  avaiable, which is the strategy for combining the individual errors, each of type , into an aggregated error of the same type. If you choose  as your error type, the  concatenates the strings; if you choose , the error(s) from each step are concatenated into a longer  of errors. This concatenation happens below when two  are combined, using the  operator (which expands with implicits to, for example, .Still reading? ()I've shown that sub-computations based on  or  can be combined with either  or with . When would you choose one over the other?When you use , the structure of the computation is fixed. All sub-computations will be executed; the results of one can't influence the the others. Only the 'pure' function has an overview of what happened. Monadic computations, on the other hand, allow the first sub-computation to influence the later ones.If we used a Monadic validation structure, the first failure would short-circuit the entire validation, as there would be no  value to feed into the subsequent validation. However, we are happy for the sub-validations to be independent, so we can combine them through the Applicative, and collect all the failures we encounter. The weakness of Applicative Functors has become a strength!"},
{"body": "While looking for something else, quite out of mere coincidence I stumbled upon few comments about how diabolical case class inheritance is. There was this thing called  , wretches and kings, elves and wizards and how some kind of a very desirable property is lost with case classes inheritance. So what is so wrong with case class inheritance ?One word:    classes come with a supplied implementation of  and . The equivalence relation, known as  works like this (i.e. must have the following properties):As soon as you allow for equality within an inheritance hierarchy you can break 2 and 3. this is trivially demonstrated by the following example:Then we have:But You might argue that all class hierarchies may have this problem, and this is true. But case classes exist specifically to simplify equality from a developer's perspective (among other reasons), so having them behave  would be the definition of an own goal!There were other reasons as well; notably the fact that  and .That is not overall true. And this is worse than lie.As mentioned by  in any case class successor which constricts a definition area must redefine the equality because pattern matching must work exactly as equality (if try to match  as  then it will not matched since  is not exists).That give understanding to how the equality of case class hierarchy could be implemented.Eventually it is possible to satisfy requirements of the equality relation even for case class successor (without overriding of equality)."},
{"body": "My method definition looks as followsSuppose I wish to know whether the second string is blankDoes not compile.  How do I go about doing this?If you want to pattern match on the array to determine whether the second element is the empty string, you can do the following:The  binds to any number of elements including none. This is similar to the following match on Lists, which is probably better known:Pattern matching may not be the right choice for your example. You can simply do:Pattern matching is more approriate for cases like:which print something for each token. if you want to check if there exist any token which is an empty string, you can also try: statement doesn't work like that. That should be:What is extra cool is that you can use an alias for the stuff matched by  with something like "},
{"body": "NOTE: I am on Scala 2.8\u2014can that be a problem?Why can't I use the  function the same way as  or ?In the  it says that:But I see no type parameter  in the function signature:What is the difference between the  and , and how do I use the latter?EDIT: For example how would I write a fold to add all elements in a list? With  it would be:You're right about the old version of Scala being a problem. If you look at the  for Scala 2.8.1, you'll see no fold defined there (which is consistent with your error message).  Apparently,  was introduced in Scala 2.9.Short answer: associates to the right. I.e. elements will be accumulated in right-to-left order: associates to the left. I.e. an accumulator will be initialized and elements will be added to the accumulator in left-to-right order: is  in that the order in which the elements are added together is not defined. I.e. the arguments to  form a ., contrary to  and , does not offer any guarantee about the order in which the elements of the collection will be processed. You'll probably want to use , with its more constrained signature, with parallel collections, where the lack of guaranteed processing order helps the parallel collection implements folding in a parallel way. The reason for changing the signature is similar: with the additional constraints, it's easier to make a parallel fold.For your particular example you would code it the same way you would with foldLeft.Agree with other answers. thought of giving a simple illustrative example: "},
{"body": "What is the difference between the  and  functions of ?Here is a pretty good explanation:Using  list as an example:Map's signature is: and flatMap's isSo flatMap takes a type [A] and returns an iterable type [B] and map takes a type [A] and returns a type [B]This will also give you an idea that flatmap will \"flatten\" lists.The above is all true, but there is one more thing that is handy:   turns a  into , with any  that drills down to , removed.  This is a key conceptual breakthrough for getting beyond using .From :You can see this better in for comprehensions:this translates into:Each iterator inside for will be translated into a \"flatMap\", except the last one, which gets translated into a \"map\". This way, instead of returning nested collections (a list of an array of a buffer of blah, blah, blah), you return a flat collection. A collection formed by the elements being yield'ed -- a list of Integers, in this case.Look here:\n\"Search for flatMap\"  - there is a really good explanation of it there.  (Basically it is a combination of \"flatten\" and \"map\" -- features from other languages)."},
{"body": "I guess it is not possible to invoke methods implemented in Scala traits from Java, or is there a way? Suppose I have in Scala:and in Java if I use it as Java complains that From Java perspective  is compiled into  . Hence implementing  in Java is interpreted as implementing an interface - which makes your error messages obvious. Short answer: you can't take advantage of trait implementations in Java, because this would enable multiple inheritance in Java (!)Long answer: so how does it work in Scala? Looking at the generated bytecode/classes one can find the following code:Note that  is neither a member of , nor does  extend it. It simply delegates to it by passing .To continue the digression on how Scala works... That being said it is easy to imagine how mixing in multiple traits works underneath:translates to:Now it's easy to imagine how mixing in multiple traits overriding same method:Again  and  will become interfaces extending . Now if  comes last when defining :you'll get:However switching  and  (making  to be last) will result in:Now consider how traits as stackable modifications work. Imagine having a really useful class Foo:which you want to enrich with some new functionality using traits:Here is the new 'Foo' on steroids:It translates to:So the whole stack invocations are as follows:And the result is \"Foo, Trait1, Trait2\".If you've managed to read everything, an answer to the original question is in the first four lines...It's indeed not abstract since  is returning an empty  (a kind of NOP). Try:Then  will be a Java abstract method returning ."},
{"body": "Given a typeclass where instance selection should be performed based on the return type:why does Scala (2.11.6) fail to resolve the proper instance:when it has no problems finding an implicit based on the return type when using the  function (we redefine it here as  to illustrate how similar it is to )The , instead of  in the error message is puzzling.I would assume many scalaz contributors to be familiar with this problem as it seems to limit the convenience of typeclasses in scala.EDIT: I'm looking into getting this working without forgoing type inference. Otherwise I'd like understand why that's not possible. If this limitation is documented as a Scala issue, I could not find it.1) After rewriting your code as follows:then becomes clearly why that works so.2) After you had to set mzero as  then you enabled additional type inference to resolve existencial type. Compiler got actual type from required return type. \nYou could check it with  if you want.3) Of course all that behaviour is just subtleties of compiler and I not think that such partial cases really required deep understanding."},
{"body": "I'm only beginning to become familiar with the concept of kinds, so bear with me if I am not formulating my questions well...  Do they kinds have ilk, or genres, or breeds, or varieties?How far does this sequence of abstraction go?  Do we stop because we run out of words, or do we stop because going farther has no value?  Or, perhaps, because we quickly reach the limits of human cognition and just can't wrap our heads around higher-genred kinds?A related question:  languages give us value-constructors (like a cons operator) to make values. Languages also give us type-constructors like (,) or [] to make types.  Are there any languages that expose kind-constructors to make kinds?  Another edge case that I'm curious about:  We apparently have a type that has no value, denoted as \u22a5, called \"the bottom type\".  Is there a kind that has no type:  a bottom kind?The terminology  and  does not scale well.  Type theorists since Bertrand Russell have used a hierarchy of \"types.\"  One version of this has that   In dependently typed languages like Coq and Agda, one frequently needs these \"higher sorts.\"Levels like this are helpful for avoiding .  Using  tends to cause contradiction (see  for alternative designs).This use of numbers is the standard notation when we need it.  Some type theories have a notion of \"cumulative kinds\", \"cumulative levels\" or \"cumulative sorts\" which says \"if  then also \".Cumulative levels + \"level polymorphism\" give a theory almost as flexible as , but avoids paradoxes.  Coq makes the levels implicit mostly, although the sorts  and  are both typed , .  That is, you don't usually see the numbers, and most of the time it just works. Agda has a language pragma which provides level polymorphism, and makes things very flexible, but can be slightly bureaucratic (Agda is however usually less \"bureaucratic\" than Coq in other areas).  Another good word is \"universe.\"You should probably read Tim Sheard's paper about Omega, a dialect of Haskell with an infinite tower of types/kinds/sorts, but without full-blown dependent types.  It explains why you'd want this, and mentions that the levels above \"sort\" are in practice (at least so far) not directly used very much."},
{"body": "I just recently started learning the Scala language and would like to do it in -way. Could you share your experiences on the unit testing frameworks there are for Scala and the pros/cons of them.I'm using IntelliJ IDEA for Scala development, so it would be nice to be able to run the tests with IDE-support.Have you looked at  ?I've not used it, but it comes from Bill Venners and co at Artima, and consequently I suspect it'll do the job. It doesn't appear to have IDE integration, however. is a little old, but suggests that TestNG is the best option for testing Scala. TestNG will certainly have IDE integrations.EDIT: I've just realised that I wrote this answer in 2009, and the world has moved on (!). I am currently using ScalaTest, the IDE integration works fine, and I can strongly recommend it. In particular the  works very nicelyI'm the author of specs. If you're a Intellij user, I advise you to mix-in in the org.specs.runner.ScalaTest trait to your specification and run it as a ScalaTest suite.If you have any issue with that, or anything else feel free to send a message to the  mailing list.You could also check out  it's fairly complete and IIRC is heavily used as part of Lift."},
{"body": "I'm trying to find the 'right' actor implementation. I realized there is a bunch of them and it's a bit confusing to pick one. Personally I'm especially interested in remote actors, but I guess a complete overview would be helpful to many others. This is a pretty general question, so feel free to answer just for the implementation you know about.I know about the following Scala Actor implementations (SAI). Please add the missing ones.This is the most comprehensive comparison I have seen so far: via As of , scala actors is now deprecated and Akka Actors is now part of standard distributionScala 2.7.7. vs 2.8 after :New Reactors provide more lightweight, purely event-based actors with optional, implicit sender identification. Support for actors with daemon-style semantics was added. Actors can be configured to use the efficient JSR166y fork/join pool, resulting in significant performance improvements on 1.6 JVMs. Schedulers are now pluggable and easier to customize.There's also a design document of Haller: As far as I know, only Scala and Akka support remote actors.Akka is backed up by scalablesolutions, which offer commerical support and plug ins for akka. \nAkka seems like a heavyweight solution, which targets integration with existing frameworks (camel, AMQP, JTA, Comet, Spring, Redis) and additionally STMs and persistence.Akka compared to Scala doesn't support nested receives, but supports hotswapping the actors message loop and has both, thread based and event based actors and so called \"Event-based single-threaded\" ones.I realized that akka enforces exhaustive matches. So even if technically receive expects a partial function, the function must not be partial. This means you have to handle every message immediately."},
{"body": "When I type  on the terminal to start the repl, it throws this errorWhen I hit  and type , it again throws thisI am using  and  givesEither update to a newer scala version (2.10.3+) or downgrade java to java 6/7. As you have seen in the output, 2.9.2 was here long before java 8 was introduced (), so they don't work well together. contains exact instructions on ubuntu's java downgrade.remove the scala 2.9.2 using terminal download the latest scala from Installation instruction are giving on this You might have to run it with JDK 7 or 6"},
{"body": "My scala application will only run with Java 7 as it depends on libraries that only appeared in that version of the JDK.How do I enforce that in sbt, so that the correct error message is shown immediately to the user if she is using the wrong version of Java when starting sbt to run/compile the application?NOTE: There is  Java\u2122 source code to compile here. I  have Scala source code. The Scala code requires an  that's available from Java 7.Using  does not work if you have no Java sources.But you can set the target JVM for the Scala compiler in build.sbt or Build.scala:As a result it prints on a JDK 6:Note: Maybe it works only for the latest SBT/Scalac version.Being Scala code, you can put assertions in the build definition.  sbt defines the  as a common place for things like this, but you can use any setting, including a custom one.  For example,For anybody in the future, this is also a good way to do it. It halts execution immediately if it cannot find the right Java version:In SBT 0.13.6 there is a new  class and  trait. Tweaking the approach recommended by @MarkHarrah to use this one might do the following:In order to compile in Java 7, you should add the javac option to compile with source 1.7.You should add  to your SBT build config that can be found in the /project folder.Here's the reference from SBT:\nJust in-case if you use eclipse based scala-ide change settings in window --> pref -- scala compiler --> standard --> target --> jvm-1.7 "},
{"body": "I have a scala Map and would like to test if a certain value exists in the map. There are several different options, depending on what you mean.If you mean by \"value\" , then you can use something likeIf you mean , then you canIf you wanted to just test , thenNote that although the tuple forms (e.g. ) end up being shorter, the slightly longer forms are more explicit about what you want to have happen.Do you want to know if the  exists on the map, or the key? If you want to check the key, use :you provide a test that one of the map values will pass, i.e.The ScalaDocs say of the method \"Tests whether a predicate holds for some of the elements of this immutable map.\", the catch is that it receives a tuple (key, value) instead of two parameters.What about this:Yields  if map contains  value.If you insist on using :Per answers above, note that exists() is significantly slower than contains() (I've benchmarked with a Map containing 5000 string keys, and the ratio was a consistent x100). I'm relatively new to scala but my guess is exists() is iterating over all keys (or key,value tupple) whereas contains uses Map's random access"},
{"body": "I am doing a project in Scala, but am fairly new to the language and have a Java background.  I see that Scala doesn't have ArrayList, so I am wondering what Scala's equivalent of Java's ArrayList is called, and if there are any important differences between the Java and Scala versions.  I'm not looking for a specific behavior so much as an internal representation (data stored in an array, but the whole array isn't visible, only the part you use).I can think of 3 more specific questions to address yours:So here are the answers for these:Scala's equivalent of Java's  interface is the . A more general interface exists as well, which is the  -- the main difference being that a  may have operations processed serially or in parallel, depending on the implementation.Because Scala allows programmers to use  as a factory, they don't often bother with defining a particular implementation unless they care about it. When they do, they'll usually pick either Scala's  or . They are both immutable, and  has good indexed access performance. On the other hand,  does very well the operations it does well.That would be .Well, the good news is, you can just use  in Scala! In Java,  is often avoided because of its general incompatibility with generics. It is a co-variant collection, whereas generics is invariant, it is mutable -- which makes its co-variance a danger, it accepts primitives where generics don't, and it has a pretty limited set of methods.In Scala,  -- which is still the same  as in Java -- is invariant, which makes most problems go away. Scala accepts  (the equivalent of primitives) as types for its \"generics\", even though it will do auto-boxing. And through the \"enrich my library\" pattern,  of  methods are available to .So, if you want a more powerful , just use an .The default methods available to all collections all produce  collections. For example, if I do this:Then  will be a  collection, while  will still be the same as before this command. This is true no matter what  was: , , etc.Naturally, this has a cost -- after all, you  producing a new collection. Scala's immutable collections are much better at handling this cost because they are , but it depends on what operation is executed.No collection can do much about , but a  has excellent performance on generating a new collection by prepending an element or removing the head -- the basic operations of a stack, as a matter of fact.  has good performance on a bunch of operations, but it only pays if the collection isn't small. For collections of, say, up to a hundred elements, the overall cost might exceed the gains.So you can actually add or remove elements to an , and Scala will produce a   for you, but you'll pay the cost of a full copy when you do that.Scala mutable collections add a few other methods. In particular, the collections that can increase or decrease size -- without producing a new collection -- implement the  and  traits. They don't guarantee good performance on these operations, though, but they'll point you to the collections you want to check out.It's  from . You can find the scaladocs .Did you have a look at ?It's hard to say exactly what you should do because you haven't said what behavior of  you're interested in using. It's more useful to think in terms of which scala traits you want to take advantage of. Here's a good explanation: .That said, you probably want some sort of ."},
{"body": "This is not about command-line compiler options. How do I programmatically obtain the Scala version inside code?Alternatively, where does the Eclipse Scala plugin v2 store the path to ?This will work without access to :There are three ways to get the Scala version -You can get the Scala version like this:I don't know the specifics of the plugin, though.We can also get installed Scala versionWelcome to Scala version 2.10.3 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_6\n0).\nType in expressions to have them evaluated.\nType :help for more information."},
{"body": "I am trying to figure out how to make Eclipse recognize dependencies that are retrieved using SBT?  SBT download the correct dependencies and puts them in my ~/.ivy directory but eclipse doesn't see them.  Is there a way to do this?thanksThis is probably not the answer you are looking for and I admit it is not elegant but it currently works for me, meaning that I think it takes less time for me to periodically do the following instead of researching and finding a more elegant solution.I assume you are using the sbt-eclipse plugin (  ). When I add new dependencies to my project ( which is actually pretty rare ) I simply regenerate my eclipse project files from the plugin. The downside of this is that I have a multiple module project and after I refresh the projects in eclipse I need to re-add the inter-project dependencies in the eclipse build path editor. Like I mentioned it is pretty hacky but all in all I really don't loose that much time doing it. It's not pretty but it works.Best of luck,\nAndyIf you are using sbteclipse plugin it's achievable in a simple way. In sbt type: Then in eclipse, hit F5 on a project folder to refresh it. Or right-click and choose \"Refresh\". Just works.I use the  plugin for Eclipse, and I've had more luck with this approach. It's only , but works with sbt 0.11First, install the IvyDE plugin in Eclipse and restart.Run the sbt command  - this will create an XML ivy file of your dependencies.In Eclipse, under your Project/Properties - Java Build Path - Libraries, click \"Add Library\" and choose \"IvyDE Managed Dependencies\" then select the file target/scala-2.9.1/ivy-.xmlWhen you add a new dependency to build.sbt, run the sbt commands  and  again. Then right-click the Ivy library for your project in the Package Explorer - it will be called \"target/scala-2.9.1/ivy-.xml [compile,test]\", and click the second \"Refresh\" menu item (between \"Refresh\" and \"Reload Settings\" -  \"F5 Refresh\" ). In command prompt go to the project folder and typeThis should generate the appropriate  entries in eclipse project.Go back to eclipse, select the project and , this will reload the referenced libs.The following works for me:1) close project in Eclipse2) in my file explorer, browse to my Eclipse project, make sure hidden files are visible3) open .classpath in a simple text editor4) copy the bottom entry. For example, in my current project, it is 5) navigate to my .ivy folder, cache, then down to the library I have added via sbt6) right click on jar file, select properties, copy the path and jar file name and replace it in the entry I copied in step 47) save .classpath8) open my project on EclipseNew dependency is now available in Eclipse.All you need is execute from your project home:or enter  console and write:BTW: I don't know if from Jan '12 something has changed but now it seems much more simple."},
{"body": "I really don't seem to be understanding Map and FlatMap. What I am failing to understand is how a for-comprehension is a sequence of nested calls to map and flatMap. The following example is from translates toThe mkMatcher method is defined as follows:And the pattern method is as follows:It will be great if someone could shed some light on the rationale behind using map and flatMap here.I'll try and recapThe  comprehension is a syntax shortcut to combine  and  in a way that's easy to read and reason about.Let's simplify things a bit and assume that every  that provides both aforementioned methods can be called a  and we'll use the symbol  to mean a  with an inner type .Some commonly seen monadsDefined in a generic monad e.g.As you can see, the  operation preserves the \"shape\" of the original , so the same happens for the  expression: a  remains a  with the content transformed by the operation in the On the other hand each binding line in the  is just a composition of successive , which must be \"flattened\" in order to maintain a single \"external shape\"Suppose for a moment that each internal binding was translated to a  call, but the right-hand was the same  function, you would end up with a  for each line in the comprehension.\nThe intent of the whole  syntax is to easily \"flatten\" the concatenation of successive monadic operations (i.e. operations that \"lift\" a value in a \"monadic shape\": ), with the addition of a final  operation that  performs a concluding transformationI hope this explains the logic behind the choice of translation, which is applied in a mechanical way, that is:   nested calls concluded by a single  call.\nMeant to show the expressiveness of the  syntaxCan you guess the type of ?As already said, the shape of the  is mantained through the comprehension, so we start with a  in , and must end with a .\nThe inner type instead changes and is determined by the  expression: which is  should be a The rationale is to chain monadic operations which provides as a benefit, proper \"fail fast\" error handling.It is actually pretty simple. The  method returns an  (which is a Monad).\nThe result of , the monadic operation, is either a  or a .Applying the  or  function to a  always returns a  - the function passed as a parameter to  and  is not evaluated.Hence in your example, if  returns a None, the flatMap applied to it will return a  (the second monadic operation  will not be executed) and the final will again return a .\nIn other words, if any of the operations in the for comprehension, returns a None, you have a fail fast behavior and the rest of the operations are not executed.This is the monadic style of error handling. The imperative style uses exceptions, which are basically jumps (to a catch clause)A final note: the  function is a typical way of \"translating\" an imperative style error handling (...) to a monadic style error handling using This can be traslated as:Run this for a better view of how its expandedresults are:This is similar to  - loop through each element in  and foreach element  it to each element in I'm not a scala mega mind so feel free to correct me, but this is how I explain the  saga to myself!To understand  and it's translation to  we must take small steps and understand the composing parts -  and .  But isn't  just  with  you ask thyself! if so why do so many developers find it so hard to get the grasp of it or of . Well, if you just look at scala's  and  signature you see they return the same return type  and they work on the same input argument  (at least the first part to the function they take) if that's so what makes a difference?scala map signature:But there is a big part missing when we look at this signature, and it's - where does this  comes from? our container is of type  so its important to look at this function in the context of the container - .  Our container could be a  of items of type  and our  function takes a function which transform each items of type  to type , then it returns a container of type  (or )Let's write map's signature taking into account the container:Note an  - it bundles  in the output container  you have no control over it.  Let's us stress it again:You see you did not specify how to  the item you just specified how to transform the internal items.  And as we have the same container  for both  and  this means  is the same container, meaning if you have  then you are going to have a  and more importantly  is doing it for you!Now that we have dealt with  let's move on to .Let's see its signature:You see the big difference from map to  in flatMap we are providing it with the function that does not just convert from  but also containerizes it into .So why do we so much care of the input function to map/flatMap does the containerization into  or the map itself does the containerization for us?You see in the context of  what's happening is multiple transformations on the item provided in the  so we are giving the next worker in our assembly line the ability to determine the packaging.  imagine we have an assembly line each worker does something to the product and only the last worker is packaging it in a container! welcome to  this is it's purpose, in  each worker when finished working on the item also packages it so you get containers over containers.Now let's looks into your for comprehension taking into account what we said above:What have we got here:First,  returns a function whose signature is , that's a regular java procedure which just run , as shown in the  function.\nThen, look at this lineThe  function is applied to the result of , which is , so the  in  is just the pattern you compiled. So, given a pattern , a new function is constructed, which takes a String , and check if  matches the pattern. Note, the  variable is bounded to the compiled pattern. Now, it's clear that how a function with signature  is constructed by .Next, let's checkout the  function, which is based on . To show how  works, we first look at this part:Since we got a function with signature  from , which is  in this context,  is equivalent to , which returns if the String s matches pattern . So how about , it's same as , the only difference is that, the first call of  uses , instead of ,  Why? Because  returns , you will get a nested result  if you use  for both call, that's not what you want ."},
{"body": "Can Scala be used to script a Java application?I need to load a piece of Scala code from Java, set up an execution scope for it (data exposed by the host application), evaluate it and retrieve a result object from it. The Scala documentation shows how easy it is to call compiled Scala code from Java (because it gets turned into to regular JVM bytecode).But how can I evaluate a Scala expression on the fly (from Java or if that is easier, from within Scala) ? For many other languages, there is the javax.scripting interface. Scala does not seem to support it, and I could not find anything in the Java/Scala interoperability docs that does not rely on ahead-of-time compilation.Scala is not a scripting language.  It may  somewhat like a scripting language, and people may advocate it for that purpose, but it doesn't really fit well within the JSR 223 scripting framework (which is oriented toward dynamically typed languages).  To answer your original question, Scala does not have an  function just like Java does not have an .  Such a function wouldn't really make sense for either of these languages given their intrinsically static nature.My advice: rethink your code so that you don't need  (you rarely do, even in languages which have it, like Ruby).  Alternatively, maybe you don't want to be using Scala at all for this part of your application.  If you really need , try using JRuby.  JRuby, Scala and Java mesh very nicely together.  It's quite easy to have part of your system in Java, part in Scala and another part (the bit which requires ) in Ruby.it's now 2011, and you can do so with see Scala has added official support to JSR-223 in 2.11 ().So if you still need it after thinking about the considerations made in the currently accepted answer from Daniel Spiewak (about rethinking in a way it is not needed), this should be the official alternative.You can emulate \"eval\" by taking scala code, wrapping it in a class, compiling that class, using reflection to create a new instance, and then calling it. It's a little involved, and the scala compiler is very slow (on the order of 2 seconds) to initialize, but it works fine.There's a library for it here, called \"util-eval\": The code in question lives here: It works like this:I am not sure, if this is a good way, but I solved this problem with using  and  To have an eval in Scala you have to:You can always use scalac to compile a scala class and then load that class dynamically. But I guess that's not what you're after."},
{"body": "I've seen questions about IDE's here --  and , but I've had mixed experiences with IDEs. Right now, I'm using the Eclipse IDE with the automatic workspace refresh option, and KDE 4's Kate as my text editor. Here are some of the problems I'd like to solve:I looked at Ant and Maven, though haven't employed either yet (I'll also need to spend time solving #3 and #4). I wanted to see if anyone has other suggestions before I spend time getting a suboptimal build system working. Thanks in advance! - I'm now using Maven, passing a project as a compiler plugin to it. It seems fast enough; I'm not sure what kind of jar caching Maven does. A current repository for Scala 2.8.0 is available []. The archetypes are very cool, and cross-platform support seems very good. However, about compile issues, I'm not sure if fsc is actually fixed, or my project is stable enough (e.g. class names aren't changing) -- running it manually doesn't bother me as much. If you'd like to see an example, feel free to browse the pom.xml files I'm using []. - from benchmarks I've seen, Daniel Spiewak is right that buildr's faster than Maven (and, if one is doing incremental changes, Maven's 10 second latency gets annoying), so if one can craft a compatible build file, then it's probably worth it...Points 2 and 4 are  difficult to manage with the current scalac.  The problem is that Scala's compiler is a little dumb about building files.  Basically, it will build whatever you feed it, regardless of whether or not that file really needs to be built.  Scala 2.8.0 will have some tremendous improvements in this respect, but until then...  Eclipse SDT actually has some very elaborate (and very hackish) code for doing change detection and dependency tracking.  On the whole, it does a decent job, but as you have seen, there are wrinkles.  Eclipse SDT 2.8.0 will rely on the aforementioned improvements to scalac itself.So, building only modified files is pretty much out of the question.  Aside from SDT, the only tool I know of which even tries this is SBT ().  It uses a compiler plugin to track files as they are compiled and query the dependency graph computed by the compiler itself.  In practice, this yields about a 50% improvement over the recompile-the-world approach.  Once again, this is a hack to get around deficiencies in pre-2.8.0 scalac.The good news is that reasonably fast compilation is still achievable even without worrying about change detection.  FSC uses the same technology (ooh, that sounded so \"Charlie Eppes\") that Eclipse SDT uses to implement fast incremental compilation.  In short, it's pretty snappy.Personally, I use .  Its configuration is significantly cleaner than either Maven's or SBT's and its startup time is orders of magnitude less (when running under MRI).  It integrates with FSC and attempts to do some basic change detection on its own (fairly primitive).  It also has auto-magical support for the major Scala test frameworks (ScalaTest, ScalaCheck and Specs) as well as support for joint compilation with Java sources and IDE meta generation for IntelliJ and Eclipse.  Oh, and it supports all of Maven's features (dependency resolution, etc) and then some.  I'm even working on an extension which would allow interactive shell support integrated with JavaRebel and supporting several shell providers (Scala, JIRB, Clojure REPL, etc).  It's not ready for the SVN yet, but I'll commit once it's ready (possibly in time for 1.3.5).As you can see, I'm very firmly of the opinion that Buildr is the best Scala build tool out there.  Its documentation is a little spotty where Scala is concerned, but that's because everything is so straightforward that it's hard to document without feeling verbose.  You can always check out one of  for examples.  Good luck!Have you looked at Intellij IDEA and its  ? Intellij has a loyal (fanatical?) following amongst Java developers, so you may find this is appropriate for your needs.Am also quite frustrated with the scala plugin on Eclipse and I can add a few more problems to the list: I'm glad to hear that Buildr sounds like a better alternative (on the build front anyhow), I'll give that a try - thanks!For the reasons of completeness, I have to say that there is also  -- the build tool that in use in Twitter (one of the early scala adopters)The main difference it that it is intended not only for scala (and written in python, by the way) and is modeled after .It's not so bloated as sbt, so for the freshmans it's much simplier, but I've never heard about Pants usage outside of twitter and foursquare.If you scared of SBT, maybe another no-so-popular build tool, , could be an  for you?I went down the same road, and here is where I am at:\n- After some initial investigation, I dropped Kate.  I love to use it for most things, but when it came to things like defining tab completions, I found it sorely lacking.  I would recommend that you look into gedit instead, which is much more robust for Scala development\n- With gedit as my editor, I use SBT and have found it to be a great build tool.  I can put it into a 'test' mode where when any code changes it recompiles the relevant files and runs my test suite.  This has been an extremely effective way to work.I have not taken a look at Buildr yet.  I would like to say that I will, but honestly with SBT at my disposal I don't really have a compelling need to look at another build tool.If you use Emacs, I think  is a pretty good IDE. I think at the time writing, Ensime is the only IDE that will give you fast and accurate autocompletion on both Scala and Java objects, including implicit conversions.There's code browsing support using Speedbar, code templates using the excellent Yasnippet, and code completion menu using Autocomplete. These are all very modern, actively maintained Emacs packages. There's also out of the box incremental building support for Maven and SBT. There's a lot more in there such as interactive debugging, refactoring, and the Scala interpreter in an inferior process. All the things you want in a modern IDE for Scala is already there in Ensime. Highly recommended for Emacsens.If you want to use Eclipse, but build the project using sbt, and still be able to debug, take a look at this post here:zikaprog.wordpress.com/2010/04/19/scala-eclipse-sbt-and-debugging/It also can be applied to builders other than sbt.The latest version of the Maven Scala plugin supports Zinc/Nailgun for faster start times and faster incremental builds.  See ."},
{"body": "I saw a StackOverflow question regarding static analysis in Scala, but that  one was answered in 2009. As you know, the Scala tools are changing very rapidly. I was therefore wondering if someone familiar with the current state of static analysis tools in Scala could tell me if there's, say, a Findbugs equivalent for Scala. I found that Findbugs issues many unnecessary warnings for Scala, probably having to do with the way the \"object\" singleton compiles to bytecode, due to traits, etc. I heard that Scalastyle is not only a Scala version of Java's CheckStyle, that it also includes bits of Findbugs and PMD. But if it doesn't implement all of Findbugs and/or PMD, then are there other tools that supplement it? Or, is Scalastyle good not only for style checking, but is it good for improving code quality?Also, what about Scala's integration with, say, Sonar? Is the Scala Sonar plugin (which works with Scalastyle) reliable?Here is an updated answer as of August 2014 for some that are aimed or work well with Scala. Personally I think the JVM or Java ones end up with far too many false positives, or have inspections that are aimed mostly at Java specific classes. For example, since in Scala we don't tend to use the Java Collections, all the findbugs collection based inspections are not needed. Another example is the inspections for use of static fields which are irrelevant in Scala.Would the  be counted as an alternative? Also  and  command line parameters of scalac might help catching certain types of bugs. developed a easy to customize tool - scala-meta: take a lookand I think  has some support for scalaWhy don't you like to use a very powerful Scalac compiler? E.g. it has some useful options, e.g. ...etc.According to Codacy, following are the tools (updated till Oct 2015)Source: \n"},
{"body": "I've searched for a half-hour, and still cannot figure it out.In  there are a number of features which will require explicit \"enabling\" in Scala 2.10 ().\nAmongst them there is , to which I just cannot find a reference anywhere. What exactly does this feature allow?It allows you to use operator syntax in postfix position. For examplerather thanIn this harmless example it is not a problem, but it can lead to ambiguities. This will not compile:And the error message is not very helpful:It tries to call the  method on the result of the  call, which is of type . This is likely not what the programmer intended. To get the correct result, you need to insert a semicolon after the first line. Dropping dot from methods without parameters is !OK to drop dot in methods that take one parameter of  like map, filter, count and be safe!\nAlso,  methods like zip. Postfix operator - is actually a method call with no parameters (a!==a.!) and without brackets. (considered not safe and deprecated) Postfix operator is method, that should end the line, or else it will be treated as infix. Infix operator is method with one parameter, that can be called without dot and parentheses. Only for  methods Method with one or more parameters will chain without dot if you call it with parameters. def a(), def a(x), def a(x,y)\nBut you should do this only for methods that use  as parameter!Sample warnings:It refers to the ability to call a nullary (with no arg list or empty arg list) method as a postfix operator:By example:See: "},
{"body": "According to :and attempts to store custom type in a  lead to following error like:or:Are there any existing workarounds?Note this question exists only as an entry point for a Community Wiki answer. Feel free to update / improve both question and answer.Unfortunately, virtually nothing has been added to help with this. Searching for  in  or  finds things mostly to do with primitive types (and some tweaking of case classes). So, first thing to say: . With that out of the way, what follows is some tricks which do as good a job as we can ever hope to, given what we currently have at our disposal. As an upfront disclaimer: this won't work perfectly and I'll do my best to make all limitations clear and upfront.When you want to make a dataset, Spark \"requires an encoder (to convert a JVM object of type T to and from the internal Spark SQL representation) that is generally created automatically through implicits from a , or can be created explicitly by calling static methods on \" (taken from the ). An encoder will take the form  where  is the type you are encoding. The first suggestion is to add  (which gives you  implicit encoders) and the second suggestion is to explicitly pass in the implicit encoder using  set of encoder related functions.There is no encoder available for regular classes, sowill give you the following implicit related compile time error:However, if you wrap whatever type you just used to get the above error in some class that extends , the error confusingly gets delayed to runtime, soCompiles just fine, but fails at runtime withThe reason for this is that the encoders Spark creates with the implicits are actually only made at runtime (via scala relfection). In this case, all Spark checks at compile time is that the outermost class extends  (which all case classes do), and only realizes at runtime that it still doesn't know what to do with  (the same problem occurs if I tried to make a  - Spark waits until runtime to barf on ). These are central problems that are in dire need of being fixed:The solution everyone suggests is to use the  encoder.This gets pretty tedious fast though. Especially if your code is manipulating all sorts of datasets, joining, grouping etc. You end up racking up a bunch of extra implicits. So, why not just make an implicit that does this all automatically?And now, it seems like I can do almost anything I want (the example below won't work in the  where  is automatically imported)Or almost. The problem is that using  leads to Spark just storing every row in the dataset as a flat binary object. For , ,  that is enough, but for operations like , Spark really needs these to be separated into columns. Inspecting the schema for  or , you see there is just one binary column:So, using the magic of implicits in Scala (more in ), I can make myself a series of implicits that will do as good a job as possible, at least for tuples, and will work well with existing implicits:Then, armed with these implicits, I can make my example above work, albeit with some column renamingI haven't yet figured out how to get the expected tuple names (, , ...) by default without renaming them - if someone else wants to play around with this,  is where the name  gets introduced and  is where the tuple names are usually added. However, the key point is that that I now have a nice structured schema:So, in summary, this workaround:This one is less pleasant and has no good solution. However, now that we have the tuple solution above, I have a hunch the implicit conversion solution from another answer will be a bit less painful too since you can convert your more complex classes to tuples. Then, after creating the dataset, you'd probably rename the columns using the dataframe approach. If all goes well, this is  an improvement since I can now perform joins on the fields of my classes. If I had just used one flat binary  serializer that wouldn't have been possible.Here is an example that does a bit of everything: I have a class  which has fields of types , , and . The first takes care of itself. The second, although I could serialize using  would be more useful if stored as a  (since s are usually something I'll want to join against). The third really just belongs in a binary column.Now, I can create a dataset with a nice schema using this machinery:And the schema shows me I columns with the right names and with the first two both things I can join against.Related questions:In case of Java Bean class, this can be usefulNow you can simply read the dataFrame as custom DataFrame This will create a custom class encoder and not a binary one. Encoders work more or less the same in . And  is still the recommended  choice.You can look at following example with spark-shellTill now] there were no  in present scope so our persons were not encoded as  values. But that will change once we provide some  encoders using  serialization.I have been quite successful converting  to  using  and  as long as  is a simple .I'd stick to classes with primitive types and String as fields before the DataBricks folks beef up their Encoders.  Sure it's a little extra work, but I imagine it'll help a lot on performance working with a flat schema.And voila! Lather, rinse, repeat."},
{"body": "I was thinking of making a new, light-weight database population framework. I absolutely hate dbunit. Before I do, I want to know if someone already did it.Things i dislike about dbunit:1) The simplest format to write and get started is deprecated. They want you to use formats that are bloated. Some even require xml schemas. Yeah, whatever.2) They populate rows not in the order you write them, but in the order tables are defined in the xml file. This is really bad because you can't order your data in such a way that foreign key constraints won't cause problems. This just forces you to go through the hassle of turning them off altogether. This also wastes time and bloats up your junit base classes to include code to disable the foreign key constraints. You will probably have to test for the database type (hsqldb, etc.) and disable them in database-specific ways. This is way bad.It could be better if dbunit helped in disabling foreign key constraints as part of their framework automatically, but they don't do this. They do keep track of dialects... so why not use them for this? Ultimately, all of this does is force the programmer to waste time and not get up and testing quickly.3) XML is a pain to write. I don't need to say more about this. They also offer so many ways to do it, that I think it just complicates matters. Just offer one really solid way and be done with it.4) When your data gets large, keeping track of the ids and their consistent/correct relationships is a royal pain. Also, if you don't work on a project for a month, how are you to remember that user_id 1 was an admin, user_id 2 was a business user, user_id 3 was an engineer and user_id 4 was something else? Going back to check this is wasting more time. There should be a meaningful way to retrieve it other than an arbitrary number.5) It's slow. I've found that unless hsqldb is used, it is painfully slow. It doesn't have to be. There are also numerous ways to mess up its configuration as it is not easy to do \"out of the box\". There is a hump that you must go through to get it working right. All this does is encourage people to not use it, or be pissed of when they do start to use it.6) Some values tend to repeat a lot, likes dates. It'd be nice to specify defaults, or even have the framework put defaults in automatically, even without you telling it to put defaults in there. That way you can create objects just with the values you want, and leave the rest off. This sure beats specifying every nook and cranny of a column if it's not required.7) Probably the most annoying thing is that the first entry must include ALL the values - even null placeholders - or future rows won't pick the columns that you actually specified.DBunit doesn't have a sensible default for translating [NULL] to a real null value either. You have to manually add it. Tell me, who hasn't done this with dbunit? Everyone has. It shouldn't be like this!What this means is that if you have a polymorphic object, you must declare all the foreign keys to the joining tables of each subclass in the first row, even though they are null. If you do a table for all subclasses pattern, you still have to specify all the fields on the first row. This is just awful.Anything out there to satisfy me, or should I become the next framework developer of a much better database testing framework?I'm not aware of any real alternative to DbUnit and none of the tools mentioned by  are in my eyes:That being said, I've personally used DbUnit successfully several times, on small and huge projects, and I find it pretty usable, especially when using  and its DbUnit module. This doesn't mean it's perfect and can't be improved but with decent tooling (either custom made or something like Unitils), using it has been a decent experience.So let me answer some of your points:DbUnit supports flat or structured XML, XLS, CSV. What revolutionary format would you like to use? By the way, a DTD or schema is not mandatory when using XML. But it gives you nice things like validation and auto-completion, how is that bad? And Unitils can generate it easily for you, see .They are waiting for your patch. Meanwhile, Unitils provides support to handle constraints transparently, see . I guess pain is subjective but I don't find it painful, especially when using a schema and autocompletion. What is the silver bullet you're suggesting?Keep them small, that's a know . You're going against a known best practice and then complain... Yes, task switching is counter productive. But since you're working with low level data, you have to know how they are represented, there is no magic solution unless you use a higher level API of course (but that's not the purpose of DbUnit).That's inherent to databases and JDBC, not DbUnit. Use a fast database like H2 if you want things to be as fast as possible (if you have a better agnostic way to do things, I'd be glad to learn about it).Not when using Unitils as mentioned in presentations like  or . If you think you can make things better, maybe contribute to existing solutions. If that's not possible and if you think you can create the killer database testing framework, what can I say, do it. But don't forget, ranting is easy, coming up with solutions using your own solutions is less so.As a DbUnit developer I'm grateful for criticism and I must partially agree with you. We are currently starting the design of the next DbUnit major release and I wish to invite you to participate both in the discussion and development.I'm not going to answer your points as your question is not really related to DbUnit, but to DbUnit alternatives. Anyway, I just want to highlight your point 7 is completely false: you do not need to specify all the columns on first row any more, the feature is called column sensing. I'm not going to tell you why it's not enabled by default as you are surely smart enough to understand it by yourself.I'll give scaladbtest a deep examination in the hope we can integrate their ideas.Faced with similar concerns using DBUnit I have found this :  which may address concerns. Such as instead of representing test data in separate files all DB content is contained within the java class itself.Here's a short list of a few tools in this vein (besides DBunit) that I particularly like, or find interesting. At the very least they may offer some inspiration:Note that none of these are really competitors to DBunit in terms of scope or feature sets. However, there are some interesting ideas there that might be worth taking a look at. Good luck!I use DBUnit, with a few wrappers to smooth over the rough edges. A nice tool that can either complement or overlap the functionality is . It can extract subsets of data from a reference database, and store this as either DBUnit compatible XML files, or as \"topologically sorted DML files\", which respect the foreign key constraints.You're making excellent point.   I've been working for a lot of web portals over the last years, mostly with PHP, but also some Java now and then.\nAnd like you I don't get that after all these years framework and unittesting developers don't seem to realize how much storage handling has changed in the last decade.\nIt's not enough to just send create/insert/truncate statements to some database!\nIf you're operating at large scale you end up employing all sorts of storage backends, organized in layers to push hot content out fast. Plus on the Database front there's the issue of data partitioning. If you don't have a proper foreign key abstraction provided you will certainly go nuts when your storage setup changes. And while we're at it: fixture ordering by foreign key precedence has many pitfalls and I have yet to see a real solution for that with .  Anyway, the point is having just a basic database storage in place for unittesting is not enough for complex storage setups, since they often fail to reproduce problems in the live environment and are a pain in the ass to maintain.  Without wanting to sound like a fanboy: one place where things are okay is .\nThat has a persistent model concept that people seem to have actually put some thought into. If you're dealing in ,  is the place to go. It is limited via the default inclusion of , with is also quite DB-centric, but it has clean interfaces and great extensibility and copied the rails fixture system completely. Professionally I need to stick to homebrew solutions for now, but they work okay.If you use the Spring Framework (or don\u2019t mind using it at least for testing), then  is currently the best (maintained) alternative to plain DBUnit that I know and use. Quoting their website:Spring DBUnit appears to be the \u2018somewhat official\u2019 Spring solution for DB unit testing (with DBUnit); at least the author/maintainer of the library, Phil Webb, is working at SpringSource/Pivotal.We are writing  as a wrapper around DbUnit to address some of the mentioned concerns. It allows populating a DB just within your unit test rather than relying on editing XML files. I too had similar issues with DBUnit. Especially for using it to populate local development data and exporting data from a real database. I ran into several cases where it would export a dataset that it couldn't then import.This inspired me to write a new library for it: This uses a groovy DSL to define the datasets which makes for a very compact representation of the data and makes it possible to do cool things like generate random data since it's just groovy code.An alternative using  configuration and  testing can be found I just released a groovy DSL based framework called pedal-loader available via . Documentation .It allows you to work with JPA entity level abstraction directly. Since it is a groovy script, you can use all of the groovy constructs. To insert rows into a table backed by a JPA entity called Student, with fields (not database columns, but mapped fields) called id, name and grade, you would do something like this:Grade is an enum in the Student class that is mapped to the database column (perhaps using JPA 2.1 @Convert annotation). allStudents is a list that will hold the rows and rowOfInterest is a reference to a particular row. These properties (allStudents and rowOfInterest) become available to your unit test.The situation of DBUnit is indeed sometimes frustrating. Some of the problem are solved from  with , specially if you combine it with the , which is in a very early stage. You can see it in action at .Disclaimer: All referenced github-resources are maintained by me.I just released a library called JDBDT (Java Database Delta Testing) that\nyou may use for database setup and validation in software tests.Have a look at  Best,\nEduardo"},
{"body": "I'm having problems with a maven dependency which is in my local respository.SBT can't find it. Already set log level to debug, but not getting anything new.The files are in the repository. I copy paste paths from the console to file explorer and they are there.The output: like described in \"sbt can search your local Maven repository if you add it as a repository:\"That made sbt look in the local repository. Before it didn't.So the scala file looks like this:(I hardcoded Path.userHome to exclude possible error reason. As expected it didn't change anything).You need three slashes after the  specifier. This is because between the second and third slash, you have an optional hostname.  has a good explanation of  URL'sYou're having a problem because the typical pattern of  assumes a Unix filesystem, where the path begins with a , contains no , and usually contains no spaces.To have a non-hardcoded path that works on both Windows and Linux/Unix, use: Just add this line in the build.scala or build.sbt fileTo get this to work for newer versions of sbt, add the following to build.sbt:Watch out when you have a project defined, you'll need to include the resolver in the settings. Global resolver will not be identified.Example:"},
{"body": "Are there any sane analogues to LINQ (.NET) exists for Scala? It depends on what exactly you mean by \"LINQ\". LINQ is many things.The most obvious answer would be: just use the .NET port of Scala. It gives you full native access to everything in .NET, which obviously includes LINQ.Unfortunately, the .NET port of Scala was dropped a couple of years ago. Fortunately, it was picked up again a couple of months ago, with official funding directly from Microsoft no less. You can expect a release sometime in the 2011/2012 timeframe.Anyway, what is LINQ?A couple of features where added to .NET and specifically C# and VB.NET  LINQ. They are not technically part of LINQ, but are necessary prerequisites: type inference, anonymous (structural) types, lambda expressions, function types ( and ) and expression trees. All of these have been in Scala for a long time, most have been there forever.Also not directly part of LINQ, but in C#, LINQ query expressions can be used to generate XML, to emulate VB.NET's XML literals. Scala has XML literals, like VB.NET.More specifically, LINQ isIn Scala, like in pretty much any other functional language (and in fact also pretty much any other object-oriented language, too), the query operators are simply part of the standard collections API. In .NET, they have a little bit weird names, whereas in Scala, they have the same standard names they have in other languages:  is ,  is  (or ),  is ,  is  or ,  is  or  or , and there are ,  and  and so on. So, that takes care of both the specification and the LINQ-to-Objects implementation. Scala's XML libraries also implement the collections APIs, which takes care of LINQ-to-XML.SQL APIs are not built into Scala, but there are third-party APIs which implement the collection API.Scala also has specialized syntax for those APIs, but unlike Haskell, which tries to make them look like imperative C blocks and C#, which tries to make them look like SQL queries, Scala tries to make them look like  loops. They are called  and are the equivalent to C#'s query comprehensions and Haskell's monad comprehensions. (They also replace C#'s  and generators ()).But if you  want to know whether or not there are analogues for LINQ in Scala, you will first have to specificy what exactly you mean by \"LINQ\". (And of course, if you want to know whether they are \"sane\", you will have to define that, too.)All LINQ  extensions are available in Scala. For example:Linq:scala:There are many situations in Scala where you can use monadic constructs as a sort of query language.For example, to query XML (in this case, extracting URLs from links in some XHTML):For an analogue of LINQ to SQL, the closest thing is probably . To lift an example right out of the docs: is a modern database query and access library for Scala. \n()"},
{"body": "This might be the least important Scala question ever, but it's bothering me.  How would I generate a list of n random number.  What I have so far:Which works, but doesn't look very Scalarific to me.  I'm open to suggestions.Not because it's relevant so much as it's amusing and obvious in retrospect, the following looks like it works:But the index of each entry in the returned list is also the upper bound of that last.  The first number must be less than 1, the second less than 2, and so on.  I ran it three or four times and noticed \"Hmmm, the result always starts with 0...\"You can either use  or:Note that you don't need to create a new  object, you can use the default companion object Random, as stated above.How about:regarding your EDIT, can take an  argument as an upper bound for the random number, so  is the same as , rather than a more useful compilation error. does what you intended. But it's better to use  since that more accurately represents what you're doing."},
{"body": "How can you write to multiple outputs dependent on the key using Spark in a single Job. I could of course use  for all the possible keys, but that is a horrible hack, which will fire up many jobs Related: E.g.would ensure  isand  would beI know that the answer to this question will involve using MultipleOutputFormat in Hadoop.UPDATE: Please do not place any limitations on the resulting output, e.g. supply a solution where the number of files is fixed, or where the number of keys must be known a priori, or where the compression type is limited.UPDATES: In Scalding this is now super easy thanks to  I want an answer just like that!I would do it like this which is scalableJust saw similar answer above, but actually we don't need customized partitions. The MultipleTextOutputFormat will create file for each key. It is ok that multiple  record with same keys fall into the same partition. new HashPartitioner(num), where the num is the partition number you want. In case you have a big number of different keys, you can set number to big. In this case, each partition will not open too many hdfs file handlers.If you use Spark 1.4+, this has become much, much easier thanks to the . (DataFrames were introduced in Spark 1.3, but , which we need, was .)If you're starting out with an RDD, you'll first need to convert it to a DataFrame:In Python, this same code is:Once you have a DataFrame, writing to multiple outputs based on a particular key is simple. What's more -- and this is the beauty of the DataFrame API -- the code is pretty much the same across Python, Scala, Java and R:And you can easily use other output formats if you want:In each of these examples, Spark will create a subdirectory for each of the keys that we've partitioned the DataFrame on:If you potentially have many values for a given key, I think the scalable solution is to write out one file per key per partition. Unfortunately there is no built-in support for this in Spark, but we can whip something up.(Replace  with your choice of distributed filesystem operation.)This makes a single pass over the RDD and performs no shuffle. It gives you one directory per key, with a number of files inside each.I have a similar need and found an way. But it has one drawback (which is not a problem for my case): you need to re-partition you data with one partition per output file.To partition in this way it generally requires to know beforehand how many files the job will output and find a function that will map each key to each partition.First let's create our MultipleTextOutputFormat-based class:With this class Spark will get a key from a partition (the first/last, I guess) and name the file with this key, so it's not good to mix multiple keys on the same partition.For your example, you will require a custom partitioner. This will do the job:Now let's put everything together:This will generate 3 files under prefix (named 1, 2 and 7), processing everything in one pass.As you can see, you need some knowledge about your keys to be able to use this solution.For me it was easier because I needed one output file for each key hash and the number of files was under my control, so I could use the stock HashPartitioner to do the trick.I was in need of the same thing in Java. Posting my translation of  to Spark Java API users:I had a similar use case where I split the input file on Hadoop HDFS into multiple files based on a key (1 file per key). Here is my scala code for sparkI have grouped the records based on key. The values for each key is written to separate file.saveAsText() and saveAsHadoop(...) are implemented based on the RDD data, specifically by the method:    which takes the data from the PairRdd where it's executed.\nI see two possible options: If your data is relatively small in size, you could save some implementation time by grouping over the RDD, creating a new RDD from each collection and using that RDD to write the data. Something like this:Note that it will not work for large datasets b/c the materialization of the iterator at  might not fit in memory.The other option I see, and actually the one I'd recommend in this case is: roll your own, by directly calling the hadoop/hdfs api.Here's a discussion I started while researching this question:\ngood news for python user in the case you have multi columns and you want to save all the other columns not partitioned in csv format which will failed if you use \"text\" method as Nick Chammas' suggestion .error message is \"AnalysisException: u'Text data source supports only a single column, and you have 2 columns.;'\"In spark 2.0.0 (my test enviroment is hdp's spark 2.0.0) package \"com.databricks.spark.csv\" is now integrated , and it allow us save text file partitioned by only one column, see the example blow:In my spark 1.6.1 enviroment ,the code didn't throw any error,however ther is only one file generated. it's not partitioned by two folders.Hope this can help ."},
{"body": "I want to be able to use  and , the problem is that the compiler is triggering an error on the file which contains the play routes for my application:same goes for other routes.Is it possible maybe to tell scalac to ignore a file?Scala version is .A horrible \"solution\" could be to remove those unused imports after the routes are generated but before the compile task runs. Here's a sketch:You'll then want to sort out the task dependencies:You'll also likely need to set  to a conservative list before enabling . Something like this, depending on what types are used in your views:I also had to knock unused  imports out of Twirl templates (YMMV):If you do that, make sure the task dependencies are set up appropriately.This works for me. Scala 2.11.8, Play 2.5.10, both  and  enabled. It's hideous, but it works."},
{"body": "I have a suite of scalatest tests that test different endpoints of a RESTful API.\nI really want them separated into different files for best organization.My problem is how to start something (an HTTP server in my case, but it doesn't matter what it is) before all the tests and shut it down after all the tests are done.I know about BeforeAndAfterAll, but that only accomplishes before/after inside one test file.  I need something like that, but for all tests, for example:-- start http server before tests\n-- run all test suites\n-- shut down http serverThe intended way to do this is to use nested suites. Suite has a nestedSuites method that returns an IndexedSeq[Suite] (in 2.0, in 1.9.1 it was a List[Suite]). Suite also has a runNestedSuites method that is responsible for executing any nested suites. By default runNestedSuites calls nestedSuites, and on each returned Suite either invokes run directly, or if a Distributor is passed, puts the nested suites in the distributor so that they can be run in parallel.So what you really probably want to do is make Foo and Bar into classes, and return instances of them from the nestedSuites method of EndpointTests. There's a class that makes that easy called Suites. Here's an example of its use:One potential problem, though, is that if you are using discovery to find Suites to run, all three of EndpointTests, Foo, and Bar will be discovered. In ScalaTest 2.0 you can annotate Foo and Bar with @DoNotDiscover, and ScalaTest's Runner will not discover them. But sbt still will. We are currently enhancing sbt so that it passes over otherwise discoverable Suites that are annotated with DoNotDiscover, but this will be in sbt 0.13, which isn't out yet. In the meantime you can get sbt to ignore them by adding an unused constructor parameter to Foo and Bar.Alternatively you can just use an object.When you access the object it will be initialised, starting the server.\nJust create a common trait in the body of which you access the object.\nThen mixin that trait into all your tests. Done.If your server runs in daemon mode (e.g. a Play! application in test mode) it will be automatically shut down after all tests are run.Ok, found a way.  It seems (unless someone here can correct me) that Scalatest does not have the facility of a \"master\" suite. But... you can kinda build one.You can compose a suite from traits. So using my endpoint example:Ok, but what about the tests?  Notice the with  with . I'm bringing the dependent tests in as traits.\nSee here:"},
{"body": "I'm following the Scala course on Coursera.\nI've started to read the Scala book of Odersky as well.What I often hear is that it's not a good idea to throw exceptions in functional languages, because it breaks the control flow and we usually return an Either with the Failure or Success.\nIt seems also that Scala 2.10 will provide the Try which goes in that direction.But in the book and the course, Martin Odersky doesn't seem to say (at least for now) that exceptions are bad, and he uses them a lot.\nI also noticed the methods assert / require...Finally I'm a bit confused because I'd like to follow the best practices but they are not clear and the language seems to go in both directions...Can someone explain me what i should use in which case?The basic guideline is to use exceptions for something really exceptional**.  For an \"ordinary\" failure, it's far better to use  or .  If you are interfacing with Java where exceptions are thrown when someone sneezes the wrong way, you can use  to keep yourself safe.Let's take some examples.Suppose you have a method that fetches something from a map.  What could go wrong?  Well, something dramatic and dangerous like a * stack overflow, or something expected like the element isn't found.  You'd let the  stack overflow throw an exception, but if you merely don't find an element, why not return an  instead of the value or an exception (or )?Now suppose you're writing a program where the user is supposed to enter a filename.  Now, if you're not just going to instantly bail on the program when something goes wrong, an  is the way to go:Now suppose you want to parse an string with space-delimited numbers.So you have three ways (at least) to deal with different types of failure:  for it worked / didn't, in cases where not working is expected behavior, not a shocking and alarming failure;  for when things can work or not (or, really, any case where you have two mutually exclusive options) and you want to save some information about what went wrong; and  when you don't want the whole headache of exception handling yourself, but still need to interface with code that is exception-happy.Incidentally, exceptions make for good examples--so you'll find them more often in a textbook or learning material than elsewhere, I think: textbook examples are very often incomplete, which means that serious problems that normally would be prevented by careful design ought instead be flagged by throwing an exception.***So this is one of those places where Scala specifically trades off functional purity for ease-of-transition-from/interoperability-with legacy languages and environments, specifically Java.  Functional purity is broken by exceptions, as they break referential integrity and make it impossible to reason equationally.  (Of course, non-terminating recursions do the same, but few languages are willing to enforce the restrictions that would make those impossible.)  To keep functional purity, you use Option/Maybe/Either/Try/Validation, all of which encode success or failure as a referentially-transparent type, and use the various higher-order functions they provide or the underlying languages special monad syntax to make things clearer.  Or, in Scala, you can simply decide to ditch functional purity, knowing that it might make things easier in the short term but more difficult in the long.  This is similar to using \"null\" in Scala, or mutable collections, or local \"var\"s.  Mildly shameful, and don't do  to much of it, but everyone's under deadline.   "},
{"body": "True ... it has been discussed quite a lot.However there is a lot of ambiguity and some of the answers provided ... including duplicating jar references in the jars/executor/driver configuration or options.Following ambiguity, unclear, and/or omitted details should be clarified for each option:I am aware where I can find the , and specifically about , the  available, and also the . However that left for me still quite some holes, although it answered partially too.I hope that it is not all that complex, and that someone can give me a clear and concise answer.If I were to guess from documentation, it seems that , and the   and  methods are the ones that will automatically distribute files, while the other options merely modify the ClassPath.Would it be safe to assume that for simplicity, I can add additional application jar files using the 3 main options at the same time:Found a nice article on . However nothing new learned. The poster does make a good remark on the difference between Local driver (yarn-client) and Remote Driver (yarn-cluster). Definitely important to keep in mind.ClassPath is affected depending on what you provide. There are a couple of ways to set something on the classpath:If you want a certain JAR to be effected on both the Master and the Worker, you have to specify these separately in BOTH flags.:This depends on the mode which you're running your job under:In , the Spark documentation does a good job of explaining the accepted prefixes for files:As noted, JARs are copied to the  for each Worker node. Where exactly is that? It is  under , you'll see them like this:And when you look inside, you'll see all the JARs you deployed along:The most important thing to understand is . If you pass any property via code, it will take precedence over any option you specify via . This is mentioned in the Spark documentation:So make sure you set those values in the proper places, so you won't be surprised when one takes priority over the other.Lets analyze each option in question:You can safely assume this only for Client mode, not Cluster mode. As I've previously said. Also, the example you gave has some redundant arguments. For example, passing JARs to  is useless, you need to pass them to  if you want them to be on your classpath. Ultimately, what you want to do when you deploy external JARs on both the driver and the worker is:Another approach in  is to use  during spark-submit which changes the priority of dependency load, and thus the behavior of the spark-job, by giving priority to the jars the user is adding to the class-path with the  option."},
{"body": "Scala seems to have a .NET implementation too. I was wondering if it's a complete implementation with no loose ends or just a showcase thing.It's important, because the app we are about to develop should have Windows GUI besides the main implementation on web. Having a language where the core code can be ported between two implementations looks like a deal maker.Anyone worked on the .NET implementation of Scala? Any feedback?I have heard new funding has been acquired for the .NET side, but at the moment it is a great and increasing distance from \"production ready\" or even \"usable\".  There hasn't been a check-in which meaningfully touched the .NET side in a long time. A recent message to one of the scala lists.From: Lukas RytzOn Fri, Jan 15, 2010 at 03:18, Naftoli Gugenheim wrote:We're working on bootstrapping the compiler and we're fixing MSIL-Backend-Bugs along the way. EPFL will provide a compiler running on .NET (which uses IKVM.OpenJDK.Core.dll, at least in a first version)Right now, we have a cross-compiler running on the JVM, and a reduced version of scala-library.jar which runs on .NET.For some parts we will rely on the community (e.g. porting more of scala-library.jar to .NET).LukasMartin Odersky says in this  (January 2011): He starts talking about .NET at the 15 minute mark.The  claims:The  has this entry:The  talks about version 1.4, which is rather old by now. On the other hand, it looks like documentation was written as recently as 2008, including the  page which sounds pretty important.Without having any experience of it myself, it sounds like support is limping along, but that the .NET port isn't really a first-class citizen. The fact that there's no \"buzz\" around Scala on .NET (compared with the Java version) isn't encouraging either. It doesn't sound like something I'd want to use for commercial software at the moment. Of course, it's possible that it's fine, and just not well marketed. If you're really keen on the idea, I'd ask on a .Why not either use the  for a Java/Scala-based Windows client, or use .NET for the web app?In an interview on scala-lang.org on 18th July 2011:Link: \nLink: At the moment -- no, not really, which is a great pity, since it's quite a fun language.\"Hello world\" type programs, fine -- even fancy tricks like writing a simple stack where a small piece of Python (the same code under Jython or IronPython) drives the same piece of Scala.Anything moderately taxing -- the scalac-net compilation can and will throw, even if the code doesn't use anything outside the scala namespace, and builds and passes a fairly exhaustive set of unit tests on the JVM.  It can balk at building against even fairly simple assemblies (even if built using .net 1.0, to match the low spec of the mscorlib included in the scala-msil bundle).My test-sets for \"moderately taxing\" have included the following implementations intended to compensate for current lacks on .net As of Scala Days 2010 in April, Miguel Garcia was working on Scala.NET.  I believe he's working on a visual studio plugin and I'm not sure how far along things are, however you can find evidence of his work here:\n\nMiguel is now doing his postdoc at EPFL: Paul (extempore)'s answer probably has more up-to-date information, but if I were curious I might ask Miguel directly.Looking in the SVN repo, there's definitely some activity going on in the .net space.  In  there was a  directory, but that's gone in  and .  Instead there is a  directory which has had some non-trivial things going on as recently as .All the documentation available seems to be way out of date, but maybe it's worth asking one of the main contributors what the current status is.  Note that extempore (who as already answered this over a year ago) seems to be one of them."},
{"body": "In Scala there is a Stream class that is very much like an iterator.  The topic  offers some insights into the similarities and differences between the two.Seeing how to use a stream is pretty simple but I don't have very many  use-cases where I would use a stream instead of other artifacts.  The ideas I have right now:So have I missed any big uses?  Or is it a developer preference for the most part?ThanksThe main difference between a  and an  is that the latter is mutable and \"one-shot\", so to speak, while the former is not.  has a better memory footprint than , but the fact that it  mutable can be inconvenient.Take this classic prime number generator, for instance:It can be easily be written with an  as well, but an  won't  the primes computed so far.So, one important aspect of a  is that you can pass it to other functions without having it duplicated first, or having to generate it again and again.As for expensive computations/infinite lists, these things can be done with  as well. Infinite lists are actually quite useful -- you just don't know it because you didn't have it, so you have seen algorithms that are more complex than strictly necessary just to deal with enforced finite sizes.In addition to Daniel's answer, keep in mind that  is useful for short-circuiting evaluations.  For example, suppose I have a huge set of functions that take  and return , and I want to keep executing them until one of them works:Well, I certainly don't want to execute the  list, and there isn't any handy method on  that says, \"treat these as functions and execute them until one of them returns something other than \".  What to do?  Perhaps this:This takes a list and treats it as a  (which doesn't actually evaluate anything), then defines a new  that is a result of applying the functions (but that doesn't evaluate anything either yet), then searches for the first one which is defined--and here, magically, it looks back and realizes it has to apply the map, and get the right data from the original list--and then unwraps it from  to  using .Here's an example:But does it work?  If we put a  into our functions so we can tell if they're called, we get(This is with Scala 2.8; 2.7's implementation will sometimes overshoot by one, unfortunately.  And note that you  accumulate a long list of  as your failures accrue, but presumably this is inexpensive compared to your true computation here.)I could imagine, that if you poll some device in real time, a Stream is more convenient. Think of an GPS tracker, which returns the actual position if you ask it. You can't precompute the location where you will be in 5 minutes. You might use it for a few minutes only to actualize a path in OpenStreetMap or you might use it for an expedition over six months in a desert or the rain forest. Or a digital thermometer or other kinds of sensors which repeatedly return new data, as long as the hardware is alive and turned on - a log file filter could be another example. is to  as  is to . Favouring immutability prevents a class of bugs, occasionally at the cost of performance.scalac itself isn't immune to these problems: As Daniel points out, favouring laziness over strictness can simplify algorithms and make it easier to compose them."},
{"body": "I've a SBT multi-project where some projects have dependencies to each other. Like this:Now I have some test-code in the 'core' project in the test-folder. There are also stuff like mocks and test-utilities. Now I would like to use those test utilities in the tests of the extensions. For production code this works, since I've declared a dependency. However it seems that dependency doesn't hold for the tests. When I run the tests I get compilation error for missing classes. Those classes are from the test-code in the core-project.How can I tell sbt that the dependency also should include the test-code for the test-scope? So that I can reuse my mocks in the test-code of the 'exension'-project?Like so:This is discussed in the section \"Per-configuration classpath dependencies\" in then  guide.You can also do this with a  after the initial project declaration."},
{"body": "I have two RDD's that I want to join and they look like this:It happens to be the case that the key values of  are unique and also that the tuple-key values of  are unique.  I'd like to join the two data sets so that I get the following rdd:What's the most efficient way to achieve this?  Here are a few ideas I've thought of.  Option 1:Option 2:Option 1 will collect all of the data to master, right?  So that doesn't seem like a good option if rdd1 is large (it's relatively large in my case, although an order of magnitude smaller than rdd2).  Option 2 does an ugly distinct and cartesian product, which also seems very inefficient.  Another possibility that crossed my mind (but haven't tried yet) is to do option 1 and broadcast the map, although it would be better to broadcast in a \"smart\" way so that the keys of the map are co-located with the keys of .Has anyone come across this sort of situation before?  I'd be happy to have your thoughts.Thanks!One option is to perform a broadcast join by collecting  to the driver and broadcasting it to all mappers; done correctly, this will let us avoid an expensive shuffle of the large  RDD:  The  tells Spark that this map function doesn't modify the keys of ; this will allow Spark to avoid re-partitioning  for any subsequent operations that join based on the  key.This broadcast could be inefficient since it involves a communications bottleneck at the driver.  In principle, it's possible to broadcast one RDD to another without involving the driver; I have a prototype of this that I'd like to generalize and add to Spark.Another option is to re-map the keys of  and use the Spark  method; this will involve a full shuffle of  (and possibly ):On my sample input, both of these methods produce the same result:A third option would be to restructure  so that  is its key, then perform the above join.Another way to do it is to create a custom partitioner and then use zipPartitions to join your RDDs."},
{"body": "In the work that I do on a day to day in Java, I use builders quite a lot for fluent interfaces, e.g.: With a quick-and-dirty Java approach, each method call mutates the builder instance and returns . Immutably, it involves more typing, cloning the builder first before modifying it. The build method eventually does the heavy lifting over the builder state.What's a nice way of achieving the same in Scala?If I wanted to ensure that  was called only once, and then subsequently only  and  could be called, a-la a directed builder, how would I go about approaching this?Another alternative to the Builder pattern in Scala 2.8 is to use immutable case classes with default arguments and named parameters. Its a little different but the effect is smart defaults, all values specified and things only specified once with syntax checking...The following uses Strings for the values for brevity/speed...You can then also use existing immutable instances as kinda builders too...You have three main alternatives here.It's the same exact pattern.  Scala allows for mutation and side effects.  That said, if you'd like to be more of a purest, have each method return a new instance of the object that you're constructing with the element(s) changed.  You could even put the functions within the Object of a class so that there's a higher level of separation within your code.so that your code might look like(Note: I've probably screwed up some syntax here.)Case classes solve the problem as shown in previous answers, but the resulting api is difficult to use from java when You have scala collections in your objects. To provide a fluent api to java users try this:Then in java code the api is really easy to useusing Scala partial applies are feasible if you are building a smallish object that you don't need to pass over method signatures. If any of those assumptions don't apply, I recommend using a mutable builder to build an immutable object. With this being scala you could implement the builder pattern with a case class for the object to build with a companion as the builder.Given that the end result is a constructed immutable object I don't see that it defeats any of the Scala principles.  I think scala's Stackable Modification could be used here. Check this out : \nIt works for me, but I think it is write Java for Scala compiler. I`m still learning the scala way, functionnaly way.The usage is:The structure is:"},
{"body": "As shown below, in Haskell, it's possible to store in a list values with heterogeneous types with certain context bounds on them:How can I achieve the same in Scala, preferably without subtyping?As @Michael Kohl commented, this use of forall in Haskell is an existential type and can be exactly replicted in Scala using either the forSome construct or a wildcard. That means that @paradigmatic's answer is largely correct.Nevertheless there's something missing there relative to the Haskell original which is that instances of its ShowBox type also capture the corresponding Show type class instances in a way which makes them available for use on the list elements even when the exact underlying type has been existentially quantified out. Your comment on @paradigmatic's answer suggests that you want to be able to write something equivalent to the following Haskell,@Kim Stebel's answer shows the canonical way of doing that in an object-oriented language by exploiting subtyping. Other things being equal, that's the right way to go in Scala. I'm sure you know that, and have good reasons for wanting to avoid subtyping and replicate Haskell's type class based approach in Scala. Here goes ...Note that in the Haskell above the Show type class instances for Unit, Int and Bool are available in the implementation of the useShowBox function. If we attempt to directly translate this into Scala we'll get something like,and this fails to compile in useShowBox as follows,The problem here is that, unlike in the Haskell case, the Show type class instances aren't propagated from the ShowBox argument to the body of the useShowBox function, and hence aren't available for use. If we try to fix that by adding an additional context bound on the useShowBox function,this fixes the problem within useShowBox, but now we can't use it in conjunction with map on our existentially quantified List,This is because when useShowBox is supplied as an argument to the map function we have to choose a Show instance based on the type information we have at that point. Clearly there isn't just one Show instance which will do the job for all of the elements of this list and so this fails to compile (if we had defined a Show instance for Any then there would be, but that's not what we're after here ... we want to select a type class instance based on the most specific type of each list element).To get this to work in the same way that it does in Haskell, we have to explicitly propagate the Show instances within the body of useShowBox. That might go like this,then in the REPL,Note that we've desugared the context bound on ShowBox so that we have an explicit name (showInst) for the Show instance for the contained value. Then in the body of useShowBox we can explicitly apply it. Also note that the pattern match is essential to ensure that we only open the existential type once in the body of the function.As should be obvious, this is a lot more vebose than the equivalent Haskell, and I would strongly recommend using the subtype based solution in Scala unless you have extremely good reasons for doing otherwise.As pointed out in the comments, the Scala definition of ShowBox above has a visible type parameter which isn't present in the Haskell original. I think it's actually quite instructive to see how we can rectify that using abstract types.First we replace the type parameter with an abstract type member and replace the constructor parameters with abstract vals,We now need to add the factory method that case classes would otherwise give us for free,We can now use plain ShowBox whereever we previously used ShowBox[_] ... the abstract type member is playing the role of the existential quantifier for us now,(It's worth noting that prior to the introduction of explict forSome and wildcards in Scala this was exactly how you would represent existential types.)We now have the existential in exactly the same place as it is in the original Haskell. I think this is as close to a faithful rendition as you can get in Scala.The  example you gave involves an . I'm renaming the  data constructor to  to distinguish it from the :We say  is \"existential\", but the  here is a universal quantifier that pertains to the  data constructor. If we ask for the type of the  constructor with explicit  turned on, this becomes much clearer:That is, a  is actually constructed from three things:Because the type  becomes part of the constructed , it is . If Haskell supported a syntax for existential quantification, we could write  as a type alias:Scala does support this kind of existential quantification and Miles's answer gives the details using a trait that consists of exactly those three things above. But since this is a question about \"forall in Scala\", let's do it exactly like Haskell does.Data constructors in Scala cannot be explicitly quantified with forall. However, every method on a module can be. So you can effectively use type constructor polymorphism as universal quantification. Example:A Scala type , given some , is then equivalent to a Haskell type .We can use this technique to add constraints to the type argument.A value of type  is like a value of the Haskell type . The instance of  is implicitly looked up by Scala if it exists.Now, we can use this to encode your  ...The  method is the universally quantified data constructor. You can see that it takes a type , an instance of , and a value of type , just like the Haskell version.Here's an example usage: in Scala might be to use a case class:Then:In this case, a  is basically equivalent to a , but you can use this technique with traits other than  to get something more interesting.This is all using the  typeclass from .I don't think a 1-to-1 translation from Haskell to Scala is possible here. But why don't you want to use subtyping? If the types you want to use (such as Int) lack a show method, you can still add this via implicit conversions.( : Adding methods to show, to answer comment. )I think you can get the same using implicit methods with context bounds:Why not:As the authorities' answers suggested,\nI'm often surprised that Scala can translate \"Haskell type monsters\" into very simple one."},
{"body": "I'm trying to understand the advantages of currying over partial applications in Scala. Please consider the following code:So, if I can calculate partially applied function that easy, what are the advantages of currying?Currying is mostly used if the second parameter section is a function or a by name parameter. This has two advantages. First, the function argument can then look like a code block enclosed in braces. E.g.This reads better than the uncurried alternative:Second, and more importantly, type inference can usually figure out the function's parameter type, so it does not have to be given at the call site.\nFor instance, if I define a  function over lists like this:I can call it like this:or even shorter:If I defined  as an uncurried function, this would not work, I'd have to call it like this:If the last parameter is not a function or by-name parameter, I would not advise currying. Scala's  notatation is amost as lightweight, more flexible, and IMO clearer.I think it becomes clearer if you invert your curried example:It is more of an optical effect but you don\u2019t need to use any parentheses around the whole expression. Of course you can achieve the same result using partial application or by creating a helper method to invert the arguments, sure. The point is, that you don\u2019t have to do all of this if you start with a curried method in the first place. In that sense currying is more of an API and syntax sugar thing. It is not expected that you useanywhere in your code or that this is in any way especially meaningful to do. It is just that you get a nicely looking expression easily.(Well, and there are some advantages with respect to type inference\u2026)"},
{"body": "It seems to me like the  syntax for partial functions require at least one :So, what's the best way to define an \"empty\" partial function? Is there a better way than \"manually\" overriding  and ?Map is a PartialFunction so you can do:Since Scala 2.10 you can use:As @didierd said in the comments, due to argument variances, a single instance can cover all possible argument types.Stealing from everyone, a possible mix of it all : Shortest one I can think of:A solution (which is more a hack) is to ensure that the case is never true: Simple curiosity, why do you need such a function?It may be interesting to know that it is planned to add an empty member to the scala library and to see how it is implemented:\n"},
{"body": "With too many arguments,  easily gets too confusing. Is there a more powerful way to format a String. Like so:Or is this not possible because of type issues ( would need to take a Map[String, Any], I assume; don\u2019t know if this would make things worse).Or is the better way doing it like this:even though it pollutes the name space?While a simple pimping might do in many cases, I\u2019m also looking for something going in the same direction as Python\u2019s  (See: )In Scala 2.10 you can use .Well, if your only problem is making the order of the parameters more flexible, this can be easily done:And there's also regex replacement with the help of a map:Maybe the Scala-Enhanced-Strings-Plugin can help you. Look here:You can easily implement a richer formatting yourself (with pimp-my-library approach):If you're using 2.10 then go with built-in interpolation. Otherwise, if you don't care about extreme performance and are not afraid of functional one-liners, you can use a fold + several regexp scans:This the answer I came here looking for:  You might also consider the use of a template engine for really complex and long strings. On top of my head I have  which implements amongst others the  template engine. Might be overkill and performance loss for simple strings, but you seem to be in that area where they start becoming real templates."},
{"body": "I have the following code which recursively operates on each element within a ListNow, ignoring that this functionality is available through  and , this works just fine. However, if I try to change it to accept any , I run into problems:Here is how I think the code should look, except it doesn't work:Edit: So many good answers! I'm accepting agilesteel's answer as his was the first that noted that :: isn't an operator in my example, but a case class and hence the difference.There are two  (pronounced cons) in Scala. One is an operator defined in  and one is a  (subclass of ), which represents a non empty list characterized by a head and a tail. is a constructor pattern, which is syntactically modified from . is a case class, which means there is an extractor object defined for it.Kind of cheating, but here it goes:Don't ask me why  doesn't work...As of the ides of March 2012, this works in 2.10+:More generally, two different head/tail and init/last decomposition objects mirroring append/prepend were added for  in :You can actually define an object for  to do exactly what you are looking for:Then your code works exactly as expected.This works because  is equivalent to  when used for patten matching.I don't think there is pattern matching support for arbitrary sequences in the standard library. You could do it with out pattern matching though:You can define your own extractor objects though. See  A simple tranformation from Seq to List would do the job:"},
{"body": "I have a Scala data processing tool which is failing with a  exception. The tool needs to make a couple passes over a large data file (the one I'm working on is over 700MB), so it would be convenient if the entire thing could be stored in memory.I run the tool from the command line or from a Bash script using the \"scala\" runner. How do I increase the JVM heap size for this? I've tried passing , but it does not recognize this argument. I'm using a nightly build of Scala 2.8.0 (r18678).The JAVA_OPTS environment variable is used to specify the options passed to the java command. See the  for details.You can pass options to the JVM via -J.  E.g.,Based on Joe's suggestion, I looked at the scala program, which is a Bash script that calls java with various Scala specific arguments. It allows extra command line arguments to be passed to Java through the JAVA_OPTS environment variable. So you can increase the heap size like this:Edit .As a hack you can edit the  file to edit the parameters for the  command. Not pretty."},
{"body": "Say I'm writing an extension methodShould you always include  in the class defininition? Under what circumstances would you not want to make an implicit class a value class?Let's look at the  and think when they may not be suitable for implicit classes:In addition, making your implicit class a value class could possibly change some behavior of code using reflection, but reflection shouldn't normally see implicit classes.If your implicit class does satisfy all of those limitations, I can't think of a reason not to make it a value class.I sense you're confusing  with . You'd rarely extend anything when defining an Implicit Class for an enhancement while Value Classes  extend ."},
{"body": "I am trying to parse a CSV file, ideally using weka.core.converters.CSVLoader.\nHowever the file I have is not a valid UTF-8 file.\nIt is mostly a UTF-8 file but some of the field values are in different encodings,\nso there is no encoding in which the whole file is valid,\nbut I need to parse it anyway.\nApart from using java libraries like Weka, I am mainly working in Scala.\nI am not even able to read the file usin scala.io.Source:\nFor examplethrows:I am perfectly happy to throw all the invalid characters away or replace them with some dummy.\nI am going to have lots of text like this to process in various ways\nand may need to pass the data to various third party libraries.\nAn ideal solution would be some kind of global setting that would\ncause all the low level java libraries to ignore invalid bytes in text,\nso that that I can call third party libraries on this data without modification.SOLUTION:Thanks to +Esailija for pointing me in the right direction.\nThis lead me to \nwhich provides the core java solution. In Scala I can make this the default behaviour by making the codec implicit. I think I can make it the default behaviour for the entire package by putting it the implicit codec definition in the package object.This is how I managed to do it with java:The invalid file is created with bytes:Which is  in UTF-8 with 4 invalid bytes mixed in.With  you see the standard unicode replacement character being used:With , you see the invalid bytes ignored:Without specifying , you get The solution for scala's Source (based on @Esailija answer):Scala's Codec has a decoder field which returns a :The problem with ignoring invalid bytes is then deciding when they're valid again. Note that UTF-8 allows variable-length byte encodings for characters, so if a byte is invalid, you need to understand which byte to start reading from to get a valid stream of characters again.In short, I don't think you'll find a library which can 'correct' as it reads. I think a much more productive approach is to try and clean that data up first.I'm switching to a different codec if one fails.In order to implement the pattern, I got inspiration from .I use a default List of codecs, and recursively go through them.  If they all fail, I print out the scary bits:I'm just learning Scala, so the code may not be optimal.A simple solution would be to interpret your data stream as ASCII, ignore all non-text characters. However, you would lose even valid encoded UTF8-characters. Don't know if that is acceptable for you.EDIT: If you know in advance which columns are valid UTF-8, you could write your own CSV parser that can be configured which strategy to use on what column.Use  as the encoder; this will just give you byte values packed into a string.  This is enough to parse CSV for most encodings.  (If you have mixed 8-bit and 16-bit blocks, then you're in trouble; you can still read the lines in ISO-8859-1, but you may not be able to parse the line as a block.)Once you have the individual fields as separate strings, you can tryto generate the string with the proper encoding (use the appropriate encoding name per field, if you know it).Edit: you will have to use  if you want to detect errors.  Mapping to UTF-8 this way will just give you 0xFFFF in your string when there's an error."},
{"body": "There is an Idris tutorial, an Agda tutorial and many other tutorial style papers and introductory material with never ending references to things yet to learn. I'm kind of crawling in the middle of all these and most of the time I'm stuck with mathematical notations and new terminology appearing suddenly with no explanation. Perhaps my math sucks :-)Is there any disciplined way to approach dependent type programming? Like when you want to learn Haskell, you start with \"Teach yourself a Haskell\", when you want to learn Scala, you start with Odersky's book, for Ruby you read that weird tutorial with mutated bugs in it. But I can't start Agda or Idris with their books. They are way above my head. I tried Coq and got stuck in its all-about-teorm-proving style. Agda requires a huge math background and Idris, well, let's leave that for now!I understand static type systems very well, I am kind of proficient with Scala and I can use Haskell if necessary. I understand the Functional Paradigm and use it day to day, I understand Algebraic Data Types and GADTs (quite smoothly actually) and I recently managed to comprehend the Lambda Cube. I'm lacking in the math and logic parts, though. I would highly recommend . This book is quite good at introducing you to Coq one step at a time. There is a lot of theorem proving, yes, but that's part of the deliciousness of dependent types. It's a great feeling when the line between \"programming\" and \"proving\" starts to blur.I think Software Foundations does a pretty good job of bringing you up to speed for the logic you need to know. Already being comfortable with the concept of implication helps, though.(Notice: This is a self advertisement)I am writing an  and my primary goal is to\nlet people to play with Agda without theoretical background.This tutorial may solve most of your problems:It is under development, but the first half is kind of ready."},
{"body": "Given a trait :it can be mixed into a class with  or :It can also be mixed upon instantiating a new instance:But...can the trait (or any other if that makes a difference) be added to an existing instance?I'm loading objects using JPA in Java and I'd like to add some functionality to them using traits. Is it possible at all?I'd like to be able to mix in a trait as follows:I have a idea for this usage:you can use this trait as below:for your example code:I hope this can help you.UPDATEDbut this pattern has some restrict, you can't use some implicit helper method that defined already.An existing runtime object in the JVM has a certain size on the heap. Adding a trait to it would mean altering its size on the heap, and changing its signature.So the only way to go would be to do some kind of transformation at compile time.Mixin composition in Scala occurs at compile time. What compiler could potentially do is create a wrapper B around an existing object A with the same type that simply forwards all calls to the existing object A, and then mix in a trait T to B. This, however, is not implemented. It is questionable when this would be possible, since the object A could be an instance of a final class, which cannot be extended.In summary, mixin composition is not possible on existing object instances.UPDATED:Related to the smart solution proposed by Googol Shan, and generalizing it to work with any trait, this is as far as I got. The idea is to extract the common mixin functionality in the  trait. The client should then create a companion object extending  for each trait he wants to have the dynamic mixin functionality for. This companion object requires defining the anonymous trait object gets created ().I usually used a  to mix in a new method to an existing object.See, if I have some code as below:and then you can use  method with an already existing object Test.in your example, you can use like this:I am thinking out a prefect syntax to define this HelperObject:What about an implicit class? It seems easier to me compared to the way in the other answers with a final inner class and a \"mixin\"-function. "},
{"body": "What are the available code coverage tools for Scala? I have Scala spec tests and a Hudson continuous integration set-up. Is there something I can hook-in to this setup to measure and track code coverage?SCCT is a compiler plugin which instruments the classes to gather coverage data:I use .  However, any Java coverage tool should work just fine.  The only catch is that you will end up with a large number of auto-generated classes in your coverage list.  This is because while Scala compiles down into very natural JVM bytecode, it is forced to produce an unnaturally large number of classes to accommodate common functional features like lazy evaluation. is little better.One problem with non-mainstream languages (such as Scala) is that tools are hard to find, because they are hard to build.This technical paper  (I'm the author) describes how to build test coverage tools for langauges in systematic way to help get around this problem, using a generic tool-building infrastructure.We've implemented  this way, including instrumenters, data collection and test coverage display and reporting.\nIt would be straightforward to implement Scala like this.The solutions posed by other answers produces confusing information from the implementation of Scala (\"auto genreated classes\").  What developers want to see is coverage data in terms of their code.   The approach we use instruments the source code, so the results are stated entirely and only in terms of the source code; even the test coverage viewer shows the source code covered with coverage information.I've put together a SBT plugin called  that uses scct under the hood, but publishes the results to .  Still, it's good if you want to code coverage reports to be publicly visible. Check out an example of the results I use jacoco. It does not require compile- or runtime- dependencies, instruments classes on the fly w/o special instrumentation phase. Also it integrated with Sonar and published on Maven Central. Here is example: I would like to add better reporting: more detailed branch coverage makrup, excluding of generated classes/methods, and to be handy like ScalaDoc (see SCCT reports for example)"},
{"body": "I'm about to write a Scala command-line application that relies on a MySQL database. I've been looking around for ORMs, and am having trouble finding one that will work well.The  looks nice, but I'm not sure it can be decoupled from the entire Lift web framework. ActiveObjects also looks OK, but the author says that it may not work well with Scala.I'm not coming to Scala from Java, so I don't know all the options. Has anyone used an ORM with Scala, and if so, what did you use and how well did it work?There are several reasons why JPA-oriented frameworks (Hibernate, for instance) do not fit into idiomatic Scala applications elegantly:There are more reasons, I'm sure. That's why we have started the . This pure-Scala ORM tries it's best to eliminate the nightmares of classic Java ORMs. Specifically, you define your entities in pretty much way you would do this with classic DDL statements:As you can see, these declarations are a bit more verbose, than classic JPA POJOs. But in fact there are several concepts that are assembled together:The only things Circumflex ORM lacks are:P.S. I hope this post will not be considered an advertisement. It isn't so, really -- I was trying to be as objective as possible. I experimented with  and basic operations worked fine for me.  JPA is a Java standard and there are other implementations that may also work (, etc). Here is an example of what a JPA class in Scala looks like:I am happy to announce the 1st release of a new ORM library for Scala. MapperDao maps domain classes to database tables. It currently supports mysql, postgresql (oracle driver to be available soon), one-to-one, many-to-one, one-to-many, many-to-many relationships, autogenerated keys, transactions and optionally integrates nicely with spring framework. It allows freedom on the design of the domain classes which are not affected by persistence details, encourages immutability and is type safe. The library is not based on reflection but rather on good Scala design principles and contains a DSL to query data, which closely resembles select queries. It doesn't require implementation of equals() or hashCode() methods which can be problematic for persisted entities. Mapping is done using type safe Scala code.Details and usage instructions can be found at the mapperdao's site:The library is available for download on the above site and also as a maven dependency (documentation contains details on how to use it via maven)Examples can be found at:Very brief introduction of the library via code sample:Querying is very familiar:I encourage everybody to use the library and give feedback. The documentation is currently quite extensive, with setup and usage instructions. Please feel free to comment and get in touch with me at kostas dot kougios at googlemail dot com.Thanks,Kostantinos KougiosHere's basically the same example with @Column annotation: is a perfect match for a functional world. Traditional ORM's are not a perfect fit for Scala. Slick composes well and uses a DSL that mimics Scala collection classes and for comprehensions.Of course, any Java database access framework will work in Scala as well, with the usual issues that you may encounter, such as collections conversion, etc. jOOQ for instance, has been observed to work well in Scala. An example of jOOQ code in Scala is given in the manual:Taken from\n"},
{"body": "Are there any advantages to using object-oriented programming (OOP) in a functional programming (FP) context?I have been using  for some time now, and I noticed that the more my functions are stateless, the less I need to have them as methods of objects. In particular, there are advantages to relying on type inference to have them usable in as wide a number of situations as possible.This does not preclude the need for namespaces of some form, which is orthogonal to being OOP. Nor is the use of data structures discouraged. In fact, real use of FP languages depend heavily on data structures. If you look at the F# stack implemented in , you will find that it is not object-oriented.In my mind, OOP is heavily associated with having methods that act on the state of the object mostly to  the object. In a pure FP context that is not needed nor desired.A practical reason may be to be able to interact with OOP code, in much the same way F# works with . Other than that however, are there any reasons? And what is the experience in the Haskell world, where programming is more pure FP?I will appreciate any references to papers or counterfactual real world examples on the issue.The disconnect you see is not of FP vs. OOP. It's mostly about immutability and mathematical formalisms vs. mutability and informal approaches.First, let's dispense with the mutability issue: you can have FP with mutability and OOP with immutability just fine. Even more-functional-than-thou Haskell lets you play with mutable data all you want, you just have to be explicit about what is mutable and the order in which things happen; and efficiency concerns aside, almost any mutable object could construct and return a new, \"updated\" instance instead of changing its own internal state.The bigger issue here is mathematical formalisms, in particular heavy use of algebraic data types in a language little removed from lambda calculus. You've tagged this with Haskell and F#, but realize that's only half of the functional programming universe; the Lisp family has a very different, much more freewheeling character compared to ML-style languages. Most OO systems in wide use today are very informal in nature--formalisms do exist for OO but they're not called out explicitly the way FP formalisms are in ML-style languages.Many of the apparent conflicts simply disappear if you remove the formalism mismatch. Want to build a flexible, dynamic, ad-hoc OO system on top of a Lisp? Go ahead, it'll work just fine. Want to add a formalized, immutable OO system to an ML-style language? No problem, just don't expect it to play nicely with .NET or Java.Now, you may be wondering, what  an appropriate formalism for OOP? Well, here's the punch line: In many ways, it's more function-centric than ML-style FP! I'll refer back to  for what seems to be the key distinction: structured data like algebraic data types in ML-style languages provide a concrete representation of the data and the ability to define operations on it; objects provide a black-box abstraction over  and the ability to easily replace components.There's a duality here that goes deeper than just FP vs. OOP: It's closely related to what some programming language theorists call : With concrete data, you can easily add new operations that work with it, but changing the data's structure is more difficult. With objects you can easily add new data (e.g., new subclasses) but adding new operations is difficult (think adding a new abstract method to a base class with many descendants).The reason why I say that OOP is more function-centric is that functions themselves represent a form of behavioral abstraction. In fact, you can simulate OO-style structure in something like Haskell by using records holding a bunch of functions as objects, letting the record type be an \"interface\" or \"abstract base class\" of sorts, and having functions that create records replace class constructors. So in that sense, OO languages use higher-order functions far, far more often than, say, Haskell would.For an example of something like this type of design actually put to very nice use in Haskell, read the source for , in particular the way that it uses an opaque record type containing functions and combines things only in terms of their behavior. A few final things I forgot to mention above.If OO indeed makes extensive use of higher-order functions, it might at first seem that it should fit very naturally into a functional language such as Haskell. Unfortunately this isn't quite the case. It is true that  as I described them (cf. the paper mentioned in the LtU link) fit just fine. in fact, the result is a more pure OO style than most OO languages, because \"private members\" are represented by values hidden by the closure used to construct the \"object\" and are inaccessible to anything other than the one specific instance itself. You don't get much more private than that!What doesn't work very well in Haskell is . And, although I think inheritance and subtyping are all too often misused in OO languages, some form of subtyping is quite useful for being able to combine objects in flexible ways. Haskell lacks an inherent notion of subtyping, and hand-rolled replacements tend to be exceedingly clumsy to work with.As an aside, most OO languages with static type systems make a complete hash of subtyping as well by being too lax with substitutability and not providing proper support for variance in method signatures. In fact, I think the only full-blown OO language that hasn't screwed it up completely, at least that I know of, is Scala (F# seemed to make too many concessions to .NET, though at least I don't think it makes any  mistakes). I have limited experience with many such languages, though, so I could definitely be wrong here.On a Haskell-specific note, its \"type classes\" often look tempting to OO programmers, to which I say: Don't go there. Trying to implement OOP that way will only end in tears. Think of type classes as a replacement for overloaded functions/operators, not OOP.As for Haskell, classes are less useful there because some OO features are more easily achieved in other ways.Encapsulation or \"data hiding\" is frequently done through function closures or existential types, rather than private members.  For example, here is a data type of random number generator with encapsulated state.  The RNG contains a method to generate values and a seed value.  Because the type 'seed' is encapsulated, the only thing you can do with it is pass it to the method.Dynamic method dispatch in the context of parametric polymorphism or \"generic programming\" is provided by type classes (which are not OO classes).  A type class is like an OO class's virtual method table.  However, there's no data hiding.  Type classes do not \"belong\" to a data type the way that class methods do.Dynamic method dispatch in the context of subtyping polymorphism or \"subclassing\" is almost a translation of the class pattern in Haskell using records and functions.I think that there are several ways of understanding what OOP means. For me, it is not about encapsulating , but more about organizing and structuring programs. This aspect of OOP can be used perfectly fine in conjunction with FP concepts.I believe that mixing the two concepts in F# is a very useful approach - you can associate  state with operations working on that state. You'll get the nice features of 'dot' completion for identifiers, the ability to easy use F# code from C#, etc., but you can still make your code perfectly functional. For example, you can write something like:In the beginning, people don't usually declare types in F# - you can start just by writing functions and later evolve your code to use them (when you better understand the domain and know what is the best way to structure code). The above example:"},
{"body": "Is there a way to write an anonymous function that is recursive in Scala? I'm thinking of something like this:(Related question: )As described in the link you posted. You can use Y-combinator. Here is example:Note it doesn't work with big numbers. \nBe careful with tail call optimization.If you don't want to hit the \"Amazing mathematics\" you could just revert to the object aspects of scala.in order to make it look geekier you can also use this code style:Adding to the many good responses here in this thread, the fact that Scala is not giving us tail call optimizable Fixed-point combinator has been bothering me so much so that I've decided to write a macro to translate Y-combinator-like call to an ordinary, idiomatic recursive call (with tail call optimization, of course). The idea is that a call likeis readily translatable intoI've put up macro implementation targeting Scala 2.11 (with minor tweak should also work with 2.10) into . With this macro, we can perform ordinary recursive tasks in anonymous manner without fearing stack overflow e.g.givesA very simple approach:"},
{"body": "I saw that there are two methods to cast an object in Scala:When I tried, I found that  doesn't use implicit conversion whereas the other one does. What are the differences of behavior between these two methods? And where is it recommended to use one over the other?Type ascriptions can also be used to trigger implicit conversions. For instance, you could define the following implicit conversion:and then ensure its use like so:Ascribing type  to a  would normally be a compile-time type error, but before giving up the compiler will search for available implicit conversions to make the problem go away. The particular implicit conversion that will be used in a given context is known at compile time.Needless to say, runtime errors are undesirable, so the extent to which you can specify things in a  manner (without using ), the better! If you find yourself using , you should probably be using  instead.Pelotom's answer covers the theory pretty nice, here are some examples to make it clearer:As you can see the type ascription can be used to resolve disambiguations. Sometimes they can be unresolvable by the compiler (see later), which will report an error and you must resolve it. In other cases (like in the example) it just uses the \"wrong\" method, not that you want.  does not compile, showing that the type ascription is not a cast. Compare it with the previous line, where the compiler is happy, but you get an exception, so the error is detected then with the type ascription.You will get an unresolvable ambiguity if you also add a methodIn this case the first and third invocation of foo will not compile, as the compiler can not decide if you want to call the foo with a single Any param, or with the varargs, as both of them seems to be equally good => you must use a type ascription to help the compiler. see also \"Programming in Scala\" covers this in a bit of detail in Chapter 15 - Case Classes and Pattern Matching.Basically the second form can be used as 'Typed Pattern' in a pattern match, giving the isInstanceOf and asInstanceOf functionality. Comparevs.The authors hint that the verbosity of the isInstance* way of doing things is intentional to nudge you into the pattern matching style.I'm not sure which pattern is more effective for a simple typecast without a test though.There is example of the difference:Example:We can also call method using multiple-dispatch:We can define implicit conversion:"},
{"body": "E.g. why doeswork, butfails (because arrays are invariant). What is the desired effect behind this design decision?Because it would break type-safety otherwise.\nIf not, you would be able to do something like this:and the compiler cant catch it.On the other hand, lists are immutable, so you can't add something that is not This is because lists are immutable and arrays are mutable.The difference is that s are immutable while s are mutable.To understand why mutability determines variance, consider making a mutable version of  - let's call it . We'll also make use of some example types: a base class  and 2 subclasses named  and .Notice that  has one more method () than .Then, define a function that accepts a mutable list of animals and modifies the list:Now, horrible things will happen if you pass a list of cats into the function:If we were using a careless programming language, this will be ignored during compilation. Nevertheless, our world will not collapse if we only access the list of cats using the following code:But if we do this:A runtime error will occur. With Scala, writing such code is prevented, because the compiler will complain. The normal answer to give is that mutability combined with covariance would break type safety. For collections, this can be taken as a fundamental truth. But the theory actually applies to any generic type, not just collections like  and .The real answer has to do with the way function types interact with subtyping. The short story is if a type parameter is used as a return type, it is covariant. On the other hand, if a type parameter is used as an argument type, it is contravariant. If it is used both as a return type and as an argument type, it is invariant.Let's look at the . The two obvious methods to look at are for the ones for lookup and update:In the first method  is a return type, while in the second  is an argument type. The rules of variance dictate that  must therefore be invariant.We can compare the  to see why it is covariant. Confusingly, we would find these methods, which are analogous to the methods for :Since  is used as both a return type and as an argument type, we would expect  to be invariant just like  is for . However, unlike with , the documentation is lying to us about the type of . The lie is good enough for most calls to this method, but isn't good enough to decide the variance of . If we expand the documentation for this method and click on \"Full Signature\", we are shown the truth:So  does not actually appear as an argument type. Instead,  (which can be any supertype of ) is the argument type. This does not place any restriction on , so it really can be covariant. Any method on  which has  as an argument type is a similar lie (we can tell because these methods are marked as )."},
{"body": "So say i have some list likeAnd then i want to filter one of them out, i can do something likeIs there any way i can \"unpack\" the tuple as the parameter to the lambda directly, instead of having this intermediate  variable?Something like the following would be ideal, but eclipse tells me Any help would be appreciated - using 2.9.0.1This is about the closest you can get:It's basically pattern matching syntax inside an anonymous . There are also the  methods in  object and traits, but they are just a wrapper around this pattern matching expression.Hmm although Kipton has a good answer. You can actually make this shorter by doing.There are a bunch of options:... is a type mismatch because that lambda defines aYou can convert between them like this:I've pondered the same, and came to your question today. I'm not very fond of the partial function approaches (anything having ) since they imply that there could be more entry points for the logic flow. At least to me, they tend to blur the intention of the code. On the other hand, I really do want to go straight to the tuple fields, like you.Here's a solution I drafted today. It seems to work, but I haven't tried it in production, yet.Output:Maybe this turns out to be useful. The  object should naturally be put aside in some tool namespace.Applied to your case:"},
{"body": "Given:How can I get  from  and  from ?Or if you want only the name without the package:See the documentation of  for more information.You can use the property  of the case class:N.B.\nIf you pass to scala 2.8 extending a case class have been deprecated, and you have to not forget the left and right parent Here is a Scala function that generates a human-readable string from any type, recursing on type parameters:"},
{"body": "What is the canonical way to get an empty array in Scala?  is too verbose.You can leave out the  part if it can be inferred (e.g. ). will be enough, most of the times. It will be of type .If you use implicit conversions, you might need to actually write Array[Nothing], due to :This will not work:  This will:But this is only a weird corner case of implicits. It's is quite possible that this problem will disappear in the future."},
{"body": "I am new to Play framework and tried to mimic the  sample in my local machine but I encountered an error:I have Play 2.4 installed and created the project using IntelliJ Idea 14 via  template.After adding  parameters to views you can just add the following imports and use the old controller classes or even objects without any additional changes:Using view form helpers (such as ) requires you to pass an implicit  parameter to your view. You can do this adding  to the signature in your view. Your view becomes this:Then in your application controller you must make this parameter implicitly available in your scope. The simplest way to do this is to implement play's  trait.In your example, this would look like this:In your controller you can of course use your own implementation of . Since play knows out of the box how to inject a  you can simply annotate your controller with  and let play do the work for you.As Matthias Braun mentioned, you also have to setin your See  for more information about I18n."},
{"body": "I am not sure whether mergestrategy or exclude jars is the best option here. Any help with how do I proceed further with this error will be great!// Here is where I start seeing errors:// Here are the error messages.Use the  configuration, which will scope your dependent library.For example:If needed, read more atAdd the code below to your build.sbt fileThis helped me a lot."},
{"body": "given:why does the compiler complain I'm not sure about the error, but you can achieve what you want as follows:That is,  takes a function that takes a pair and returns , not a function that takes two arguments: here,  has type .Another way to write it is:In this case, the  block is a partial function.oops, read the doco wrong, map.foreach expects a function literal with a tuple argument!soworksThe confusing error message is a compiler bug, which should be :You need to patter-match on the  argument to assign variables to its subparts , . You can do with very few changes:Excellent question!\nEven when explicitly typing the foreach method, it still gives that very unclear compile error. There are ways around it, but I can't understand why this example does not work.Yet another way:However it requires explicit type annotations, so I prefer partial functions.Docs says argument is tuple -> unit, so We can easily do this"},
{"body": "I've read a few questions such as Scala vs Haskell discussing the merits of both languages or which to learn, but I already know that I'd like to learn Scala. I was a Java programmer at uni and now mainly use PHP. I want to learn Scala as it looks like an improvement on Java for personal projects and I'd also like to learn a functional language to improve my knowledge as a programmer.I am wondering if it would be a good idea to learn Haskell as an introduction to functional programming as it is purely functional so I'd properly learn it rather than haphazard using bits of functional in Scala without knowing why? I'd also like to use Haskell for personal projects etc as it looks great, but I don't really see many real world applications of it, seems more used for academic stuff hence wanting to learn it to get the functional understanding then move on to Scala.Speaking as someone who came from Java, and for whom Scala was a gateway drug to Haskell, I happen to think this is a great idea (learn Haskell first)! Haskell is conceptually a much simpler language than Scala, and if your goal is to learn how to program functionally, you can't help but do so if you start with Haskell. By design, Scala supports a kind of \"legacy mode\" of coding in which you don't really have to change your Java (or PHP) habits too much if you don't want to. I think this is a strategic decision--and a good one!--meant to increase adoption amongst crusty Java stalwarts. But that's not you! You're actually  in learning something new... so why not go all-out? Learning functional programming in a pure setting, without the clutter and the temptation to regress into old habits, will crystallize the concepts in your brain.Then by all means return to Scala and learn how it differs from Haskell; it is both weaker in some respects and stronger in others, but you will then be on a much better foundation to appreciate these differences.I wouldn't do that. If you go straight to Scala (your final aim) you save time. You'll learn functional programming anyway, but as Scala is \"less pure\" you'll be able to start writing programs in Scala usign some Java-like constructs, which will make the transition simpler.Even if you go into Haskell, your first programs will be of less \"functional quality\" than later ones, as only practice improves that. Starting Scala your first programs will be of \"less quality\" as your first programs in Java or PHP were, but with practice you'll improve. Spending time in unrelated tasks hoping they MAY help, it can result in you wasting that time.Focus on your aim. If that's Scala, go for it. I believe  may be relevant to the question. It raises a point on productivity (deliver software) vs seeking \"pure knowledge\".I came to Scala from a Java background. Initially I just focused on Scala as a better Java. As time went on I started to use more and more functional concepts in Scala. Eventually I reached a point where I felt I needed a purer understanding of functional programming. It was at this point that I went away and learnt Haskell. When I cam back to Scala I found it much easier to use a functional style and some of the the concepts I struggled with previously made much more sense.You don't have to learn Haskell to become a good Scala developers, but I think once you get to a certain level in Scala that having a deeper understanding of functional programming concepts really helps.The risk of starting directly from Scala, without having played any purely functional programming language first, is that you'll probably be drawn to the procedural solutions a bit too often.I wouldn't try Haskell as an intro to functional programming, though: not that it's particularly hard - its syntax is amazingly terse - but it's definitely very peculiar (even in the peculiar world of functional programming) and I think you should try with a language you can port more easily to Scala, like Racket or plain Scheme.To better understand what I mean, look at list comprehensions in Haskell, Scala and Scheme.I learned Haskell first and then Scala and never regretted. You will gain a much deeper appreciation of FP if you learn Haskell first. Your understanding of the concepts behind FP and why they are important will be seared in. Much of Scala is taken from Haskell (IMHO). It will make more sense when you have the Haskell background. Don't shy away! Your efforts will be rewarded.For me, I want to learn Scala and functional programming; and, thus simply, I am learning Scala. My study material is . After I have read the early chapter on syntx and stuff, I have skipped chapter covering OO stuff and jumped to Chapter 8 which cover the functional programming stuff. There I let myself struggle with FP. I read  scala code, which is written in FP way and analyze . You want to learn Scala for \"...an improvement on Java for personal projects.\" and Haskell for FP intro. So, it seems that first of all, you want to be productive with your java project and then grasp FP as a side effect. To be productive, you have to master the language. There are much to learn and master the language and the only way to master the language is that you have to write the code in that language. As you learn/use Scala, you can't avoid FP out right even if you want to. I have been reading the Scala blogs(the book too) and the authors  go out of their way to do explanation when it comes to FP concept. You don't have to worry about not grasping FP by going Scala route. Yes, you can draw inspiration from Haskell like  when you are learning Scala. Or as  above have pointed out, you can delve into more FP when you are already productive with Scala. You wanna go to point A, why not go to point A directly? If you go via the route of point B (Haskell), you will have to postpone your productivity as  mentioned."},
{"body": "is there a way in Scala to sort an array of tuples using and arbitrary comparison function? In particular I need to sort and array of tuples by their second element, but I wanted to know a general technique to sort arrays of tuples.Thanks!You can use this code:Unfortunetly, it seems that Scala cannot infer the type of the array passed to . I hope that's ok for you.In scala 2.8, there is a method sortBy. Here is a simple use case:If it's an , it's probably typical to use in-place sorting algorithms. However, in idiomatic Scala code, mutable collections are usually not encouraged/used. If that's the case and you have am immutable collection (or would like to not modify the  in place), use :sorting an  or any other collection of tuples:On Scala 2.8 (yes, again :), you can also do this:In the specific case of pairs, this can also work to sort  by the second element, and then by the first:2.7 and not in place:You probably want  from scala.util.Sorting.\nYour comparison function would be something like "},
{"body": "In one of my first attempts to create functional code, I ran into a performance issue.I started with a common task - multiply the elements of two arrays and sum up the results:Here is how I reformed the work:When I benchmarked the two approaches, the second method takes 40 times as long to complete!  Why does the second method take so much longer? How can I reform the work to be both speed efficient and use functional programming style?The main reasons why these two examples are so different in speed are:Let's consider the slower one by parts. First:That creates a new array, an array of . It will copy all elements from both arrays into  objects, and then copy a reference to each of these objects into a third array. Now, notice that  is parameterized, so it can't store  directly. Instead, new instances of  are created for each number, the numbers are stored in them, and then a reference for each of them is stored into the . Now a fourth array is created. To compute the values of these elements, it needs to read the reference to the tuple from the third array, read the reference to the  stored in them, read the numbers, multiply, create a new  to store the result, and then pass this reference back, which will be -referenced again to be stored in the array (arrays are not type-erased).We are not finished, though. Here's the next part:That one is relatively harmless, except that it still do boxing/unboxing and  creation at iteration, since  receives a , which is parameterized.Scala 2.8 introduces a feature called specialization which will get rid of a lot of these boxing/unboxing. But let's consider alternative faster versions. We could, for instance, do  and  in a single step:We could use  (Scala 2.8) or  (Scala 2.7) to avoid creating intermediary collections altogether:This last one doesn't save much, actually, so I think the non-strictness if being \"lost\" pretty fast (ie, one of these methods is strict even in a view). There's also an alternative way of zipping that is non-strict (ie, avoids some intermediary results) by default:This gives much better result that the former. Better than the  one, though not by much. Unfortunately, we can't combined  with  because the former doesn't support the latter.The last one is the fastest I could get. Faster than that, only with specialization. Now,  happens to be specialized, but for ,  and . The other primitives were left out, as specialization increases code size rather dramatically for each primitive. On my tests, though  is actually taking longer. That might be a result of it being twice the size, or it might be something I'm doing wrong.So, in the end, the problem is a combination of factors, including producing intermediary copies of elements, and the way Java (JVM) handles primitives and generics. A similar code in Haskell using supercompilation would be equal to anything short of assembler. On the JVM, you have to be aware of the trade-offs and be prepared to optimize critical code.I did some variations of this with Scala 2.8. The loop version is as you write but the \nfunctional version is slightly different:I ran with Double instead of Float, because currently specialization only kicks in for Double. I then tested with arrays and vectors as the carrier type. Furthermore, I tested Boxed variants which work on java.lang.Double's instead of primitive Doubles to measure \nthe effect of primitive type boxing and unboxing. Here is what I got (running Java 1.6_10 server VM, Scala 2.8 RC1, 5 runs per test).The first thing to notice is that by far the biggest difference is between primitive array loops and primitive array functional reduce. It's about a factor of 15 instead of the 40 you have seen, which reflects improvements in Scala 2.8 over 2.7. Still, primitive array loops are the fastest of all tests whereas primitive array reduces are the slowest. The reason is that primitive Java arrays and generic operations are just not a very good fit. Accessing elements of primitive Java arrays from generic functions requires a lot of boxing/unboxing and sometimes even requires reflection. Future versions of Scala will specialize the Array class and then we should see some improvement. But right now that's what it is.If you go from arrays to vectors, you notice several things. First, the reduce version is now faster than the imperative loop! This is because vector reduce can make use of efficient bulk operations. Second, vector reduce is faster than array reduce, which illustrates the inherent overhead that arrays of primitive types pose for generic higher-order functions.If you eliminate the overhead of boxing/unboxing by working only with boxed java.lang.Double values, the picture changes. Now reduce over arrays is a bit less than 2 times slower than looping, instead of the 15 times difference before. That more closely approximates the inherent overhead of the three loops with intermediate data structures instead of the fused loop of the imperative version. Looping over vectors is now by far the slowest solution, whereas reducing over vectors is a little bit slower than reducing over arrays.So the overall answer is: it depends. If you have tight loops over arrays of primitive values, nothing beats an imperative loop. And there's no problem writing the loops because they are neither longer nor less comprehensible than the functional versions. In all other situations, the FP solution looks competitive.This is a microbenchmark, and it depends on how the compiler optimizes you code. You have 3 loops composed here,zip . map . foldNow, I'm fairly sure the Scala compiler cannot fuse those three loops into a single loop, and the underlying data type is strict, so each (.) corresponds to an intermediate array being created. The imperative/mutable solution would reuse the buffer each time, avoiding copies.Now, an understanding of what composing those three functions means is key to understanding performance in a functional programming language -- and indeed, in Haskell, those three loops will be optimized into a single loop that reuses an underlying buffer -- but Scala cannot do that.There are benefits to sticking to the combinator approach, however -- by distinguishing those three functions, it will be easier to parallelize the code (replace map with parMap etc). In fact, given the right array type, (such as a ) a sufficiently smart compiler will be able to automatically parallelize your code, yielding more performance wins.So, in summary:Don Stewart has a fine answer, but   I'll add to his answer that Scala compiles to JVM bytecodes, and not only does the Scala compiler not fuse the three loops into one, but the Scala compiler is almost certainly allocating all the intermediate arrays.  Notoriously, .  Allocation is a significant cost in functional programs, and that's one the loop-fusion transformations that Don Stewart and his colleagues have implemented for Haskell are so powerful: they eliminate lots of allocations.  When you don't have those transformations, plus you're using an expensive allocator such as is found on a typical JVM,  that's where the big slowdown comes from.Scala is a great vehicle for experimenting with the expressive power of an unusual mix of language ideas: classes, mixins, modules, functions, and so on.  But it's a relatively young research language, and it runs on the JVM, so it's unreasonable to expect great performance except on the kind of code that JVMs are good at.  If you want to experiment with the mix of language ideas that Scala offers, great\u2014it's a really interesting design\u2014but don't expect the same performance on pure functional code that you'd get with a mature compiler for a functional language, like  or ..  Stuff to do with first-class functions, pattern matching, and currying need not be especially slow.  But with Scala, more than with other implementations of other functional languages, you really have to \u2014they can be very expensive.The Scala collections library is fully generic, and the operations provided are chosen for maximum capability, not maximum speed.  So, yes, if you use a functional paradigm with Scala without paying attention (especially if you are using primitive data types), your code will take longer to run (in most cases) than if you use an imperative/iterative paradigm without paying attention.That said, you can easily create non-generic functional operations that perform quickly for your desired task.  In the case of working with pairs of floats, we might do the following:and then these operations will be much faster.  (Faster still if you use Double and 2.8.RC1, because then the functions  will be specialized, not generic; if you're using something earlier, you can create your own   and then call with  instead of .)Anyway, the point is that it's not the functional style that makes functional coding in Scala slow, it's that the library is designed with maximum power/flexibility in mind, not maximum speed.  This is sensible, since each person's speed requirements are typically subtly different, so it's hard to cover everyone supremely well.  But if it's something you're doing more than just a little, you can write your own stuff where the performance penalty for a functional style is extremely small.I am not an expert Scala programmer, so there is probably a more efficient method, but what about something like this. This can be tail call optimized, so performance should be OK.To answer the question in the title: Simple functional constructs may be slower than imperative on the JVM.But, if we consider only simple constructs, then we might as well throw out all modern languages and stick with C or assembler. If you look a the programming language shootout, C always wins.So why choose a modern language? Because it lets you express a cleaner design. Cleaner design leads to performance gains in the overall operation of the application. Even if some low-level methods can be slower. One of my favorite examples is the performance of BuildR vs. Maven. BuildR is written in Ruby, an interpreted, slow, language. Maven is written in Java. A build in BuildR is twice as fast as Maven. This is due mostly to the design of BuildR which is lightweight compared with that of Maven. Your functional solution is slow because it is generating unnecessary temporary data structures. Removing these is known as deforesting and it is easily done in strict functional languages by rolling your anonymous functions into a single anonymous function and using a single aggregator. For example, your solution written in F# using ,  and :may be rewritten using  so as to avoid all temporary data structures:This is a  faster and the same transformation can be done in Scala and other strict functional languages. In F#, you can also define the  as  in order to have the higher-order function inlined with its functional argument whereupon you recover the optimal performance of the imperative loop.Here is dbyrnes solution with arrays (assuming Arrays are to be used) and just iterating over the index:That needs about 1/40 of the time of the list-approach. I don't have 2.8 installed, so you have to test @tailrec yourself. :)"},
{"body": "What changes to the JVM would most benefit the Scala compiler and runtime?The dynamic languages will benefit greatly in performance from the introduction of the  byte code scheduled to arrive in JVM 7 and Scala will probably benefit from tail recursion (not sure if it will appear in JVM 8 or later).What other changes could Scala, with its present feature set, benefit from in the JVM? Are these changes on the horizon?Specifically, are there changes to the JVM that would improve performance with closures and functions-as-objects?Basically, everything that John Rose has been campaigning for :)Reification is often quoted as something that would benefit Scala's pattern matching, but this would come at a high cost in terms of interop, given the different variance schemes that the two languages use.  At this point, I believe that reification may actually cause more harm than it would do good.I also think it unreasonable to expect anything that would break backwards compatibility in Java.  That just ain't gonna happen.There are a couple of features of Scala that would be better implemented in the JVM, such has:EDIT: Added declaration-site variance to list.Value types would help quite a bit performance wise for tuples and case classes. Escape analysis helps reduce heap allocations of such objects a bit, but at the moment the JVM can't inline some method calls aggressively enough thus can't eliminate heap allocations for these small immutable objects. This leads to heap trashing and 3-4x higher execution time.Value types also helps increase data locality and reduce memory usage. For example, in a simple Array[(Int, Int)] each array entry will have an overhead of one pointer + object header. With value types this overhead could be eliminated completely.People often focus on InvokeDynamic - without realizing that the MethodHandles framework has plenty of other advantages, even if the method references that MH provides are never dynamically invoked. MethodHandles are almost like a performant form of \"Reflection done right\".Any language which makes heavy use of reflection may well be able to get benefits from using MethodHandles inside the language runtime."},
{"body": "When I'm working with libraries that support type-level programming, I often find myself writing comments like the following (from  presented by ):Or this, from  in the  repository:This is a very rough way of indicating some fact about the behavior of these methods, and we could imagine wanting to make these assertions more formal\u2014for unit or regression testing, etc.To give a concrete example of why this might be useful in the context of a library like Shapeless, a few days ago I wrote the following as a quick first attempt at an answer to :Where the intention is that this will compile:While this will not:I was surprised to find that this implementation of a type-level  for  didn't work, because Shapeless would happily find a  instance in the latter case. In other words, the following would compile, even though you'd probably expect it not to:In this case, what I was seeing was \u2014or at least something bug-ish\u2014and it .More generally, we can imagine wanting to check the kind of invariant that was implicit in my expectations about how   work with something like a unit test\u2014as weird as it may sound to be talking about testing type-level code like this, with all the recent debates about the relative merit of types  tests.The problem is that I don't know of any kind of testing framework (for any platform) that allows the programmer to assert that something must not .One approach that I can imagine for the  case would be to use the old :Which would let you write the following in your unit test:The following would be a heck of a lot more convenient and expressive, though:I want this. My question is whether anyone knows of any testing library or framework that supports anything remotely like it\u2014ideally for Scala, but I'll settle for anything.Not a framework, but Jorge Ortiz () mentioned some utilities he added to the tests for Foursquare's Rogue library at NEScala in 2012 which support tests for non-compilation: you can find examples . I've been meaning to add something like this to shapeless for quite a while.More recently, Roland Kuhn () has added a similar mechanism, this time using Scala 2.10's runtime compilation, to the .These are both dynamic tests of course: they fail at (test) runtime if something that shouldn't compile does. Untyped macros might provide a static option: ie. a macro could accept an untyped tree, type check it and throw a type error if it succeeds). This might be something to experiment with on the macro-paradise branch of shapeless. But not a solution for 2.10.0 or earlier, obviously.Since answering the question, another approach, due to Stefan Zeiger (), . This one is interesting because, like the untyped macro one alluded to above, it is a compile time rather than (test) runtime check, however it is also compatible with Scala 2.10.x. As such I think it is preferable to Roland's approach.I've now added implementations to shapeless for , for  and for . Examples of the corresponding tests can be found ,  and .The untyped macro tests are the cleanest, but Stefan's 2.10.x compatible approach is a close second.ScalaTest 2.1.0 has the following syntax for :And for :Do you know about  in the Scala project? E.g.  has the following doc:It is able to check for example whether this source  will have this result It's not a perfect fit for your question (since you don't specify your test cases in terms of asserts), but may be an approach and/or give you a head start.Based on the links provided by  I was able to use the akka versionThen in my tests I used it like thisThe  macro in \u00b5Test does just that:"},
{"body": "I have the following setup of a Scala application and a common core library:\nrootI want to add a reference from ApplicationA to CoreLibrary \u00e0 la Eclipse project reference, so that every time CoreLibrary changes ApplicationA is built as well. I\u00b4ve tried the following contents of build.Scala for ApplicationA:However, when compiling ApplicationA SBT complains that a dependency can only be a subdirectory!!:This seems completely straightforward, what's the correct way of having this project dependency?You can do a source dependency on your project like that : I have a working example with a multimodule play build : But I think the proper way (it depends of your context) of doing it is to create a referencing the two projects and the dependencies between them.With sbt 0.12.1 it seems possible to get a simple reference to a project :I used  instead of sbt-eclipse also works.Since , you may create multi-project definitions directly in  without needing a  file.So adding the following to   would be sufficient."},
{"body": "When I read the source of scalatra, I found there are some code like:There is an interesting class named . I've looked at the doc of this class, but I don't know when and why we should use it? It has a  which is usually be used.If we don't use it, then what code we should use instead, to solve the problem it solved?(I'm new to scala, if you can provide some code, that will be great) is an implementation of the loan and dynamic scope patterns. Use-case of  is pretty much similar to  in Java (as a matter of fact,  uses  behind the scenes) - it's used, when you need to do a computation within an enclosed scope, where every thread has it's own copy of the variable's value:Given that  uses an inheritable , value of the variable is passed to the threads spawned in the context: (and ) is used in Scalatra for the same reason it's used in many other frameworks (Lift, Spring, Struts, etc.) - it's a non-intrusive way to store and pass around context(thread)-specific information. Making  and  dynamic variables (and, thus, binding to a specific thread that processes request) is just the easiest way to obtain them anywhere in the code (not passing through method arguments or anyhow else explicitly).This is well answered by Vasil, but I'll add an additional simple example that might further help understanding.Suppose we must use some code that uses println() to write all over stdout.  We want this output to go to a log file, but we don't have access to the source.We can use this to simply fix our issue:This is a minimal snippet:The output will be:"},
{"body": "I am trying to learn Scala now, with a little bit of experience in Haskell. One thing that stood out as odd to me is that all function parameters in Scala  be annotated with a type - something that Haskell does not require. Why is this? To try to put it as a more concrete example: an add function is written like this:But, this only works for doubles(well, ints work too because of the implicit type conversion). But what if you want to define your own type that defines its own  operator. How would you write an add function which works for any type that defines a  operator?Haskell uses Hindley-Milner type inference algorithm whereas Scala, in order to support Object Oriented side of things, had to forgo using it for now.In order to write an add function for all applicable types easily, you will need to use Scala 2.8.0:In order to solidify the concept of using  for myself, I wrote an example that does not require scala 2.8, but uses the same concept. I thought it might be helpful for some.\nFirst, you define an generic-abstract class :Now you can write the  function like this:This is used like a type class in Haskell. Then to  this generic class for a specific type, you would write(examples here for Int, Double and String):At this point you can call the  function with all three types:Certainly not as nice as Haskell which will essentially do all of this for you. But, that's where the trade-off lies.Haskell uses the  type inference. This kind of type-inference is powerful, but limits the type system of the language. Supposedly, for instance, subclassing doesn't work well with H-M.At any rate, Scala type system is too powerful for H-M, so a more limited kind of type inference must be used.I think the reason Scala requires the type annotation on the parameters of a newly defined function comes from the fact that Scala uses a more local type inference analysis than that used in Haskell.If all your classes mixed in a trait, say , that declared the  operator, you could write your generic add function as:This restricts the add function to types T that implement the Addable trait.Unfortunately, there is not such trait in the current Scala libraries. But you can see how it would be done by looking at a similar case, the  trait. This trait declares comparison operators and is mixed in by the , , etc. classes. Then you can write a sort function that can take, for example, a  where  to sort a list of elements that mix in the ordered trait. Because of implicit type conversions like  to , you can even use your sort function on lists of , or  or .As I said, unfortunately, there is no corresponding trait for the  operator. So, you would have to write out everything yourself. You would do the Addable[T] trait, create , , etc., classes that extend Int, Float, etc. and mix in the Addable trait, and finally add implicit conversion functions to turn, for example, and Int into an , so that the compiler can instantiate and use your add function with it.The function itself will be pretty straightforward:Better yet, you can just overload the + method:There's a missing piece, though, which is the type parameter itself.  As written, the method is missing its class. The most likely case is that you're calling the + method on an instance of T, passing it another instance of T.  I did this recently, defining a trait that said, \"an additive group consists of an add operation plus the means to invert an element\"Then, later, I define a Real class that knows how to add instances of itself (Field extends GroupAdditive):That may be more than you really wanted to know right now, but it does show both how to define generic arguments and how to realize them.Ultimately, the specific types aren't required, but the compiler does need to know at least the type bounds."},
{"body": "Is there a way, only using the Scala collection API, to get an Option in a List when trying to get an element by its index?I'm looking for the equivalent of this function, does it exist?ThanksYes, you can lift your collection to a function :"},
{"body": "I was taught about  at university, but I was disappointed how they didn't seem to be used in the real word.I like the idea of being able to know that some code (object, function, whatever) works, not by testing, but by .I'm sure we're all familiar with the parallels that don't exist between physical engineering and software engineering (steel behaves predictably, software can do anything - who knows!), and I would love to know if there are any languages that can be use in the real word (is asking for a web framework too much to ask?)I've heard interesting things about the testability of functional languages like scala.As software  what options do we have?Yes, there are languages designed for writing provably correct software. Some are even used in industry.  is probably the most prominent example. I've talked to a few people at Praxis Critical Systems Limited who used it for code running on Boings (for engine monitoring) and it seems quite nice. (Here is a great .) This language and accompanying proof system uses the second technique described below. It doesn't even support dynamic memory allocation!My impression and experience is that there are two techniques for writing correct software:Note that both approaches hinges on the fact you have a formal specification at hand (how else would you tell what is correct / incorrect behavior), and a formally defined semantics of the language (how else would you be able to tell what the actual behavior of your program is).Here are a few more examples of formal approaches. If it's \"real-world\" or not, depends on who you ask :-)I know of only one \"provably correct\" : . A Ur-program that \"goes through the compiler\" is guaranteed not to:In order to answer this question, you really need to define what you mean by \"provable\".  As Ricky pointed out, any language with a type system is a language with a built-in proof system which runs every time you compile your program.  These proof systems are almost always sadly impotent \u2014 answering questions like  vs  and avoiding questions like \"is the list sorted?\" \u2014 but they are proof systems none-the-less.Unfortunately, the more sophisticated your proof goals, the harder it is to work with a system which can check your proofs.  This escalates pretty quickly into undecidability when you consider the sheer size of the class of problems which are undecidable on Turing Machines.  Sure, you can theoretically do basic things like proving the correctness of your sorting algorithm, but anything more than that is going to be treading on very thin ice.Even to prove something simple like the correctness of a sorting algorithm requires a comparatively sophisticated proof system.  (note: since we have already established that type systems are a proof system built into the language, I'm going to talk about things in terms of type theory, rather than waving my hands still more vigorously)  I'm fairly certain that a full correctness proof on list sorting would require some form of dependent types, otherwise you have no way of referencing relative values at the type level.  This immediately starts breaking into realms of type theory which have been shown to be undecidable.  So, while you may be able to prove correctness on your list sorting algorithm, the only way to do it would be to use a system which will also allow you to express proofs which it cannot verify.  Personally, I find this very disconcerting.There is also the ease-of-use aspect which I alluded to earlier.  The more sophisticated your type system, the less pleasant it is to use.  That's not a hard and fast rule, but I think it holds true for the most part.  And as with any formal system, you will often find that expressing the proof is more work (and more error prone) than creating the implementation in the first place.With all that out of the way, it's worth noting that Scala's type system (like Haskell's) is Turing Complete, which means that you can  use it to prove any decidable property about your code, provided that you have written your code in such a way that it is amenable to such proofs.  Haskell is almost certainly a better language for this than Java (since type-level programming in Haskell is similar to Prolog, while type-level programming in Scala is more similar to SML).  I don't advise that you use Scala's or Haskell's type systems in this way (formal proofs of algorithmic correctness), but the option is theoretically available.Altogether, I think the reason you haven't seen formal systems in the \"real world\" stems from the fact that formal proof systems have yielded to the merciless tyranny of pragmatism.  As I mentioned, there's so much effort involved in crafting your correctness proofs that it's almost never worthwhile.  The industry decided a long time ago that it was easier to create ad hoc tests than it was to go through any sort of analytical formal reasoning process.Typed languages prove the absence of certain categories of fault.  The more advanced the type system, the more faults they can prove the absence of.For proof that a whole program works, yes, you're stepping outside the boundaries of ordinary languages, where mathematics and programming meet each other, shake hands and then proceed to talk using Greek symbols about how programmers can't handle Greek symbols.  That's about the \u03a3 of it anyway.You're asking a question a lot of us have asked over the years.  I don't know that I have a good answer, but here are some pieces:An interesting question to ask, I think, is what would sufficient conditions be to get formal approaches into practice?  I'd love to hear some suggestions.You can investigate  like Haskell, which are used when one of your requirements is . is a fun read if you're interested about functional programming languages and pure functional programming languages.real world uses of such provable languages would be incredibly difficult to write and understand afterwoods.\nthe business world needs working software, fast.\"provable\" languages just dont scale for writing/reading a large project's code base and only seem to work in small and isolated use cases.I'm involved in two provable languages. The first is the language supported by Perfect Developer, a system for specifying sotfware, refining it and generating code. It's been used for some large systems, including PD itself, and it's taught in a number of universities. The second provable language I'm involved with is a provable subset of MISRA-C, see  for more.Scala is meant to be \"real world\", so no effort has been made to make it provable. What you can do in Scala is to produce functional code. Functional code is much easier to test, which is IMHO a good compromise between \"real world\" and provability.EDIT =====\nRemoved my incorrect ideas about ML being \"provable\".Check out .This  contains a relatively painless implementation of AVL trees with included correctness proof.My (controversial) definition of \"real-world\" in the context of provably-correct code is:The above are relatively more universal requirements. Others, such as the ability to model imperative code, or the ability to prove higher-order functions correct, may be important or essential for some tasks, but not all.In view of these points, many of the tools listed in  may be useful - although they are nearly all either new and experimental, or unfamiliar to the vast majority of industry programmers. There are some very mature tools covered there though.How about  and ? Real world enough?As a good real life example, there's a project that aims to provide a verified HTTP REST library written in Agda, called Lemmachine: "},
{"body": "In C# you can write:How would this simple  look like in Scala?The  pattern is the analogous construction:Per @Daniel Spiewak's comments, this will avoid reflection on method invocation, aiding performance:Since version 2.10 of Scala, it is possible to make an entire class eligible for implicit conversionIn addition, it is possible to avoid creating an instance of the extension type by having it extend AnyValFor more information on implicit classes and AnyVal, limitations and quirks, consult the official documentation:This would be the code after Daniel's  .In Scala we use the so-called (by the inventor of the language)  pattern, which is much discussed and pretty easy to find on the Web, if you use a string (not keyword) search."},
{"body": "Suppose I have two arrays: Now for each element of , I want to first concatenate that element with the corresponding element of , and then print the result. One way to do would be something like:It would have been nicer if there was a  variant that would allow me to work directly with the indices of  instead of first constructing the integer list.Perhaps there is a better way?One very convenient way to do this is with the  method on tuples.  Put two collections in, get out two arguments to a function!This is both convenient to write and fast, since you don't need to build a tuple to store each pair (as you would with ) which you then have to take apart again.  Both forms of zip stop when the shorter of the two collections is exhausted.If you have something more complicated (e.g. you need to do math on the index), the canonical solution is to zip in the index:The method you are using is more rapidly and compactly done as follows, an can be useful:although this only works if the first collection is no longer than the second.  You can also specify your ranges explcitly:to get around this problem.  (You can see why  and  are preferred unless what you're doing is too complicated for this to work easily.)If it is not a parallel collection (and usually it is not unless you call ), it's also possible, though not recommended, to keep track with a mutable variable:There are a very limited number of cases where this is necessary (e.g. if you may want to skip or backtrack on some of the other collection(s)); if you can avoid having to do this, you'll usually end up with code that's easier to reason about.This is how you loop with an index in idiomatic Scala:And here is the idiomatic Scala way to do what you are trying to achieve in your code:I did not had the opportunity to test it, but this should do the trick: will do it:This will yield:Note that you don't need  generic type, will be infered by the compiler:"},
{"body": "I am trying to put a Google map inside a scroll view, so that the user can scroll down other contents to see the map. The problem is that this scroll view is eating up all the vertical touching events, so the UI experience of map becomes very weird.I know that in V1 of the google map, you could override onTouch, or setOnTouchListener to call requestDisallowInterceptTouchEvent upon MotionEvent.ACTION_DOWN. I have tried to implement the similar trick with V2 to no avail.So far I have tried:None of these remedied the scrolling problem. Am I missing something here? If anyone has a working example of a map inside scrolling view, could you please kindly share code example?Apply a transparent image over the mapview fragment.Then set  for the main ScrollView. When the user touches the transparent image and moves disable the touch on the transparent image for  and  so that map fragment can take Touch Events.This worked for me. Hope it helps you..I encountered a similar problem and came up with a more general working solution based on In-Ho Yi and \u0414\u0430\u043d\u0430\u0438\u043b \u0414\u0438\u043c\u0438\u0442\u0440\u043e\u0432 answers above.Thank you for suggestions,After much try-and-error, pulling off my hairs and swearing at monitor and my poor Android test phone, I've figured that if I customise ScrollView, override onInterceptTouchEvent in which we return false when the event is on a map view no matter what, then the scrolling on a map does happen as expected.This code is in Scala but you get the idea. Note I've ended up using a raw map view (as shown in android-sdks\\extras\\google\\google_play_services\\samples\\maps\\src\\com\\example\\mapdemoRawMapViewDemoActivity.java). Guess you can do the pretty much same thing with fragments, I just never liked fragments in the first place.I think Google owes me an apology.I had the same issue, so here is how I used the solution as java code just in case anyone needs it. You just have to set the mapView field when using it.The accepted answer did not work in my case. The Guest's answer did neither (but almost). If this is the case for someone else try this edited version of the Guest's answer.I have commented out the action bar height if someone needs to use it when calculating the hitbox.Improving code if you do not need transparent Image again:Most of the options listed above did not work for me but the following proved to be a great solution to the problem for me:I also had to implement is slightly differently for my implementation, as I am using the map in a fragment which made things slightly more complicated but easy to get working."},
{"body": "Am I right understanding thatYes, though for the 3rd one I would say \"when that statement is executed\", because, for example:This gives .  is never evaluated and its error is never thrown. But it is in scope as soon as control enters the block.Yes, but there is one nice trick: if you have lazy value, and during first time evaluation it will get an exception, next time you'll try to access it will try to re-evaluate itself.Here is example:One good reason for choosing  over , especially in abstract classes (or in traits that are used to mimic Java's interfaces), is, that you can override a  with a  in subclasses, but not the other way round.Regarding , there are two things I can see that one should have in mind. The first is that  introduces some runtime overhead, but I guess that you would need to benchmark your specific situation to find out whether this actually has a significant impact on the runtime performance. The other problem with  is that it possibly delays raising an exception, which might make it harder to reason about your program, because the exception is not thrown upfront but only on first use.I would like to explain the differences through the example that i executed in REPL.I believe this simple example is easier to grasp and explains the conceptual differences.Here,I am creating a val result1, a lazy val result2 and a def result3 each of which has a type String.Here, println is executed because the value of result1 has been computed here. So, now result1 will always refer to its value i.e \"returns val\".So, now, you can see that result1 now refers to its value. Note that, the println statement is not executed here because the value for result1 has already been computed when it was executed for the first time. So, now onwards, result1 will always return the same value and println statement will never be executed again because the computation for getting the value of result1 has already been performed.As we can see here, the println statement is not executed here and neither the value has been computed. This is the nature of lazyness.Now, when i refer to the result2 for the first time, println statement will be executed and value will be computed and assigned.Now, when i refer to result2 again, this time around, we will only see the value it holds and the println statement wont be executed. From now on, result2 will simply behave like a val and return its cached value all the time.In case of def, the result will have to be computed everytime result3 is called. This is also the main reason that we define methods as def in scala because methods has to compute and return a value everytime it is called inside the program.You are correct. For evidence from :From \"3.3.1 Method Types\" (for ):From \"4.1 Value Declarations and Definitions\": defines a method. When you call the method, the method ofcourse runs. defines a value (an immutable variable). The assignment expression is evaluated when the value is initialized. defines a value with delayed initialization. It will be initialized when it's first used, so the assignment expression will be evaluated then.A name qualified by def is evaluated by replacing the name and its RHS expression every time the name appears in the program. Therefore, this replacement will be executed every where the name appears in your program.A name qualified by val is evaluated immediately when control reaches its RHS expression. Therefore, every time the name appears in the expression, it will be seen as the value of this evaluation. A name qualified by lazy val follows the same policy as that of val qualification with an exception that its RHS will be evaluated only when the control hits the point where the name is used for the first timeShould point out a potential pitfall in regard to usage of val when working with values not known until runtime.Take, for example, If you were to say:You would get a null pointer exception as at the point of initialization of the , request has no foo (would only be know at runtime).So, depending on the expense of access/calculation, def or lazy val are then appropriate choices for runtime-determined values; that, or a val that is itself an anonymous function which retrieves runtime data (although the latter seems a bit more edge case)"},
{"body": "I just wonder why there is no  to increase a number. As what I know, languages like Ruby or Python doesn't support it because they are dynamically typed. So it's obviously we cannot write code like  because maybe  is a string or something else. But Scala is statically typed - the compiler absolutely can infer that if it is legal or not to put  behind a variable.So, why doesn't  exist in Scala?Scala doesn't have  because it's a functional language, and in functional languages, operations with side effects are avoided (in a purely functional language, no side effects are permitted at all). The side effect of  is that  is now 1 larger than it was before. Instead, you should try to use immutable objects (e.g.  not ).Also, Scala doesn't really need  because of the control flow constructs it provides. In Java and others, you need  often to construct  and  loops that iterate over arrays. However, in Scala, you can just say what you mean:  or  or something along those lines.  is useful in imperative programming, but when you get to a higher level, it's rarely necessary (in Python, I've never found myself needing it once).You're spot on that   be in Scala, but it's not because it's not necessary and would just clog up the syntax. If you really need it, say , but because Scala calls for programming with immutables and rich control flow more often, you should rarely need to. You certainly could define it yourself, as operators are indeed just methods in Scala.Of course you can have that in Scala, if you really want:And here you go:No need to resort to violence against variables.Scala is perfectly capable of parsing  and, with a small modification to the language, could be made to modify a variable.  But there are a variety of reasons not to.First, it saves only one character,  vs. , which is not very much savings for adding a new language feature.Second, the  operator is widely used in the collections library, where  takes collection  and  and produces a new collection that contains both.Third, Scala tries to encourage you, without forcing you, to write code in a functional way.   is a mutable operation, so it's inconsistent with the idea of Scala to make it especially easy.  (Likewise with a language feature that would allow  to mutate a variable.)Scala doesn't have a  operator because it is not possible to implement one in it.: As just pointed out in response to this answer, Scala 2.10.0  implement an increment operator through use of macros. See  for details, and take everything below as being pre-Scala 2.10.0. Let me elaborate on this, and I'll rely heavily on Java, since it actually suffers from the same problem, but it might be easier for people to understand it if I use a Java example.To start, it is important to note that one of the goals of Scala is that the \"built-in\" classes must not have any capability that could not be duplicated by a library. And, of course, in Scala an  is a class, whereas in Java an  is a primitive -- a type entirely distinct from a class.So, for Scala to support  for  of type , I should be able to create my own class  also supporting the same method. This is one of the driving design goals of Scala.Now, naturally, Java does not support symbols as method names, so let's just call it . Our intent then is to try to create a method  such that  works just like .Here's a first pass at it:We can test it with this:Everything seems to work, too:And, now, I'll show what the problem is. Let's change our demo program, and make it compare  to :As we can see in the output,  and  are behaving differently:The problem is that we implemented  by mutating , which is not how primitives work.  needs to be immutable, which means that  must produce a  object. Let's do a naive change:However, this doesn't work:The problem is that, while,   a new object, that new object hasn't been  to . There's no existing mechanism in Java -- or Scala -- that would allow us to implement this method with the exact same semantics as .Now, that doesn't mean it would be impossible for Scala to make such a thing . If Scala supported parameter passing by reference (see \"call by reference\" in ), like C++ does, then we  implement it!Here's a fictitious implementation, assuming the same by-reference notation as in C++.This would either require JVM support or some serious mechanics on the Scala compiler.In fact, Scala  something similar to what would be needed that when it create closures -- and one of the consequences is that the original  becomes boxed, with possibly serious performance impact.For example, consider this method:The code being passed to , , is  part of this method. The method  takes an  of the type  whose  method implements that little code. That means  is not only on a different method, it is on a different class altogether! And yet, it can change the value of  just like a  operator would need to.If we use  to look at it, we'll see this:Note that instead of creating an  local variable, it creates an  on the heap (at 0), which is boxing the . The real  is inside , as we see on 25. Let's see this same thing implemented with a while loop to make clear the difference:That becomes:No object creation above, no need to get something from the heap.So, to conclude, Scala would need additional capabilities to support an increment operator that could be defined by the user, as it avoids giving its own built-in classes capabilities not available to external libraries. One such capability is passing parameters by-reference, but JVM does not provide support for it. Scala does something similar to call by-reference, and to do so it uses boxing, which would seriously impact performance (something that would most likely come up with an increment operator!). In the absence of JVM support, therefore, that isn't much likely.As an additional note, Scala has a distinct functional slant, privileging immutability and referential transparency over mutability and side effects. The  purpose of call by-reference is to cause side effects ! While doing so can bring performance advantages in a number of situations, it goes very much against the grain of Scala, so I doubt call by-reference will ever be part of it.Other answers have already correctly pointed out that a  operator is neither particularly useful nor desirable in a functional programming language. I would like to add that since Scala 2.10, you can add a  operator, if you want to. Here is how:You need an implicit macro that converts ints to instances of something that has a  method. The  method is \"written\" by the macro, which has access to the variable (as opposed to its value) on which the  method is called. Here is the macro implementation:Now, as long as the implicit conversion macro is in scope, you can writeRafe's answer is true about the rationale for why something like i++ doesn't belong in Scala. However I have one nitpick. It's actually not possible to implement i++ in Scala without changing the language.In Scala, ++ is a valid method, and no method implies assignment. Only  can do that.Languages like C++ and Java treat  specially to mean both increment and assign. Scala treats  specially, and in an inconsistent way.In Scala when you write  the compiler first looks for a method called  on the Int. It's not there so next it does it's magic on  and tries to compile the line as if it read . If you write  then Scala will call the method  on  and assign the result to... nothing. Because only  means assignment. You could write  but that kind of defeats the purpose.The fact that Scala supports method names like  is already controversial and some people think it's operator overloading. They could have added special behavior for  but then it would no longer be a valid method name (like ) and it would be one more thing to remember.Quite a few languages do not support the ++ notation, such as Lua. In languages in which it is supported, it is frequently a source of confusion and bugs, so it's quality as a language feature is dubious, and compared to the alternative of  or even just , the saving of such minor characters is fairly pointless.This is not at all relevant to the type system of the language. While it's true that most static type languages do offer and most dynamic types don't, that's a correlation and definitely not a cause.Scala encourages using of FP style, which  certainly is not.The question to ask is why there should be such an operator, not why there shouldn't be.  Would Scala be improved by it?The  operator is single-purpose, and having an operator that can change the value of a variable can cause problems.  It's easy to write confusing expressions, and even if the language defines what  means, for example, that's a lot of detailed rules to remember.Your reasoning on Python and Ruby is wrong, by the way.  In Perl, you can write  or  just fine.  If  turns out to be something that can't be incremented, you get a run-time error.  It isn't in Python or Ruby because the language designers didn't think it was a good idea, not because they're dynamically typed like Perl.You could simulate it, though. As a trivial example:Add some implicit conversions and you're good to go. However, this sort of changes the question into: why isn't there a mutable RichInt with this functionality?"},
{"body": "I have a parser that was written using Scala's  - It had some serious performance problems when parsing a grammar which had deeply nested expressions. As such I have created a version where I mix in Scala's  - The Packrat version does not exhibit the same performance issue and correctly parses the grammar. However, when I provide an invalid grammar for testing, e.g. The old (non-packrat) parser used to correctly report the 'Invalid rule' failure, via the  parser combinator  here - When using the packrat-parser version, if I enable tracing I can see from the trace that the failure is created just as it is in the non-packrat version, however the PackratParser seems to ignore this and always return  instead.Is there something different about failure handling when using PackratParsers which I need to understand?Seems that you need to use  instead of , as it guarantees that no backtracking will be performed."},
{"body": "I've seen quite a   recently of the new  pattern matcher for scala. I missed the memo explaining what it actually was...The  pattern matcher is a rewrite of the existing matcher. The motivation for doing this was to support virtualization of pattern matching for the , not relevant for 2.10.As Iulian says in the comments below: The \"polymorphic embedded DSLs\" are the idea that one might write programs in scala that are not supposed to be run on the JVM. That is,  will produce an output which describes what the program is doing. This may then be re-compiled against a specific architecture. Such things .This rewrite will eventually become the standard scala pattern matcher. The old pattern matcher was (as I understand it) unmaintainable.Sadly, the (sole) existing answer is low on juicy bits, and the links on the commentary are broken. So let me try to add some juice here, for, if no other reason, my own reference when I actually decide to do something with it in the future, seeing as this answer is on top of every google search I do.The virtualized pattern matcher, as mentioned, is a rewrite of how the Scala compiler handles pattern matching. It served many purposes, with the \"virtualization\" part of it meaning it is part of the virtualized scala effort. That effort is a bit the opposite of macros: it takes things that are \"run\" at compile time, and move then to run time.For example, given the presence of the proper definition in scope, a statement like this:instead of being compiled to bytecode branches and literals, or even optimized to the literal \"2\", actually gets compiled as the following statement:Please see the  for more information and some examples of what this can be useful for.I said, however, that the pattern matcher rewrite served many purposes. One other very important goal was to turn the spaghetti code that was the old pattern matcher, full or special and corner cases and bugs, into something that can be reasoned with, extended and improved more easily. This rewrite fixed so many problems that people just went through the issues list running sample code for issues related to the pattern matcher and marking the issues as \"fixed\" as they worked. It does have new bugs of its own, but on a much, much smaller scale.Now, there's very little information about how the new pattern matcher works, but, basically, it translates into a few method calls which are \"implemented\" in the compiler with the  monad. That then goes into an optimization phase that produces optimal bytecode.It is possible to introduce your own matcher, though that's locked behind an  flag. Try the following code, copied from Scala's test suite, with and without that flag:The result without the flag is just what you'd expect:With , however, the alternative matching \"engine\" gets compiled:See also, for more information, the scaladocs for  and the .Disclaimer: the above was extract and ran from a Scala version on the master branch, after 2.10.0, so there may be differences. I find myself sadly lacking in a pure 2.10.0 or 2.10.1 environment to test it out, though."},
{"body": "Just now I am surprised to learn that  produces a view. The consequence is shown in the following example:The idea is that I have a distribution, which is perturbed with some randomness then I renormalize it. The code actually fails in its original intention: since  produces a ,  is always re-evaluated whenever  is used.I am now doing something like , but that's just a little bit verbose. So the purpose of this question is:Thanks.There's a ticket about this,  (by YT).The commit that introduces it has this to say:I have not been able to find the original suggestion by jrudolph, but I assume it was done to make  more efficient. Give the question, that may come as a surprise, but   more efficient if you are not likely to iterate over the values more than once.As a work-around, one can do  to produce a new .The scala doc say:So this should be expected, but this scares me a lot, I'll have to review bunch of code tomorrow. I wasn't expecting a behavior like that :-(Just an other workaround:You can call  to get a copy, and if you need it back to map , but this unnecessary create objects, and have a performance implication over using One can relatively easy write, a  which doesn't create a view, I'll do it tomorrow and post the code here if no one do it before me ;)I found an easy way to 'force' the view, use '.map(identity)' after mapValues (so no need of implementing a specific function):It's a shame the type returned isn't actually a view! othewise one would have been able to call 'force' ..."},
{"body": " has  method, which can interrupt the thread, which runs the  task. For example, if I wrap an  blocking call in a  I can interrupt it later. provides no  method. Suppose I wrap an  blocking call in a . How can I interrupt it? This is not yet a part of the s API, but may be added as an extension in the future.As a workaround, you could use the  to wrap 2 futures - the future you want to cancel and a future that comes from a custom . You could then cancel the thus created future by failing the promise:Now you can call this on any future to obtain a \"cancellable wrapper\". Example use-case:EDIT:For a detailed discussion on cancellation, see Chapter 4 in the  book.I haven't tested this, but this expands on the answer of Pablo Francisco P\u00e9rez Hidalgo. Instead of blocking waiting for the java , we use an intermediate  instead.By cancelling I guess you would like to violently interrupt the .Found this segment of code: Did a few tests and seems to work fine!Enjoy :)I think it is possible to reduce the complexity of the implementations provided by making use of the  and its implementations. can build a Java future which is the one to be cancelled by its  method. Another future can wait for its completion thus becoming the observable interface which is itself immutable in state:"},
{"body": "Is it possible to implement in Scala something equivalent to the Python  statement where it remembers the local state of the function where it is used and \"yields\" the next value each time it is called?I wanted to have something like this to convert a recursive function into an iterator. Sort of like this:Except,  may be more complex and recurs through some acyclic object graph.\nLet me add a more complex example (but still simple):\nI can write a simple recursive function printing things as it goes along:Ideally I would like to have a library that allows me to easily change a few statements and have it work as an Iterator:It does seem continuations allow to do that, but I just don't understand the  concept. Will continuation eventually make it into the main compiler and would it be possible to extract out the complexity in a library?\ncheck  in that other thread.While Python generators are cool, trying to duplicate them really isn't the best way to go about in Scala. For instance, the following code does the equivalent job to what you want:In it the stream is generated lazily, so it won't process any of the elements until asked for, which you can verify by running this:The result can be converted into an  simply by calling  on the resulting :The  definition, using , would be rendered thus:Another alternative would be concatenating the various iterators, taking care to not pre-compute them. Here's an example, also with debugging messages to help trace the execution:This is pretty close to your code. Instead of , I have definitions, and then I just concatenate them as I wish.So, while this is a non-answer, I just think you are barking up the wrong tree here. Trying to write Python in Scala is bound to be unproductive. Work harder at the Scala idioms that accomplish the same goals.Another continuations plugin based solution, this time with a more or less encapsulated Generator type,To do this in a general way, I think you need the .A naive implementation (freehand, not compiled/checked):What is happening is that any call to  captures the control flow from where it is called to the end of the  block that it is called in.  This is passed as the  argument into the shift function.So, in the example above, each  returns the value of  (up to that point) and saves the rest of the computation in the  variable.  It is driven from outside by the  and  methods.Go gentle, this is obviously a terrible way to implement this.  But it best I could do late at night without a compiler handy.Scala's for-loop of the form  translates into a  call, and not directly into  / .In the  we don't need to linearize objects' creations and have them in one place, as it would be needed for iterator's . The consumer-function  can be inserted multiple times, exactly where it is needed (i.e. where an object is created).This makes implementation of use cases for generators simple and efficient with  /  in Scala.The initial Foo-example:The initial printClass-example:Or with indentation:Output:"},
{"body": "In Scala 2.10 how do I generate a class from string (probably, using the Toolbox api) later to be instantiated with Scala's reflection?W.r.t compilation toolboxes can only run expressions = return values, but not resulting classes or files/byte arrays with compilation results. However it's still possible to achieve what you want, since in Scala it's so easy to go from type level to value level using implicit values:. In 2.10.0-RC1 some methods of  have been renamed.  is now just , and  is now called .Update #1. If you don't need a java.lang.Class and just need to instantiate the compiled class, you can write  directly in the string submitted to .Update #2. It is also possible to have  use custom mapping from variable names to runtime values. For example:In this example I create a free term that has a value of 2 (the value doesn't have to be a primitive - it can be your custom object) and bind an identifier to it. This value is then used as-is in the code that is compiled and run by a toolbox.The example uses manual AST assembly, but it's possible to write a function that parses a string, finds out unbound identifiers, looks up values for them in some mapping and then creates corresponding free terms. There's no such function in Scala 2.10.0 though."},
{"body": "I was just comparing the performance of scala actors vs java threads.I was amazed to see the difference, I observed that with my system I was able to spawn maximum ~2000 threads (live at a time) only But with the same system I was able to spawn ~500,000 actors of scala. Both programs used around 81MB of Heap memory of JVM.Can you explain How java thread are this much heavy weight than scala / akka actors?\nWhat is the key factor which made scala-actor this much light weight? Thanks.Scala actors (including the Akka variety) use Java threads.  There's no magic: more than a few thousand threads running simultaneously is a problem for most desktop machines.The Actor model allows for awake-on-demand actors which do not occupy a thread unless they have work to do.  Some problems can be modeled effectively as lots of sleeping agents waiting to get some work, who will do it relatively quickly and then go back to sleep.  In that case, actors are a very efficient way to use Java threading to get your work done, especially if you have a library like Akka where performance has been a high priority.The  explain the basics pretty well.All reasonably scalable web servers have to solve this sort of problem one way or another; you probably ought not be basing your decision for web server primarily on whether actors are used under the hood, and regardless of what you use you can always add actors yourself.An Akka actor is not equivalent to a thread. It is more like a  that is executed on a threadpool.When a message is dispatched to an actor, that actor is placed on a threadpool to process the message. When it is done, the pooled thread can be used to execute other actors."},
{"body": "I was working with a variable that I had declared as an Integer and discovered that > is not a member of Integer. Here's a simple example:Compare that to an Int:What are the differences between Integer and Int? I see the deprecation warning but it's unclear to me why it was deprecated and, given that it has been, why it doesn't have a > method.\"What are the differences between Integer and Int?\"Integer is just an alias for java.lang.Integer.  Int is the Scala integer with the extra capabilities.Looking in Predef.scala you can see this the alias:However, there is an implicit conversion from Int to java.lang.Integer if you need it, meaning that you can use Int in methods that take an Integer.As to why it is deprecated, I can only presume it was to avoid any confusion over which kind of integer you were working with.I think the problem you're seeing has has to do boxing/unboxing of value types and the use of the Java class Integer.I think the answer is here: .  There is no implict unboxing in Scala.  You've defined  as the Java class Integer but in the , the 3 is being treated and an int. Integer gets imported from java.lang.Integer and is only for compatibility with Java. Since it is a Java class, of course it can't have a method called \"<\".\nEDIT: You can mitigate this problem by declaring an implicit conversion from Integer to Int.You'll still get deprecation warning though.Integer gets imported from java.lang.Integer and is only for compatibility with Java. Since it is a Java class, of course it can't have a method called \"<\". EDIT: You can mitigate this problem by declaring an implicit conversion from Integer to Int. is a Java class, . It's different from Java's primitive type , which is not a class. It can't have  defined, because Java does not allow operators to be defined for classes.Now, you might wonder why such a type exist at all? Well, primitive types cannot be passed as references, so you can't pass an  to a method expecting , equivalent to Scala's , for example. To do that, you put that  inside an  object, and then pass the ."},
{"body": "What is the different between the following Generics definitions in Scala:and My gut tells me they are about the same but that the latter is more explicit. I am finding cases where the former compiles but the latter doesn't, but can't put my finger on the exact difference.Thanks!Can I throw another into the mix?OK, I figured I should have my take on it, instead of just posting comments. Sorry, this is going to be long, if you want the TLDR skip to the end.As Randall Schulz said, here  is a shorthand for an existential type. Namely,is a shorthand forNote that contrary to what Randall Shulz's answer mentions (full disclosure: I got it wrong too in an earlier version fo this post, thanks to Jesper Nordenberg for pointing it out) this not the same as:nor is it the same as:Beware, it is easy to get it wrong (as my earlier goof shows): the author of the article referenced by Randall Shulz's answer got it wrong himself (see comments), and fixed it later. My main problem with this article is that in the example shown, the use of existentials is supposed to save us from a typing problem, but it does not. Go check the code, and try to compile  or . Yep, does not compile. Simply making  generic in  would make the code compile, and it would be much simpler.\nIn short, that's probably not the best article to learn about existentials and what they are good for (the author himself acknowledge in a comment that the article \"needs tidying up\").So I would rather recommend reading this article: , in particular the sections named \"Existential types\" and \"Variance in Java and Scala\".The important point that you you should get from this article is that existentials are useful (apart from being able to deal with generic java classes) when dealing with non covariant types.\nHere is an example.This class is generic (note also that is is invariant), but we can see that  really don't make use of the type parameter (unlike ), so if I get an instance of  I should always be able to call it, whatever  is.  If I want to define a method that takes a  instance and just calls its  method, I could try this:Sure enough, this does not compile, as  comes out of nowhere here.OK then, let's make the method generic:Great, this works. We could also use existentials here:Works too. So all in all, there is no real benefit here from using an existential (as in ) over type parameter (as in ).However, this changes if  appears itself as a type parameter to another generic class. Say by example that we want to store several instances of  (with different ) in a list. Let's try it:The last line does not compile because  is invariant, so a  and  cannot be treated as a  even though  and  both extends .OK, let's try with an existential, using the shorthand notation :This compiles fine, and you can do, as expected:Now, remember that the reason we had a type checking problem in the first place was because  is invariant. If it was turned into a covariant class () then everything would have worked out of the box and we would never have needed existentials.So to sum up, existentials are usefull to deal with generic invariant classes, but if the generic class does not need to appear itself as a type parameter to another generic class, chances are that you don't need existentials and simply adding a type parameter to your method will workNow come back(at last, I know!) to your specific question, regardingBecause  is covariant, this is for all intents and purpose the same as just saying:So in this case, using either notation is really just a matter of style.However, if you replace  with , things change: is invariant and thus we are in the same situation as with the  class from my example. Thus the above really is very different fromThe former is a shorthand for an existential type when the code doesn't need to know what the type is or constrain it:This form says that the element type of  is unknown to  rather than your second form, which says specifically that the 's element type is .Check out this brief explanatory  on Existential Types in Scala."},
{"body": "'map' preserves the number of elements, so using it on a Tuple seems sensible.My attempts so far:It looks quite painful... And I haven't even begun to try to convert it back to a tuple.\nAm I doing it wrong? Could the library be improved? Supports mapping and folding over tuples via an intermediary  representation,Sample REPL session,Where the elements of the tuple are of different types you can map with a polymorphic function with type-specific cases,As of shapeless  mapping over tuples is supported directly. The above examples now look like this,In general, the element types of a tuple aren't the same, so map doesn't make sense. You can define a function to handle the special case, though:You could use the Pimp My Library pattern to call this as .Even when the types of the elements are not the same,  is a , which can be mapped with the  operation."},
{"body": "One thing I find quite confusing is knowing which characters and combinations I can use in method and variable names. For instanceAs I understand it, there is a distinction between  and . You can mix an match one or the other but not both, unless separated by an underscore (a ).From  section 6.10,So we are excluded from using  and `I looked up Unicode mathematical symbols on , but the ones I found didn't include , ,  etc. Is there a definitive list somewhere of what the operator characters are?Also, any ideas why Unicode mathematical operators (rather than symbols) do not count as operators?Working from the EBNF syntax in the spec:But also taking into account the very beginning on Lexical Syntax that defines:Here is what I come up with. Working by elimination in the range , eliminating letters, digits, parentheses and delimiters, we have for ... (drumroll):\nand also  and  - except for parentheses and periods.(Edit: adding valid examples here:). In summary, here are some valid examples that highlights all cases - watch out for  in the REPL, I had to escape as :I found this Unicode  to figure out :Other operator-looking things that are reserved words:  and also  \u21d2 and  \u2190. gives the rule in Chapter 1, lexical syntax (on page 3):This is basically the same as your extract of Programming in Programming in Scala.  is not an Unicode mathematical symbol, but it is definitely an  not listed above (not a letter, including _ or $, a digit, a paranthesis, a delimiter).In your list:"},
{"body": "I would like to disable the color escape codes logged from sbt/play. Is this possible? And if it is, is there a way to do it without making changes to the config - i.e. via a command line switch or system property.You can simply set the system property  to . If you want to e.g. use SBT inside Vim you can create a script like this:Since version  (and possibly earlier) you can now simply use the  option to sbt. e.g.This sounds like your platform does not match the actual jline.terminal property. I am just guessing here but when I pass the parameter as Daniel suggested on a Windows command line I see the color escape codes as well.Therefore, you have to make sure the property matches your platform, i.e. WindowsTerminal on Windows and UnixTerminal on Unix.If this does not help, then you might be on an unsupported platform in which case the  suggests to use:Well, you can  colors on Windows by installing Cygwin and passing this parameter:So I'd look up jline parameters to see what disables color coding.I was able to get colored output from SBT in Cygwin by adding:Additionally I figured out that I also needed to add the following line to Cygwin.bat:After that is added SBT gives very nice colored output. Additionally I would recommend looking into Console2 as it can hook through Cygwin, but provides a much better interface in my opinion:"},
{"body": "By day I write C#. Everything I do goes through Microsoft code analysis and static analysis tools, so my C# has a very regular structure and layout. Obviously, I write code with a certain style. It is partially, because I have no choice (it won't compile if I miss out a space before that comma), but it's also nice to have regular looking code, knowing where to look for things, etc.At the weekends I'm getting into Scala. Looking at the Scala API and  web framework source, I can't obviously see any standardised style. One thing that jumps out at me, for example, is the lack of a separate file per class. The lack of consistency with brackets and braces is another example.I understand that there are probably a few reasons driving this: firstly, with open source (or hobby) code making sure that an obvious method isn't completely documented is less of a priority. Secondly, things like case classes cut down 20-line class declarations into a single line. Thirdly, C# is a much 'flatter' language: unless it's a complex  statement, the numbers of nested parens, braces and brackets isn't that deep. In Scala, things tend to get a little nested.Do regular Scala users have a specific style that they stick to? Am I just being stupid putting a one-line case-class in its own file for the sake of [alien] convention? Any tips?Having multiple classes and objects inside a single file is considered good form in Scala, as long as the classes are tightly related.While not necessary, the type returned by a method -- a named function declared on a trait, class or object -- is expected to be declared for non-private methods. Spaces are expected after , but not before it. Spaces are expected between keywords and parenthesis, but not between a method name and the following parenthesis, in dot notation. For operator notation, there doesn't seem to be an accepted style regarding parenthesis -- or when to use that notation, for that matter, but spaces are expected around non-alphanumeric methods in such notation.Exceptionally, when concatenating strings with , the recommended style is not to use spaces around it. For example:Declarations that can be one-liners are expected to be one-liners, unless the nesting isn't obvious.Methods that do not expect parameters, and do not have side effects, are supposed to be used without parenthesis, except for Java methods, which are expected to be used with parenthesis. Parameter-less methods with side effects are supposed to be used with parenthesis.Declarations which contains a single expression are expected not to be enclosed inside curly braces unless other syntactic considerations make that impossible. Enclosing an expression within parenthesis to enable multi-line expressions is accepted, but I have seen little use of that.In for-comprehensions, keeping generators and conditions vertically aligned seems to be an accepted style. As for , I have seen it both aligned with  and indented.It's also accepted style to vertically align parameters of a class declaration. Speaking of indentation, two-spaces is the accepted convention.Curly braces are expected to start on the same line of the declaration, and end vertically aligned with that line by itself.For procedures -- functions whose return type is  --, the expected style was supposed to be to leave out the type of the method and the equal sign:Some people think this style is error prone, however, as a missed equal sign will change a function returning something other than  into a procedure.Identifiers are written in camel case (eg: ), like in Java. For names of fields, method parameters, local variables and functions, start with a lower case letter. For classes,traits and types, start with an upper case letter.Departing from Java convention are constant names. In Scala, the practice is to use standard camel case starting with an upper case letter. For example  and not , XOffset and not . This rule is usually followed by any singleton. Having a constants and singletons be represented that way has a practical consequence, for case matches:Package names are written beginning with a lower case letter. This is particularly helpful when distinguishing in an import statement what is a package and what is not. In the previous example,  is not a package (it's a singleton), as it begins with an upper case letter.Using the underline character --  -- is not recommended, as that character has many special meanings in Scala. These rules for identifiers can be found on pages 141 and 142 of Programming in Scala, by Odersky, Spoon & Venners.Right now, I can't recall other situations, but feel free to ask for clarification on specific points. Some of these rules were explicitly stated, others are more of a community consensus. I tried to leave out my own preferences, but I may have failed.More importantly, perhaps, there just isn't really much of an unified convention. One reason for that may be that Scala is attracting people from many very different backgrounds, such as functional language mavens, Java programmers and web 2.0 enthusiasts.There is now a full Scala style guide which has been .  It isn't even remotely official yet, but it is the only (to my knowledge) codification of the community-accepted conventions.Now  is available. It's not 100% official (located on \"\" site) but seems the most standard style guide for Scala.This is a very important question.  Generally, Scala style seems to be picked up just by hanging out with other members of the Scala community, reading Scala source code, etc.  That's not very helpful for newcomers to the language, but it does indicate that some sort of de facto standard  exist (as chosen by the wisdom of the masses).  I am currently working on a fully-realized style guide for Scala, one which documents the community-chosen conventions and best-practices.  However, a) it's not finished yet, and b) I'm not sure yet that I'll be allowed to publish it (I'm writing it for work).To answer your second question (sort of): in general, each class/trait/object should get its own file named according to Java naming conventions.  However, in situations where you have a lot of classes which share a single common concept, sometimes it is easiest (both in the short-term and in the long-term) to put them all into the same file.  When you do that, the name of the file should start with a lower-case letter (still camelCase) and be descriptive of that shared concept.I don't really care for the typical style that many of the Scala coders use, so I just apply the same standard I would using in C#, Java or especially JavaScript.Scala  be very expressive, but using an unfamiliar format will increase your barrier to entry.  Especially considering internal , which could not possibly have a \"Standard\".So, I say to do whatever make your code more readable to you and your team."},
{"body": "I'm new to Scala and don't know Java. I want to create a jar file out of a simple Scala file. So I have my HelloWorld.scala, generate a HelloWorld.jar. Manifest.mf:In the console I run:Sample directory structure:HelloWorld.scala:MANIFEST.MF:build.bat:In order to successfully use the  switch, you need two entries in the  file: the main class; relative URLs to any dependencies. The documentation notes:For completeness, an equivalent bash script:Because Scala scripts require the Scala libraries to be installed, you will have to include the Scala runtime along with your JAR.There are many strategies for doing this, such as , but ultimately the issue you're seeing is that the Java process you've started can't find the Scala JARs.For a simple stand-alone script, I'd recommend using jar jar, otherwise you should start looking at a dependency management tool, or require users to install Scala .You can also use maven and the maven-scala-plugin. Once you set up maven, you can just do mvn package and it will create your jar for you. I ended up using , it is really simple to use. I added a file called  into the  directory at the root of the project with a one liner (Note your version might need to be changed).Then just run the  task in :It will first run your tests and then it will generate the new jar into the  directory (given that my  already lists all my dependencies).In my case, I just make that  file executable, rename to remove the extension and it is ready to ship!Also, if you are doing a command line tool, don't forget to add a  (I hate scripts without proper manpages or with multi-page plain text documentation that is not even piped into a pager for you).I don't want to write why's and how's rather just show the solution which worked in my case (via Linux Ubuntu command line):1) 2)  3)  3) 4)  5)I modified the bash script adding some intelligence including auto-manifest generation.This script assumes that the main object is named the same as the file it is in (case sensitive).  Also, either the current directory name must equal to the main object name or the main object name should be provided as a command line parameter.  Launch this script from the root directory of your project.  Modify the variables at the top as required.Be aware that the script will generate the bin and dist folders and will ERASE any existing contents in bin.One thing which may cause a similar problem (although it's not the problem in the initial question above) is that the Java vm seems to demand that the main method returns . In Scala we can write something like ():where main actually returns a -object (i.e. it's not ), but the program will run nicely when we start it using the scala command.If we package this code into a jar-file, with  as the Main-Class, the Java vm will complain that there's no main function, since the return type of our main is not  (I find this complaint somewhat strange, since the return type is not part of the signature).We would have no problems if we left out the =-sign in the header of main, or if we explicitly declared it as .I tried to reproduce MyDowell's method. Finally I could make it work. However I find that the answer though correct a bit too complicated for a novice ( in particular the directory structure is unnecessarily complicated ).I can reproduce this result with very simplistic means. To start with there is only one directory which contains three files:helloworld.scalaMANIFEST.MF:first compile helloworld.scala:then create the jar:now you can run it with:I found this simple solution because the original one did not work. Later I found out that not because it is wrong, but because of a trivial error: if I don't close the second line in MANIFEST.MF with a newline, then this line will be ignored. This took me an hour to find out and I tried all other things before, in the process finding this very simple solution.If you do not wish to use sbt facilities I recommend the use of a makefile.Here is an example  where  package is replaced by  for completeness.makefile"},
{"body": "I need to check if a string is present in a list, and call a function which accepts a boolean accordingly.Is it possible to achieve this with a one liner?The code below is the best I could get:I'm sure it's possible to do this with less coding, but I don't know how!Just use And if you didn't want to use strict equality, you could use exists:this should work also with different predicate    In your case I would consider using Set and not List, to ensure you have unique values only. unless you need sometimes to include duplicates.In this case, you don't need to add any wrapper functions around lists."},
{"body": "I'm new to spark. I want to perform some operations on particular data in a CSV record.I'm trying to read a CSV file and convert it to RDD. My further operations are based on the heading provided in CSV file.(From comments)\nThis is my code so far:I can get the header values like this. I want to map this to each record in CSV file.I can get the header values like this. I want to map this to each record in CSV file.In java I\u2019m using  to get the particular value. I need to do something similar to that here.A simplistic approach would be to have a way to preserve the header.Let's say you have a file.csv like:We can define a header class that uses a parsed version of the first row:That we can use that header to address the data further down the road:Note that the  is not much more than a simple map of a mnemonic to the array index. Pretty much all this could be done on the ordinal place of the element in the array, like PS: Welcome to Scala :-)You can use the spark-csv library: This is directly from the documentation: Firstly I must say that it's much much simpler if you put your headers in separate files - this is the convention in big data.Anyway Daniel's answer is pretty good, but it has an inefficiency and a bug, so I'm going to post my own. The inefficiency is that you don't need to check every record to see if it's the header, you just need to check the first record for each partition.  The bug is that by using  you could get an exception thrown or get the wrong column when entries are the empty string and occur at the start or end of the record - to correct that you need to use .  So here is the full code:Final points, consider Parquet if you want to only fish out certain columns.  Or at least consider implementing a lazily evaluated split function if you have wide rows.We can use the new DataFrameRDD for reading and writing the CSV data.\nThere are few advantages of DataFrameRDD over NormalRDD:You will be required to have this library: Add it in build.sbtSpark Scala code for it:To convert to normal RDD by taking some of the columns from it and Saving the RDD to CSV format:Since the header is set to true we will be getting the header name in all the output files.I'd recommend reading the header directly from the driver, not through Spark. Two reasons for this: 1) It's a single line. There's no advantage to a distributed approach. 2) We need this line in the driver, not the worker nodes.It goes something like this:Now when you make the RDD you can discard the header.Then we can make an RDD from one column, for example:Here is another example using Spark/Scala to . For a more detailed description see this .Another alternative is to use the  method as you'll get the partition index number and a list of all lines within that partition. Partition 0 and line 0 will be be the headerHow about this? I would suggest you to tryYou have to have a class in this example person with the spec of your file header and associate your data to the schema and apply criteria like in mysql.. to get desired result I think you can try to load that csv into a RDD and then create a dataframe from that RDD, here is the document of creating dataframe from rdd:For spark scala I typically use when I can't use the spark csv packages... "},
{"body": "Reading the Programming in Scala 2nd Ed and I came across this:literal identifier\n\"The idea is that you can put any string that's accepted by the runtime as an identifier between backtick\"I'm not entirely sure why I would use this? The book gave a use case of accessing the static yield method in Java's Thread class.So since in Scala, yield is a reserve word, if I use yield with backticks, it would ignore the Scala's yield and let me access the Java's Thread class's method yield instead? Thank you in advance.Exactly. Using backticks, you can more or less give any name to a field identifier. In fact, you can even saywhich defines a variable with name (one character of whitespace).The literal definition of identifiers is useful in two cases. The first case is, when there is already a reserved word of the same name in Scala and you need to use a Java library which does not care about that (and of course, why should it).The other use case comes with  statements. The convention is that lower case names refer to match variables, whereas upper case names refer to identifiers from the outer scope. So,prints  (if the compiler were dumb enough not to fail with saying  were unreachable). If you want to refer to the originally defined , you need to use backticks as a marker.Which prints . There is a more advanced use case in this recent question  where the backticks were needed to get the compiler to digesting the code for a setter method (which in itself uses some \u2018magic\u2019 syntax).Thank you @Debilski, it helps me to understand this code below from AKKA doc :The case :matches a message of type Terminated with ActorRef field equals to child which is defined earlier.With this statement :We match every Terminated messages with any reference of ActorRef mapped in ."},
{"body": "How to catch multiple exceptions at once in Scala? Is there a better way than in C#: You can bind the whole pattern to a variable like this:See  called Pattern alternatives.As you have access to the full pattern matching capabilities of scala in the catch clause, you can do a lot : Note that if you need to write the same handlers time and time again you can create your own control structure for that : Some such methods are available in object . failing, failAsValue, handling may be just what you needUnfortunately, with this solution, you have no access to the exception where you use the alternative patterns. To my knowledge, you cannot bind on an alternative pattern with case . So if you need access to the exception, you have to nest matchers : You can also use :This specific example might not be the best example to illustrate how you can use it, but I find it pretty useful in many occasions."},
{"body": "Is there a pattern-matching way to see if two arrays (or sequences) are equivalent?You need to change your last line toto do a deep comparison of the arrays.From :"},
{"body": "I'm new to Scala, I'm using 2.9.1, and I'm trying to get my head around how to use partial functions.  I have a basic understanding of curried functions, and I know that partial functions are kind of like curried functions where they are only 2nary or some such.  As you can tell I'm a bit green at this.It does seem that in certain cases like XML filtering, being able to partial functions would be highly advantageous, so I'm hoping get a better understanding of how to use them.I have a function that uses the RewriteRule structure, but I need it to work with two arguments, whereas the RewriteRule structure only takes one, OR a partial function.  I think this is one of the cases I'm thinking about it being helpful.Any advice, links, words of wisdom etc. welcome!The answers so far are excellent, and have cleared up a few fundamental misconceptions I have.  I think they also explain where I'm struggling - I think maybe posting a new question being a bit more specific will help, so I'll do that too.A partial function is a function that is valid for only a subset of values of those types you might pass in to it.  For example:This is useful when you have something that knows how to check whether a function is defined or not.  Collect, for instance:This is  going to help you place two arguments where you really want one.In contrast, a  is a function where some of its arguments have already been filled in.Now you only need one argument--the thing to add 5 to--instead of two:You can see from this example how to use it.But if you need to specify those two arguments, this  won't do it--say you want to use , for instance, and you need to give it a function of one argument, but you want it to add two different things.  Well, then you canwhich will partially apply the function (really, just create a function out of the method, since nothing has been filled in) and then combine the separate arguments into a tuple.  Now you can use this in places that require a single argument (assuming that the type is correct):In contrast,  is different yet again; it turns functions of the form  into .  That is, given a function of multiple arguments, it will produce a chain of functions that each take one argument and return a chain one shorter (you can think of it as partially applying one argument at a time).Rex Kerr's explanation is very good -- and no surprise there either. The question is clearly mixing up  with . For whatever it is worth, I made the same confusion myself when I learned Scala.However, since the question does draw attention to partial functions, I'd like to speak of them a bit.Many people say that partial functions are functions that are not defined for all input, and that's true of mathematics, but not of Scala. In Scala, a function may not be defined for all input either. In fact, since partial function inherit from function, then function includes all partial functions as well, making that inevitable.Others mention the method , which is, indeed, a difference, but one mostly about implementation. That's so true that Scala 2.10 will probably be released with a \"fast partial function\", which does not rely on .And a few people even imply that the  method for partial functions do something different than the  method for functions, like only executing for the input that is defined -- which could not be farther from the truth. The  method is .What partial functions really come down to is another method: . That sums up all use cases for partial functions much better than , because partial functions are really about doing one of the following things:I'm not saying everything can be  implemented in terms of , mind you. I'm just saying that partial functions are about doing something else when an input isn't defined for it."},
{"body": "In  I have set the  property of my Scala project to  where  is the location of my scala project.Now, if I am in a Scala file that implements a class/trait, when I type +, Eclipse opens the Scaladoc page of that class/trait. However, this does not work if it is an object since Eclipse tries to open  whereas the Scaladoc-generated file name is .Is there any workaround?Scaladoc is now integrated since  of the Scala-IDE plugin. You can just hover over your term, or press F2 while the cursor is on the identifier.Typesafe has  a plugin for the Scala compiler which generates  for your  API. The plugin and its documentation is available on . Maybe this helps as a workaround."},
{"body": "Suppose I've got a type class that proves that all the types in a Shapeless coproduct are singleton types:We can show that it works with a simple ADT:And then:Now we want to combine this with Shapeless's  mechanism that'll give us a coproduct representation of our ADT:I'd expect  to work, but it doesn't. We can use  to get some information about why: obviously  a singleton type, though. We can try putting the  instances in scope manually just for fun:And somehow now it works:I don't understand why these instances would work in this context while the ones generated by the  macro method (which we used to create them) don't. What's going on here? Is there a convenient workaround that doesn't require us to enumerate the constructors manually?This works as written as of the most recent shapeless 2.1.0-SNAPSHOT."},
{"body": "I started to program in Scala recently. I'm looking for a free Scala profiler. Reading from the language's official site led me to , but the program was not a free one.Googling \"scala profiler\" didnt give me any relevant result.So how do I profile my program written in Scala? I prefer a graphical plugin for Netbeans or Eclipse. But if there is no such thing, then a console one will be fine.Thanks :)Given that Scala runs on the Java Virtual Machine (), you can use the  tools  and  to profile the application. Alternatively  Java profiler should work (e.g. YourKit, as you've already mentioned) has been bundled with the JDK since  and it is based on the NetBeans profiler. You can capture memory usage, code performance hotspots etc:I have tried with jvisualvm (both inside and outside netbeans) but I cannot see any scala method call in the profiling reports (only the underlying java libraries method calls). To me it looks like a bug but I might have missed snomething obvious. Anyway here is a issue report I wrote on netbeans.org with the details:If anybody here has successfully used JVisualVM to CPU-profile Scala code please let me know."},
{"body": " monad is a great expressive way to deal with something-or-nothing things in Scala. But what if one needs to log a message when \"nothing\" occurs? According to the Scala API documentation,However, I had no luck to find best practices using Either or good real-world examples involving Either for processing failures. Finally I've come up with the following code for my own project:(Please note this is a snippet from a real project, so it will not compile on its own)I'd be grateful to know how you are using  in your code and/or better ideas on refactoring the above code.Either is used to return one of possible two meaningful results, unlike Option which is used to return a single meaningful result or nothing.An easy to understand example is given below (circulated on the Scala mailing list a while back):As the function name implies, if the execution of \"block\" is successful, it will return \"Right(<result>)\".  Otherwise, if a Throwable is thrown, it will return \"Left(<throwable>)\".  Use pattern matching to process the result:Hope that helps.Scalaz library has something alike Either named Validation. It is more idiomatic than Either for use as \"get either a valid result or a failure\".Validation also allows to accumulate errors.Edit: \"alike\" Either is complettly false, because Validation is an applicative functor, and scalaz Either, named \\/ (pronounced \"disjonction\" or \"either\"), is a monad. \nThe fact that Validation can accumalate errors is because of that nature. On the other hand, / has a \"stop early\" nature, stopping at the first -\\/ (read it \"left\", or \"error\") it encounters. There is a perfect explanation here:  See: As requested by the comment, copy/paste of the above link (some lines removed):The snippet you posted seems very contrived. You use Either in a situation where:Turning an exception into a Left is, indeed, a common use case. Over try/catch, it has the advantage of keeping the code together, which makes sense if the exception is . The most common way of handling Either is pattern matching:Another interesting way of handling  is when it appears in a collection. When doing a map over a collection, throwing an exception might not be viable, and you may want to return some information other than \"not possible\". Using an Either enables you to do that without overburdening the algorithm:Here we get a list of all authors in the library,  a list of books without an author. So we can then further process it accordingly:So, basic Either usage goes like that. It's not a particularly useful class, but if it were you'd have seen it before. On the other hand, it's not useless either."},
{"body": "I see that traits in Scala are similar to interfaces in Java (but interfaces in Java extend other interfaces, they don't extend a class). I saw  where a trait extends a class.What is the purpose of this? Why can traits extend classes?Yes they can, a  that extends a  puts a restriction on what  can extend that  - namely, all  that mix-in that  must extend that ."},
{"body": "The package \"scala\" has a number of classes named Product, Product1, Product2, and so on, up to Product22.The descriptions of these classes are surely precise. For example:Precise, yes. Communicative? Not so much. I expect that this is the perfect wording for someone who already understands the sense of \"cartesian product\" being used here. For someone who doesn't, it sounds a bit circular. \"Oh yes, well of course Product4 is the  product of 4 .\"Please help me understand the correct functional-language viewpoint.  What is the sense of \"cartesian product\" being used here?  What do the Product classes' \"projection\" members indicate?Perhaps better understanding can be gained by knowing who derives from it:Or by, knowing it \"\", know what other classes can make use of it, by virtue of extending  itself. I won't quote that here, though, because it's rather long.Anyway, if you have types A, B, C and D, then Product4[A,B,C,D] is a class whose instances are all possible elements of the cartesian product of A, B, C and D. Literally.Except, of course, that Product4 is a Trait, not a class. It just provides a few useful methods for classes that are cartesian products of four different sets.Everyone else has gone for the maths so I'll go for the silly answer just in case! You have a simple car which has a gearbox, a steering wheel, an accelerator and a number of passengers. These can each vary:  etc. The gearbox, steering, accelerator etc are therefore  and each has its own  of possible values. The cartesian product of each of these sets is basically . So a few possible values are:the size of the cartesian product is of course the product (multiplication) of the possibilities of each set. hence if you car has 5 gears (+ reverse + neutral), steering is left/straight/right, accelerator is on/off and up to 4 passengers, then there are 7 x 3 x 2 x 4 or 168 possible states.This last fact is the reason that the cartesian product (named after  by the way) has the multiplication symbol From :The projection allows to get the instance of the 'n' class referenced by the Product.A cartesian product is a product of sets. Given sets A and B, A x B (\"A cross B\") is the set of all tuples (x, y) such that x is in A and y is in B. A cartesian product may be analogously defined on types: given types A and B, A x B is the type of tuples (x, y) where x is of type A and y is of type B.So Product4 is the type of tuples (w, x, y, z), where w, x, y, z are components."},
{"body": "Scala makes a big deal about how what seem to be language features are implemented as library features.Is there a list of types that are treated specially by the language?Either in the specification or as an implementation detail?That would include, for example, optimizing away matches on tuples.What about special conventions related to pattern matching, for comprehensions, try-catch blocks and other language constructs?Is String somehow special to the compiler? I see that String enhancement is just a library implicit conversion, and that String concatenation is supported by , but is that somehow special-cased by the language?Similarly, I see questions about  and  and , and it's not clear what is a magical intrinsic. Is there a way to tell the difference, either with a compiler option or by looking at byte code?I would like to understand if a feature is supported uniformly by implementations such as Scala.JS and Scala-native, or if a feature might actually prove to be implementation-dependent, depending on the library implementation.There is an incredible amount of types that are \"known\" of the compiler, and are special to varying degrees. You can find a complete list in scalac's .We can probably classify them according to the degree of specialness they bear.Disclaimer: I have probably forgotten a few more.The following types are crucial to Scala's type system. They have an influence on how type checking itself is performed. All these types are mentioned in the specification (or at least, they definitely should be).The following types are not crucial to the type system. They do not have an influence on type checking. However, the Scala language does feature a number of constructs which desugar into expressions of those types.These types would also be mentioned in the specification.This is probably the list that you care most about, given that you said you were interested in knowing what could go differently on different back-ends. The previous categories are handled by early (front-end) phases of the compiler, and are therefore shared by Scala/JVM, Scala.js and Scala Native. This category is typically known of the compiler back-end, and so potentially have different treatments. Note that both Scala.js and Scala Native do try to mimic the semantics of Scala/JVM to a reasonable degree.Those types might not be mentioned in the language specification per se, at least not all of them.Here are those where the back-ends agree (re Scala Native, to the best of my knowledge):And here are those where they disagree:Also, although not  per se, but  are also handled specifically by the back-ends.In addition to the types mentioned above, which are available on all platforms, non-JVM platforms add their own special types for interoperability purposes.See Plus, a dozen more .See "},
{"body": "I have been looking for an explanation for why twitter had to migrate part of its middle ware from Rails to Scala. What prevented them from scaling the way facebook has, by adding servers as its user base expanded. More specifically what about the Ruby/Rails technology prevented the twitter team from taking this approach?It's not that Rails doesn't scale, but rather, requests for \"live\" data in Ruby (or any interpreted language) do not scale, as they are comparatively far more expensive both in terms of CPU & memory utilization than their compiled language counterparts.Now, were Twitter a different type of service, one that had the same enormous user base, but served data that changed less frequently, Rails could be a viable option via caching; i.e. avoiding live requests to the Rails stack entirely and offloading to front end server and/or in-memory DB cache. An excellent article on this topic: However, Twitter did not ditch Rails for scaling issues alone, they made the switch because Scala, as a language, provides certain built-in guarantees about the state of your application that interpreted languages cannot provide: if it compiles, time wasting bugs such as fat-fingered typos, incorrect method calls, incorrect type declarations, etc. simply cannot exist.For Twitter TDD was not enough. A quote from Dijkstra in  illustrates this point: \"testing can only prove the presence of errors, never their absence\".  As their application grew, they ran into more and more hard to track down bugs. The magical mystery tour was becoming a hindrance beyond performance, so they made the switch. By all accounts an overwhelming success, Twitter is to Scala what Facebook is to PHP (although Facebook uses their own ultra fast C++ preprocessor so cheating a bit ;-))To sum up, Twitter made the switch for both performance and reliability. Of course, Rails tends to be on the innovation forefront, so the 99% non-Twitter level trafficked applications of the world can get by just fine with an interpreted language (although, I'm now solidly on the compiled language side of the fence, Scala is just too good!)No platform can infinitely scale out whilst still dealing with complex sets of data that change moment to moment. \nLanguage and infrastructure matters, but how you build your site and the data access patterns matter more.If you've ever played games like Transport Tycoon or Settlers where you have to transport resources around, you'll know how you need to stay on top of upgrading infrastructure as usage increases. Scaling platforms like Facebook and Twitter is a never-ending task. \nYou have an ever increasing number of users, and you're being pushed to add more features and functionality. It's a continual process of upgrading one bit, which causes more stress on another bit. Throwing servers at the problem isn't always the answer, and sometimes can cause more problems.  links to a set of posts about the changes, including a decent history of the steps taken over time.The short version is that Ruby and Rails didn't deliver the performance and reliability they required for the service.  Given the scale, this isn't surprising; most COTS solutions are not satisfactory at the super-large end of scale.High Scalability covers a lot of questions about architecture at that top end, for other sites, so helps answer broader questions in the area too.They could have thrown more hardware at the problem, but it is a good deal more expensive then simply writing more efficient code. Like many high-level frameworks, Ruby on Rails is great at many things, but high-performance isn't one of them. Compiled languages will always be faster than interpreted languages.Facebook (and Google) scale by adding more servers, but at the same time they break their application out into various services.   Those services communicate via an agreed upon interface and type, and they are now free to build these services out in any technology they see fit.  Just because you read that facebook uses php doesn't mean that all their backend services are being served by php (and it doesn't make sense either since in SOA you can choose any tech stack).I think this video is the best answer to your question:\"From Ruby to the JVM\"\nI think one important bit missing here is the platform. Yes we had the compiled vs interpreted argument and a couple of others. \nBut one very important aspect was indeed the platform. There are different Ruby VMs but none did please twitter, although they tuned it quite a bit. But scala runs on the JVM and twitter engineers hat pretty goog experience with that. Why they they didnt try/choose JRuby? Well I guess the reasons mentioned above come here into play.Linear gains with parallelism (which is what multiple servers is) is exceedingly rare, and very application dependent. Yes, it exists -- that's how GPU do most of their work. If you are serving static pages, with no session state, that would also be the case.For the most part, however, adding servers do not increase performance linearly (ie, 10 servers are not 10 times faster than 1 server), and  means that any gains you can make on a single server will have much more impact than just adding servers. It's not like Twitter doesn't have a bunch of servers, now is it?"},
{"body": "my Haskell* is a bit rusty, so i can imagine that I\u2019m missing the obvious:Does one of these properties apply to the it?*actually SML, but that\u2019s 99% the same, but known by nobody under the sun.Methods exist on the Traversable trait which are equivalent to  and :BTW, another function that I find missing is a  function.How about :It's on ."},
{"body": "In Scala I can enforce type equality at compile time. For example:Is there a way to enforce that type A and type B should be different ?Riffing off of Jean-Philippe's ideas, this works:Then:I'd probably simplify this as follows, since the checks for \"cheating\" can always be circumvented anyway (e.g.  or ):I have a simpler solution, which also leverages ambiguity,The original use case,We can relate this to my  (thanks @jpp ;-) as follows,Sample REPL session,I liked the simplicity and effectiveness of Miles Sabin's first solution, but was a bit dissatisfied with the fact that the error we get is not very helpful:By example with the following definition:Attemtping to do  will fail to compile with:I'd rather have the compiler tell me something along the line of \"T is not different from String\"\nIt turns out that it's quite easy if add yet another level of implicits in such a way that we turn the  error\ninto an  error. From then we can use the  annotation to emit a custom error message:Now let's try to call :That's better. Thanks compiler.As a last trick for those that like the context bound syntactic sugar, one can define this alias (based on type lambdas):Then we can define  like this:Whether it is easier to read is highly subjective. In any case is definitly shorter than writing the full implicit parameter list.: For completeness, here the equivalent code for expressing that  is is not a sub-type of :And for expressing that  is not convertible to  :Based on 's idea, the following seems to work:Here's another attempt:Then, again:Like in my other proposal, the aim here is to introduce a compile-time ambiguity when  and  are the same. Here, we provide two implicits for the case where  is the same as , and an unambiguous implicit when this is not the case.Note that the problem is that you could still explicitly provide the implicit parameter by manually calling  or . I couldn't think of a way to prevent this at compile time. However, we can throw an exception at runtime, with the trick that  requires an evidence parameter itself indicating whether  is the same as . But wait... Aren't we trying to prove the exact opposite? Yes, and that's why that  implicit parameter has a default value of . And we expect it to be  for all legal uses \u2014 the only time where it would not be  is when a nasty user calls e.g. , and there we can prevent this cheating by throwing an exception.How about something like this, then?Then:The idea is to make resolution ambiguous when is the same as , and unambiguous when they are not the same. To further emphasize that the ambiguous methods should not be called, I added an implicit of type , which should never be around (and should certainly look wrong to the caller if they try to insert one explicitly). (The role of the  is just to give a different signature to the first two methods.)This is not an answer, just the beginnings of what I could think is an answer. The code below will return either an  or an  depending on whether the types are equal or not, if you ask for . How to go from there to actually making a check I haven't been able to figure out. Maybe the whole approach is doomed, maybe someone can make something out of it. Mind you,  will  return something, one can't use that. :-(Test:"},
{"body": "Let's say I have a  instance which is not initialized:is this the proper way to initialize it to null?Use  as a last resort. As already mentioned,  replaces most usages of null. If you using  to implement deferred initialisation of a field with some expensive calculation, you should use a .That said, Scala does support . I personally use it in combination with Spring Dependency Injection.Your code is perfectly valid. However, I suggest that you use  to initialize  to it's default value. If  is a primitive, you get the default specific to the type. Otherwise you get .Not only is this more concise, but it is neccessary when you don't know in advance what  will be:Using  is an compile error if T is unbounded:You can add an implicit parameter as evidence that  is nullable -- a subtype of  not a subtype of  This isn't fully , even in Scala 2.8, so just consider it a curiousity for now. The canonical answer is . Instead, use an option type:When you want to set it:And when you want to read from it, test for None:As David and retronym have already mentioned, it's a good idea to use  in most cases, as  makes it more obvious that you have to handle a no-result situation.  However, returning  requires an object creation, and calling  or  can be more expensive than an if-statement.  Thus, in high-performance code, using  is not always the best strategy (especially in collection-lookup code, where you may look up a value very many times and do not want correspondingly many object creations).  Then again, if you're doing something like returning the text of an entire web page (which might not exist), there's no reason  to use Option.Also, just to add to retronym's point on generics with , you can do this in a fully-baked way if you really mean it should be :and this works in 2.7 and 2.8.  It's a little less general than the  method, because it doesn't obey  AFAIK, but it otherwise does exactly what you'd hope it would do.I came across this question since scalastyle told me to not use null when initialising an object within my test with . My solution without changing any type that satisfied scalastyle:"},
{"body": "I'm novice in Scala. Recently I was writing a hobby app and caught myself trying to use pattern matching instead of if-else in many cases.instead ofAre these approaches equal? Is one of them more preferrable than another for some reason?I'm not sure why you'd want to use the longer and clunkier second version.And that's a lot more bytecode for the match also.  It's  efficient even so (there's no boxing unless the match throws an error, which can't happen here), but for compactness and performance one should favor /.  If the clarity of your code is greatly improved by using match, however, go ahead (except in those rare cases where you know performance is critical, and then you might want to compare the difference).Don't pattern match on a single boolean; use an if-else.Incidentally, the code is better written without duplicating .One arguably better way would be to pattern match on the string directly, not on the result of the comparison, as it avoids \"boolean blindness\". One downside is the need to use backquotes to protect the enteredPassword variable from being shadowed.Basically, you should tend to avoid dealing with booleans as much as possible, as they don't convey any information at the type level.Both statements are equivalent in terms of code semantics. But it might be possible that the compiler creates more complicated (and thus inefficient) code in one case (the ).Pattern matching is usually used to break apart more complicated constructs, like polymorphic expressions or deconstructing (ing) objects into their components. I would not advice to use it as a surrogate for a simple  statement - there's nothing wrong with . Note that you can use it as an expression in Scala. Thus you can writeI apologize for the stupid example.I'v came across same question, and had written tests:timeIf  : Long = 22\ntimeMatch  : Long = 1092                     For the large majority of code that isn't performance-sensitive, there are a lot of great reasons why you'd want to use pattern matching over if/else:Here's an equivalent if/else block implementation:Yes, you can argue that for something as simple as a boolean check you can use an if-expression.  But that's not relevant here and doesn't scale well to conditions with more than 2 branches.If your higher concern is maintainability or readability, pattern matching is awesome and you should use it for even minor things!"},
{"body": "Scala's  trait has a methodBut I sometimes want a different type:Is there a simple way to do this which I am missing? Of course, this can be done with a fold. method iterates though all  pairs. You can use it like this:What about this code:Which produces:This can be packaged into utility method:Usage:With some Scalaz:I like @tenshi's solution better.You could create a utility class:Code not tested but it should work somehow simmilar like that."},
{"body": "Given an Option, what is the idiomatic way to get its value or throw an exception trying?(EDIT: this is not the best or most idiomatic way to do it. I wrote it when I was not familiar with Scala. I leave it here for an example of how  to do it. Nowadays I would do as @TravisBrown)I think it really boils down to two things:If at that point in your code you  the value to be there, and in the remote case that it isn't you want your program to  fast, then I would only do a normal  and let Scala throw a  if there was no value:If you want to handle the case differently (throw your own exception) I think a more elegant way would look like this:And of course if you want to supply your own alternative value for the case that the  is  you would just use :A  \"statement\" is really an expression in Scala, and it has type , which is a subtype of every other type. This means you can just use plain old :You really, really shouldn't be doing this, though.Just use the .get method. It will throw a NoSuchElementException if o is an instance of None. Basically, I would work with options like this:to avoid your specific question.I think  will help you stay on the \"Functional\" side :)About  and "},
{"body": "It seems like the support for printing arrays is somewhat lacking in Scala.  If you print one, you get the default garbage you'd get in Java:Furthermore, you cannot use the Java toString/deepToString methods from the java.util.Arrays class: (or at least I cannot figure it out)The best solution I could find for printing a 2D array is to do the following:Is there a more idiomatic way of doing this?  In Scala 2.8, you can use the  method defined on Array, that returns an IndexedSeq cointaining all of the (possibly nested) elements of this array, and call mkString on that:The IndexedSeq returned does have a stringprefix 'Array' by default, so I'm not sure whether this gives precisely what you wanted.How about this:Adding little more to Arjan's answer - you can use the mkString method to print and even specify the separator between elements.\nFor instance :The \"functional programming\" way to do this (as far as I concern) is:Or if you don't really care about the spacing:IMHO, functional programming can get a little messy, if it takes too long to make this, I'd say just go with the imperative way.You can get neat formatting of Array[Array[Somethings]] with custom separators for the inner as well as the outer array follows:This results in:Try simply this:"},
{"body": "I recently gave up trying to use Scala in Eclipse (basic stuff like completion doesn't work). So now I'm trying IntelliJ. I'm not getting very far.I've been able to edit programs (within syntax highlighting and completion... yay!). But I'm unable to run even the simplest \"Hello World\".  This was the original error:But that was yesterday with IDEA 9.0.1. See below...Today I uninstalled IntelliJ 9.0.1, and installed 9.0.2 Early Availability, with the 4/14 stable version of the Scala plug-in.Then I setup a project from scratch through the wizards:Created a new class:For my efforts, I now have a brand-new error :)Here it is:Scalac internal error: class java.lang.ClassNotFoundException [java.net.URLClassLoader$1.run(URLClassLoader.java:202), java.security.AccessController.doPrivileged(Native Method), java.net.URLClassLoader.findClass(URLClassLoader.java:190), java.lang.ClassLoader.loadClass(ClassLoader.java:307), sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301), java.lang.ClassLoader.loadClass(ClassLoader.java:248), java.lang.Class.forName0(Native Method), java.lang.Class.forName(Class.java:169), org.jetbrains.plugins.scala.compiler.rt.ScalacRunner.main(ScalacRunner.java:72)]I uninstalled 9.0.2 EA and reinstalled 9.0.1, but this time went with the 2.7.3 version of Scala rather than the default 2.7.6, because 2.7.3 is the one shown in the screen-shots at the IntelliJ website (I guess the screen-shots prove that they actually tested this version!). Now everything works!!!I have encountered the same scalac error when trying to run a Scala project in Intellij Idea 9.0.2 and I've managed to find a solution by chance :). These are the steps I took in creating the project and running it.I have created a Scala project in Intellij Idea 9.0.2 final (it was released today). I have installed the Scala plugin, restarted the IDE and created a new Scala project (with the name \"TestScala\") with scala-2.8.0.Beta1 as project library. Once the project is created and the scala libraries downloaded, I have created a Test.scala file with the following content:After that, I created a launch configuration (\"Edit Configurations\"), choosing the \"Application\" template. I set as main class Test and choose the project name (\"TestScala\") in the \"Use classpath and JDK of module\" combo box. When I run the configuration I get the same error as you reported  (\"Scalac internal error: class java.lang.ClassNotFoundException\") .Now comes the freaky part :). I right click on the project, choose \"Module Settings\", have a look on all settings but  . Click \"apply\" and \"ok\", try to run configuration again and  :) .I use Intellij Idea 9.0.2 the final release (build 95-66); Ubuntu 9.10 and JDK 1.6.0_18. I also have to mention that I had a JDK configured in Intellij, otherwise there is an extra step to configure it. When checking the setting of the module, one needs to click on the  and  . Both of these settings are about the scala compiler and scala library location. I would guess these values are not properly set when the project is created but are saved once the user touches them and saves the settings.  To answer your question, it's difficult to get a working IDE for Scala for two reasons:\n(a) Scala is only just beginning to reach a wide audience and\n(b) due to (a), there is no business case for spending time on a Scala IDE.Also, if you are old enough to cast your mind back and young enough to still remember, you would know that for the first five or more years of Java, we were stuck with okay-ish tools like JBuilder that did little more than compile your code when you said so - no error highlighting, no auto-importing, and the word refactoring didn't even exist. If you want to pioneer, you need to be prepared to cut some of the road yourself, or at least bush-bash.I know it won't help you, but I have successfully used IDEA for Scala on Linux, Mac and Windows. I typically have the Scala SDK installed somewhere locally and point IDEA at that rather than using the 'download' option.Presently, I am mostly using an  on Mac OS X with Scala 2.8.0.Beta1-RC5 and it's working well (except that fsc doesn't seem to worked with mixed sources).You could try your luck over at the , though I haven't had a great lot of responses to my own postings there.Installing the plug-in is prerequisite one.The next thing you should do is define a library (global or project-specific; I use global) that holds the Scala library  compiler JAR files (at a minimum, that's  and ). Adding source JARs and a documentation JAR or URLs is a good idea, too. Then make this library a dependency of any modules in your project that include Scala code.Lastly, find the Scala facets in those modules and de-select both check-boxes there.I just did a fresh install and had exactly this same problem myself.\nIt turned out that, because I had created the file in the root package, IDEA had added a package statement at the top with naming a package. I assume that this then got compiled as \"package object Main\" - valid syntax in 2.8? Anyway, I deleted the line that said package and it all worked fine.I had the same problem yesterday while trying to set it up. Solution is pretty simple, you just have to set scala somewhere in project settings.You are mixing code compiled with two different Scala versions.I use Netbeans to write scala programs. So far it works very well with my codes. You can try the plugin here: .I was getting this error and also had to right click on the project and \"Open Module Settings\".  However, it was more than just hitting apply.  I had to make sure that my Content Root was correct for each project.  For some reason, there were some incorrect Source and Test Folders.My project uses maven as the main build tool and importing the project into Intellij is probably what created these incorrect settings.I had similar problem, following  blog post instructions solved the problem for me"},
{"body": "I heard a lot of good things about  and the  Web framework recently, especially from  hence, I might use this technology in my next projects.  Share your experiences on the Scala/Lift Platform.I'm working on my second Lift app at the moment - it's very strongly in Lift's sweet spot - very realtime, lots of concurrency.The first one we wimped out after a few days of wrestling with the DB layer (it's better now, I am led to believe), and went to Play/Scala instead.  That maximized the existing knowledge of our team and made it possible to make deadline.  But the hot code reloading pretty much stopped happening once our project got moderately large (kept running out of PermGen - it's an ongoing problem with Scala compilation pretty much anywhere), and the manual juggling of things like method call parameters and location security in different places in the website got quite cumbersome.  We were glad when it was done - in the same way as I tended to find Rails 1, the speed increases shrank as the project size increased, and by the end it was every bit as tedious and error-prone as working in a Velocity/Spring/XML++ whatever).This time we've been committed to just working out how Lift does what it does, and the right ways to do things.  This has meant a lot of casual browsing through the mailing list (discussions that are several versions old are often still relevant), and most importantly a new ethos for the team.  It's been necessary to internalize very strongly the motto:\"This is feeling hard and repetitive.  I bet they made an easier way to do this.\"So far Lift has never disappointed us.  I'm not talking, by the way, about stuff like the Sitemap and list concatenation syntax - you MUST have a pretty good handle on functional Scala, or you just won't be able to read the source code or even configure your app.That said it's not crazy IO monads or anything, just some common idioms that you'd pick up in a few weeks of Scala anyway.The biggest problem for us has been a slow compile cycle.  It takes about 20 seconds to jetty:run our project, which is a different feeling to Play which (when it's working) hot compiles all your stuff.  On the other hand, we actually timed that the other day when one of our devs complained about it, and it worked out that although Play technically hot compiled it, the page still took 12 seconds to load in Dev mode.  So there's not a huge loss, it just feels a bit slow to have to hop out to the command line.Lift lets you do a great deal, and there are many places in our app where (because it's available), we've said \"Yeah, we really WOULD prefer to have that live updated immediately to all viewers of that page, instead of them discovering later that they're out of date (think of all the times you've posted simultaneously to someone on SO, with the same answer).  COMET is everywhere, it turns out - it's not a specialist use case, it's the way things should work.  And Lift makes it really easy.We also love the strong, programatically configurable, security model - once we switched our mindsets to \"We have to whitelist every location, and specify the necessary entrance conditions\", we never saw another session problem - you know, those ones where you assumed that the user would have traversed a certain path, and thus would know a whole bunch of parameters?  Like, a valid username, and an area of interest or whatever?  (I'm being intentionally vague).  That can be one of the awkward things about a stateful framework, that you're going to want to have usable state when the user hits a page, instead of (for instance) just demanding that all the state gets carried along at each request.My takeaway from this renewed shot at Lift:It's worth it.  Not just to build the app that you're trying to build, but to build the app that you didn't know you needed.There's a lot of head scratching, but not a lot of code.  And when it works it really works.  It's fast, and clean, and for all of the miracles that it's working between the browser and the server, I've never yet seen it get confused.I'm developing enterprise financial application in Lift for more than 6 months and I was JAVA programmer former. I have noticed a few points, which could help you:I can't imagine that I have to return to JAVA code now. But my little advice is to try to code some simple application and you will see."},
{"body": "Are there any tools for performing static analysis of Scala code, similar to FindBugs and PMD for Java or Splint for C/C++? I know that FindBugs works on the bytecode produced by compiling Java, so I'm curious as to how it would work on Scala.Google searches (as of 27 October 2009) reveal very little.Google searches (as of 01 February 2010) reveal this question.FindBugs analyzes JVM byte codes, regardless of the tool that generated them. I've tried using FindBugs to check .class files generated by Scala. Unfortunately, FindBugs produced many warnings, even for trivial Scala programs.There is now  which does the job that Checkstyle does for Java. This includes not only formatting checks, but also some checks for known sources of bugs, such as a class which implements hashCode() but not equals.There are currently about 40 checks, but we're adding them all of the time.For more information, see .There is some work going on in that direction. Some links:There is also a discussion on scala mail list, archive available .Here is an updated answer as of August 2014 for some that are aimed or work well with Scala. Personally I think the JVM or Java ones end up with far too many false positives, or have inspections that are aimed mostly at Java specific classes. For example, since in Scala we don't tend to use the Java Collections, all the findbugs collection based inspections are not needed. Another example is the inspections for use of static fields which are irrelevant in Scala.Findbugs and other tools that are bytecode based will work, in the sense that they will find faults in your code.  Unfortunately, the bytecode based approaches have been tuned against the output of the javac compilers, meaning they are likely to produce very high false positive rates, and miss basic issues, because Scala will be producing different idioms than the javac compiler.   I'm having a lot of fun with Codacy (e.g. ) for Open Source projectsThere is a SBT plugin for PMD copy paste detector CPD. scala copy paste detector, based on AST. Looking for copy-pasted subtrees/ASTs, and inform about it.It's plugin for sbt. I don't know much about Scala but if is Java compatible Klocwork's Solo product might work.  You can find it here "},
{"body": "Is it always more performant to use withFilter instead of filter, when afterwards applying       functions like map, flatmap etc.?Why are only map, flatmap and foreach supported? (Expected functions like forall/exists as well)  From the scala docs:So  will take the original collection and produce a new collection, but  will non-strictly (ie. lazily) pass unfiltered values through to later map/flatMap/withFilter calls, saving a second pass through the (filtered) collection. Hence it will be more efficient when passing through to these subsequent method calls.In fact,  is specifically designed for working with chains of these methods, which is what a for comprehension is de-sugared into. No other methods (such as /) are required for this, so they have not been added to the  return type of . Using for yield can be a work around, for example:As a workaround, you can implement other functions with only  and .Moreover, this optimisation is useless on small collections\u2026"},
{"body": "There's not much info in the spec on what type ascription is, and there certainly isn't anything in there about the purpose for it.  Other than \"making passing varargs work\", what would I use type ascription for?  Below is some scala REPL for the syntax and effects of using it.Type ascription is just telling the compiler what type you expect out of an expression, from all possible valid types.A type is valid if it respects existing constraints, such as variance and type declarations, and it is either one of the types the expression it applies to \"\", or there's a conversion that applies in scope.So, , therefore any  is also an . In your example you declared you want the expression  to be treated as an , not a . Since there is no constraints preventing that and the desired type is one of the types  , it works.Now, why would you want that? Consider this:You could also have fixed this by type ascripting  at  declaration, or you could have declared 's type to be .However, type declarations achieve the same thing only as long as you are assigning a value to an identifier. Which one can always do, of course, if one doesn't care about littering the code with one-shot identifiers. For example, the following does not compile:But this does:It would be silly to use an identifier here in place of . And though I could just write  instead, that isn't always an option. Consider this, for instance:For the reference, this is what Scala 2.7 spec (march 15, 2009 draft) has to say about type ascription:One possibility is when network and serial protocol level stuff, then this:is far cleaner thanThe second form is also a runtime conversion (not handled by the compiler) and could lead to some interesting over/underflow conditions.I use type ascription to paper over holes in Scala's type inference. For example, foldLeft over a collection of type A takes an initial element of type B and a function (B, A) => B that is used to fold the elements of the collection into the initial element. The actual value of type B is inferred from the type of the initial element. Since Nil extends List[Nothing], using it as an initial element causes problems:Alternatively, you could just use List.empty[Int] instead of Nil:List[Int].edit: List.empty[A] is implemented as This is effectively a more verbose form of Nil:List[A]You may find  illuminating, if a bit convoluted to follow.  The important thing to note is that you're adding constraint hints to the type checker - it gives you a little more control over what that compilation phase is doing.Type Inference: We can skip Explicitly giving Name of Type of Something in source code, called Type Inference.(Although required in some exceptional cases.)Type Ascription: Being explicit about the type of something is called a Type Ascription.\nWhat Difference It can make?ex: val x = 2 : Bytealso see:\n1. We can explicitly give return type to our functionsAnother way of declaring this could be:Here we did not give  return type explicitly and Compiler inferred it as .\nWhy  is because we used type ascription in the definition.This time We did not explicitly tell the compiler anything(neither this defi). And It inferred our definition as Some[None.type]"},
{"body": "I have a folder structure like below:and in that folders I have my files that I have to read.\nHere is the code:Resources in Scala work exactly as they do in Java. \nIt is best to follow the  and put all resources in  and .Example folder structure:To read resources the object  provides the method .To read resources you can use  and  .Keep in mind that  also works fine when the resources are part of a , , which returns a URL which is often used to create a file can lead to problems there. To avoid undebuggable Java NPEs (Java is so horrible), consider:To get meaningful FNFEs instead.For production code I would also suggest to make sure that the source is closed again.For Scala >= 2.12, use :"},
{"body": "I used play2 before with java. It felt a little bit like boilerplate especially if you used akka with java. But that is not the fault of the framework.Yesterday I read \"Scala for the impatient\" and I really enjoy the language.Now I looked at both frameworks Lift 2.5 and Play 2.0.3. I think lift has a higher learning curve and I could not just do something with lift. This is not a con for me. From what I saw, Lift has a very nice and clean design. But for me it is hard to tell what the main differences are. I think both frameworks are great.Posting this after spending a week or two with Lift doesn't really\nserve anybody's interests. However, I want to spend some time correcting\nsome mistakes and mis-perceptions.You're dead wrong. Security is the job of the framework. It's critical that security\nis done by default rather than relying on each developer to understand every\nsecurity vulnerability and make sure every line of code takes that into account.All we have to do is look at what happened to \nto understand that even the best coders using well known technology\ncan make a critical mistake.Lift gives a solid security layer on top, so by default, there's no XSS, CSRF, etc.\nbut the developer can dig down as deep as he wants to the HTTP request and deal\nwith the bytes on the wire.Lift is very clear about where you need state and where you don't. Lift can support\nstateless, partially stateful, and completely stateful apps. On a page-by-page and\nrequest-by-request basis, a Lift app can be stateful or stateless (for example,\nin , the venue pages are stateless for\nsearch engine crawls, but stateful for browsers that are logged in.) For\nmore on the design decisions around state, please see .Lift uses Maven, sbt, Buildr, and even Ant. Lift is agnostic about the build environment\nand about the deploy environment (Java EE container, Netty, whatever). This is important\nbecause it make Lift easier to integrate with the rest of your environment.Lift has been around for 5+ years and has a lot of modules and stuff for it. The Lift web framework (as distinguished from the modules) is agnostic about persistence, authentication, etc., so you can use anything with Lift.Lift has had Async support for more than 5 years. It's baked into the framework. Lift's Comet support is  because,\namong other things, it multiplexes all the \"push\" requests on a page through a single request\nto the server which avoids connection starvation. How Lift does async is a whole lot\nless important because one of the core philosophies with Lift is that we remove the\nplumbing from the developer so the developer can focus on business logic.But for those who care, Lift has the best and lightest weight actors of any framework in\nScala-land. We were the first to break away from the Scala Actor's library and worked\nto blaze the trail for different Actor libraries that allowed Akka and ScalaZ Actors\nto flourish.This is part of Lift's commitment to security. It's .Lift apps can be as stateful or as stateless as you want. It's your choice and Lift\nmakes very clear how to make the decision.Also, as I pointed out in the Lift, State, and Scaling post, making the developer figure out\nhow to serialize state in a secure, scalable, performant way\n(because virtually every request on a web\napp that recognizes specific users is stateful) should be done in a predictable,\nsecure way by the framework with reasonable overrides for the developers.Play is a lot like Rails: it's quick to get a site knocked together and it's \nbased on MVC, so a lot of developers understand it. But Play lacks the\ndepth and breadth of Rails (community, plugins, expertise, talent, etc.) If you\nwant quick, easy MVC, then go with Rails and JRuby and write your\nback end in Scala (they work together extraordinarily well.)Lift is a different beast. There's a significant unlearning curve (stop thinking\nMVC and start thinking about user experience first that flows to business logic.)\nBut once you're up the unlearning curve, Lift sites are more secure, highly\nscalable, super-interactive, and much easier to maintain over time.To see what can be done with Play (it can be cool), have a look at the To get going quick with Lift, use a .For a sample of using Mongo with Play, look at .In summary, I don't think you will go wrong with either Lift or Play. Both are active projects, with good communities and good backing from the authors. It really depends on your business problem. If tool support are important to you, then you may want to look at using Play (it's well supported on IntelliJ Idea).Take note that Play, being part of the TypeSafe technology stack, will have builds up to the latest versions of Scala, so if using Scala 2.10 features are important to you, then you may want to keep that in mind. Lift is currently using Scala 2.9.2, which is fine also.For my current project I use lift-mapper for ORM (It's great and rock solid), with Spray for REST (which is simply amazing). This approach avoids frameworks altogether, but it depends on what you want to do. Frameworks are quite often the way to go."},
{"body": "I'm following the great so i'm trying at 46m:00s to load the  but fail to what i'm doing is this:how can I load that ?Try explicitly specify . The error occurs when Hadoop environment is set. SparkContext.textFile internally calls , which in turn uses  if schema is absent. This method reads \"fs.defaultFS\" parameter of Hadoop conf. If you set HADOOP_CONF_DIR environment variable, the parameter is usually set as \"hdfs://...\"; otherwise \"file://\".gonbe's answer is excellent. But still I want to mention that  = , not . Hope this could save some time for newbs like me.You need just to specify the path of the file as example:This has been discussed into spark mailing list, and please refer this .You should use  copy the file into :  I have a file called NewsArticle.txt on my Desktop. In Spark, I typed:I needed to change all the \\ to / character for the filepath. To test if it worked, I typed:I'm running Windows 7 and I don't have Hadoop installed. This is the solution for this error that i was getting on Spark cluster that is hosted in Azure on a windows cluster:Load the raw HVAC.csv file, parse it using the functionWe use (wasb:///) to allow Hadoop to access azure blog storage file and the three slashes is a relative reference to the running node container folder.For example: If the path for your file in File Explorer in Spark cluster dashboard is:sflcc1\\sflccspark1\\HdiSamples\\SensorSampleData\\hvacSo to describe the path is as follows: sflcc1: is the name of the storage account. sflccspark: is the cluster node name.So we refer to the current cluster node name with the relative three slashes.Hope this helps.If the file is located in your Spark master node (e.g., in case of using AWS EMR), then launch the spark-shell in local mode first.Alternatively, you can first copy the file to HDFS from the local file system and then launch Spark in its default mode (e.g., YARN in case of using AWS EMR) to read the file directly.try "},
{"body": "I frequently find myself working with Lists, Seqs, and Iterators of Tuples and would like to do something like the following,However, the compiler never seems to agree with this syntax.  Instead, I end up writing,Which is just silly.  How can I get around this?A work around is to use  :I like the tupled function; it's both convenient and not least, type safe:Why don't you use If you need the parameters multiple time, or different order, or in a nested structure, where _ doesn't work, seems to be a short, but readable form.Another option is"},
{"body": "In Java you can write . However in Scala,  is hidden by  which lacks this function. It's easy enough to switch to using the original Java version of a boolean, but that just doesn't seem right.So what is the one-line, canonical solution in Scala for extracting  from a string?Ah, I am silly. The answer is .How about this:If the input string does not convert to a valid Boolean value  is returned as opposed to throwing an exception.  This behavior more closely resembles the Java behavior of .Note: Don't write  in Java - always use . Using the  variant unnecessarily creates a  object; using the  variant doesn't do this.The problem with  is that it will throw an exception if  isn't exactly one of  or  (even extra white space in the string will cause an exception to be thrown).If you want exactly the same behaviour as , then use it fully qualified, or import Boolean under a different name, eg, . Or write your own method that handles your own particular circumstances (eg, you may want  to be  as well).I've been having fun with this today, mapping a 1/0 set of values to boolean. I had to revert back to Spark 1.4.1, and I finally got it working with:where p(11) is the dataframe fieldmy previous version didn't have the \"Try\", this works, other ways of doing it are available ... "},
{"body": "I have installed sbt on Ubuntu.However, whenever I try to launch sbt (from the same directory where sbt is located) it does not work:I am new to linux and I have no idea how to tackle this issue.It seems like you installed a zip version of sbt, which is fine. But I suggest you install the native debian package if you are on Ubuntu. That is how I managed to install it on my Ubuntu 12.04. Check it out here:\n\nOr simply directly download it from .The simplest way of installing SBT on ubuntu is the  package provided by Typesafe.Run the following shell commands:And you're done !It's saying that  is not on your path. Try to run  from  or wherever the  executable is to verify that it runs correctly. Also check that you have execute permissions on the  executable. If this works , then add  to your path and  should run from anywhere.See this  about adding a directory to your path.To verify the  use the  command on LINUX. The output will look something like this:Lastly, to verify  is working try running  or likewise. The output with -help will look something like this:As an alternative approach, you can save the  script to a file called sbt.sh and set the permission to executable. Then add this file to your path, or just put it under your ~/bin directory.The bonus here, is that it will download and use the correct version of SBT depending on your project properties. This is a nice convenience if you tend to compile open source projects that you pull from GitHub and other.My guess is that the directory ~/bin/sbt/bin is not in your PATH.To execute programs or scripts that are in the current directory you need to prefix the command with ./, as in:This is a security feature in linux, so to prevent overriding of system commands (and other programs) by a malicious party dropping a file in your home directory (for example).  Imagine a script called 'ls' that emails your /etc/passwd file to 3rd party before executing the ls command...  Or one that executes 'rm -rf .'...That said, unless you need something specific from the latest source code, you're best off doing what paradigmatic said in his post, and install it from the Typesafe repository."},
{"body": "I'm trying to make a EnumListField in Lift/Record/Squeryl, similar to MappedEnumList in LiftMapper. The storage type should be Long/BIGINT. I understand that if I define:Then Squeryl will know it should create a BIGINT column. And I know it uses setFromAny() to set the value, passing in the Long. The one piece I don't get is:How will it read the field's value? If it uses valueBox, it will get a Seq[Enum#Value], and it won't know how to turn that into a Long. How do I tell Squeryl to convert my Seq[Enum#Value] to a Long, or define a \"getter\" that returns a Long, and that doesn't conflict with the \"normal\" getter(s)?"},
{"body": "I am looking for a high-performance, concurrent, MultiMap. I have searched everywhere but I simply cannot find a solution that uses the same approach as ConcurrentHashMap (Only locking a segment of the hash array).The multimap will be both read, added to and removed from often.The multimap key will be a String and it's value will be arbitrary.I need O(1) to find all values for a given key, O(N) is OK for removal, but O(logN) would be preferred.It is crucial that removal of the last value for a given key will remove the container of values from the key, as to not leak memory.HERE'S THE SOLUTION I BUILT, availbable under ApacheV2:\nWhy not wrap ConcurrentHashMap[T,ConcurrentLinkedQueue[U]] with some nice Scala-like methods (e.g. implicit conversion to Iterable or whatever it is that you need, and an update method)?Have you tried Google Collections?  They have various  implementations.There is  although I haven't used it.I made a  mixin which extends the mutable.MultiMap mixin and has a concurrent.Map[A, Set[B]] self type.  It locks per key, which has O(n) space complexity, but its time complexity is pretty good, if you aren't particularly write-heavy.you should give  a try. here is the .I had a requirement where I had to have a  where insertion on the Map be concurrent and also on the corresponding Set, but once a Key was consumed from the Map, it had to be deleted, think if as a Job running every two seconds which is consuming the whole  from an specific Key but insertion be totally concurrent so that most values be buffered when the Job kicks in, here is my implementation: I use Guava's helper class Maps to create the concurrent Maps, also, this solution emulates :I am a bit late on this topic but I think, nowadays, you can use Guava like this:Have you taken a look to  which is intended for Real time etc. and of course high performance.It's late for the discussion, yet...When it comes to high performance concurrent stuff, one should be prepared to code the solution. \nWith Concurrent the statement  has a complete meaning. \nIt's possible to implement the structure fully concurrent and lock-free. Starting base would be the NonBlocking Hashtable  and then depending how many values per key and how often need to add/remove some copy on write Object[] for values or an array based Set with semaphore/spin lock."},
{"body": "I am new to Scala started learning the language for fun and I am still trying to get my head around it. My understanding of Scala traits is that they are like java interfaces except that some methods can have an implementation.Java 8 is adding interfaces that can have default methods where an implementation can be provided. What are the similarities and differences between Java 8 interfaces and Scala traits?Motivations for  and  differ.The former was introduced to support safe API evolution and a limited form of multiple inheritance. With leveraging functional programming idioms in Project Lambda it's been beneficial to add, for example, a  method to  interface without altering all possible implementers (which is actually impossible to do without breaking backward compatibility). As a side effect this also offered a form of . were designed from scratch as building blocks for modular components composition. They are multiple inheritance friendly and don't have  by having strict rules on evaluation order of mix-ins due to linearization. They also support state, can reference the implementing class and place restrictions on which type can mix-in them. Look at Scala collections library where traits are used thoroughly.Note that with ,  now compiles to an interface.\nScala 2.12 is all about making optimal use of Java 8\u2019s new featuresSee  more the difference of implementation."},
{"body": "I use 's iteratees in a number of projects, primarily for processing large-ish files. I'd like to start switching to Scalaz , which are designed to replace the iteratee package (which frankly is missing a lot of pieces and is kind of a pain to use).Streams are based on  (another variation on the iteratee idea), which have  in Haskell. I've used the Haskell machines library a bit, but the relationship between machines and streams isn't completely obvious (to me, at least), and the documentation for the streams library is .This question is about a simple parsing task that I'd like to see implemented using streams instead of iteratees. I'll answer the question myself if nobody else beats me to it, but I'm sure I'm not the only one who's making (or at least considering) this transition, and since I need to work through this exercise anyway, I figured I might as well do it in public.Supposed I've  got a file containing sentences that have been tokenized and tagged with parts of speech:There's one token per line, words and parts of speech are separated by a single space, and blank lines represent sentence boundaries. I want to parse this file and return a list of sentences, which we might as well represent as lists of tuples of strings:As usual, we want to fail gracefully if we hit invalid input or file reading exceptions, we don't want to have to worry about closing resources manually, etc.First for some general file reading stuff (that really ought to be part of the iteratee package, which currently doesn't provide anything remotely this high-level):And then our sentence reader:And finally our parsing action:We can demonstrate that it works:And we're done.More or less the same program implemented using Scalaz streams instead of iteratees.A scalaz-stream solution: We can demonstrate that it works:And we're done.The  function is a common Scalaz combinator. In this case it's being used to traverse, in the  monad, the sentence  generated by . It's equivalent to  followed by ."},
{"body": "Where can differences between a class and a type be observed in Scala and why is this distinction important?Is it only a consideration from the language design point-of-view or has it \"practical\" impact when programming Scala?Or is it fundamental to \"securing the boundaries\" of the type system (,  come to my mind)?How many of the considerations/differences/problems mentioned above can also be recognized in Java?(See  as a language-agnostic introduction.)When you say \"type\" I'm going to assume you mean static type mostly.  But I'll talk about dynamic types shortly.A static type is a property of a portion of a program that can be statically proven (static means \"without running it\").  In a statically typed language, every expression has a type whether you write it or not.  For instance, in the Cish \"int x = a * b + c - d\", a,b,c,and d have types, a * b has a type, a * b + c has a type and a * b + c -d has a type.  But we've only annotated x with a type.  In other languages, such as Scala, C#, Haskell, SML, and F#, even that wouldn't be necessary.Exactly what properties are provable depends on the type checker.A Scala style class, on the other hand, is just the specification for a set of objects.  That specification includes some type information and includes a lot of implementation and representation details such as method bodies and private fields, etc.  In Scala a class also specifies some module boundaries.Many languages have types but don't have classes and many languages have classes but don't have (static) types.There are several observable differences between types and classes. List[String] is a type but not a class. In Scala List is class but normally not a type (it's actually a higher kinded type).  In C# List isn't a type of any sort and in Java it's a \"raw type\". Scala offers structural types.  {def foo : Bar} means any object that provably has a foo method that returns a Bar, regardless of class.  It's a type, but not a class.Types can be abstracted using type parameters.   When you write def foo[T](x : T) = ..., then inside the body of foo T is a type.  But T is not a class.Types can be virtual in Scala (i.e. \"abstract type members\"), but classes can't be virtual with Scala today (although there's a boilerplate heavy way to encode virtual classes )Now, dynamic types.  Dynamic types are properties of objects that the runtime automatically checks before performing certain operations.  In dynamically typed class-based OO languages there's a strong correlation between types and classes.  The same thing happens on JVM languages such as Scala and Java which have operations that can only be checked dynamically such as reflection and casting.  In those languages, \"type erasure\" more or less means that the dynamic type of most objects is the same as their class.  More or less.  That's not true of, e.g., arrays which aren't typically erased so that the runtime can tell the difference between Array[Int] and Array[String].  But remember my broad definition \"dynamic types are properties of objects that the runtime automatically checks.\"  When you use reflection it is possible to send any message to any object.  If the object supports that message then everything works out.  Thus it makes sense to talk of all objects that can quack like a duck as a dynamic type, even though it's not a class.  That's the essence of what the Python and Ruby communities call \"duck typing.\"  Also, by my broad definition even \"zeroness\" is a dynamic type in the sense that, in most languages, the runtime automatically checks numbers to make sure you don't divide by zero.  There are a very, very few languages that can prove that statically by making zero (or not-zero) a static type.Finally, as other's have mentioned, there are types like int which don't have a class as an implementation detail, types like Null and Any which are a bit special but COULD have classes and don't, and types like Nothing which doesn't even have any values let alone a class.Okay, I'll bite...  James has a good answer, so I'm going to try a different tact and give a more down-to-earth viewpoint.Broadly speaking, a class is . singleton objects (scala) traits (Scala) and interfaces (Scala) are also commonly considered to be classes.  This makes sense, as singletons are still instantiated (via compiler-generated code) and an interface can be instantiated as part of a subclass.Which brings us to the second point.  classes are the primary unit of design in most object-oriented languages (though not the prototype-based ones like javascript).  Polymorphism and subclassing are both defined in terms of classes.  classes also provide a namespace and visibility controls.types are a very different beast, every possible value that the system can express will have one or more types, and these can sometimes be equated to classes, for example:You also get some interesting differences between Scala and Java:and the really fun types that aren't classes at all.  For example,  always refers to the unique type of .  It's unique to a single instance and isn't even compatible with other instances of the same class.There are also abstract types and type parameters.  For example: is interesting as it's a class, but not a type.  More accurately, it's a \"type constructor\"; something that will construct a valid type when supplied with the necessary type parameter. Another term for type constructors is \"higher kinded types\", I personally don't like this term, as \"type constructor\" encourages me to think in terms of supplying types like any other form of argument - a mental model that has served me well for Scala.\"higher-kinded\" rightly implies that  has a \"kind\", which is , this notation states that  will take a single type and yield a single type (this is similar to curried notation for describing functions). By way of comparison, the kind of  is  because it takes two type parameters.A type can be useful by itself, without any instances. One example for this is called \"phantom type\". Here is an example for Java: In that example we have , where  and  take some types (represented by the abstract classes  and ), without ever beeing instantiated. I hope this shows that types and classes are something different, and that types can be useful by itself.(Java only) I'd say, a type is a set of objects. Object  is type , if  is a member of set . Type  is a  of , if set  is a  of . For every class C (not interface) there is a set of objects, created from . Interestingly, we rarely cares about this set. (but every object does belong to a set like this, a fact that may be useful)For every class C, there is a type , generally refered to as \"the type C\", which is the set of all objects that can be created from  where S is C or a subclass of C. Similarly, for every interface I, there is a type , \"the type I\", which is the set of all objects that can be created from  where S implements I.Obviously, if class  is a subclass of , type S is a subtype of type C. Similar for interface There is a null type, which is the empty set. The null type is a subtype of every type.There is a set of all objects, which is the type Object. It's a super type of every type.So far, this formalism is pretty useless. A type is basically the same as a class or an interface, and the subtype relation is basically the subclass/subinterface relation. The triviality is a good thing, the language was understandable! But entering generics, there are more complicated types, and operations like unions and intersections of types. Types are no longer only classes and interfaces, and subtype relations are much richer and harder to understand."},
{"body": "I was going through the  and it mentions on slide 10 to never use  in a  for abstract members and use  instead. The slide does not mention in detail why using abstract  in a  is an anti-pattern. I would appreciate it if someone can explain best practice around using val vs def in a trait for abstract methods  A  can be implemented by either of a , a , a  or an . So it's the most abstract form of defining a member. Since traits are usually abstract interfaces, saying you want a  is saying  the implementation should do. If you ask for a , an implementing class cannot use a .A  is needed only if you need a stable identifier, e.g. for a path-dependent type. That's something you usually don't need.Compare:If you hadyou wouldn't be able to define  or .Ok, and to confuse you and answer @om-nom-nom\u2014using abstract s can cause initialisation problems:This is an ugly problem which in my personal opinion should go away in future Scala versions by fixing it in the compiler, but yes, currently this is also a reason why one should not use abstract s. (Jan 2016): You are allowed to override an abstract  declaration with a  implementation, so that would also prevent the initialisation failure.I prefer not use  in traits because the val declaration has unclear and non-intuitive order of initialization. You may add a trait to already working hierarchy and it would break all things that worked before, see my topic: You should keep all things about using this val declarations in mind which eventually road you to an error.But there are times when you could not avoid using . As @0__ had mentioned sometimes you need a stable identifier and  is not one.I would provide an example to show what he was talking about:This code produces the error:If you take a minute to think you would understand that compiler has a reason to complain. In the  case it could not derive return type by any means.  means that it could be implemented in broad way. It could return different holders for each call and that holders would incorporate different  types. But Java virtual machine expects the same type to be returned.Always using def seems a bit awkward since something like this won't work:You will get the following error:"},
{"body": "When using Scala in Spark, whenever I dump result out using saveAsTextFile, it seems to split the output into multiple part. I'm just passing a parameter(path) to it. does the number of output correspond to the number of reducer it uses? \ndoes this mean the output is compressed? I know I can combine the output together using bash, but is there an option to store the output in a single text file, without splitting?? I looked at the API docs, but it doesn't say much about this.The reason it saves it as multiple files is because the computation is distributed. If the output is small enough such that you think you can fit it on one machine, then you can end your program with And then save the resulting array as a file, Another way would be to use a custom partitioner, , and make it so everything goes to one partition though that isn't advisable because you won't get any parallelization.If you require the file to be saved with  you can use . This basically means do the computation then coalesce to 1 partition. You can also use  which is just a wrapper for  with the shuffle argument set to true. Looking through the source of  is how I figured most of this stuff out, you should take a look.You could call  and then  - but it might be a bad idea if you have a lot of data. Separate files per split are generated just like in Hadoop in order to let separate mappers and reducers write to different files. Having a single output file is only a good idea if you have very little data, in which case you could do collect() as well, as @aaronman said.You will be able to do it in the next version of Spark, in the current version 1.0.0 it's not possible unless you do it manually somehow, for example, like you mentioned, with a bash script call. In Spark 1.6.1 the format is as shown below. It creates a single output file.It is best practice to use it if the output is small enough to handle.Basically what it does is that it returns a new RDD that is reduced into numPartitions partitions.If you're doing a drastic coalesce, e.g. to numPartitions = 1, this may result in your computation taking place on fewer nodes than you like (e.g. one node in the case of numPartitions = 1)As others have mentioned, you can collect or coalesce your data set to force Spark to produce a single file. But I prefer to let it create a hundred files in the output HDFS directory, then use  to extract the results into a single file in the local filesystem. This makes the most sense when your output is a relatively small report, of course.I also want to  mention that the documentation clearly states that users should be careful when calling coalesce with a real small number of  partitions  . this can cause upstream partitions to inherit this number of partitions.I would not recommend  using coalesce(1) unless really required. "},
{"body": "In Martin Odersky's  in Scala, in the  section, he includes the term .These are not mentioned in . What are they?Early initializers are part of the constructor of a subclass that is intended to run before its superclass. For example:If the code was written instead asthen a null pointer exception would occur when  got initialized, because  is initialized before  in the normal ordering of initialization (superclass before class).For syntax and motivation look here:  (they are called \"early definitions\" there, but it's the same)As far as I can tell, the motivation (as given in the link above) is:\"Naturally when a val is overridden, it is not initialized more than once. So though x2 in the above example is seemingly defined at every point, this is not the case: an overridden val will appear to be null during the construction of superclasses, as will an abstract val.\"I don't see why this is natural at all. It is completely possible that the r.h.s. of an assignment might have a side effect. Note that such code structure is completely impossible in either C++ or Java (and I will guess Smalltalk, although I can't speak for that language). In fact you have to make such dual assignments implicit...ticilpmi...EXplicit in those languages via constructors. In the light of the r.h.s. side effect uncertainty, it really doesn't seem like much of a motivation at all: the ability to sidestep superclass side effects (thereby voiding superclass invariants) via ASSIGNMENT? Ick!Are there other \"killer\" motivations for allowing such unsafe code structure? Object-oriented languages have done without such a mechanism for about 40 years (30-odd years, if you count from the creation of the language), why include it now?It...just...seems...dangerous.On second thought, a year layer...This is just cake. Literally.Not an early ANYTHING. Just cake (mixins).Cake is a term/pattern coined by The Grand Pooh-bah himself, one that employs Scala's trait system, which is halfway between a class and an interface. It is far better than Java's decoration pattern.The so-called \"interface\" is merely an unnamed base class, and what used to be the base class is acting as a trait (which I frankly did not know could be done). It is unclear to me if a \"with'd\" class can take arguments (traits can't), will try it and report back.This question and its answer has stepped into one of Scala's coolest features. Read up on it and be in awe."},
{"body": "I wrote this in scala and it won't compile:the compiler notify:I know JVM has no native support for generics so I understand this error.I could write wrappers for  and  but I'm lazy :)I'm doubtful but, is there another way expressing  is not the same type than ?Thanks.I like Michael Kr\u00e4mer's idea to use implicits, but I think it can be applied more directly:I think this is quite readable and straightforward.There is another easy way which seems to work:For every version you need an additional type parameter, so this doesn't scale, but I think for three or four versions it's fine.For exactly two methods I found another nice trick:Instead of inventing dummy implicit values, you can use the  defined in  which seems to be made exactly for that:Due to the wonders of type erasure, the type parameters of your methods' List get erased during compilation, thus reducing both methods to the same signature, which is a compiler error.To understand , it's necessary to recognize that the types of the implicit parameters are unimportant. What  important is that their types are distinct.  The following code works in the same way:At the bytecode level, both  methods become two-argument methods since JVM bytecode knows nothing of implicit parameters or multiple parameter lists. At the callsite, the Scala compiler selects the appropriate  method to call (and therefore the appropriate dummy object to pass in) by looking at the type of the list being passed in (which isn't erased until later).While it's more verbose, this approach relieves the caller of the burden of supplying the implicit arguments.  In fact, it even works if the dummyN objects are private to the  class.As Viktor Klang already says, the generic type will be erased by the compiler. Fortunately, there's a workaround:Thanks for  for the tip!If I combine s   and s response here  I get: I get a typesafe(ish) variant The logic may also be included in the type class as such (thanks to ): \n    @annotation.implicitNotFound(msg = \"Foo does not support ${T} only Int and String accepted\")\n    sealed trait Foo[T] { def apply(list : List[T]) : Unit }Which gives: Note that we have to write  since the compiler thinks that   means that we call  with parameters.There is (at least one) another way, even if it is not too nice and not really type safe:The implicit manifest paramenter can be used to \"reify\" the erased type and thus hack around erasure. You can learn a bit more about it in many blog posts,e.g. .What happens is that the manifest param can give you back what T was before erasure. Then a simple dispatch based on T to the various real implementation does the rest.Probably there is a nicer way to do the pattern matching, but I haven't seen it yet. What people usually do is matching on m.toString, but I think keeping classes is a bit cleaner (even if it's a bit more verbose). Unfortunately the documentation of Manifest is not too detailed, maybe it also has something that could simplify it. A big disadvantage of it is that it's not really type safe: foo will be happy with any T, if you can't handle it you will have a problem. I guess it could be worked around with some constraints on T, but it would further complicate it. And of course this whole stuff is also not too nice, I'm not sure if it worth doing it, especially if you are lazy ;-)Instead of using manifests you could also use dispatchers objects implicitly imported in a similar manner. I blogged about this before manifests came up: This has the advantage of type safety: the overloaded method will only be callable for types which have dispatchers imported into the current scope. I tried improving on Aaron Novstrup\u2019s and Leo\u2019s answers to make one set of standard evidence objects importable and more terse.But that will cause the compiler to complain that there are ambiguous choices for the implicit value when  calls another method which requires an implicit parameter of the same type.Thus I offer only the following which is more terse in some cases. And this improvement works with value classes (those that ).If the containing type name is rather long, declare an inner  to make it more terse.However, value classes do not allow inner traits, classes, nor objects. Thus also note Aaron Novstrup\u2019s and Leo\u2019s answers do not work with a value classes.Nice trick I've found from \nby Aaron Novstrup[...]I didn't test this, but why wouldn't an upper bound work?Does the erasure translation to change from foo( List[Any] s ) twice, to foo( List[String] s ) and foo( List[Int] i ):I think I read that in version 2.8, the upper bounds are now encoded that way, instead of always an Any.To overload on covariant types, use an invariant bound (is there such a syntax in Scala?...ah I think there isn't, but take the following as conceptual addendum to the main solution above):then I presume the implicit casting is eliminated in the erased version of the code.UPDATE: The problem is that JVM erases more type information on method signatures than is \"necessary\". I provided a link. It erases type variables from type constructors, even the concrete bound of those type variables. There is a conceptual distinction, because there is no conceptual non-reified advantage to erasing the function's type bound, as it is known at compile-time and does not vary with any instance of the generic, and it is necessary for callers to not call the function with types that do not conform to the type bound, so how can the JVM enforce the type bound if it is erased? Well  says the type bound is retained in metadata which compilers are supposed to access. And this explains why using type bounds doesn't enable overloading. It also means that JVM is a wide open security hole since type bounded methods can be called without type bounds (yikes!), so excuse me for assuming the JVM designers wouldn't do such an insecure thing.At the time I wrote this, I didn't understand that stackoverflow was a system of rating people by quality of answers like some competition over reputation. I thought it was a place to share information. At the time I wrote this, I was comparing reified and non-reified from a conceptual level (comparing many different languages), and so in my mind it didn't make any sense to erase the type bound."},
{"body": "Why does this construction cause a Type Mismatch error in Scala?If I switch the Some with the List it compiles fine:This also works fine:For comprehensions are converted into calls to the  or  method. For example this one:becomes that:Therefore, the first loop value (in this case, ) will receive the  method call. Since  on a  returns another , the result of the for comprehension will of course be a . (This was new to me: For comprehensions don't always result in streams, not even necessarily in s.)Now, take a look at how  is declared in :Keep this in mind. Let's see how the erroneous for comprehension (the one with ) gets converted to a sequence of map calls:Now, it's easy to see that the parameter of the  call is something that returns a , but not an , as required.In order to fix the thing, you can do the following:That compiles just fine. It is worth noting that  is not a subtype of , as is often assumed.An easy tip to remember,  will try to return the type of the collection of the first generator, Option[Int] in this case. So, if you start with  you should expect a result of Option[T]. If you want a result of  type, you should start with a List generator.Why have this restriction and not assume you'll always want some sort of sequence? You can have a situation where it makes sense to return . Maybe you have an  that you want to combine with something to get a , say with the following function: ; you could then write this and get None when things don't \"make sense\":How  are expanded in the general case are in fact a fairly general mechanism to combine an object of type  with a function  to get an object of type . In your example, M can be Option or List. In general it has to be the same type . So you can't combine Option with List. For examples of other things that can be , look at .Why did combining  with  work though when you started with the List? In this case the library use a more general type where it makes sense. So you can combine List with Traversable and there is an implicit conversion from Option to Traversable.The bottom line is this: think about what type you want the expression to return and start with that type as the first generator. Wrap it in that type if necessary. It probably has something to do with Option not being an Iterable. The implicit  will handle the case where compiler is expecting second to be an Iterable. I expect that the compiler magic is different depending on the type of the loop variable."},
{"body": "How can I convert an  to a 7-character long , so that  is turned into ?The Java library has pretty good (as in excellent)  which is accessible from  enriched String class:: as suggested by fommil, from 2.10 on, there is also a formatting string interpolator:Scala  (which contains a nice set of methods that Scala string objects have because of implicit conversions) has a  method, which appends a certain amount of characters to your string. For example:Will return \"alohaaaaaa\" (actually it will return a Vector but it's not important for this case).Your problem is a bit different since you need to  characters instead of  them. That's why you need to reverse the string, append the fill-up characters (you would be prepending them now since the string is reversed), and then reverse the whole thing again to get the final result.Hope this helps!huynhjl beat me to the right answer, so here's an alternative:The  is denoted by  for  to be prefixed to make the length :In case this Q&A becomes the canonical compendium,Do you need to deal with negative numbers? If not, I would just doorOtherwise, you can use :"},
{"body": "How large is Scala's thread pool for futures?My Scala application makes many millions of s and I wonder if there is anything I can do to optimize them by configuring a thread pool.Thank you.You can specify your own ExecutionContext that your futures will run in, instead of importing the global implicit ExecutionContext.This answer is from monkjack, a comment from the accepted answer. However, one can miss this great answer so I'm reposting it here.If you just need to change the thread pool count, just use the global executor and pass the following system properties.best way to specify threadpool in scala futures:"},
{"body": "I want to know the technical reasons why the lift webframework has high performance and scalability?  I know it uses scala, which has an actor library, but according to the install instructions it default configuration is with jetty. So does it use the actor library to scale?Now is the scalability built right out of the box.  Just add additional servers and nodes and it will automatically scale, is that how it works?  Can it handle 500000+ concurrent connections with supporting servers.  I am trying to create a web services framework for the enterprise level, that can beat what is out there and is easy to scale, configurable, and maintainable.  My definition of scaling is just adding more servers and you should be able to accommodate the extra load.ThanksLift's approach to scalability is within a single machine. Scaling across machines is a larger, tougher topic.  The short answer there is: Scala and Lift don't do anything to either help or hinder horizontal scaling.As far as actors within a single machine, Lift achieves better scalability because a single instance can handle more concurrent requests than most other servers. To explain, I first have to point out the flaws in the classic thread-per-request handling model.  Bear with me, this is going to require some explanation.A typical framework uses a thread to service a page request. When the client connects, the framework assigns a thread out of a pool. That thread then does three things: it reads the request from a socket; it does some computation (potentially involving I/O to the database); and it sends a response out on the socket.  At pretty much every step, the thread will end up blocking for some time.  When reading the request, it can block while waiting for the network. When doing the computation, it can block on disk or network I/O. It can also block while waiting for the database.  Finally, while sending the response, it can block if the client receives data slowly and TCP windows get filled up. Overall, the thread might spend 30 - 90% of it's time blocked.  It spends 100% of its time, however, on that one request.A JVM can only support so many threads before it really slows down. Thread scheduling, contention for shared-memory entities (like connection pools and monitors), and native OS limits all impose restrictions on how many threads a JVM can create.Well, if the JVM is limited in its maximum number of threads, and the number of threads determines how many concurrent requests a server can handle, then the number of concurrent requests will be determined by the number of threads.(There are other issues that can impose lower limits---GC thrashing, for example. Threads are a fundamental limiting factor, but not the only one!)Lift decouples thread from requests. In Lift, a request does  tie up a thread. Rather, a thread does an action (like reading the request), then sends a message to an actor. Actors are an important part of the story, because they are scheduled via \"lightweight\" threads.  A pool of threads gets used to process messages within actors. It's important to avoid blocking operations inside of actors, so these threads get returned to the pool rapidly.  (Note that this pool isn't visible to the application, it's part of Scala's support for actors.)  A request that's currently blocked on database or disk I/O, for example, doesn't keep a request-handling thread occupied. The request handling thread is available, almost immediately, to receive more connections.This method for decoupling requests from threads allows a Lift server to have many more concurrent requests than a thread-per-request server.  (I'd also like to point out that the Grizzly library supports a similar approach without actors.)  More concurrent requests means that a single Lift server can support more users than a regular Java EE server.at mtnyguard\"Scala and Lift don't do anything to either help or hinder horizontal scaling\"Ain't quite right. Lift is highly statefull framework. For example if a user requests a form, then he can only post the request to the same machine where the form came from, because the form processeing action is saved in the server state.And this is actualy a thing which hinders scalability in a way, because this behaviour is inconistent to the shared nothing architecture.No doubt that lift is highly performant but perfomance and scalability are two different things. So if you want to scale horizontaly with lift you have to define sticky sessions on the loadbalancer which will redirect a user during a session to the same machine.Jetty maybe the point of entry, but the actor ends up servicing the request, I suggest having a look at the twitter-esque example, 'skitter' to see how you would be able to create a very scalable service. IIRC, this is one of the things that made the twitter people take notice.I really like @dre's reply as he correctly states the statefulness of lift being a potential problem for horizontal scalability. The problem - \nInstead of me describing the whole thing again, check out the discussion (Not the content) on this post. Solution would be as @dre said sticky session configuration on load balancer on the front and adding more instances. But since request handling in lift is done in thread + actor combination you can expect one instance handle more requests than normal frameworks. This would give an edge over having sticky sessions in other frameworks. i.e. Individual instance's capacity to process more may help you to scale"},
{"body": "Akka 2.x requires many commands to reference an .  So, to create an instance of an actor  you might say:Because of the frequent need for an : many code examples omit the creation from the code and assume that the reader knows where a  variable has come from. If your code produces actors in different places, you could duplicate this code, possibly creating additional  instances, or you could try to share the same  instance by referring to some global or by passing the  around.The Akka documentation provides a  under the heading 'Actor Systems', and there is .  But neither of these help a great deal in explaining why a user of Akka can't just rely on Akka to manage this under-the-hood.Creating an ActorSystem is very expensive, so you want to avoid creating a new one each time you need it. Also your actors should run in the same ActorSystem, unless there is a good reason for them not to. The name of the ActorSystem is also part the the path to the actors that run in it. E.g. if you create an actor in a system named  it will have a path like . If you are in an actor context, you always have a reference to the ActorSystem. In an Actor you can call . I don't know what akka-testkit expects, but you could take a look at the akka tests. So to sum it up, you should always use the same system, unless there is a good reason not to do so.Here are some materials which might be helpful to understand \"Why does document always suggest to use one ActorSystem for one logical application\" : Here is a  , if you have time, you can read it. They discuss a lot, ActorSystem, local or remote, etc. "},
{"body": "I'm tearing my hair out trying to figure out how to do the following:There's a reason why I have to declare the method with an  and an   separately. Basically, I end up with the format method called with a single object parameter (of type ). Attempting:Gives me the type error:I've tried casting, which compiles but fails for pretty much the same reason as the first example. When I trythis fails to compile with implicit conversion ambiguity ( and )orI much prefer the latter, though it has no locale* support."},
{"body": "I am exploring the Scala language. One claim I often hear is that Scala has a  type system than Java. By this I think what people mean is that:Am I right in thinking so? If so, please point to articles/blogs/papers which illustrate such examples.The main advantage of the Scala Type system is not so much being  but rather being far  (see \"\").\n(Java can define some of them, and implement others, but Scala has them built-in).\nSee also , commenting , where he \"disses\" Scala as \"Frankenstein's Monster\" because \"there are type types, and type type types\".The main safety problem with Java relates to variance. Basically, a programmer can use incorrect variance declarations that may result in exceptions being thrown at run-time in Java, while Scala will not allow it.In fact, the very fact that Java's  is co-variant is already a problem, since it allows incorrect code to be generated. For instance, as exemplified by :Then, of course, there are raw types in Java, which allows all sort of things.Also, though Scala has it as well, there's casting. Java API is rich in type casts, and there's no idiom like Scala's . Sure, one case use  to accomplish that, but there's no incentive to do it. In fact, Scala's  is intentionally verbose.These are the things that make Scala's type system stronger. It is also much richer, as  shows."},
{"body": "I have a base abstract class (trait). It has an abstract method . It is extended and implemented by several derived classes. I want to create a trait that can be mixed into the derived classes so that it implements  and then calls the derived class's .Something like:I tried self types and structural types, but I can't get it to work.You were very close. Add the abstract modifier to M.foo, and you have the 'Stackable Trait' pattern: "},
{"body": "So far implicit parameters in Scala do not look good for me -- it is too close to global variables, however since Scala seems like rather strict language I start doubting in my own opinion :-). could you show a real-life (or close) good example when implicit parameters really work. IOW: something more serious than , that would justify such language design.Or contrary -- could you show reliable language design (can be imaginary) that would make implicit not neccessary. I think that even no mechanism is better than implicits because code is clearer and there is no guessing.Thank you for all great answers. Maybe I clarify my \"global variables\" objection. Consider such function:you call it you could (!) do it like this:but in my eyes  works like this:it is not very different from such construct (PHP-like)This is great example, however if you can think of as flexible usage of sending message not using  please post an counter-example. I am really curious about purity in language design ;-).In a sense, yes, implicits represent global state. However, they are not mutable, which is the true problem with global variables -- you don't see people complaining about global constants, do you? In fact, coding standards usually dictate that you transform any constants in your code into constants or enums, which are usually global.Note also that implicits are  in a flat namespace, which is also a common problem with globals. They are explicitly tied to types and, therefore, to the package hierarchy of those types.So, take your globals, make them immutable and initialized at the declaration site, and put them on namespaces. Do they still look like globals? Do they still look problematic?But let's not stop there. Implicits  tied to types, and they are just as much \"global\" as types are. Does the fact that types are global bother you?As for use cases, they are many, but we can do a brief review based on their history. Originally, afaik, Scala did not have implicits. What Scala had were view types, a feature many other languages had. We can still see that today whenever you write something like , which means the type  can be viewed as a type . View types are a way of making automatic casts available on type parameters (generics).Scala then  that feature with implicits. Automatic casts no longer exist, and, instead, you have  -- which are just  values and, therefore, can be passed as parameters. From then on,  meant a value for an implicit conversion would be passed as parameter. Since the cast is automatic, the caller of the function is not required to explicitly pass the parameter -- so those parameters became .Note that there are two concepts -- implicit conversions and implicit parameters -- that are very close, but do not completely overlap.Anyway, view types became syntactic sugar for implicit conversions being passed implicitly. They would be rewritten like this:The implicit parameters are simply a generalization of that pattern, making it possible to pass  kind of implicit parameters, instead of just . Actual use for them then followed, and syntactic sugar for  uses came latter.One of them is , used to implement the  (pattern because it is not a built-in feature, just a way of using the language that provides similar functionality to Haskell's type class). A context bound is used to provide an adapter that implements functionality that is inherent in a class, but not declared by it. It offers the benefits of inheritance and interfaces without their drawbacks. For example:You have probably used that already -- there's one common use case that people usually don't notice. It is this:That uses a context bound of a class manifests, to enable such array initialization. We can see that with this example:You can write it like this:On the standard library, the context bounds most used are:The latter three are mostly used with collections, with methods such as ,  and . One library that makes extensive use of context bounds is Scalaz.Another common usage is to decrease boiler-plate on operations that must share a common parameter. For example, transactions:Which is then simplified like this:This pattern is used with transactional memory, and I think (but I'm not sure) that the Scala I/O library uses it as well.The third common usage I can think of is making proofs about the types that are being passed, which makes it possible to detect at compile time things that would, otherwise, result in run time exceptions. For example, see this definition on :That makes this possible:One library that makes extensive use of that feature is Shapeless.I don't think the example of the Akka library fits in any of these four categories, but that's the whole point of generic features: people can use it in all sorts of way, instead of ways prescribed by the language designer.If you like being prescribed to (like, say, Python does), then Scala is just not for you.Sure.  Akka's got a great example of it with respect to its Actors.  When you're inside an Actor's  method, you might want to send a message to another Actor.  When you do this, Akka will bundle (by default) the current Actor as the  of the message, like this:The  is implicit.  In the Actor there is a definition that looks like:This creates the implicit value within the scope of your own code, and it allows you to do easy things like this:Now, you can do this as well, if you like:ororBut normally you don't.  You just keep the natural usage that's made possible by the implicit value definition in the Actor trait.  There are about a million other examples.  The collection classes are a huge one.  Try wandering around any non-trivial Scala library and you'll find a truckload.One example would be the comparison operations on . E.g.  or :These can only be sensibly defined when there is an operation  on . So, without implicits we\u2019d have to supply the context  every time we\u2019d like to use this function. (Or give up type static checking inside  and risk a runtime cast error.)If however, an implicit comparison  is in scope, e.g. some , we can just use it right away or simply change the comparison method by supplying some other value for the implicit parameter.Of course, implicits may be shadowed and thus there may be situations in which the actual implicit which is in scope is not clear enough. For simple uses of  or  it might indeed be sufficient to have a fixed ordering  on  and use some syntax to check whether this trait is available. But this would mean that there could be no add-on traits and every piece of code would have to use the traits which were originally defined.\nI think you\u2019re correct that in a code snipped likeit may smell of rotten and evil global variables. The crucial point, however, is that there may be only one implicit variable  in scope. Your example with two s is not going to work.Also, this means that practically, implicit variables are employed only when there is a not necessarily unique yet distinct primary instance for a type. The  reference of an actor is a good example for such a thing. The type class example is another example. There may be dozens of algebraic comparisons for any type but there is one which is special.\n(On another level, the actual  in the code itself might also make for a good implicit variable as long as it uses a very distinctive type.)You normally don\u2019t use s for everyday types. And with specialised types (like ) there is not too much risk in shadowing them.Another good general usage of implicit parameters is to make the return type of a method depend on the type of some of the parameters passed to it. A good example, mentioned by Jens, is the collections framework, and methods like , whose full signature usually is:Note that the return type  is determined by the best fitting  that the compiler can find.For another example of this, see . There, the return type of the method  is determined according to a certain implicit parameter type ().It's easy, just remember:e.g.Implicit parameters are heavily used in the collection API. Many functions get an implicit CanBuildFrom, which ensures that you get the 'best' result collection implementation.Without implicits you would either pass such a thing all the time, which would make normal usage cumbersome. Or use less specialized collections which would be annoying because it would mean you loose performance/power.Based on my experience there is no real good example for use of implicits parameters or implicits conversion.The small benefit of using implicits (not needing to explicitly write a parameter or a type) is redundant in compare to the problems they create. I am a developer for 15 years, and have been working with scala for the last 1.5 years.I have seen many times bugs that were caused by the developer not aware of the fact that implicits are used, and that a specific function actually return a different type that the one specified. Due to implicit conversion.I also heard statements saying that if you don't like implicits, don't use them.\nThis is not practical in the real world since many times external libraries are used, and a lot of them are using implicits, so your code using implicits, and you might not be aware of that.\nYou can write a code that has either:or:Both codes will compile and run.\nBut they will not always produce the same results since the second version imports implicits conversion that will make the code behave differently.The 'bug' that is caused by this can occur a very long time after the code was written, in case some values that are affected by this conversion were not used originally.Once you encounter the bug, its not an easy task finding the cause.\nYou have to do some deep investigation.Even though you feel like an expert in scala once you have found the bug, and fixed it by changing an import statement, you actually wasted a lot of precious time.Additional reasons why I generally against implicits are:There is no option to compile scala without implicits (if there is please correct me), and if there was an option, none of the common community scala libraries would have compile. For all the above reasons, I think that implicits are one of the worst practices that scala language is using.Scala has many great features, and many not so great.When choosing a language for a new project, implicits are one of the reasons against scala, not in favour of it. In my opinion."},
{"body": "As I understand from   \"type classes\" in Scala is just a \"pattern\" implemented with traits and implicit adapters.As the blog says if I have trait  and an adapter  then I can invoke a function, which requires argument of type , with an argument of type  without invoking this adapter explicitly.I found it nice but not particularly useful. Could you give a use case/example, which shows what this feature is useful for ? One use case, as requested...Imagine you have a list of things, could be integers, floating point numbers, matrices, strings, waveforms, etc.  Given this list, you want to add the contents.One way to do this would be to have some  trait that must be inherited by every single type that can be added together, or an implicit conversion to an  if dealing with objects from a third party library that you can't retrofit interfaces to.This approach becomes quickly overwhelming when you also want to begin adding other such operations that can be done to a list of objects.  It also doesn't work well if you need alternatives (for example; does adding two waveforms concatenate them, or overlay them?) The solution is ad-hoc polymorphism, where you  pick and chose behaviour to be retrofitted to existing types.For the original problem then, you could implement an  type class:You can then create implicit subclassed instances of this, corresponding to each type that you wish to make addable:The method to sum a list then becomes trivial to write...The beauty of this approach is that you can supply an alternative definition of some typeclass, either controlling the implicit you want in scope via imports, or by explicitly providing the otherwise implicit argument.  So it becomes possible to provide different ways of adding waveforms, or to specify modulo arithmetic for integer addition.  It's also fairly painless to add a type from some 3rd-party library to your typeclass.Incidentally, this is exactly the approach taken by the 2.8 collections API.  Though the  method is defined on  instead of on , and the type class is  (it also contains a few more operations than just  and )Reread the first comment there:I think this is the most important advantage of type classes.Also, they handle properly the cases where the operations don't have the argument of the type we are dispatching on, or have more than one. E.g. consider this type class:I think of type classes as the ability to add type safe metadata to a class. So you first define a class to model the problem domain and then think of metadata to add to it. Things like Equals, Hashable, Viewable, etc. This creates a separation of the problem domain and the mechanics to use the class and opens up subclassing because the class is leaner. Except for that, you can add type classes anywhere in the scope, not just where the class is defined and you can change implementations. For example, if I calculate a hash code for a Point class by using Point#hashCode, then I'm limited to that specific implementation which may not create a good distribution of values for the specific set of Points I have. But if I use Hashable[Point], then I may provide my own implementation.[Updated with example]\nAs an example, here's a use case I had last week. In our product there are several cases of Maps containing containers as values. E.g.,  or . Adding to these collections can be verbose:So I wanted to have a function that wraps this so I could writeThe main issue is that the collections don't all have the same methods for adding elements. Some have '+' while others ':+'. I also wanted to retain the efficiency of adding elements to a list, so I didn't want to use fold/map which create new collections.The solution is to use type classes:Here I defined a type class  that can add an element C to a collection CC. I have 2 default implementations: For Lists using  and for other collections, using the builder framework.Then using this type class is:The special bit is using  to add the elements and  to create new collections for new keys. To compare, without type classes I would have had 3 options:\n1. to write a method per collection type. E.g.,  and  etc. This creates a lot of boilerplate in the implementation and pollutes the namespace\n2. to use reflection to determine if the sub collection is a List / Set. This is tricky as the map is empty to begin with (of course scala helps here also with Manifests)\n3. to have poor-man's type class by requiring the user to supply the adder. So something like , which is plain uglyYet another way I find this blog post helpful is where it describes typeclasses:  Search the article for typeclass.  It should be the first match.  In this article, the author provides an example of a Monad typeclass.One way to look at type classes is that they enable  or .  There are a couple of great posts by  and  that show examples of using Type Classes in Scala to achieve this.Here's a \n that explores various methods in scala of , a kind of retroactive extension, including a typeclass example.The forum thread \"\" makes some interesting points:I don't know of any other use case than  which is explained  the best way possible.Both  and  are used for . The major use-case for both of them is to provide (i.e) on classes that you can't modify but expect inheritance kind of polymorphism. In case of implicits you could use both an implicit def or an implicit class (which is your wrapper class but hidden from the client). Typeclasses are more powerful as they can add functionality to an already existing inheritance chain(eg: Ordering[T] in scala's sort function). \nFor more detail you can see  is an important difference (needed for functional programming):consider :  received is the same that is returned, this cannot be done with subtypingIn scala type classesBehavior can be extended\n - at compile-time\n - after the fact\n - without changing/recompiling existing codeScala ImplicitsThe last parameter list of a method can be marked implicitBelow Example extension on String class with type class implementation extends the class with a new methods even though string is final :) "},
{"body": "This weekend I decided to try my hand at some Scala and Clojure.  I'm proficient with object oriented programming, and so Scala was easy to pick up as a language, but wanted to try out functional programming.  This is where it got hard.  I just can't seem to get my head into a mode of writing functions.  As an expert functional programmer, how do you approach a problem?Given a list of values and a defined period of summation, how would you generate a new list of the simple moving average of the list?For example: Given the list  (2.0, 4.0, 7.0, 6.0, 3.0, 8.0, 12.0, 9.0, 4.0, 1.0), and the  4, the function should return: (0.0, 0.0, 0.0, 4.75, 5.0, 6.0, 7.25, 8.0, 8.25, 6.5)After spending a day mulling it over, the best I could come up with in Scala was this:I know this is horribly inefficient, I'd much rather do something like:  Now that would be easily done in a imperative style, but I can't for the life of me work out how to express that functionally.Interesting problem. I can think of many solutions, with varying degrees of efficiency. Having to add stuff repeatedly isn't really a performance problem, but let's assume it is. Also, the zeroes at the beginning can be prepended later, so let's not worry about producing them. If the algorithm provides them naturally, fine; if not, we correct it later.Starting with Scala 2.8, the following would give the result for  by using  to get a sliding window of the List:Nevertheless, although this is rather elegant, it doesn't have the best performance possible, because it doesn't take advantage of already computed additions. So, speaking of them, how can we get them?Let's say we write this:We have a list of the sum of each two pairs. Let's try to use this result to compute the moving average of 4 elements. The above formula made the following computation:So if we take each element and add it to the second next element, we get the moving average for 4 elements:We may do it like this:We could then compute the moving average for 8 elements, and so on. Well, there is a well known algorithm to compute things that follow such pattern. It's most known for its use on computing the power of a number. It goes like this:So, let's apply it here:So, here's the logic. Period 0 is invalid, period 1 is equal to the input, period 2 is sliding window of size 2. If greater than that, it may be even or odd.If odd, we add each element to the  of the next  elements. For example, if 3, we add each element to the  of the next 2 elements.If even, we compute the  for , then add each element to the one  steps afterwards.With that definition, we can then go back to the problem and do this:There's a slight inefficiency with regards to the use of , but it's O(period), not O(values.size). It can be made more efficient with a tail recursive function. And, of course, the definition of \"sliding\" I provided is horrendous performance-wise, but there will be a much better definition of it on Scala 2.8. Note that we can't make an efficient  method on a , but we can do it on an .Having said all that, I'd go with the very first definition, and optimize only if a critical path analysis pinpointed this as a big deal.To conclude, let's consider how I went about the problem. We have a moving average problem. A moving average is the sum of a moving \"window\" on a list, divided by the size of that window. So, first, I try to get a sliding window, sum everything on it, and then divide by the size.The next problem was to avoid repetition of already computed additions. In this case, I went to the smallest addition possible, and tried to figure out how to compute bigger sums reusing such results.Finally, let's try to solve the problem the way you figured it, by adding and subtracting from the previous result. Getting the first average is easy:Now we make two lists. First, the list of elements to be subtracted. Next, the list of elements to be added:We can add these two lists by using . This method will only produce as many elements as the smaller list has, which avoids the problem of  being bigger than necessary:We finish by composing the result with a fold:which is the answer to be returned. The whole function looks like this:I know Clojure better than Scala, so here goes. As I write this the other Clojure entry here is imperative; that's not really what you're after (and isn't idiomatic Clojure). The first algorithm that comes to my mind is repeatedly taking the requested number of elements from the sequence, dropping the first element, and recurring. The following works on any kind of sequence (vector or list, lazy or not) and gives a lazy sequence of averages---which could be helpful if you're working on a list of indefinite size. Note that it takes care of the base case by implicitly returning nil if there aren't enough elements in the list to consume.Running this on your test data yieldsIt doesn't give \"0\" for the first few elements in the sequence, though that could easily be handled (somewhat artificially). The easiest thing of all is to see the pattern and be able to bring to mind an available function that fits the bill.  gives a lazy view of portions of a sequence, which we can then map over:Someone asked for a tail recursive version; tail recursion vs. laziness is a bit of a tradeoff. When your job is building up a list then making your function tail recursive is usually pretty simple, and this is no exception---just build up the list as an argument to a subfunction. We'll accumulate to a vector instead of a list because otherwise the list will be built up backwards and will need to be reversed at the end. is a way to make an anonymous inner function (sort of like Scheme's named let);  must be used in Clojure to eliminate tail calls.  is a generalized , appending in the manner natural for the collection---the beginning of lists and the end of vectors.Here is another (functional) Clojure solution: The zeros at the beginning of the sequence must still be added if that is a requirement.Here's a purely functional solution in Clojure. More complex than those already provided, but it is  and . It's actually slower than a simple solution which calculates a new average at each step if the period is small; for larger periods, however, it experiences virtually no slowdown, whereas something doing  will perform worse for longer periods.Here's a partially  one line Haskell solution:First it applies  to the list to get the \"tails\" lists, so:Reverses it and drops the first 'p' entries (taking p as 2 here):In case you aren't familiar with the  symbol, it is the operator for 'functional composition', meaning it passes the output of one function as the input of another, \"composing\" them into a single function. (g . f) means \"run f on a value then pass the output to g\", so ((f . g) x) is the same as (g(f x)). Generally its usage leads to a clearer programming style.It then maps the function ((/ (fromIntegral p)) . sum . take p) onto the list. So for every list in the list it takes the first 'p' elements, sums them, then divides them by 'p'. Then we just flip the list back again with \"reverse\".This all looks a lot more inefficient than it is; \"reverse\" doesn't physically reverse the order of a list until the list is evaluated, it just lays it out onto the stack (good ol' lazy Haskell). \"tails\" also doesn't create all those separate lists, it just references different sections of the original list. It's still not a great solution, but it one line long :)Here's a slightly nicer but longer solution that uses mapAccum to do a sliding subtraction and addition:First we split the list into two parts at \"p\", so:Sum the first bit:Zip the second bit with the original list (this just pairs off items in order from the two lists). The original list is obviously longer, but we lose this extra bit:Now we define a function for our mapAccum(ulator).  is the same as \"map\", but with an extra running state/accumulator parameter, which is passed from the previous \"mapping\" to the next one as map runs through the list. We use the accumulator as our moving average, and as our list is formed of the element that has just left the sliding window and the element that just entered it (the list we just zipped), our sliding function takes the first number 'x' away from the average and adds the second number 'y'. We then pass the new 's' along and return 's' divided by 'p'. \"snd\" (second) just takes the second member of a pair (tuple), which is used to take the second return value of mapAccumL, as mapAccumL will return the accumulator as well as the mapped list.For those of you not familiar with the , it is the \"application operator\". It doesn't really do anything but it has a has \"low, right-associative binding precedence\", so it means you can leave out the brackets (take note LISPers), i.e. (f x) is the same as f $ xRunning (ma 4 [2.0, 4.0, 7.0, 6.0, 3.0, 8.0, 12.0, 9.0, 4.0, 1.0]) yields [4.75, 5.0, 6.0, 7.25, 8.0, 8.25, 6.5] for either solution.Oh and you'll need to import the module \"List\" to compile either solution.Here are 2 more ways to do moving average in Scala 2.8.0(one strict and one lazy).  Both assume there are at least  Doubles in .The J programming language facilitates programs such as moving average. Indeed, there are fewer characters in  than in their label, 'moving average.'For the values specified in this question (including the name 'values') here is a straightforward way to code this:We can describe this by using labels for components.Both examples use exactly the same program. The only difference is the use of more names in the second form. Such names can help readers who don't know the J primaries.Let's look a bit further into what's going on in the subprogram, .  denotes summation (\u03a3) and  denotes division (like the classical sign \u00f7). Calculating a tally (count) of items is done by  . The overall program, then, is the sum of values divided by the tally of values: The result of the moving-average calculation written here does not include the leading zeros expected in the original question. Those zeros are arguably not part of the intended calculation.The technique used here is called tacit programming. It is pretty much the same as the point-free style of functional programming.Here is Clojure pretending to be a more functional language.  This is fully tail-recursive, btw, and includes leading zeroes.Usually I put the collection or list parameter last to make the function easier to curry.  But in Clojure...... is so cumbersome, I usually end up doing this ...... in which case, it doesn't really matter what order the parameters go.Here's a clojure version:Because of the lazy-seq, it's perfectly general and won't blow stack;; To help see what it's doing:;; ExampleThis example makes use of state, since to me it's a pragmatic solution in this case, and a closure to create the windowing averaging function:It is still functional in the sense of making use of first class functions, though it is not side-effect free.  The two languages you mentioned both run on top of the JVM and thus both allow for state-management when necessary.  This solution is in Haskell, which is more familiar to me:Scala translation:A short Clojure version that has the advantage of being O(list length) regardless of your period:This exploits the fact that you can calculate the sum of a range of numbers by creating a cumulative sum of the sequence (e.g. [1 2 3 4 5] -> [0 1 3 6 10 15]) and then subtracting the two numbers with an offset equal to your period.I know how I would do it in python (note: the first 3 elements with the values 0.0 are not returned since that is actually not the appropriate way to represent a moving average). I would imagine similar techniques will be feasible in Scala. Here are multiple ways to do it.It looks like you are looking for a recursive solution. In that case, I would suggest to slightly change the problem and aim for getting (4.75, 5.0, 6.0, 7.25, 8.0, 8.25, 6.5, 0.0, 0.0, 0.0) as a solution.In that case, you can write the below elegant recursive solution in Scala:Being late on the party, and new to functional programming too, I came to this solution with an inner function:I adopted the idea, to divide the whole list by the period (len) in advance. \nThen I generate the sum to start with for the len-first-elements.\nAnd I generate the first, invalid elements (0.0, 0.0, ...) .Then I recursively substract the first and add the last value.\nIn the end I listify the whole thing. In Haskell pseudocode:or pointfree(Now one really should abstract the 4 out ....)Using Haskell:The key is the tails function, which maps a list to a list of copies of the original list, with the property that the n-th element of the result is missing the first n-1 elements.SoWe apply fmap (avg . take n) to the result, which means we take the n-length prefix from the sublist, and compute its avg.  If the length of the list we are avg'ing is not n, then we do not compute the average (since it is undefined).  In that case, we return Nothing.  If it is, we do, and wrap it in \"Just\".  Finally, we run \"catMaybes\" on the result of fmap (avg . take n), to get rid of the Maybe type.I was (surprised and) disappointed by the performance of what seemed to me the most idiomatic Clojure solutions, @JamesCunningham 's  .So here's a combination of James' solution with @DanielC.Sobral 's  of adapting  to moving sums : this one -based on @mikera 's - is even faster."},
{"body": "I'm trying to incorporate ScalaTest into my Java project, replacing all JUnit tests by ScalaTests. At one point, I want to check if Guice's Injector injects the correct type. In Java, I have a test like this:But I have a problem doing the same with ScalaTest:It complaints that the value instanceof is not a member of Door/Window/Roof. Can't I use instanceof that way in Scala? Scala is not Java. Scala just does not have the operator  instead it has a parametric method called .If you want to be less JUnit-esque and if you want to use ScalaTest's matchers, you can write your own property matcher that matches for type (bar type erasure).I found this thread to be quite useful: You can then write assertions like:instead of With Scalatest 2.2.x (maybe even earlier) you can use:The current answers about isInstanceOf[Type] and junit advice are good but I want to add one thing (for people who got to this page in a non-junit-related capacity).  In many cases scala pattern matching will suit your needs.  I would recommend it in those cases because it gives you the typecasting for free and leaves less room for error.Example:Consolidating Guillaume's ScalaTest discussion reference (and another discussion linked to by James Moore) into two methods, updated for ScalaTest 2.x and Scala 2.10 (to use ClassTag rather than manifest):"},
{"body": "I am new to Scala and I really like it, but sometimes it surprises me. For instance:Could anyone tell me what  and  mean in Scala? is syntactic sugar for creating instances of functions. Recall that every function in scala is an instance of a class.For example, the type , is equivalent to the type  i.e. a function that takes an argument of type  and returns a .Here  is binded to the argument value passed to  and . is the type of a function that takes no arguments and returns a . It is equivalent to .  is called a zero parameter list I believe. has several meanings in Scala, all related to its mathematical meaning as implication. means: So if you declare a value  to be a function that takes no parameters and returns nothing its type would be: Since this is a  you have to assign a value, for instance a function that prints That reads: *\"f is a function that takes no parameters and returns nothing initialized with the code Since in Scala you can use type inference you don't have to declare the type so the following would be equivalent: To invoke it you can just: Or, since the functions are also objects you can invoke the  method:That's why in your declaration you're saying   hence  List[()=>Unit]I hope this helps.  is the \"function arrow\". It is used both in function type signatures as well as anonymous function terms.  is a shorthand for , which is the type of functions which take no arguments and return nothing useful (like  in other languages).As the most simplified answer, you can substitute whatever is on the left-hand side of => with the word \"LEFT\" and whatever is on the right-hand side with the word \"RIGHT\".Then, the meaning of \"\" becomes:This means that if you have a \"()=>\" that you can take nothing (that is, no parameters) and then do whatever is on the right-hand side.This is the most common meaning."},
{"body": "In his talk , Rick Hickey talks about \"\" (about 30:00 into the video). In the same context, he also mentions Haskell's  and Clojure's  (and protocols).Since I am not very familiar with these concepts, I would like to understand its usefulness when trying to achieve . I am particularly interested in any examples or showcases of this concept in . You can take  as . Clojure community are proud of the term  because of the fact that Clojure support multiple polymorphism strategies. Some of them are: means \"Select whatever polymorphism strategy best for your case. They're all in your toolbox.\"You can implement  pattern in Scala using implicits. Read  if you want real world examples. Scala does not support multimethods at language level, but I guess it is possible with the help of the upcoming 2.10 macro. As for the benefits, advanced polymorphism strategies such as TypeClass and Multimethod can help solve the . BTW, this question is too big to fit into a single StackOverflow question. My suggestion is to get familiar with these concepts, and then you'll understand their usefulness.   "},
{"body": "I came across the  method, and according to the Scala documentation this is...I have some doubts:EDIT:Consider taking a look at Chapter 4 in the .I always found the official documentation a little bit poor in regard to the blocking construct; therefore I tried to explain it more clearly in this  with examples of how you could implement your own and how it works under the hood."},
{"body": "I am trying to create a generic method for object updates using scala / java but I can't get the class for a type parameter.Here is my code:The error i get is I know in java you can't do it but is this possible in scala at all?Thanks!Due  is  (Since Scala 2.10.0) this is the updated answer -You should use  instead of \nand  instead of \nYes, you can do that using manifests:Vasil's and Maxim's answer helped me.Personally, I prefer the syntax where  is used for adding such parameters (the presented  is shorthand for it. So here, in case someone else also sees this to be a better way:Disclaimer: I did not compile the above, but my own similar code works this way. "},
{"body": "I'm new to the Akka framework and I'm building an HTTP server application on top of Netty + Akka.My idea so far is to create an actor for each type of request. E.g. I would have an actor for a POST to /my-resource and another actor for a GET to /my-resource.Where I'm confused is how I should go about actor creation? Should I:Thanks for any feedback.Well, you create an Actor for each instance of mutable state that you want to manage. In your case, that might be just one actor if  is a single object and you want to treat each request serially - that easily ensures that you only return consistent states between modifications.If (more likely) you manage multiple resources, one actor per resource instance is usually ideal unless you run into many thousands of resources. While you can also run per-request actors, you'll end up with a strange design if you don't think about the state those requests are accessing - e.g. if you just create one Actor per POST request, you'll find yourself worrying how to keep them from concurrently modifying the same resource, which is a clear indication that you've defined your actors wrongly. I usually have fairly trivial request/reply actors whose main purpose it is to abstract the communication with external systems. Their communication with the \"instance\" actors is then normally limited to one request/response pair to perform the actual action.If you are using Akka, you can create an actor per request. Akka is extremely slim on resources and you can create literarily millions of actors on an pretty ordinary JVM heap. Also, they will only consume cpu/stack/threads when they actually do something.A year ago I made a  between the resource consumption of the thread-based and event-based standard actors. And Akka is even better than the event-base.One of the big points of Akka in my opinion is that it  you to design your system as \"one actor per usage\" where earlier actor systems often forced you to do \"use only actors for shared services\" due to resource overhead.I would recommend that you go for option 1.Options 1) or 2) have both their drawbacks. So then, let's use options  Router is an element which act as a Akka providers different Router implementations with different logic to route a message (for example SmallestMailboxPool or RoundRobinPool).Every Router may have several children and its task is to supervise their Mailbox to further decide where to route the received message.This procedure is well explained in .For scaling up the serial requests handling, add a master actor () which in turn will delegate to the worker actors () ()."},
{"body": "I know that  can be expressed in Scala as follows:I see why it is useful. For example, given two functions:I can easily write function  since  is a monad:...Now I see  in Scala:I wonder when I should use it  monad . I guess both Option and List are . Could you give simple examples of using  with Option and List and explain  I should use it   ?To :To expand a bit on the first paragraph: sometimes you don't have a choice between monadic and applicative code. See the rest of  for a discussion of why you might want to use Scalaz's  (which doesn't and can't have a monad instance) to model\nvalidation.About the optimization point: it'll probably be a while before this is generally relevant in Scala or Scalaz, but see for example :Writing applicative code allows you to avoid making unnecessary claims about dependencies between computations\u2014claims that similar monadic code would commit you to. A sufficiently smart library or compiler  in principle take advantage of this fact.To make this idea a little more concrete, consider the following monadic code:The -comprehension desugars to something more or less like the following:We know that  doesn't depend on  (assuming these are well-behaved methods that aren't changing some mutable state behind the scenes), but the compiler doesn't\u2014from its perspective it needs to know  before it can start computing .The applicative version (using Scalaz) looks like this:Here we're explicitly stating that there's no dependency between the two computations.(And yes, this  syntax is pretty horrible\u2014see  for some discussion and alternatives.)The last point is really the most important, though. Picking the  powerful tool that will solve your problem is a tremendously powerful principle. Sometimes you really do need monadic composition\u2014in your  method, for example\u2014but often you don't.It's a shame that both Haskell and Scala currently make working with monads so much more convenient (syntactically, etc.) than working with applicative functors, but this is mostly a matter of historical accident, and developments like  are a step in the right direction.Functor is for lifting computations to a category.And it works perfectly for a function of one variable.But for a function of 2 and more, after lifting to a category, we have the following signature:And it is the signature of a applicative functor. And to apply the\u00a0following  value to a function  \u2014 an aplicative functor is needed.And finally:Applicative functor is a functor for applying a special value (value in category) to a lifted function."},
{"body": "I'm building a few Java-only projects using simple-build-tool.  When I publish the artifacts from the projects using, say, sbt publish-local then the resulting artifacts have the Scala version appended to their name.  With a Scala project this would make sense, but since these are Java only projects it doesn't.  How would I disable this postfixing of the Scala version?  Or can I?For reference I'm using sbt 0.11.1, Scala 2.9.1 and a .sbt file for build configuration (though moving to a full project config would be no problem).After looking around how Artifact.artifactName is implemented and ultimately used it seems that the way to turn this off is to specify false for the crossPath setting.  This is documented in one of the quick configuration examples on the xsbt wiki.This is documented on the xsbt wiki under . From that page:While the accepted answer is strictly correct, you should  set  to  on publicly published Scala artifacts. The embedded scala version is an important compatibility feature, since different versions of the Scala libraries may not be binary compatible.Only set  to  for projects, like those in the question, that are strictly Java only."},
{"body": "So here's the situation.  I want to define a case class like so:and I want to define an object to ensure that when I create instances of the class, the value for 's' is always uppercase, like so:However, this doesn't work since Scala is complaining that the apply(s: String) method is defined twice.  I understand that the case class syntax will automatically define it for me, but isn't there another way I can achieve this?  I'd like to stick with the case class since I want to use it for pattern matching.The reason for the conflict is that the case class provides the exact same apply() method (same signature).First of all I would like to suggest you use require:This will throw an Exception if the user tries to create an instance where s includes lower case chars. This is a good use of case classes, since what you put into the constructor also is what you get out when you use pattern matching ().If this is not what you want, then I would make the constructor  and force the users to  use the apply method:As you see, A is no longer a . I am not sure if case classes with immutable fields are meant for modification of the incoming values, since the name \"case class\" implies it should be possible to extract the (unmodified) constructor arguments using . \nWhile the answer I wrote below remains sufficient, it's worth also referencing another related answer to this regarding the case class's companion object. Namely,  which occurs when one only defines the case class itself. For me, it turned out to be counter intuitive.\nYou can alter the value of a case class parameter before it is stored in the case class pretty simply while it still remaining a valid(ated) ADT (Abstract Data Type). While the solution was relatively simple, discovering the details was quite a bit more challenging.   \nIf you want to ensure only valid instances of your case class can ever be instantiated which is an essential assumption behind an ADT (Abstract Data Type), there are a number of things you must do.  For example, a compiler generated  method is provided by default on a case class. So, even if you were very careful to ensure only instances were created via the explicit companion object's  method which guaranteed they could only ever contain upper case values, the following code would produce a case class instance with a lower case value:  Additionally, case classes implement . This means that your careful strategy to only have upper case instances can be subverted with a simple text editor and deserialization.  So, for all the various ways your case class can be used (benevolently and/or malevolently), here are the actions you must take:  Here's your code modified with the above actions:  And here's your code after implementing the require (suggested in the @ollekullberg answer) and also identifying the ideal place to put any sort of caching:And this version is more secure/robust if this code will be used via Java interop (hides the case class as an implementation and creates a final class which prevents derivations):  While this directly answers your question, there are even more ways to expand this pathway around case classes beyond instance caching. For my own project needs, I have  which I have  (a StackOverflow sister site). If you end up looking it over, using or leveraging my solution, please consider leaving me feedback, suggestions or questions and within reason, I will do my best to respond within a day.I don't know how to override the  method in the companion object (if that is even possible) but you could also use a special type for upper case strings:The above code outputs:You should also have a look at this question and it's answers: Another idea while keeping case class and having no implicit defs or another constructor is to make the signature of  slightly different but from a user perspective the same. \nSomewhere I have seen the implicit trick, but can\u00b4t remember/find which implicit argument it was, so I chose  here. If someone can help me out and finish the trick...It works with var variables:This practice is apparently encouraged in case classes instead of defining another constructor. . When copying an object, you also keep the same modifications.I faced the same problem and this solution is ok for me:And, if any method is needed, just define it in the trait and override it in the case class.I think this works exactly how you want it to already.  Here's my REPL session:This is using Scala 2.8.1.final"},
{"body": "I am learning Scala now and I want to write some silly little app like a console Twitter client, or whatever. The question is, how to structure application on disk and logically. I know python, and there I would just create some files with classes and then import them in the main module like  or  (strongly hoping you wouldn't mind that names, they are just for reference). But how  I do this stuff using Scala? Also, I have not much experience with JVM and Java, so I am a complete newbie here.I'm going to disagree with , here, though not all that much.My own suggestion is that you model your efforts on . Previous versions of SBT (before SBT 0.9.x) would create it automatically for you:So you'll put your source files inside , for the main program, or , for the tests.Since that doesn't work anymore, there are some alternatives:Install , clone ymasory's  template and adapt it to your necessities, and use that. See below, for example, this use of unmodified ymasory's sbt.g8 template. I think this is one of the best alternatives to starting new projects when you have a good notion of what you want in all your projects.Use softprops's  for sbt. In the example below, the plugin is configured on , and its settings on , with standard sbt script. If you use paulp's sbt-extras, you'd need to install these things under the right Scala version subdirectory in , as it uses separate configurations for each Scala version. In practice, this is the one I use most often.You could simply create it with :Now, about the source layout. Jens recommends following Java style. Well, the Java directory layout is a  -- in Java. Scala does not have the same requirement, so you have the option of following it or not.If you do follow it, assuming the base package is , then source code for that package would be put inside , and so on for sub-packages.Two common ways of diverging from that standard are:And this deals with directory layout.Next, let's talk about files. In Java, the practice is separating each class in its own file, whose name will follow the name of the class. This is good enough in Scala too, but you have to pay attention to some exceptions.First, Scala has , which Java does not have. A  and  of the same name are considered , which has some practical implications, but  if they are in the same file. So, place companion classes and objects in the same file.Second, Scala has a concept known as  (or ), which limits subclasses (or implementing s) to those declared in the same file. This is mostly done to create algebraic data types with pattern matching with exhaustiveness check. For example:If  was not , then anyone could extend it, making it impossible for the compiler to know whether the match was exhaustive or not. Anyway,  classes go together in the same file.Another naming convention is to name the files containing a  (for that package) .The most basic rule is that stuff in the same package see each other. So, put everything in the same package, and you don't need to concern yourself with what sees what.But Scala also have relative references and imports. This requires a bit of an explanation. Say I have the following declarations at the top of my file:Everything following will be put in the package . Also, not only everything inside that package will be visible, but everything inside  will be visible as well. If I just declared  instead, then  would not be visible.The rule is pretty simple, but it can confuse people a bit at first. The reason for this rule is relative imports. Consider now the following statement in that file:This import may be relative -- all imports can be relative unless you prefix it with . It can refer to the following packages: , ,  and . It could also refer to an object named  inside . Or it could be an absolute import refering to a package named .If more than one such package exists, it will pick one according to some precedence rules. If you needed to import something else, you can turn the import into an absolute one.This import makes everything inside the  package (wherever it is) visible in its scope. If it happens inside a , and  or a , then the visibility will be restricted to that. It imports everything because of the , which is a wildcard.An alternative might look like this:In that case, the   and  would be visible, but you'd have to name them explicitly when using them:Or you could use further relative imports:The  statement also enable you to rename stuff, or import everything but something. Refer to relevant documentation about it for more details.So, I hope this answer all your questions.Scala supports and encourages the package structure of Java /JVM and pretty much the same recommendation apply:"},
{"body": "How can I convert a list with (say) 3 elements into a tuple of size 3?For example, let's say I have  and I want to convert this into . How can I do this?You can't do this in a typesafe way. Why? Because in general we can't know the length of a list until runtime. But the \"length\" of a tuple must be encoded in its type, and hence known at compile time. For example,  has the type , which is sugar for . The reason tuples have this restriction is that they need to be able to handle a non-homogeneous types.an example using  :You can do it using scala extractors and pattern matching ():Which returns a tupleAlso, you can use a wildcard operator if not sure about a size of the List changed some syntax. Here's the updated solution using shapeless.The main issue being that the type for  has to be specified ahead of time. More generally, since tuples are limited in their arity, the design of your software might be better served by a different solution.Still, if you are creating a list statically, consider a solution like this one, also using shapeless. Here, we create an HList directly and the type is available at compile time. Remember that an HList has features from both List and Tuple types. i.e. it can have elements with different types like a Tuple and can be mapped over among other operations like standard collections. HLists take a little while to get used to though so tread slowly if you are new.Despite the simplicity and being not for lists of any length, it is type-safe and the answer in most cases:Another possibility, when you don't want to name the list nor to repeat it (I hope someone can show a way to avoid the Seq/head parts):You can't do this in a type-safe way. In Scala, lists are  sequences of elements of some type. As far as the type system knows,  could be a list of arbitrary length.In contrast, the arity of a tuple must be known at compile time. It would violate the safety guarantees of the type system to allow assigning  to a tuple type.In fact, for technical reasons, Scala tuples , but the limit no longer exists in 2.11 The case class limit has been lifted in 2.11 It would be possible to manually code a function that converts lists of up to 22 elements, and throws an exception for larger lists. Scala's template support, an upcoming feature, would make this more concise. But this would be an ugly hack.FWIW, I wanted a tuple to  and wanted to use the syntactic sugar of tuple assignment.\nEG:It turns out that there is syntactic sugar for assigning the contents of a list too...So no need for tuples if you've got the same problem.as far as you have the type:you can do this either2015 post.\nFor the   to be more , here is a real example.At first, I got confused about it. Because I come from Python, where you can just do .\nIs it short of Scala language ? (the answer is -- it's not about Scala or Python, it's about static-type and dynamic-type.)   That's causes me trying to find the crux why Scala can't do this .The following code example implements a  method, which has type-safe  and type-unsafe . The  method get the type-length information at run-time, i.e no type-length information at compile-time, so the return type is  which is very like the Python's  indeed (no type at each position, and no length of types).\nThat way is proned to runtime error like type-mismatch or . (so Python's convenient list-to-tuple  is not free lunch. )Contrarily , it is the length information user provided that makes  compile-time safe.If you are very sure that your list.size<23 use it:"},
{"body": "What's the difference between and in Scala? means that class  can take any class  that is a subclass of . means that  can take  class, but if  is a subclass of , then  is considered to be a subclass of . means that class  can only take subclasses of  as well as propagating the subclass relationship.The first is useful when you want to do something generic, but you need to rely upon a certain set of methods in .  For example, if you have an  class with a  method, you could use that method in any class that could be passed into .The second is useful when you want to make collections that behave the same way as the original classes.  If you take  and you make a subclass , then you can pass  in anywhere where  is expected.  But if you take a  of , , is it true that you can always pass in  instead?  In general, no; there are cases when this would be the wrong thing to do.  But you can say that this is the right thing to do by using  (covariance;  covaries--follows along with--'s subclasses' inheritance relationship).I would like to extend  with some more examples:\nLet's say we have four classes: As you can see . The developers of List did not enforce that e.g. only Cars can go inside Lists. Additionally: If a function expects a  parameter you can also pass a  as an argument to the function instead.    due to the covariance of List. It would not work if List was invariant.As you can see . No cars allowed in here.for my Understanding:The first is a parameter type bound, there a upper and lower typebounds in our case its a \"type parameter A that is a subtype of B (or B itself).The second is a Variance Annotation for a class defintion, in our case a covariance subclassing of BScala: +  Java: ? extends T    Covariant subclassingScala: -  Java: ? super T      Contravariant subclassingI found this blog post while researching this question.  Gives an even deeper explanation of Scala variance including its theoretical basis in  Category Theory"},
{"body": "Since Scala does not have old Java style  loops with index, How can we iterate efficiently, and without using 's?You could do thisbut the list is traversed twice - not very efficient.Much worse than traversing twice, it creates an intermediary array of pairs. \nYou can use . When you do , you can think of subsequent calls as acting lazily, during the iteration. If you want to get back a proper fully realized collection, you call  at the end. Here that would be useless and costly. So change your code toIt has been mentioned that Scala  have syntax for  loops:or simplyHowever, you also asked for efficiency. It turns out that the Scala  syntax is actually syntactic sugar for higher order methods such as , , etc. As such, in some cases these loops can be inefficient, e.g. (The good news is that the Scala team is working on improving this. Here's the issue in the bug tracker: ) For utmost efficiency, one can use a  loop or, if you insist on removing uses of , tail recursion:Note that the   annotation is useful for ensuring that the method is actually tail recursive. The Scala compiler translates tail-recursive calls into the byte code equivalent of while loops.One more way:Actually, scala has old Java-style loops with index:Where  or  is a  method which returns  suitable for looping.Also, you can try loop with :How about this?Output:There's nothing in the stdlib that will do it for you without creating tuple garbage, but it's not too hard to write your own.  Unfortunately I've never bothered to figure out how to do the proper CanBuildFrom implicit raindance to make such things generic in the type of collection they're applied to, but if it's possible, I'm sure someone will enlighten us. :)Some more ways to iterate:foreach, and similar, map, which would return something (the results of the function, which is, for println, Unit, so a List of Units)work with the elements, not the indexfoldingThe carry-part is just some example, to do something with i and j. It needn't be an Int. for simpler stuff, closer to usual for-loops:or without order:A simple and efficient way, inspired from the implementation of  in Looping in scala is pretty simple.\nCreate any array of your choice for ex.Types of loops,Indeed, calling  on a collection will traverse it and also create a new collection for the pairs.  To avoid this, you can just call  on the iterator for the collection.  This will just return a new iterator that keeps track of the index while iterating, so without creating an extra collection or additional traversing.This is how  is currently implemented in 2.10.3:This should even be a bit more efficient than creating a view on the collection.I have the following approachesThe proposed solutions suffer from the fact that they either explicitly iterate over a collection or stuff the collection into a function. It is more natural to stick with the usual idioms of Scala and put the index inside the usual map- or foreach-methods. This can be done using memoizing. The resulting code might look likeHere is a way to achieve this purpose. Consider the following utility:This is already all you need. You can apply this for instance as follows:which results in the listThis way, you can use the usual Traversable-functions at the expense of wrapping your effective function. Enjoy!"},
{"body": "As suggested to me, I've run through a few articles describing how to make IntelliJ Idea and Android and Scala work together, but it turned out they were all written 1 or 2 years ago. And likely, something has changed since that time and now there are other solutions to achieve this goal. Concretely, there is a new solution  for  which I don't know if it works well or not.I have IntelliJ Idea 12, Android SDK, Scala, SBT and all other stuff installed and set up. The only thing remaining is to setup IntelliJ Idea 12 to make it work with Scala instead of Java for creating Android applications. Please don't close the question. I saw the previous questions similar to mine, but, as I said, they were outdated.The question is, how do I do it ?P.S. I tried Android Development Studio but I even wasn't able to launch it due to many errors. You can read the  I wrote about the upcoming new version of SBT-Android (0.7). Most of your concerns should be addressed there. If they are not, then that's a problem. For fairness's sake, there's also a great work in progress :  by @pfn on GitHub.Regarding ProGuard and build times :I've been contributing to  for a few months now, so I know it a lot better than the other solutions.As of now, the plugin is evolving rapidly, with :The current version doesn't have most of these niceties, but they're coming up in the following weeks. Some documentation is of course going to follow very soon afterwards, with examples and everything I'll think of.I encourage you to join the  Google Group and share your thoughts, by the way. There's also a  channel on Freenode if you're into IRC.Just wanted to chime in on this.  I used , although I found the instructions to be vague, not having ever used IntelliJ before this.You've probably got it all sorted out by now and are happy with SBT, but for my purposes I just wanted to test Scala on Android without all the extra bells and whistles. So, here are some little things I had to do to get the no-sbt-plugin stuff working:I ended up including the scala-compiler.jar (along with the scala-library.jar and scala-reflect.jar) as a Global Library:\nThat way, the compiler has access to the library & reflection code, otherwise I was getting errors. If you don't include the library, the IDE complains that the compiler needs an associated library, and if you don't include reflect it spits out a bunch of errors at compile time that it cannot find reflect class definitions.I had to include scala-library.jar as a module dependency as well (which had the side effect of including it under the Libraries section also):\nBefore doing all that, I had the Scala plugin installed, created an Android project, and simply added a Scala facet (ignore the memory settings, I was fooling around):\nFinally, I had to resolve some remaining compilation errors by adding a few \"-dontwarn\" lines to the proguard txt file. Simple enough.Works like a charm. Haven't done anything substantial with it yet, but am looking forward to using Scala on my next Android project.I wrote the  library that makes Android development more concisely with Scala.For a quick starting point, I wrote a  maven project for Scala+Android.I use Intellij as a main IDE, and it imports the project with a charm.I've gone through this about a month ago and I managed to make it work quite nicely using Gradle. Here is the outcome: This is a simple client-server project using Scala and Akka, with client-side on Android, developed with IntelliJ IDEA. I know that client-server doesn't really make sense with Akka, but that's just a simple university project to prove that these technologies can work together.Note however that gradle script in this project is meant only to generate IDEA project, it has to be further extended to be able to perform proper Android application build.I have been watching this thread for several months, finally got some time to do the real tests.Using IntelliJ 13 with SBT 0.13 and Scala 2.10.3."},
{"body": "What is the difference between and The App trait is a convenient way of creating an executable scala program. The difference to the main method altenative is (apart from the obvious syntactic differences) that the App trait uses the delayed initalization feature.From the release notes for 2.9 (see  )These two cases is not same on the scala scripting. was not executed by \"\" command,\nbut the object containing the  method was executed by \"\" command.\nWhich was described as scala looking for object with main method for scripting.When using REPL or scala workseet of Eclipse, \nneed to call  explicitly for both cases.This simple tip be helpful for beginner like me."},
{"body": "I just read  and found it absolutely fascinating.What's the status of the Scala.React package described in the document? I found one tarball of a snapshot of Scala.React but there doesn't seem to be much documentation or active maintenance. I also found ScalaFX, which looks like it might be related to reactive programming, but is similarly unmaintained.Are there any projects out there that build on the ideas in this paper to create a GUI framework based on reactives?: As , there is recent progress!Like you, I have an intense interest in the project, so I creep these pages every few weeks. :)Good news!  It looks like  Ingo Maier and Martin Odersky published a new technical report this year (2012) with some updates to Scala.React.  The new code has been available on GitHub for about two weeks.Relevant links:Technical Report (2012):  GitHub:  Some similar projects:The only project that I currently know of is Naftoli Gugenheim's  project, which he announced in  on the scala mailing list. The code is hosted at "},
{"body": "Or does it just clutter up the code for something the JIT would take care of automatically anyway.I have yet to find a case where it improves performance, and I've tried in quite a few different spots.  The JVM seems to be quite good at inlining when it's possible, and even if you ask for @inline in Scala, it can't always do it (and sometimes I've noticed that it doesn't even when I think it ought to be able to).The place where you expect to see a bytecode difference is in something like this:when compiled with .  And you do see the difference, but it generally doesn't run any faster since the JIT compiler can notice that the same optimizations apply to .So it may be worth a try in the final stages of optimization, but I wouldn't bother doing it routinely without checking to see if it makes a difference in performance."},
{"body": "I'm running on a 32-bit  (Squeeze) system (a 2.5\u00a0GHz  CPU), sun-java6 6.24-1 but with the Scala 2.8.1 packages from Wheezy.This code, compiled with , takes over 30\u00a0seconds to run:But if I make the trivial change of moving the  one line down and inside the scope of , then it runs in just 1\u00a0second, which is much more like I was expecting to see and comparable with the performance of equivalent C++. Interestingly, leaving the  where it is but qualifying it with  also has the same accelerating effect.What's going on here? Why is performing the calculation outside function scope so much slower?The JVM doesn't optimize static initializers (which is what this is) to the same level that it optimizes method calls.  Unfortunately, when you do a lot of work there, that hurts performance--this is a perfect example of that.  This is also one reason why the old  trait was considered problematic, and why there is in Scala 2.9 a  trait that gets a bit of compiler help to move stuff from the initializer into a method that's called later on.(Edit: fixed \"constructor\" to \"initializer\".  Rather lengthy typo!)Code inside a top-level object block is translated to a static initializer on the object's class.  The equivalent in Java would be The HotSpot JVM doesn't perform any optimizations on code encountered during static initializers, under the reasonable heuristic that such code will only be run once.    "},
{"body": "I have some Scala functions defined in a file, not in a class, and I would like to use them in the Scala interpreter. I know I can say  to simply run the file and exit the interpreter, but I would like to run the file and then stay in the interpreter so I can do some testing. Can anyone tell me how to simply load a file into the interpreter so I can use the functions defined within it?type  in Scala REPL.You can get complete list of available commands by typing On occasions,  might be your better friend (than ). Here is an example on how to use .One can also use  to load a file using following command Just reminder, put the complete path. I found problem in Linux by doing like this:to get rid of error \"That file does not exist\" I did"},
{"body": "In theory, Dalvik executes any virtual machine byte code, created for example with the compilers of Are there already working versions of bytecode compilers for Dalvik available for other languages than Java?Scala works very well.I'm programming my Android application projects in Scala (, ), and it is pretty easy to setup the evnviroment (without IDE, using SBT as build tool).It could access every API in Android SDK, so anything you could do in Java, you could do it in Scala too.You may check this  to see how to build Android application with Scala and SBT. is a lovely but little known variant of Scheme that has existed quietly for many years and runs on both the JVM and Dalvik, .  Therefore, its output includes no extra VM and only includes explicitly imported libraries.  To the end-programmer, this means Kawa's performance and executable size are nearly identical to standard Java (ProGuard not required).Kawa also includes lots of macros (including some specific to Android APIs) that make for a nice clean syntax (assuming one is not averse to parentheses), and adds some tasty goodies on top of Scheme, like \"promises\" (lazy eval and futures in one).  The language is quite robust and well-documented, and has been actively maintained and evolving since the early days of Java. summarizes Kawa's merits with some informative examples and links.I haven't played with it but I know that Scala works.Another JVM language that works on Android is  with , both from .  also works on Android, using its Java backend.  I've written a  that should help someone get started (there are still few other examples), and  also for this purpose (although at this exact moment, it's short a few commits.  And neither are using ProGuard yet, so the  size is shocking.)Although the other posts here are cheerful about Scala-on-Android, posters in Scala forums are more concerned by Scala's ability to blow through some of Dalvik's limitations, and people who do use it say they reserve it for non-production code.  (Some discussion about Scala's problems .)  I can't say yet if Mercury has its own problems with Dalvik, but I've switched to it from Scala for the time being.The dynamically typed languages wont be possible until Dalvik supports JIT (Just In Time) compiling. I believe there is JIT support in one of the experimental Eclair branches, but it is not yet officially available/supported in Android.There is a  compiler available now which creates executables for the Android platform.With this solution, developers have access to practically the entire  and Android toolsets. This includes not only the complete set of Android widgets and a graphical designer for laying them out, but also access to the complete Android runtime. The Eclipse IDE will also build the executable and launch the Android emulator (using Run As | Android Application)."},
{"body": "My data could exist in rdds of two forms, either or as As you can see, in the second example the \"mapping\" of the data is enforced by the first element of the tuple being the key. Consider two entries in my rdd, call them R1 and R2. I'll be merging by keys in R1 and R2. When R1 and R2 both contain the same key, I do further merging on those values. As an example, say that both R1 and R2 contain an entrythen the resulting merge will produce So, my question is which data structure is faster and more memory efficient for spark to reduce by outer and inner key? Maps of maps or lists of (key, list_of_tuple). My intuition is that maps would be faster at reducing by key, given their 0(1) lookup. However, given the way that most maps are implemented, I'm sure that there is a decent amount of wasted memory for the map based RDDS. As a real life example of this type of merging, my RDDs are representing I think you have mis-understood about the concept of RDD's. You need to transform your data into appropriate structure to leverage the power of RDD's.SO, you need to think about what you want to compute for deciding your RDD's.As per my understanding of your question. You have 2 data source and you want to merge the data that you got from these 2 data sources. So you create your 2 RDD's from these sources then you merge them."},
{"body": "The following code is the recursive code for splitting a list into two parts.I want to convert this recursive function to an iterative one using a stack.I've tried to solve the problem using stack But It just brings a chaos and complexity in the code In case you want just a simple solution, this is the answerHere's a version using queues.We can change  to a stack, at the expense of a reversebut the problem for  is that we want to add things to one end and remove things from the other end. This isn't a stack, but a queue.So I think you should tell your professor it works better with queues :)"},
{"body": "this is my code:and i want this result:thanks.this is my code:"},
{"body": "Given a file like this: Is it possible to produce this from command line:When asking how to transform data in one form to data in another form it is generally better to be very explicit about what exactly the transformation consists of.  In this case it seems obvious, but who knows?This ---- will give the transformation you show.  It assumes your input data is always in exactly 4 rows.  You don't say, so who knows?  It assumes a particular transformation, but others might be possible -- i.e., it assumes that the third column in the output is the zero-indexed number of the column of the previous item in the input data, not counting the first column. to the rescue!"},
{"body": "Processing distinct and some other operations for a 5MB data using scala writes the output to multiple files. I can see the output in HDFS in 100's of 1kb files. why?Why even for a 5 MB data scala uses two executors? I studied executors are launched based on the partition and partition is based on the block size. My default block size is set to 128MB. So Normally there should be only one executor.According to  the number of executors could be set by enabling: and setting  to the number of executors that is desired."},
{"body": "In Scala, how do I declare a Java List that can hold anything? List[Object] gets upset if I try and put a tuple in it, Scala errors saysI don't know what this means, how do I declare the list to hold a triple (or a tuple, or )My code looks like this (it is twirl, so it has @, but it is just Scala code):Use a Scala (not Java) collection, e.g. . If it needs to be mutable, look at If you insist on using Java collections, you can try , as suggested by the error message.\nAlso note that you  pass Scala collections to a Java app, because at runtime the Scala code is represented by compiled bytecode that is compatible with the JVM."},
{"body": "I have to create an array or list which contains large numbers ( long ) , but I am unable to create one in an elegant way.\nI am currently doing this :How can i do this in an better way like :"},
{"body": "Being new to spark and scala. I need to check how I can achieve this:input:o/p after collecting:I guess this is what you are looking forThanksOutputs :I think this helps you to understand as per your question.OutputOutput"},
{"body": "I'm new to the Scala langauge, I don't know how to install Scala for Android in Eclipse.Try the  from .And you need code editing support to Eclipse for Android install the Scala-IDE plugin. You can get it from ."},
{"body": "I'm using Intellij to do some scala development. I normally use Eclipse but was convinced by a friend to give IntelliJ a go. Finding the lack of a 'Problems' view a real pain. But have another problem.I've got the following code fragement in a test:So firstly I get a red bar on the right hand side of the editor screen (shame there's no 'problem view'), and when I hover the mouse over it it says .Can anyone help me understand this?If I change the ArrayList import to  and remove the prefixs in the code I then get 3 error messages:this is all on the line:So can anyone help me understand this collection of errors? There is no useful help from IntelliJ and a google search didn't shed any light. Lastly if there is anyone out there who has used IntelliJ and Eclipse for scala+java development could you give me an opinion. It seems to me that IntelliJ is a bit s**t. Or maybe I'm just not getting it...Thanks All.Regarding the prefix: withyou import the util namespace i.e. to access a class within util you have to write util.ArrayList. If you want to import everything within util, you need to writeto specifically import ArrayList"},
{"body": "I'm new to Scala but I do know JAVA. Can Scala projects be compiled into web apps and deployed to Webservers like JAVA?Thanks!Scala code is converted into byte code after compilation, hence yes it can be done. this should help for starting"},
{"body": "this code could be very long and ugly, is there a way of me shortening it down using a while loop or anything else? im new to scala programming   Here you go. No explanation of the actual code deliberately, you're after an answer to an assignment. But some general hints - if you find yourself repeating code that's the same apart from the data operated on, then, yes, a while loop might be a possibility but in a language like Scala with a rich selection of functional programming features and a extensive collections library, there are other alternatives.You want to iterate over the various coin sizes. You can do this with an array, and a loop over that array. I have a rule of thumb that if I'm using an index solely to retrieve the current element of the array (that is, the value of the index isn't significant in any other way) then there's probably a more elegant way to do it.In Scala, we have many ways of iterating over a collection.  is suggestive, since it allows us to iterate over an array collecting something as we go, and here we want to collect the coins to use.Another perspective: once I've processed the largest coin, I have some smaller amount of money left, and a smaller set of coins. So I have a smaller problem. Eventually, I have no coins and no money, and nothing more to do.  This divide-and-conquer approach suggests a recursive solution.and an alternative approach"},
{"body": "This is my code:only the first line is getting written but the other lines is not getting written.I am getting the error asMost likely is that your problem is that  Has type  and are calling the  method on Option which pretty much should never be called. The world would be a better place if this method didn't even exist. I digress. If this call to  returns , then the call to  throws an exception that there is no value to get.It seems that is returning a  which will throw an exception when calling .this should never be done! Use a combination of  and  to avoid unexpected behaviour. For instance:or you can even make the  inside the map.If you are not aware, the  method executes when the Option is defined (it's a ) and ignores when the Option is a "},
{"body": "Can you recommend a better way to do this?file lifehow wholife filenow wonsaw waswas sawwho howwon nowFirst, remove the duplicate anagram pairs.Second, alternative to the for ... for ... if ... if structure?Thank you.Here's a short version using for- (which are not equivalent to for loops, and are actually common in Scala):Which prints:As we all know that functional programming paradigm in scala, usually for loop should not be used in scala because it is in imperative way.I am also new in scala. I think you can make a list of key-value pairs that is (array of word)-word and group by key(array of word). Then extract two words in the list and generate a new list.Sorry my answer seems no very clear because english is not my first language. You can use 3 println() statements to see my thought.At last, argsSeq_3 is List(now,won, the,the, file,life, saw,was, how,who)."},
{"body": "can some help me to under stand this code and operator types used or may be used hereThis code executes   times (i expected this variables to be defined in your code). Basically  is a function which will be executed given () number of times.\nTo read more about call-by-name: "},
{"body": "I have a polygon file in ^ separated format I have a case class I want to parse the file in to the class. My file is on HDFS. Can anyone guide me on this? Assuming that a line  means that it is a polygon shape with  and its has five vertices ,,, and .Now you can do the following,"},
{"body": "I have a Array of Objects that I need to apply filter.This code is not getting complied by Scala compiler. I am getting error like \n1) value ? is not a member of boolean\n2) type toUpperCase is not a member of string.Can anyone please help me how to write this ternary operator inside the filter function in scala.I agree that I can write a custom function to handle this as mentioned @  \nHowever, I am interested in why there is compilation error for this statement.\nBecause, this is valid statement in Java.The main issue is that Scala does not support the ternary operator you described. That is supported Java but there's no need for it in Scala.In Java the main difference between the   and the ternary operator is that the latter is an expression, meaning that it's evaluated to a result, while  is (as I previously suggested) a statement that relies on side-effects within its scope to make things happen.In Scala  is already an expression, hence there is no need for a ternary operator.Your code would be structured as follows:As suggested in a comment you can further improve the readability by not relying on s to express simple boolean conditions.Furthermore, in your case,  seems to be external to the list itself, so maybe pulling it out of the  (which has  complexity on ) can save you some cycles:It also looks like you are trying to make a case-insensive equality check on the two string, in which case you may be interested in using a  and not converting  to upper case at every loop:"},
{"body": "Given is data by joining two tables.When I am trying to get following \n val data = joinDataRdd.map(x=>(x._1,x._2._1.split(\",\")(3)))It's is throwing an error :\nvalue split is not a member of (String, String, String, String, String)You are trying to split the tuple so that is why the error message. At the given position  , \n(41234,(,(2014-04-04 00:00:00.0,3182,PENDING_PAYMENT))), the highlighted data is the result. So if you are looking to dig inside the tuple, then you need to advance one position.  It looks like the values are already in a tuple, so you don't need to split the string. Is what you are looking for?"},
{"body": "Pattern matching in SCALAI have 2 lists, one of which contains 20k elements and another contains 200k elements.\nI am performing pattern matching where each of the list_1 elements are matched in list_2. I have written a FOR loop which gives me correct answers but it takes more than 2 hours for complete execution.Is there any other alternative way to perform this task? How can I speed up the process?The code:Just a more scala-fashioned way to do that:Note: I'm assuming that matching at least one (1+) is acceptable, and that you need to not collect unmatched  instead of .Maybe we could improve this by knowing more about your data and eventually also choose a different  library."},
{"body": "I want to call a  with different parameters and calling to this function should happen in parallel.Also my function is not returning anything. So how to know execution is running in parallel or how many threads has been created. Please provide full code for such scenario in Scala. It's quite simple, scala gives you  and  for this reasonThey both roughly work the same way, by converting any collection of the kind  to a collection of , but they are used differently.Now  is semantically different because you feed it an , or a collection of elements, and then a function that converts those elements to a future. For instance:In terms of execution semantics, both constructs execute all futures in parallel and fail if any of the futures fail. There's no guarantee the futures are executed in order.Off topic, but fortunately it's pretty easy to write a \"one at a time logic \nyourself. This works by moving the function that produces the  from an element \"inside\" the for block. Futures in Scala are always created in an already started state, which makes them harder to reason about.Building on the excellent answer of @flavian:"},
{"body": "I need to use akka and actors, what IDE should I use? Do you have any links to add some plugin? I have scala installed but I don't know what I should install. ThanksAs vitalli wrote, when you use Akka you use a library, so you can use any of the popular scala ides ( Eclipse, Intellij, ENSIME ... ) or anything else ( vim, emacs ... )You should check TypeSafe Activator for very good Akka starter / template projects here:"},
{"body": "How could I write function below in regular Recursion and Tail Recursion.Here's a basic recursive version.Here's a tail recursive version.And this is what you do after you've studied the Standard Library.@The Archetypal Paul makes a good point (as he often does) and has offered an alternative.What we're all dancing around, without actually pointing it out, is the defining difference between basic and tail recursion: if there's any more \"work\" to be done after the recursive call returns (calculations/evaluations/etc.) then it is  tail recursive.Here is the tail recursive version of the . The idea is to call the function recursively until the list is over and return  if test fails."},
{"body": "to All!i have this data structure:all List[String] are the same size, it looks like:i need to summarize columns in this lists like this:with such algorithm...help, please!You're looking for the  method:"},
{"body": "When I tried to split a string based on the space as a delimiter, and then apply the contains method on it like belowFor that string when I tried using string_var.contains(\"JAVA\") then it returns true, but when I tried it using then it returns false, is the Array[String] searches the complete elements of the array or matches substrings insides the individual elements?A simple test in the REPL demonstrates the problem.The  method on a  will look for any sub-string, but the  method on an  will only match on individual elements of the array, not sub-strings within the elements.It is returning all the split strings. The reason it does not find \"JAVA\" is because no element in the array containing the split strings is equal to  . Although, if you type:, you will get:"},
{"body": "When trying to import a class that is in a different package, I am getting a the type cannot be resolvedsrc/foo/Foo.scala:src/bar/Bar.java: methods could be accessed in a  way and  methods like  class members."},
{"body": "I'm trying to understand every piece of the implementation of class OWritesOps explained here: The way scala / play doc is structured doesn't really help (in this area Java(doc) beats Scala(doc) hands down).Let's start with: addField:I could only guess that the  is an equivalent of \"and\" (I gather it from here:  )Alright, so let's just take it this way:  create a new instance of  (combo). What about  ? After a lot of digging Play Doc, I came to a (shaky) conclusion that it produces an instance of JsPath? How did I arrive to that conclusion? It's for that  that follows;  has that method.Beside, I read somewhere that  is a shortcut for  (I forgot where, I guess it's in Play Doc as well, but using Scala doc it's almost impossible to find my way back to that page).Ok..., so  produces an instance of . Next: what's this?: Oh..., it's for the apply the parameter to the \"apply\" method of OWrites; it takes an lambda function with the following signature Ok...., so....  is supposed to create an instance of . So, I opened the play doc, . Hmm..., I guess it's this constructor (?):  .... But,... I really doubt it.... I'm lost here.Anyway, let's see an example how it is used: So...,  is supposed to map to a block with the following signature:   ... But..., if that's really the case, I was expecting  .... so I guess  is a syntactic-sugar provided by Scala (?).So... back to the definition of :  executes the block \"_.isAdult\". I can infer here that  is an instance of  then.Can you help me decipher all this?Scala is cool, but it's doc system (still) sucks :( is the name of a method invoked on the instance of .  is the same as .  the brackets around the argument of . There are two things relevant to this equivalent form: However,  has no method called  in its interface. If you import , there's an implicit conversion and  defines the method and an alias for it: The package  defines a value , so the double underscores are just an alias for the singleton object . The singleton object  is not to be confused with the equally-named class. This construct is called a companion object in Scala. While Java puts static members next to the regular members in a class, Scala puts them inside the companion object. Apparently, the object  is at the same time the companion object of the case class  and an instance of .  is of type  and  defines a method which is equivalent to The default apply method of a case class' companion object creates a new instance, so this is equivalent to As our singleton object  is an instance of  with the empty list for , this returns us a , where It's a function taking an instance of . The function maps the argument  to the tuple , where  is a function provided as argument to your method . It has the signature , so it is a function that takes an instance of  and returns an instance of . This may be a bit confusing, so let me give you an example function that does not operate on generics. Say you want to define a function  that squares an integer number. For example,  should be ,  should be . As it takes an integer and returns an integer, its signature is . It's implementation is . Actually, this is not correct - the signatures do not match. The given lambda function is of type . Also, as pointed out in the first part of the answer, the method  returns an instance of . Unfortunatly, I am lost on what the called -method does, because it is highly generic and uses implicit arguments of a generic type. Yes, the underscore in lambda functions is syntactic sugar for capturing the arguments. For example,  is syntactic sugar for , meaning the following two values are equivalent:So  really just translates to  and in the example, .The idea behind this write builder seems to be to provide means of defining a mapping from one of your Scala classes to JSON. Let's take a look again at the . Now, let's assume we have an instance of :  and want to serialize the instance to the following JSON: The key is always a string, but the value can be of a different type. So, basically, the serialization is a function taking a  and returning a tuple of key-value pairs where the value is always a string, i.e., . Or, in the general case where we do not know what we are going to serialize, simply that it has two fields, it is  for arbitrary types , , and . This is what  emulates - the  standing for the number of members to serialize. There's also a ,  and so on. We can instantiate a  that is appropriate to our example, by calling Here, the type of  is Now, the apply method of  can be used to generate an instance of the first generic argument - . The argument to the apply method is a function that maps an instance of our type to serialize (here ) to a tuple of the desired value types, here  and  (representing  and ). This writer can now be used to generate a JSON string from any instance of . So we can pass our person and get the desired output: "},
{"body": "I am using java.text.SimpleDateFormat in Scala to convert string to date.How to fix this ? Are there any alternative?Your problem is that you have \"22\" in the month position.  The date format you set is day/month/year, but it looks like the date you sent is in the format month/day/year.The formatting is different between the dates. You have mixed the month and Day up in your statements. try this:goodluck with the code."},
{"body": "I got this problem when writing a recursive function that calculates the number of points where two general functions f1 and f2 are equal(Assuming only Integer values).And this is what compiler says :Thank you!The  construct in Scala is an expression. As the others already said, there's no ternary operator because there's  - the if being already an expression.I rewrote your function to a tail-recursive version (to avoid s), let's see how it looks:Notice how the result of the if expression is assigned to  - here you would normally use the ternary operator. Anyway, I hope this helps you.the  operator doesn't exist in scalaScala does not use the ternary operator, "},
{"body": "I have a following list-I want to find a string  in above list.\nLike first string in list contains given string .I  given string with list elementsHow do I find string in list using scala??Looking for a substringIf you need an exact match, just replace  with ."},
{"body": "I got a assignment for balancing parentheses using scala. I wrote this code:but this code fails in  \nany idea to implement this code correct?Not giving any code away, just clarifying the likely spec.  All your code is doing is counting the number of right and left parens, and making sure they are equal.  That's a necessary condition, but not sufficient.   For balanced parentheses, you also need to show that as you scan through the string, the number of '('s you have seen must always be greater than or equal to the number of ')'s seen."},
{"body": "I have just written this code for some fun and I have a question why it isn't working out? Seems like nothing is written to the list, but why is that so? Do I have to make it mutable? And why is here the  operator not working properly?Because  returns a new list, where the first element is , followed by .So, to make your code , make list a  and re-assign it inside the loop:However, the idiomatic way of doing this in Scala is to not use mutations at all (local mutable state is fine sometimes though). \"While\" loops can usually be replaced with a recursive function:This particular scenario can also be solved using  instead"},
{"body": "I have an RDD of Array[Int] and I want the sum of all elements in each array in the form of RDD[Int].\nWhat is the best way to achieve thisThe answer may be as simple as "},
{"body": "I want to know what exactly the .get method does in scala. getParam() is already returning the parameters to the post hit . I know that .get will make it easier as we dont have to \"match\" to check for null values as it will automotically thrown an exception in the former case.\nIs there more to it than meets the eye?It's usually  a function on Options (i.e. Some or None). It gets you the contained element if it exists, otherwise it throws a NoSuchElementException.As a side note, you should try to avoid using  because it lands you back in the land of null-pointer exceptions. Instead, try to use , or continue to use your Option value through higher-order functions like , , ,  etc.Here is an example of how you can use it to your advantage:You can just pretend that the value is always specified if you don't \"touch\" it directly.I suppose that's some  related code, if that's the case,  return an . s are a wrapper around types that allow you to avoid having to check for s (and other kind of utilities too), in fact a value wrapped in an  can be , in which case you can use get to access the value, e.g.Or can be a  in which case when calling  you get an exception, wether a value is a  or  can be determined via param matchOr using  which returns true if it's , false if it's .Note that your code could throw exceptions since you call  without knowing if it's a  or , you should use  which returns the value the Option holds if there's any, or a default specified parameter:"},
{"body": "I want to apply preprocessing phase on a large amount of text data in Spark-Scala such as  , there is any way to implement them in Spark - Scala ? for example here is one sample of my data: after preprocessing:and they have POS tags e.g there is a POS tagging (sister.arizona) is it applicable in Spark?  Anything is possible. The question is what YOUR preferred way of doing this would be. For example, do you have a stop word dictionary that works for you (it could just simply be a Set), or would you want to run TF-IDF to automatically pick the stop words (note that this would require some supervision, such as picking the threshold at which the word would be considered a stop word). You can provide the dictionary, and Spark's MLLib .The POS tags step is tricky. Most NLP libraries on the JVM (e.g. Stanford CoreNLP) don't implement java.io.Serializable, but you can perform the map step using them, e.g.On the other hand, don't emit an RDD that contains non-serializable classes from that NLP library, since steps such as collect(), saveAsNewAPIHadoopFile, etc. will fail. Also to reduce headaches with serialization, use Kryo instead of the default Java serialization. There are numerous posts about this issue if you google around, but see  and .Once you figure out the serialization issues, you need to figure out which NLP library to use to generate the POS tags. There are plenty of those, e.g. ,  and  for Java,  for Scala, etc. Note that you can of course use the Java NLP libraries with Scala, including with wrappers such as the University of Arizona's  around Stanford CoreNLP, etc.Also, why didn't your example lower-case the processed text? That's pretty much the first thing I would do. If you have special cases such as iPod, you could apply the lower-casing except in those cases. In general, though, I would lower-case everything. If you're removing punctuation, you should probably first split the text into sentences (split on the period using regex, etc.). If you're removing punctuation in general, that can of course be done using regex.How deeply do you want to stem? For example, the Porter stemmer (there are implementations in every NLP library) stems so deeply that \"universe\" and \"university\" become the same resulting stem. Do you really want that? There are less aggressive stemmers out there, depending on your use case. Also, why use stemming if you can use lemmatization, i.e. splitting the word into the grammatical prefix, root and suffix (e.g. walked = walk (root) + ed (suffix)). The roots would then give you better results than stems in most cases. Most NLP libraries that I mentioned above do that.Also, what's your distinction between a stop word and a non-useful word? For example, you removed the pronoun in the subject form \"I\" and the possessive form \"my,\" but not the object form \"me.\" I recommend picking up an NLP textbook like \"Speech and Language Processing\" by Jurafsky and Martin (for the ambitious), or just reading the one of the engineering-centered books about NLP tools such as  for Java,  for Python, etc., to get a good overview of the terminology, the steps in an NLP pipeline, etc.There is no built-in NLP capability in Apache Spark. You would have to implement it for yourself, perhaps based on a non-distributed NLP library, as described in marekinfo's excellent answer.I would suggest you to take a look in spark's ml pipeline. You may not get everything out of the box yet, but you can build your capabililties and use pipeline as a framework.."},
{"body": "Groovy has a nice GUI Console in which I can enter any java/groovy code and run.\nThis is distributed with groovy.As for as I know Scala does not have anything like that except for REPL.Is there any third party Scala GUI Console that I can download and install and run from command line and not from a JNLP file using webstart?One use case:Suppose I have Scala program:In Groovy Console (which is a full blown GUI) I can easily copy and paste above program. I can easily modify the program in GUI and quickly hit the run button and quickly see the result. I can even easily copy and paste the results and post it somewhere else. I can easily copy lines of Scala code from somewhere else and copy into GUI Console and hit the run button and I can also copy certain jars into a location that is in the classpath of Groovy Console and so on and on long list of lot more features. Can you do that in REPL or is there another tool you can use?For the kind of exploratory coding that you're talking about, you should check out .  Just ignore the parts designed for teaching children how to program (e.g. drag the border between the turtle window and the rest until it's as small as it can get), and you find that you have a full-fledged GUI with syntax highlighting and everything that lets you run code with a press of a button (or two presses of the keyboard).  Or maybe you want to use the turtle to do the drawing, given the example you gave.You might also be interested in  or .You definitely looking for an .There is plugin for theses IDE:My personal favourite is using a simple text editor with , you can use for example:For whatever it is worth, if you don't want to try out swing stuff, you might even go with ."},
{"body": "The below code words perfectly fine in Pyspark (Spark 1.6.2). Can any one please help me with the relevant code in Scala Output :"},
{"body": "i want to run the streaming k-means-example.scala code source (mllib) on spark , someone tell me how i can how I can display the content of clusters after clustering (for example i want to clustering data into 3 clusters , how i can display the cntent of the 3 clusters in 3 files and the content of centers in file.txt)You would have to use the predict method on your RDD( )\nThen you could zip your Rdd containing values and your RDD of predicted clusters they fall in.i am a new in this field so can you please be more clear , how can i traduct this ( use the predict method on your RDD Then you could zip your Rdd containing values and your RDD of predicted clusters they fall in ) in source code with scala !!"},
{"body": "Given these two collections :How can I check if  contains a tuple with the same elements as  ?This should work:because set are unordered dont use But use instead a tuple And then you could do"},
{"body": "Occasionally I want to print values, arrays, expressions, etc. quickly and be able to locate the print statements. How do I do this in Scala, short of \"println\"?Use this tool: ^ It's really handy for tracing values, containers, expressions, etc. ^If you want something a little more plain text editor friendly, try: or It's good for debugging things like multiple threads, separate processes, or anywhere else where debuggers tend to choke. Combine it with a debugger/logger."},
{"body": "I am new to Scala facing  in below codeif (cell != null && cell.getCellType != Cell.CELL_TYPE_BLANK) formatting was wrong..Thanks @Tzach Zohar"},
{"body": "I'm a new Scala programmer, and i would like to iterate and call another method passing the Something by parameter, but i really don't know how to do it."},
{"body": " the  Programming Language run internally ?\n of using Scala over Java?Scala is compiled to Java bytecode, executed by the same JVM that runs Java programs, using the same JDK class library plus a small extra Scala runtime library.The advantages are mostly for the programmer."},
{"body": "Has anyone successfully used sonar-scala plugin when using \"maven sonar:sonar\"?  I got NullPointerException caused by As of today (March 12th, 2014), the Scala plugin has not been released yet as you can see it on . Note that this effort is only driven by the community, so you have to periodically check this page (or subscribe to announcements on the user mailing list) to know if/when you can use this plugin."},
{"body": "I am using MVC architecture in Play Framework in which the view part is a mix of scala and html. Now, I need to sort a Map coming from the controller layer in my view. As scala has no memory its being really hard to create a sort function to sort my map by timestamp.\nCan anybody please help i'm really not able to solve this one as i am new to scala.This is my First Question. Any help would be Appreciated.Assuming the timestamps are the  of the map (i.e. it's a ):That is, you're converting the map to a  first, so that element ordering will be preserved (maps are normally s, with an unpredictable ordering), then sorting that sequence by the first element of the key-value tuple.If the timestamps are the values, replace the  with .Note that if you convert the intermediate  back to a  (e.g. with ), you'll be back in  land again, and your sort will be gone.So, this might be more like what you want:Here, we convert  to a  just so that it can be passed to the varargs factory method of ."},
{"body": "This is my scalatest file which tests savedat method.savedat enters an Employeeentry object in my table and returns number of rows saved which is 1 in my case:This works : The error was Play frequently requires a running Application as context which WithApplication object provides."},
{"body": "I have a  and I want to iterate through each object and check if one of its boolean properties are true/false. I've tried to use  followed by  but it's not working:Assuming an object like the followingand a sequence like thisYou would then use the following logic to filter out false boolean conditions on boolPropThis operation return a new sequence with only the objects that have a boolProp of true. Note, this does not  the sequence, but instead returns a new one.Imagine you have the following:And you create a list of these guys:Now if you want to find out if one of them has  set to , you just do:This is my code snippet, i am new to scala ..i am getting a error for type mismatch here ...saying that expected is boolean.. Child has property of expandable reservation is it is true then only extract that child from the sequence of childs"},
{"body": "I have a list:I will do the list that it will create the number of children equal to the dog's age. Could you help me?  Thanks!!!Result that I want is: List[Dog] = List[Dog(5),Dog(5),Dog(5),Dog(5),Dog(5), Dog(2),Dog(2), Dog(2),Dog(2)),Dog(4),Dog(4),Dog(4),Dog(4),Dog(4),Dog(4),Dog(4),Dog(4),Dog(4),Dog(4),Dog(4),Dog(4), Dog(8),Dog(8),Dog(8),Dog(8), Dog(8),Dog(8),Dog(8),Dog(8)]Your question is a bit unclear, so you might want to clean it up, but I think I understand what you mean - assuming your Dog is defined something likeThen you can create a map like thisBTW, you can create your list like this if you like:I've seen you've changed your question, you want thisres13: List[Dog] = List(Dog(5), Dog(5), Dog(5), Dog(5), Dog(5), Dog(2), Dog(2), Dog(2), Dog(2), Dog(4), Dog(4), Dog(4), Dog(4), Dog(4), Dog(4), Dog(4), Dog(4), Dog(4), Dog(4), Dog(4), Dog(4), Dog(8), Dog(8), Dog(8), Dog(8), Dog(8), Dog(8), Dog(8), Dog(8))where List.fill(n)(e) will create a list of e f'spretty simple I guess:list.flatMap(dog => {for (i <- (0 to dog.age) ) yield dog.clone} )"},
{"body": "Wordcount program is easy, but how to get word list of items based on key.(Not number of count)\nLet eg:\nLocation, Items\nBangalore,TV\nBangalore,Mobile\nHyderabad,LaptopNow I need output like this:\nBangalore, (TV, Mobile)\nHyderabad, Laptop\nIt may Json or csv or any format.Cityitems.csv:\nCity,Items\nBangalore,Mobile\nBangalore,Laptop\nBangalore,Mobile\nBangalore,Desktop\nHyderabad,Cooker\n    val data=sc.textFile(\"s3://path/Cityitems.csv\").cache()\n    val rows = data.map(line => line.split(\",\"))"},
{"body": "I want to generate Sequence of Numbers column(Seq_No) as Product_IDs changes in table. In my input table I have only Product_IDs and want output with Seq_No. We can not use GropuBy or Row Number over partition in SQL as Scala does not support.So I want to generate Seq_No as current Product_Id is not equal to previous Product_Ids. Input table has only one column Product_IDs and we want Product_IDs with Seq_No using Spark Scala.I would probably just write a function to generate the sequence numbers:UPDATE:I'm not quite clear on what you want beyond that, Nikhil, and I am not a Spark expert, but I imagine you want something like"},
{"body": "Let me first say that I have quite a lot of Java experience, but have only recently become interested in functional languages. Recently I've started looking at Scala, which seems like a very nice language.However, I've been reading about Scala's Actor framework in , and there's one thing I don't understand. In chapter 30.4 it says that using  instead of  makes it possible to re-use threads, which is good for performance, since threads are expensive in the JVM.Does this mean that, as long as I remember to call  instead of , I can start as many Actors as I like? Before discovering Scala, I've been playing with Erlang, and the author of  boasts about spawning over 200,000 processes without breaking a sweat. I'd hate to do that with Java threads. What kind of limits am I looking at in Scala as compared to Erlang (and Java)?Also, how does this thread re-use work in Scala? Let's assume, for simplicity, that I have only one thread. Will all the actors that I start run sequentially in this thread, or will some sort of task-switching take place? For example, if I start two actors that ping-pong messages to each other, will I risk deadlock if they're started in the same thread?According to , writing actors to use  is more difficult than with . This sounds plausible, since  doesn't return. However, the book goes on to show how you can put a  inside a loop using . As a result, you getwhich, to me, seems pretty similar towhich is used earlier in the book. Still, the book says that \"in practice, programs will need at least a few 's\". So what am I missing here? What can  do that  cannot, besides return? And why do I care?Finally, coming to the core of what I don't understand: the book keeps mentioning how using  makes it possible to discard the call stack to re-use the thread. How does that work? Why is it necessary to discard the call stack? And why can the call stack be discarded when a function terminates by throwing an exception (), but not when it terminates by returning ()?I have the impression that  has been glossing over some of the key issues here, which is a shame, because otherwise it's a truly excellent book.First, each actor waiting on  is occupying a thread. If it never receives anything, that thread will never do anything. An actor on  does not occupy any thread until it receives something. Once it receives something, a thread gets allocated to it, and it is initialized in it.Now, the initialization part is important. A receiving thread is expected to return something, a reacting thread is not. So the previous stack state at the end of the last  can be, and is, wholly discarded. Not needing to either save or restore the stack state makes the thread faster to start.There are various performance reasons why you might want one or other. As you know, having too many threads in Java is not a good idea. On the other hand, because you have to attach an actor to a thread before it can , it is faster to  a message than  to it. So if you have actors that receive many messages but do very little with it, the additional delay of  might make it too slow for your purposes.The answer is \"yes\" - if your actors are not blocking on anything in your code and you are using , then you can run your  program within a single thread (try setting the system property  to find out).One of the more obvious reasons why it is  is that otherwise the  method would end in a . As it is, the framework rather cleverly ends a  by throwing a , which is caught by the looping code which then runs the  again via the  method.Have a look at the  method in  and then the  method to see how the loop reschedules itself - terribly clever stuff!Those statements of \"discarding the stack\" confused me also for a while and I think I get it now and this is my understanding now. In case of \"receive\" there is a dedicated thread blocking on the message (using object.wait() on a monitor) and this means that the complete thread stack is available and ready to continue from the point of \"waiting\" on receiving a message. \nFor example if you had the following code the thread would wait in the receive call until the message is received and then would continue on and print the \"after receive and printing a 10\" message and with the value of \"10\" which is in the stack frame before the thread blocked. In case of react there is no such dedicated thread, the whole method body of the react method is captured as a closure and is executed by some arbitrary thread on the corresponding actor receiving a message. This means only those statements that can be captured as a closure alone will be executed and that's where the return type of \"Nothing\" comes to play. Consider the following codeIf react had a return type of void, it would mean that it is legal to have statements after the \"react\" call ( in the example the println statement that prints the message \"after react and printing a 10\"), but in reality that would never get executed as only the body of the \"react\" method is captured and sequenced for execution later (on the arrival of a message). Since the contract of react has the return type of \"Nothing\" there cannot be any statements following react, and there for there is no reason to maintain the stack. In the example above variable \"a\" would not have to be maintained as the statements after the react calls are not executed at all. Note that all the needed variables by the body of react is already be captured as a closure, so it can execute just fine. The java actor framework  actually does the stack maintenance by saving the stack which gets unrolled on the react getting a message. Just to have it here:These papers are linked from the scala api for Actor and provide the theoretical framework for the actor implementation. This includes why react may never return.I haven't done any major work with scala /akka, however i understand that there is a very significant difference in the way actors are scheduled. \nAkka is just a smart threadpool which is time slicing execution of actors... \nEvery time slice will be one message execution to completion by an actor unlike in Erlang which could be per instruction?!This leads me to think that react is better as it hints the current thread to consider other actors for scheduling where as receive \"might\" engage the current thread to continue executing other messages for the same actor. "},
{"body": "What are the differences among Streams, Views (SeqView), and Iterators in scala? This is my understanding:So if I want to save heap space, should I use iterators (if I won't traverse the list again) or views? Thanks. First, they are all . That has a particular mathematical meaning related to functions, but, basically, means they are computed on-demand instead of in advance. is a lazy list indeed. In fact, in Scala, a  is a  whose  is a . Once computed, a value stays computed and is reused. Or, as you say, the values are cached.An  can only be used once because it is a   into a collection, and not a collection in itself. What makes it special in Scala is the fact that you can apply transformation such as  and  and simply get a new  which will only apply these transformations when you ask for the next element.Scala used to provide iterators which could be reset, but that is very hard to support in a general manner, and they didn't make version 2.8.0.Views are meant to be viewed much like a database view. It is a series of transformation which one applies to a collection to produce a \"virtual\" collection. As you said, all transformations are re-applied each time you need to fetch elements from it.Both  and views have excellent memory characteristics.  is nice, but, in Scala, its main benefit is writing infinite sequences (particularly sequences recursively defined). One  avoid keeping all of the  in memory, though, by making sure you don't keep a reference to its  (for example, by using  instead of  to define the ).Because of the penalties incurred by views, one should usually  it after applying the transformations, or keep it as a view if only few elements are expected to ever be fetched, compared to the total size of the view."},
{"body": "On the surface Groovy and Scala look pretty similar, aside from Scala being statically typed, and Groovy dynamic.They're both object oriented languages for the JVM that have lambdas and closures and interoperate with Java.  Other than that, they're extremely different.Groovy is a \"dynamic\" language in not only the sense that it is dynamically typed but that it supports dynamic meta-programming.Scala is a \"static\" language in that it is statically typed and has virtually no dynamic meta-programming beyond the awkward stuff you can do in Java.  Note, Scala's static type system is substantially more uniform and sophisticated than Java's.Groovy is syntactically influenced by Java but semantically influenced more by languages like Ruby.Scala is syntactically influenced by both Ruby and Java.  It is semantically influenced more by Java, SML, Haskell, and a very obscure OO language called gBeta. Groovy has \"accidental\" multiple dispatch due to the way it handles Java overloading.Scala is single dispatch only, but has SML inspired pattern matching to deal with some of the same kinds of problems that multiple dispatch is meant to handle.  However, where multiple dispatch can only dispatch on runtime type, Scala's pattern matching can dispatch on runtime types, values, or both.  Pattern matching also includes syntactically pleasant variable binding.  It's hard to overstress how pleasant this single feature alone makes programming in Scala.Both Scala and Groovy support a form of multiple inheritance with mixins (though Scala calls them traits).Scala supports both partial function application and currying at the language level, Groovy has an awkward \"curry\" method for doing partial function application.Scala does direct tail recursion optimization.  I don't believe Groovy does.  That's important in functional programming but less important in imperative programming.Both Scala and Groovy are eagerly evaluated by default.  However, Scala supports call-by-name parameters. Groovy does not - call-by-name must be emulated with closures.Scala has \"for comprehensions\", a generalization of list comprehensions found in other languages (technically they're monad comprehensions plus a bit - somewhere between Haskell's do and C#'s LINQ).Scala has no concept of \"static\" fields, inner classes, methods, etc - it uses singleton objects instead.  Groovy uses the static concept.Scala does not have built in selection of arithmetic operators in quite the way that Groovy does.  In Scala you can name methods very flexibly.Groovy has the elvis operator for dealing with null.  Scala programmers prefer to use Option types to using null, but it's easy to write an elvis operator in Scala if you want to.Finally, there are lies, there are damn lies, and then there are benchmarks.  The computer language benchmarks game ranks Scala as being between substantially faster than Groovy (ranging from twice to 93 times as fast) while retaining roughly the same source size. .  I'm sure there are many, many differences that I haven't covered.  But hopefully this gives you a gist.Is there a competition between them?  Yes, of course, but not as much as you might think. Groovy's real competition is JRuby and Jython.Who's going to win?  My crystal ball is as cracked as anybody else's.Scala has Actors, which make concurrency much easier to implement. And Traits which give true, typesafe multiple inheritance.scala is meant to be an oo/functional hybrid language and is  well planned and designed. groovy is more like a set of enhancements that many people would love to use in java.\ni took a closer look at both, so i can tell :)neither of them is better or worse than the other. groovy is very good at meta-programming, scala is very good at everything that does not need meta-programming, so...i tend to use both.You've hit the nail on the head with the static and dynamic typing.  Both are part of the new generation of dynamic languages, with closures, lambda expressions, and so on.  There are a handful of syntactic differences between the two as well, but functionally, I don't see a huge difference between Groovy and Scala.Scala implements Lists a bit differently; in Groovy, pretty much everything is an instance of java.util.List, whereas Scala uses both Lists and primitive arrays.  Groovy has (I think) better string interpolation.Scala is faster, it seems, but the Groovy folks are really pushing performance for the 2.0 release.  1.6 gave a huge leap in speed over the 1.5 series.I don't think that either language will really 'win', as they target two different classes of problems.  Scala is a high-performance language that is very Java-like without having quite the same level of boilerplate as Java.  Groovy is for rapid prototyping and development, where speed is less important than the time it takes for programmers to implement the code.We have extensively used Groovy and can say with confidence that it is extremely good in developing production quality web applications, not just for prototyping. It is very easy to use and allows fast development, so much so that it is hard to wean away developers from Groovy to plan Java after they have used it for sometime, in my experience.I haven't used Scala much and so can't say much about it.Scala has a much steeper learning curve than Groovy. Scala has much more support for functional programming with its pattern matching and tail based recursion, meaning more tools for pure FP. Scala also has dynamica compilation and I have done it using twitter eval lib ( ). I kept scala code in a flat file(without any extension) and using eval created scala class at run time. \nI would say scala is meta programming and has feature of dynamic complication "},
{"body": "These lines of code outputs , even though  was successfully executed. I found that this happens because I used def in . If I use var or val the output is . I understand the default is val in scala. This:...gives a compilation error because it is a val by default. Why do the first set of lines above not work properly, and yet also don't error?There are three ways of defining things in Scala:Looking at your code:This defines a new method called . You can call this method only without  because it is defined as parameterless method. For empty-paren method, you can call it with or without '()'. If you simply write:then you are calling this method (and if you don't assign the return value, it will just be discarded). In this line of code:what happens is that you first call the  method, and on the return value (an instance of class ) you are changing the  member variable. And the last line:Here you are again calling the  method, which returns a new instance of class  (with  set to 12). It's the same as this:I'd start by the distinction that exists in Scala between ,  and .  According to above, labels from  and  cannot be reassigned, and in case of any attempt an error like the below one will be raised:When the class is defined like:and then instantiated with:an  is created for that specific instance of Person (i.e. 'personA'). Whenever the mutable field 'age' needs to be modified, such attempt fails:as expected, 'age' is part of a non-mutable label. The correct way to work on this consists in using a mutable variable, like in the following example:as clear, from the  (i.e. 'personB') it is possible to modify the class mutable field 'age'.I would still stress the fact that everything comes from the above stated difference, that has to be clear in mind of any Scala programmer.With you are defining a function/lazy variable which always returns a new Person instance with name \"Kumar\" and age 12. This is totally valid and the compiler has no reason to complain. Calling person.age will return the age of this newly created Person instance, which is always 12.When writingyou assign a new value to the age property in class Person, which is valid since age is declared as . The compiler will complain if you try to reassign  with a new Person object likeTo provide another perspective, \"def\" in Scala means something that will  when it's used, while val is something that is . Here, the expression  entails that whenever we use \"person\" we will get a  call. Therefore it's natural that the two \"person.age\" are non-related.This is the way I understand Scala(probably in a more \"functional\" manner). I'm not sure ifis really what Scala intends to mean though. I don't really like to think that way at least...As Kintaro already says, person is a method (because of def) and always returns a new Person instance. As you found out it would work if you change the method to a var or val:Another possibility would be:However,  in your code is allowed, as you get back a  instance from the  method, and on this instance you are allowed to change the value of a . The problem is, that after that line you have no more reference to that instance (as every call to  will produce a new instance).This is nothing special, you would have exactly the same behavior in Java:Let's take this:and rewrite it with equivalent codeSee,  is a method. It will execute each time it is called, and each time it will return (a) . And these is no error in the \"assignment\" because it isn't really an assignment, but just a call to the  method (provided by )."},
{"body": "I wonder if there is a way to make asynchronous calls to a database?For instance, imagine that I've a big request that take a very long time to process, I want to send the request and receive a notification when the request will return a value (by passing a Listener/callback or something). I don't want to block waiting for the database to answer.I don't consider that using a pool of threads is a solution because it doesn't scale, in the case of heavy concurrent requests this will spawn a very large number of threads.We are facing this kind of problem with network servers and we have found solutions by using select/poll/epoll system call to avoid having one thread per connection. I'm just wondering how to have a similar feature with database request?Note:\nI'm aware that using a FixedThreadPool may be a good work-around, but I'm surprised that nobody has developed a system really asynchronous (without the usage of extra thread).** Update **\nBecause of the lack of real practical solutions, I decided to create a library (part of finagle) myself: . It basically decodes/decodes mysql request/response, and use Finagle/Netty under the hood. It scales extremely well even with huge number of connections.I don't understand how any of the proposed approaches that wrap JDBC calls in Actors, executors or anything else can help here - can someone clarify.Surely the basic problem is that the JDBC operations block on socket IO. When it does this it blocks the Thread its running on - end of story. Whatever wrapping framework you choose to use its going to end up with one thread being kept busy/blocked per concurrent request.If the underlying database drivers (MySql?) offers a means to intercept the socket creation (see SocketFactory) then I imagine it would be possible to build an async event driven database layer on top of the JDBC api but we'd have to encapsulate the whole JDBC behind an event driven facade, and that facade wouldn't look like JDBC (after it would be event driven).  The database processing would happen async on a different thread to the caller, and you'd have to work out how to build a transaction manager that doesn't rely on thread affinity.Something like the approach I mention would allow even a single background thread to process a load of concurrent JDBC exec's. In practice you'd probably run a pool of threads to make use of multiple cores.(Of course I'm not commenting on the logic of the original question just the responses that imply that concurrency in a scenario with blocking socket IO is possible without the user of a selector pattern - simpler just to work out your typical JDBC concurrency and put in a connection pool of the right size).Looks like MySql probably does something along the lines I'm suggesting --- \nIt's impossible to make an asynchronous call  via JDBC, but you can make asynchronous calls  with  (e.g., actor makes calls to the DB via JDBC, and sends messages to the third parties, when the calls are over), or, if you like CPS, with  (a good implementation is  )Scala actors by default are event-based (not thread-based) - continuation scheduling allows creating millions of actors on a standard JVM setup. If you're targeting Java,  is an Actor model implementation that has a good API both for Java and Scala.Aside from that, the synchronous nature of JDBC makes perfect sense to me. The cost of a database session is far higher than the cost of the Java thread being blocked (either in the fore- or background) and waiting for a response. If your queries run for so long that the capabilities of an executor service (or wrapping Actor/fork-join/promise concurrency frameworks) are not enough for you (and you're consuming too many threads) you should first of all think about your database load. Normally the response from a database comes back very fast, and an executor service backed with a fixed thread pool is a good enough solution. If you have too many long-running queries, you should consider upfront (pre-)processing - like nightly recalculation of the data or something like that.Perhaps you could use a JMS asynchronous messaging system, which scales pretty well, IMHO:There is no direct support in JDBC but you have multiple options like MDB, Executors from Java 5.\"I don't consider that using a pool of threads is a solution because it doesn't scale, in the case of heavy concurrent requests this will spawn a very large number of threads.\"I am curious why would a bounded pool of threads is not going to scale? It is a pool not thread-per-request to spawn a thread per each request. I have been using this for quite sometime on a heavy load webapp and we have not seen any issues so far.The  might come handy.You can have a fixed number of threads to handle long-running operations. And instead of  you can use , which return a result. The result is encapsulated in a  object, so you can get it when it is back.Ajdbc project  seems to answer this problem There is currently 2 experimental natively async drivers for mysql and postgresql.Just a crazy idea : you could use an Iteratee pattern over JBDC resultSet wrapped in some Future/PromiseHammersmith does that for MongoBd.An old question, but some more information.  It is not possible to have JDBC issue asynchronous requests to the database itself, unless a vendor provides an extension to JDBC and a wrapper to handle JDBC with.  That said, it is possible to wrap JDBC itself with a processing queue, and to implement logic that can process off the queue on one or more separate connections.  One advantage of this for some types of calls is that the logic, if under heavy enough load, could convert the calls into JDBC batches for processing, which can speed up the logic significantly.  This is most useful for calls where data is being inserted, and the actual result need only be logged if there is an error.  A great example of this is if inserts are being performed to log user activity.  The application won't care if the call completes immediately or a few seconds from now.As a side note, one product on the market provides a policy driven approach to allowing asynchronous calls like those I described to be made asynchronously ().  Disclaimer:  I am co-founder of this company.  It allows regular expressions to be applied to data transformation requests such as insert/update/deletes for any JDBC data source, and will automatically batch them together for processing.  When used with MySQL and the rewriteBatchedStatements option () this can significantly lower overall load on the database.You have three options in my opinion: I am one of the developers of CoralMQ.The commons-dbutils library has support for an  which you provide an  to and it returns a . Worth checking out as it's simple to use and ensure you won't leak resources.I am just thinking ideas here. Why couldn't you have a pool of database connections with each one having a thread. Each thread has access to a queue. When you want to do a query that takes a long time, you can put on the queue and then one of threads will pick it up and handle it. You will never have too many threads because the number of your threads are bounded. Edit: Or better yet, just a number of threads. When a thread sees something in a queue, it asks for a connection from the pool and handles it. "},
{"body": "I have such a piece of code from :Everything in it is pretty clear, except this piece: \nWhat does it do? I under stand there is Seq[Node] concatenated with another Node, and then? What does  do?Thanks in advanceIt \"splats\" the sequence.Look at the constructor signaturewhich is called asbut here there is only a sequence, not , , etc. so this allows the result sequence to be used as the input to the constructor.Happy coding. This doesn't have a cutesy-name in the SLS, but here are the details. The important thing to get is that it changes how Scala binds the arguments to the method with repeated parameters (as denoted with  above).The  is covered in \"4.6.2 Repeated Parameters\" of the SLS. expands  to  (tells the compiler that we're rather working with a varargs, than a sequence). Particularly useful for the methods that can accept only varargs."},
{"body": "How in the world do you get just an element at index  from the List in scala?I tried , and   - nothing works. Googling only returns how to \"find\" an element in the list. But I already know the index of the element!Here is the code that does not compile:Looking at the  does not help, as my eyes just cross.Use parentheses:But you don't really want to do that with lists very often, since linked lists take time to traverse.  If you want to index into a collection, use  (immutable) or  (mutable) or possibly  (which is just a Java array, except again you index into it with  instead of ).Safer is to use  so you can extract the value if it exists and fail gracefully if it does not.This will return None if the list isn't long enough to provide that element, and Some(value) if it is.Whenever you're performing an operation that may fail in this way it's great to use an Option and get the type system to help make sure you are handling the case where the element doesn't exist.This works because List's  (which sugars to just parentheses, e.g. ) is like a partial function that is defined wherever the list has an element. The  method turns the partial  function (a function that is only defined for some inputs) into a normal function (defined for any input) by basically wrapping the result in an Option.Why parentheses? Here is the quote from the book . Here are a few examples how to pull certain element (first elem in this case) using functional programming style. "},
{"body": "Say I have got following two es:and the following instance of  class:Now if I want to update  of  then I will have to do:With more levels of nesting this gets even more uglier. Is there a cleaner way (something like Clojure's ) to update such nested structures? provides convenient traversal and 'mutation' of an immutable data structure. Scalaz provides Zippers for  (), and  (). It turns out that the structure of the zipper is automatically derivable from the original data structure, in a manner that resembles symbolic differentiation of an algebraic expression.But how does this help you with your Scala case classes? Well, Lukas Rytz recently  an extension to scalac that would automatically create zippers for annotated case classes. I'll reproduce his example here:So the community needs to persuade the Scala team that this effort should be continued and integrated into the compiler.Incidentally, Lukas recently  a version of Pacman, user programmable through a DSL. Doesn't look like he used the modified compiler, though, as I can't see any  annotations.In other circumstances, you might like to apply some transformation across the entire data structure, according to some strategy (top-down, bottom-up), and based on rules that match against the value at some point in the structure. The classical example is transforming an AST for a language, perhaps to evaluate, simplify, or collect information.  supports , see the examples in , and watch this . Here's a snippet to whet your appetite:Note that Kiama  the type system to achieve this.Funny that no one added lenses, since they were MADE for this kind of stuff. So,  is a CS background paper on it,  is a blog which touch briefly on lenses use in Scala,  is a lenses implementation for Scalaz and  is some code using it, which looks surprisingly like your question. And, to cut down on boiler plate,  a plugin that generate Scalaz lenses for case classes.For bonus points,  another S.O. question which touches on lenses, and a  by Tony Morris.The big deal about lenses is that they are composable. So they are a bit cumbersome at first, but they keep gaining ground the more you use them. Also, they are great for testability, since you only need to test individual lenses, and can take for granted their composition.So, based on an implementation provided at the end of this answer, here's how you'd do it with lenses. First, declare lenses to change a zip code in an address, and an address in a person:Now, compose them to get a lens that changes zipcode in a person:Finally, use that lens to change raj:Or, using some syntactic sugar:Or even:Here's the simple implementation, taken from Scalaz, used for this example:Just want to add that the  and  projects, based on Scala 2.10 macros, provides Dynamic Lens Creation.I've been looking around for what Scala library that has the nicest syntax and the best functionality and one library not mentioned here is  which for me has been really good. An example follows:These are very nice and there are many ways to combine the lenses. Scalaz for example demands a lot of boilerplate and this compiles quick and runs great.To use them in your project just add this to your dependencies:Shapeless does the trick:with: Note that whilst some other answers here let you compose lenses to go deeper into a given structure these shapless lenses (and other libraries/macros) let you combine two unrelated lenses such that you can make lens that sets an arbitrary number of parameters into arbitrary positions in your structure. For complex data structures that additional composition is very helpful.  Due to their composable nature, lenses provide a very nice solution to the problem of heavily nested structures. However with a low level of nesting, I sometimes feel lenses are a bit too much, and I don't want to introduce the whole lenses approach if there is only few places with nested updates. For sake of completeness, here is a very simple/pragmatic solution for this case:What I do is to simply write a few  helper functions in the top level structure, which deal with the ugly nested copy. For instance:My main goal (simplifying the update on client side) is achieved:Creating the full set of modify helpers is obviously annoying. But for internal stuff it is often okay to just create them the first time you try to modify a certain nested field.Perhaps  matches your question better. QuickLens uses macro's to convert an IDE friendly expression into something that is close to the original copy statement.Given the two example case classes:and the instance of Person class:you can update zipCode of raj with:"},
{"body": "I have to say I don't understand Scala enumeration classes. I can copy-paste the example from documentation, but I have no idea what is going on.the  trait has a type member  representing the individual elements of the enumeration (it's actually an inner class, but the difference doesn't matter here).Thus  inherits that type member. The line  is just a . It is useful, because after you import it elsewhere with , you can use that type, e.g.:Instead, a minimal version would just be:and you do not  import the contents of , but then you would need to use type  and to qualify individual members. So the example would becomeThe second question is about the meaning of . This is indeed very confusing if you don't look into the implementation of . This is not the assignment of a type! It is instead calling a protected , , which returns a concrete instance of type .It so happens that you can write  in Scala, and for each value , , and  the method  will be called again and again.  uses this trick to increment an internal counter so that each value is individual.If you open the Scala API docs for  and click on , you will see that method appearing."},
{"body": "Dependent method types, which used to be an experimental feature before, has now been , and apparently this seems to have created  in the Scala community. After first look, it's not immediately obvious what this could potentially be useful for. Heiko Seeberger posted a simple example of dependent method types , which as can be seen in the comment there can easily be reproduced with type parameters on methods. So that wasn't a very compelling example. (I might be missing something obvious. Please correct me if so.)What are some practical and useful examples of use cases for dependent method types where they are clearly advantageous over the alternatives? What interesting things can we do with them that weren't possible/easy before? What do they buy us over the existing type system features?Thanks! Are dependent method types analogous to / draw inspiration from any features found in the type systems of other advanced typed languages such as Haskell, OCaml?More or less any use of member (ie. nested) types can give rise to a need for dependent method types. In particular, I maintain that without dependent method types the classic cake pattern is closer to being an anti-pattern.So what's the problem? Nested types in Scala are dependent on their enclosing instance. Consequently, in the absence of dependent method types, attempts to use them outside of that instance can be frustratingly difficult. This can turn designs which initially seem elegant and appealing into monstrosities which are nightmarishly rigid and difficult to refactor.I'll illustrate that with an exercise I give during my ,It's an example of the classic cake pattern: we have a family of abstractions which are gradually refined through a heirarchy (/ are refined by / which are in turn refined by /). It's a toy example, but the pattern is real: it's used throughout the Scala compiler and was used extensively in the Scala Eclipse plugin.Here's an example of the abstraction in use,Note that the path dependency means that the compiler will guarantee that the  and  methods on  can only be called with arguments which correspond to it, ie. it's own , and nothing else.That's undeniably a desirable property, but suppose we wanted to move this test code to a different source file? With dependent method types it's trivially easy to redefine those methods outside the  hierarchy,Note the uses of dependent method types here: the type of the second argument () depends on the value of the first argument ().It is possible to do this without dependent method types, but it's extremely awkward and the mechanism is quite unintuitive: I've been teaching this course for nearly two years now, and in that time, noone has come up with a working solution unprompted.Try it for yourself ...After a short while struggling with it you'll probably discover why I (or maybe it was David MacIver, we can't remember which of us coined the term) call this the Bakery of Doom. consensus is that Bakery of Doom was David MacIver's coinage ...For the bonus: Scala's form of dependent types in general (and dependent method types as a part of it) was inspired by the programming language  ... they arise naturally from Beta's consistent nesting semantics. I don't know of any other even faintly mainstream programming language which has dependent types in this form. Languages like Coq, Cayenne, Epigram and Agda have a different form of dependent typing which is in some ways more general, but which differs significantly by being part of type systems which, unlike Scala, don't have subtyping.Somewhere else we can statically guarantee that we aren't mixing up nodes from two different graphs, e.g.: Of course, this already worked if defined inside , but say we can't modify  and are writing a \"pimp my library\" extension for it.About the second question: types enabled by this feature are  weaker than complete dependent types (See  for a flavor of that.) I don't think I've seen an analogy before.This new feature is needed when  . When type parameters are used, the  type dependency can be expressed in the latest and some older versions of Scala, as in the following simplified example.I'm  for the interoption of a form of declarative programming with environmental state. The details aren't relevant here (e.g. details about callbacks and conceptual similarity to the Actor model combined with a Serializer).The relevant issue is state values are stored in a hash map and referenced by a hash key value. Functions input immutable arguments that are values from the environment, may call other such functions, and write state to the environment. But functions are  values from the environment (so the internal code of the function is not dependent on the order of state changes and thus remains declarative in that sense). How to type this in Scala?The environment class must have an overloaded method which inputs such a function to call, and inputs the hash keys of the arguments of the function. Thus this method can call the function with the necessary values from the hash map, without providing public read access to the values (thus as required, denying functions the ability to read values from the environment).But if these hash keys are strings or integer hash values, the static typing of the hash map element type  to Any or AnyRef (hash map code not shown below), and thus a run-time mismatch could occur, i.e. it would be possible to put any type of value in a hash map for a given hash key.Although I didn't test the following, in theory I can get the hash keys from class names at runtime employing , so a hash key is a class name instead of a string (using Scala's backticks to embed a string in a class name).So static type safety is achieved."},
{"body": "Is it possible to match on a comparison using the pattern matching system in Scala?\nFor example:The second case statement is illegal, but I would like to be able to specify \"when a is greater than\".You can add a guard, i.e. an  and a boolean expression after the pattern:Edit: Note that this is more than superficially different to putting an   the , because a pattern  match if the guard is not true.As a non-answer to the question's spirit, which asked how to incorporate predicates into a match clause, in this case the predicate can be factored out before the :Now,  promises only that the non-equal outcomes will be  or . Java's  is specified similarly to Scala's. It happens to be conventional to use 1 and -1 for the positive and negative values, respectively, as Scala's  does, but one can't make such an assumption without some risk of the implementation changing out from underneath.A solution that in my opinion is much more readable than adding guards:Notes:Use guards:"},
{"body": "According to the :In practice what are the limits? Also, are there different limits that apply to inferred expression types than to parameter type bounds, and what are those limits?When inferring types, the compiler often needs to calculate the Least Upper Bound (LUB) of a list of types. For example, the type of  is the LUB of the types of  and .These types can get quite large, for example try this in a REPL:This  introduced some sanity checks to limit the depth of such inferred types.There has been some recent work to plugin to the compilation process to detect inferred types that take a long time to calculate, and suggest places where an explicit type annotation might be prudent."},
{"body": "Getting strange behavior when calling function outside of a closure:The problem is I need my code in a class and not an object. Any idea why this is happening? Is a Scala object serialized (default?)?This is a working code example:This is the non-working example :I don't think the other answer is entirely correct. , so this is not what's causing your task to fail. Spark is a distributed computing engine and its main abstraction is a resilient distributed dataset (), which can be viewed as a distributed collection. Basically, RDD's elements are partitioned across the nodes of the cluster, but Spark abstracts this away from the user, letting the user interact with the RDD (collection) as if it were a local one.Not to get into too many details, but when you run different transformations on a RDD (, ,  and others), your transformation code (closure) is:You can of course run this locally (as in your example), but all those phases (apart from shipping over network) still occur. [This lets you catch any bugs even before deploying to production]What happens in your second case is that you are calling a method, defined in class  from inside the map function. Spark sees that and since methods cannot be serialized on their own, Spark tries to serialize   class, so that the code will still work when executed in another JVM. You have two possibilities:Either you make class testing serializable, so the whole class can be serialized by Spark:or you make  function instead of a method (functions are objects in Scala), so that Spark will be able to serialize it:Similar, but not the same problem with class serialization can be of interest to you and you can read on it .As a side note, you can rewrite  to , they are exactly the same. Usually, the second is preferred as it's less verbose and cleaner to read.EDIT (2015-03-15):  introduced  and Spark 1.3.0 is the first version to use it. It adds serialization path to a . When a NotSerializableException is encountered, the debugger visits the object graph to find the path towards the object that cannot be serialized, and constructs information to help user to find the object.In OP's case, this is what gets printed to stdout: is great in explaining why the original code does not work and two ways to fix the issue. However, this solution is not very flexible; consider the case where your closure includes a method call on a non- class that you have no control over. You can neither add the  tag to this class nor change the underlying implementation to change the method into a function.  presents a great workaround for this, but the solution can be made both more concise and general:This function-serializer can then be used to automatically wrap closures and method calls:This technique also has the benefit of not requiring the additional Shark dependencies in order to access , since Twitter's Chill is already pulled in by core SparkComplete talk fully explaining the problem, which proposes a great paradigm shifting way to avoid these serialization problems: The top voted answer is basically suggesting throwing away an entire language feature - that is no longer using methods and only using functions. Indeed in functional programming methods in classes should be avoided, but turning them into functions isn't solving the design issue here (see above link). As a quick fix in this particular situation you could just use the  annotation to tell it not to try to serialise the offending value (here,  is a custom class not Spark's one following OP's naming):You can also restructure code so that rddList lives somewhere else, but that is also nasty.In future Scala will include these things called \"spores\" that should allow us to fine grain control what does and does not exactly get pulled in by a closure.  Furthermore this should turn all mistakes of accidentally pulling in non-serializable types (or any unwanted values) into compile errors rather than now which is horrible runtime exceptions / memory leaks.When using kyro, make it so that registration is necessary, this will mean you get errors instead of memory leaks:\"Finally, I know that kryo has kryo.setRegistrationOptional(true) but I am having a very difficult time trying to figure out how to use it. When this option is turned on, kryo still seems to throw exceptions if I haven't registered classes.\"Of course this only gives you type-level control not value-level control.... more ideas to come.I solved this problem using a different approach. You simply need to serialize the objects before passing through the closure, and de-serialize afterwards. This approach just works, even if your classes aren't Serializable, because it uses Kryo behind the scenes. All you need is some curry. ;)Here's an example of how I did it:Feel free to make Blah as complicated as you want, class, companion object, nested classes, references to multiple 3rd party libs.KryoSerializationWrapper refers to: I'm not entirely certain that this applies to Scala but, in Java, I solved the  by refactoring my code so that the closure did not access a non-serializable  field.I faced similar issue, and what I understand from  is your  method is trying to serialize  method, but as method are not serializable, it tries to serialize class  which is again not serializable.So make your code work, you should define  inside  method. For example:And if there are multiple functions coming into picture, then all those functions should be available to the parent context."},
{"body": "I am really interested in finding out where the differences are, and more generally, to identify canonical use cases where HLists cannot be used (or rather, don't yield any benefits over regular lists).(I am aware that there are 22 (I believe)  in Scala, whereas one only needs a single HList, but that is not the kind of conceptual difference I am interested in.)I've marked a couple of questions in the text below. It might not actually be necessary to answer them, they are more meant to point out things that are unclear to me, and to guide the discussion in certain directions.I've recently seen a couple of answers on SO where people suggested to use HLists (for example, as provided by ), including a deleted answer to . It gave rise to , which in turn sparked this question.It seems to me, that hlists are only useful when you know the number of elements and their precise types statically. The number is actually not crucial, but it seems unlikely that you ever need to generate a list with elements of varying but statically precisely known types, but that you don't statically know their number.  Could you even write such an example, e.g., in a loop? My intuition is that having a statically precise hlist with a statically unknown number of arbitrary elements (arbitrary relative to a given class hierarchy) just isn't compatible.If this is true, i.e, you statically know number and type -  why not just use an n-tuple? Sure, you can typesafely map and fold over an HList (which you can also, but  typesafely, do over a tuple with the help of ), but since number and type of the elements are statically known you could probably just access the tuple elements directly and perform the operations.On the other hand, if the function  you map over an hlist is so generic that it accepts all elements -  why not use it via ? Ok, one interesting difference could come from method overloading: if we had several overloaded 's, having the stronger type information provided by the hlist (in contrast to the productIterator) could allow the compiler to choose a more specific . However, I am not sure if that would actually work in Scala, since methods and functions are not the same.Building on the same assumption, namely, that you need to know number and types of the elements statically -  can hlists be used in situations where the elements depend on any kind of user interaction? E.g., imagine populating an hlist with elements inside a loop; the elements are read from somewhere (UI, config file, actor interaction, network) until a certain condition holds. What would the type of the hlist be? Similar for an interface specification getElements: HList[...] that should work with lists of statically unknown length, and that allows component A in a system to get such a list of arbitrary elements from component B.Addressing questions one to three: one of the main applications for  is abstracting over arity. Arity is typically statically known at any given use site of an abstraction, but varies from site to site. Take this, from shapeless's ,Without using  (or something equivalent) to abstract over the arity of the tuple arguments to  it would be impossible to have a single implementation which could accept arguments of these two very different shapes and transform them in a type safe way.The ability to abstract over arity is likely to be of interest anywhere that fixed arities are involved: as well as tuples, as above, that includes method/function parameter lists, and case classes. See  for examples of how we might abstract over the arity of arbitrary case classes to obtain type class instances almost automatically,There's no runtime  here, but there is , which the use of  (or equivalent structures) can eliminate. Of course, if your tolerance for repetitive boilerplate is high, you can get the same result by writing multiple implementations for each and every shape that you care about.In question three you ask \"... if the function f you map over an hlist is so generic that it accepts all elements ... why not use it via productIterator.map?\". If the function you map over an HList really is of the form  then mapping over  will serve you perfectly well. But functions of the form  aren't typically that interesting (at least, they aren't unless they type case internally). shapeless provides a form of polymorphic function value which allows the compile to select type-specific cases in exactly the way you're doubtful about. For instance,With respect to your question four, about user input, there are two cases to consider. The first is situations where we can dynamically establish a context which guarantees that a known static condition obtains. In these kinds of scenarios it's perfectly possible to apply shapeless techniques, but clearly with the proviso that if the static condition  obtain at runtime then we have to follow an alternative path. Unsurprisingly, this means that methods which are sensitive to dynamic conditions have to yield optional results. Here's an example using s,The type of  doesn't capture the length of the list, or the precise types of its elements. However, if we expect it to have a specific form (ie. if it ought to conform to some known, fixed schema), then we can attempt to establish that fact and act accordingly,There are other situations where we might not care about the actual length of a given list, other than that it is the same length as some other list. Again, this is something that shapeless supports, both fully statically, and also in a mixed static/dynamic context as above. See  for an extended example.It is true, as you observe, that all of these mechanism require static type information to be available, at least conditionally, and that would seem to exclude these techniques from being usable in a completely dynamic environment, fully driven by externally provided untyped data. But with the advent of the support for runtime compilation as a component of Scala reflection in 2.10, even this is no longer an insuperable obstacle ... we can use runtime compilation to provide a form of  and have our static typing performed at runtime in response to dynamic data: excerpt from the preceding below ... follow the link for the full example,I'm sure  will have something to say about that, given his  ;-)Just to be clear, an HList is essentially nothing more than a stack of  with slightly different sugar on top.So your question is essentially about the differences between using nested tuples vs flat tuples, but the two are isomorphic so in the end there really is no difference except convenience in which library functions can be used and which notation can be used.There are a lot of things you can't do (well) with tuples:You can do all of that with tuples of course, but not in the general case. So using HLists makes your code more DRY.I can explain this in super simple language:The tuple vs list naming isn't significant. HLists could be named as HTuples. The difference is that in Scala+Haskell, you can do this with a tuple (using Scala syntax):to take an input tuple of exactly two elements of any type, append a third element, and return a fully typed tuple with exactly three elements. But while this is completely generic over types, it has to explicitly specify the input/output lengths.What a Haskell style HList lets you do is make this generic over length, so you can append to any length of tuple/list and get back a fully statically typed tuple/list. This benefit also applies to homogeneously typed collections where you can append an int to a list of exactly n ints and get back a list that is statically typed to have exactly (n+1) ints without explicitly specifying n."},
{"body": "I have seen a function named  used in Scala examples. What is it, and how is it used?:Note that we have to write  since the compiler thinks that   means that we call  with parameters.Also see  and Here are a few reasons to use the delightfully simple method .An Implicit View can be triggered when the prefix of a selection (consider for example,  does not contain a member  that is applicable to  (even after trying to convert  with Implicit Views). In this case, the compiler looks for implicit members, locally defined in the current or enclosing scopes, inherited, or imported, that are either Functions from the type of that  to a type with  defined, or equivalent implicit methods.Implicit Views can also be triggered when an expression does not conform to the Expected Type, as below:Here the compiler looks for this function:Implicit parameters are arguably a more important feature of Scala than Implicit Views. They support the type class pattern. The standard library uses this in a few places -- see  and how it is used in . Implicit Parameters are also used to pass Array manifests, and  instances.Scala 2.8 allows a shorthand syntax for implicit parameters, called Context Bounds. Briefly, a method with a type parameter  that requires an implicit parameter of type :can be rewritten as:But what's the point of passing the implicit parameter but not naming it? How can this be useful when implementing the method ?Often, the implicit parameter need not be referred to directly, it will be tunneled through as an implicit argument to another method that is called. If it is needed, you can still retain the terse method signature with the Context Bound, and call  to materialize the value:Suppose you are calling a method that pretty prints a person, using a type class based approach:What if we want to change the way that the name is output? We can explicitly call , explicitly pass an alternative , but we want the compiler to pass the . is avaliable in Scala 2.8 and is  defined in  as: It is commonly used to  if so is the case. Simple example  from ) A \"teach you to fish\" answer is to use the alphabetic member index currently available in the . The letters (and the , for non-alphabetic names) at the top of the package / class pane are links to the index for member names beginning with that letter (across all classes). If you choose , e.g., you'll find the  entry with one occurrence, in , which you can visit from the link there."},
{"body": "What's the standard way to work with dates and times in Scala? Should I use Java types such as java.util.Date or there are native Scala alternatives?From Java SE 8 onwards, users are asked to migrate to java.time (JSR-310). There are efforts on creating scala libraries  wrapping java.time for scala such as . If targeting lower than SE 8 use one of the below. Also see . This project forked from scala-time since it seems that scala-time is no longer maintained. is a good Java library, there is a Scala wrapper / implicit conversion library avaliable for Joda Time at  created by . (Note implicits have a performance hit, but it depends on what you do if you will notice. And if you run into a performance problem you can just revert to the Joda interface)From the README:If you are using Java 8, then there is no need to use  anymore. The Joda-Time library has been moved into Java 8 under the  package (JSR-310). Just import that package into your Scala project.MOTIVATION:The Java Date and Calendar libraries are largely inadequate. They are mutable,\nnot thread-safe, and very inconvenient to use.The Joda Time library is a great replacement for Java's Date and Calendar\nclasses. They're immutable by default, have a much richer and nicer API, and\ncan easily be converted to Java's Date and Calendar classes when necessary.This project provides a thin layer of convenience around the Joda Time\nlibraries, making them more idiomatic to use within Scala.(copied from )There is no  way to work with dates in Scala. The options available are:I would avoid using java.util.Date due to the well-documented issues surrounding it.Everyone uses JodaTime, these Scala helper/wrapper libraries may need re-compilation with new versions of Scala. Jodatime is the only time library that's been around for a long time, and is stable and works reliably with every version of Scala."},
{"body": "I am upgrading existing code from  to  and  from .I'm having difficulty writing  that contains a scala enum, that I really could use some help with.For example,When we try to write to this field, we get the following error:We used to have this working in Rogue 1.1.8, by using our own version of the , which made the #formats method overridable. But that feature was included into lift-mongodb-record in 2.5-RC6, so we thought this should just work now?Answer coming from :  Sorry, I should have chimed in here sooner.One of the long-standing problems with Rogue was that it was too easy to\naccidentally make a field that was not serializable as BSON, and have it\nfail at runtime (when you try to add that value to a DBObject) rather than\nat compile time.I introduced the BSONType type class to try to address this. The upside is\nit catches BSON errors at compile time. The downside is you need to make a\nchoice when it comes to case classes.If you want to do this the \"correct\" way, define your case class plus a\nBSONType \"witness\" for that case class. To define a BSONType witness, you\nneed to provide serialization from that type to a BSON type. Example:That said, this can be quite burdensome if you're doing it for each case\nclass. Your second option is to define a generic witness that works for any\ncase class, if you have a generic serialization scheme:Hope this helps,"},
{"body": "Sometime I stumble into the semi-mysterious notation of in Scala blog posts, which give it a \"we used that type-lambda trick\" handwave.While I have some intutition about this (we gain an anonymous type parameter  without having to pollute the definition with it?), I found no clear source describing what the type lambda trick is, and what are its benefits. Is it just syntactic sugar, or does it open some new dimensions?Type lambdas are vital quite a bit of the time when you are working with higher-kinded types.Consider a simple example of defining a monad for the right projection of Either[A, B]. The monad typeclass looks like this:Now, Either is a type constructor of two arguments, but to implement Monad, you need to give it a type constructor of one argument. The solution to this is to use a type lambda:This is an example of currying in the type system - you have curried the type of Either, such that when you want to create an instance of EitherMonad, you have to specify one of the types; the other of course is supplied at the time you call point or bind.The type lambda trick exploits the fact that an empty block in a type position creates an anonymous structural type. We then use the # syntax to get a type member.In some cases, you may need more sophisticated type lambdas that are a pain to write out inline. Here's an example from my code from today:This class exists exclusively so that I can use a name like FG[F, G]#IterateeM to refer to the type of the IterateeT monad specialized to some transformer version of a second monad which is specialized to some third monad. When you start to stack, these kinds of constructs become very necessary. I never instantiate an FG, of course; it's just there as a hack to let me express what I want in the type system.The benefits are exactly the same as those conferred by anonymous functions.An example usage, with Scalaz 7. We want to use a  that can map a function over the second element in a .Scalaz provides some implicit conversions that can infer the type argument to , so we often avoid writing these altogether. The previous line can be rewritten as:If you use IntelliJ, you can enable Settings, Code Style, Scala, Folding, Type Lambdas. This then , and presents the more palatable:A future version of Scala might directly support such a syntax.To put things in context: This answer was originally posted in another thread. You are seeing it here because the two threads have been merged. The question statement in the said thread was as follows:The one underscore in the boxes after  implies that it is a type constructor takes one type and returns another type. Examples of type constructors with this kind: , .Give  an , a concrete type, and it gives you , another concrete type. Give  a  and it gives you . Etc.So, ,  can be considered to be type level functions of arity 1. Formally we say, they have a kind . The asterisk denotes a type.Now  is a type constructor with kind  i.e. you need to give it two types to get a new type. Since their signatures do not match, you cannot substitute  for . What you need to do is   on one of its arguments, which will give us a type constructor with kind , and we can substitue it for .Unfortunately Scala has no special syntax for partial application of type constructors, and so we have to resort to the monstrosity called type lambdas. (What you have in your example.) They are called that because they are analogous to lambda expressions that exist at value level.The following example might help:More value level and type level parallels.In the case you have presented, the type parameter  is local to function  and so you cannot simply define , because there is simply no place where you can put that synonym.To deal with such a case, I use the following trick that makes use of type members. (Hopefully the example is self-explanatory.) causes that whatever we put in  in  the  is always true I guess."},
{"body": "So Scala is supposed to be as fast as Java. I'm revisiting some  problems in Scala that I originally tackled in Java. Specifically Problem 5: \"What is the smallest positive number that is evenly divisible by all of the numbers from 1 to 20?\"Here's my Java solution, which takes 0.7 seconds to complete on my machine:Here's my \"direct translation\" into Scala, which takes 103 seconds (147 times longer!)Finally here's my attempt at functional programming, which takes 39 seconds (55 times longer)Using Scala 2.9.0.1 on Windows\u00a07 64-bit. How do I improve performance? Am I doing something wrong? Or is Java just a lot faster?The problem in this particular case is that you return from within the for-expression. That in turn gets translated into a throw of a NonLocalReturnException, which is caught at the enclosing method. The optimizer can eliminate the foreach but cannot yet eliminate the throw/catch. And throw/catch is expensive. But since such nested returns are rare in Scala programs, the optimizer did not yet address this case. There is work going on to improve the optimizer which hopefully will solve this issue soon.The problem is most likely the use of a  comprehension in the method .  Replacing  by an equivalent  loop should eliminate the performance difference with Java.As opposed to Java's  loops, Scala's  comprehensions are actually syntactic sugar for higher-order methods; in this case, you're calling the  method on a  object. Scala's  is very general, but sometimes leads to painful performance.You might want to try the  flag in Scala version 2.9. Observed performance may depend on the particular JVM in use, and the JIT optimizer having sufficient \"warm up\" time to identify and optimize hot-spots.Recent discussions on the mailing list indicate that the Scala team is working on improving  performance in simple cases:Here is the issue in the bug tracker:\n:As a follow-up, I tried the -optimize flag and it reduced running time from 103 to 76 seconds, but that's still 107x slower than Java or a while loop.Then I was looking at the \"functional\" version:and trying to figure out how to get rid of the \"forall\" in a concise manner. I failed miserably and came up with whereby my cunning 5-line solution has balooned to 12 lines. However, this version runs in , the same speed as the original Java version, and 56 times faster than the version above using \"forall\" (40.2 s)! (see EDIT below for why this is faster than Java)Obviously my next step was to translate the above back into Java, but Java can't handle it and throws a StackOverflowError with n around the 22000 mark.I then scratched my head for a bit and replaced the \"while\" with a bit more tail recursion, which saves a couple of lines, runs just as fast, but let's face it, is more confusing to read:So Scala's tail recursion wins the day, but I'm surprised that something as simple as a \"for\" loop (and the \"forall\" method) is essentially broken and has to be replaced by inelegant and verbose \"whiles\", or tail recursion. A lot of the reason I'm trying Scala is because of the concise syntax, but it's no good if my code is going to run 100 times slower!: (deleted): Former discrepancies between run times of 2.5s and 0.7s were entirely due to whether the 32-bit or 64-bit JVMs were being used. Scala from the command line uses whatever is set by JAVA_HOME, while Java uses 64-bit if available regardless. IDEs have their own settings. Some measurements here: The answer about for comprehension is right, but it's not the whole story. You should note note that the use of  in  is not free. The use of return inside the , forces the scala compiler to generate a non-local return (i.e. to return outside it's function).This is done through the use of an exception to exit the loop. The same happens if you build your own control abstractions, for example:This prints \"Hi\" only once.Note that the  in  exits  (which is what you would expect). Since the bracketed expression is a function literal, which you can see in the signature of  this forces the compiler to generate a non local return, that is, the  forces you to exit , not just the .In Java (i.e. the JVM) the only way to implement such behavior is to throw an exception.  Going back to :The  is a function literal that has a return, so each time the return is hit, the runtime has to throw and catch an exception, which causes quite a bit of GC overhead.Some ways to speed up the  method I discovered:The original: Pre-instantiating the range, so we don't create a new range every time: Converting to a List instead of a Range: I tried a few other collections but List was fastest (although still 7x slower than if we avoid the Range and higher-order function altogether).While I am new to Scala, I'd guess the compiler could easily implement a quick and significant performance gain by simply automatically replacing Range literals in methods (as above) with Range constants in the outermost scope. Or better, intern them like Strings literals in Java. :\nArrays were about the same as Range, but interestingly, pimping a new  method (shown below) resulted in 24% faster execution on 64-bit, and 8% faster on 32-bit. When I reduced the calculation size by reducing the number of factors from 20 to 15 the difference disappeared, so maybe it's a garbage collection effect. Whatever the cause, it's significant when operating under full load for extended periods.A similar pimp for List also resulted in about 10% better performance.I just wanted to comment for people who might lose faith in Scala over issues like this that these kinds of issues come up in the performance of just about all functional languages. If you are optimizing a fold in Haskell, you will often have to re-write it as a recursive tail-call-optimized loop, or else you'll have performance and memory issues to contend with.I know it's unfortunate that FPs aren't yet optimized to the point where we don't have to think about things like this, but this is not at all a problem particular to Scala.Problems specific to Scala have already been discussed, but the main problem is that using a brute-force algorithm is not very cool. Consider this (much faster than the original Java code):Try the one-liner given in the solution The time given is at least faster than yours, though far from the while loop.. :)"},
{"body": "I want to merge them, and sum the values of same keys. So the result will be:Now I have 2 solutions:andBut I want to know if there are any better solutions. has the concept of a  which captures what you want to do here, and leads to arguably the shortest/cleanest solution:Specifically, the binary operator for  combines the keys of the maps, folding 's semigroup operator over any duplicate values.  The standard semigroup for  uses the addition operator, so you get the sum of values for each duplicate key.: A little more detail, as per user482745's request.Mathematically a  is just a set of values, together with an operator that takes two values from that set, and produces another value from that set.  So integers under addition are a semigroup, for example - the  operator combines two ints to make another int.You can also define a semigroup over the set of \"all maps with a given key type and value type\", so long as you can come up with some operation that combines two maps to produce a new one which is somehow the combination of the two inputs.If there are no keys that appear in both maps, this is trivial.  If the same key exists in both maps, then we need to combine the two values that the key maps to.  Hmm, haven't we just described an operator which combines two entities of the same type?  This is why in Scalaz a semigroup for  exists if and only if a Semigroup for  exists - 's semigroup is used to combine the values from two maps which are assigned to the same key.So because  is the value type here, the \"collision\" on the  key is resolved by integer addition of the two mapped values (as that's what Int's semigroup operator does), hence .  If the values had been Strings, a collision would have resulted in string concatenation of the two mapped values (again, because that's what the semigroup operator for String does).(And interestingly, because string concatenation is not commutative - that is,  - the resulting semigroup operation isn't either.  So  is different from  in the String case, but not in the Int case.)The shortest answer I know of that uses only the standard library isWell, now in scala library (at least in 2.10) there is something you wanted -  function. BUT it's presented only in HashMap not in Map. It's somewhat confusing. Also the signature is cumbersome - can't imagine why I'd need a key twice and when I'd need to produce a pair with another key. But nevertheless, it works and much cleaner than previous \"native\" solutions.Also in scaladoc mentioned thatQuick solution:This can be implemented as a  with just plain Scala. Here is a sample implementation. With this approach, we can merge not just 2, but a list of maps.The Map based implementation of the Monoid trait that merges two maps.Now, if you have a list of maps that needs to be merged (in this case, only 2), it can be done like below.I wrote a blog post about this , check it out :basically using scalaz semi group you can achieve this pretty easilywould look something like :This is what I came up with...You can also do that with .Andrzej Doyle's answer contains a great explanation of semigroups which allows you to use the  operator to join two maps and sum the values for matching keys.There are many ways something can be defined to be an instance of a typeclass, and unlike the OP you might not want to sum your keys specifically. Or, you might want to do operate on a union rather than an intersection. Scalaz also adds extra functions to  for this purpose:You can doI've got a small function to do the job, it's in my small library for some frequently used  functionality which isn't in standard lib.\nIt should work for all types of maps, mutable and immutable, not only HashMapsHere is the usageAnd here's the body"},
{"body": "Following on from , can someone explain the following in Scala:I understand the distinction between  and  in the type declaration (it compiles if I use ). But then how does one actually write a class which is covariant in its type parameter without resorting to creating the thing ? How can I ensure that the following can only be created with an instance of ? - now got this down to the following:this is all good, but I now have two type parameters, where I only want one. I'll re-ask the question thus:: Duh! I used  and not . The following is what I wanted:Generically, a  type parameter is one which is allowed to vary down as the class is subtyped (alternatively, vary with subtyping, hence the \"co-\" prefix).  More concretely: is a subtype of  because  is a subtype of .  This means that you may provide an instance of  when a value of type  is expected.  This is really a very intuitive way for generics to work, but it turns out that it is unsound (breaks the type system) when used in the presence of mutable data.  This is why generics are invariant in Java.  Brief example of unsoundness using Java arrays (which are erroneously covariant):We just assigned a value of type  to an array of type .  For reasons which should be obvious, this is bad news.  Java's type system actually allows this at compile time.  The JVM will \"helpfully\" throw an  at runtime.  Scala's type system prevents this problem because the type parameter on the  class is invariant (declaration is  rather than ).Note that there is another type of variance known as .  This is very important as it explains why covariance can cause some issues.  Contravariance is literally the opposite of covariance: parameters vary  with subtyping.  It is a lot less common partially because it is so counter-intuitive, though it does have one very important application: functions.Notice the \"\" variance annotation on the  type parameter.  This declaration as a whole means that  is contravariant in  and covariant in .  Thus, we can derive the following axioms:Notice that  must be a subtype (or the same type) of , whereas it is the opposite for  and .  In English, this can be read as the following:The reason for this rule is left as an exercise to the reader (hint: think about different cases as functions are subtyped, like my array example from above).With your new-found knowledge of co- and contravariance, you should be able to see why the following example will not compile:The problem is that  is covariant, while the  function expects its type parameter to be .  Thus,  is varying the wrong direction.  Interestingly enough, we could solve this problem by making  contravariant in , but then the return type  would be invalid as the  function expects its return type to be .Our only two options here are to a) make  invariant, losing the nice, intuitive sub-typing properties of covariance, or b) add a local type parameter to the  method which defines  as a lower bound:This is now valid.  You can imagine that  is varying downward, but  is able to vary upward with respect to  since  is its lower-bound.  With this method declaration, we can have  be covariant and everything works out.Notice that this trick only works if we return an instance of  which is specialized on the less-specific type .  If you try to make  mutable, things break down since you end up trying to assign values of type  to a variable of type , which is disallowed by the compiler.  Whenever you have mutability, you need to have a mutator of some sort, which requires a method parameter of a certain type, which (together with the accessor) implies invariance.  Covariance works with immutable data since the only possible operation is an accessor, which may be given a covariant return type.@Daniel has explained it very well. But to explain it in short, if it was allowed: will then throw an error at runtime as it was unsuccessful in converting an  to  (duh!).In general mutability doesn't go well with co-variance and contra-variance. That is the reason why all Java collections are invariant. See , page 57+ for a full discussion of this.If I'm understanding your comment correctly, you need to reread the passage starting at the bottom of page 56 (basically, what I think you are asking for isn't type-safe without run time checks, which scala doesn't do, so you're out of luck).  Translating their example to use your construct:If you feel I'm not understanding your question (a distinct possibility), try adding more explanation / context to the problem description and I'll try again.In response to your edit: Immutable slots are a whole different situation...* smile * I hope the example above helped.You need to apply a lower bound on the parameter. I'm having a hard time remembering the syntax, but I think it would look something like this:The Scala-by-example is a bit hard to understand, a few concrete examples would have helped."},
{"body": "I have learned the basic difference between  and Is there any other difference ?Any specific reason to have two methods with similar functionality?Few things to mention here, before giving the actual answer:Here is the signature of  (could also have been  for the point I'm going to make):And here is the signature of  (again the direction doesn't matter here)These two look very similar and thus caused the confusion.  is a special case of  (which by the way means that you  can express the same thing by using either of them).When you call  say on a  it will literally reduce the whole list of integers into a single value, which is going to be of type  (or a supertype of , hence ).When you call  say on a  it will fold the whole list (imagine rolling a piece of paper) into a single value, but this value doesn't have to be even related to  (hence ).Here is an example:This method takes a  and returns a  or . It calculates the sum and returns a tuple with a list of integers and it's sum. By the way the list is returned backwards, because we used  instead of . is just a convenience method. It is equivalent to is more generic, you can use it to produce something completely different than what you originally put in. Whereas  can only produce an end result of the same type or super type of the collection type. For example:The  will apply the closure with the last folded result (first time using initial value) and the next value.  on the other hand will first combine two values from the list and apply those to the closure. Next it will combine the rest of the values with the cumulative result. See:If the list is empty  can present the initial value as a legal result.  on the other hand does not have a legal value if it can't find at least one value in the list.The basic reason they are both in Scala standard library is probably because they are both in Haskell standard library (called  and ). If  wasn't, it would quite often be defined as a convenience method in different projects.For reference,  will error if applied to an empty container with the following error.Reworking the code to use is one potential option.  Another is to use the  variant which returns an Option wrapped result.From  (Martin Odersky):[as opposed to , which throws an exception when called on an empty list.]The course (see lecture 5.5) provides abstract definitions of these functions, which illustrates their differences, although they are very similar in their use of pattern matching and recursion.Note that  returns a value of type , which is not necessarily the same type as , but reduceLeft returns a value of the same type as the list).To really understand what are you doing with fold/reduce,\ncheck this: \nvery good explanation. once you get the concept of fold,\nreduce will come together with the answer above:\nlist.tail.foldLeft(list.head)(_)"},
{"body": "I am keen to look into Scala, and have one basic question I cant seem to find an answer to:\nin general, is there a difference in performance and usage of memory between Scala and Java?Scala makes it very easy to use enormous amounts of memory without realizing it.  This is usually very powerful, but occasionally can be annoying.  For example, suppose you have an array of strings (called ), and a map from those strings to files (called ).  Suppose you want to get all files that are in the map and come from strings of length greater than two.  In Java, you mightWhew!  Hard work.  In Scala, the most compact way to do the same thing is:Easy!  But, unless you're fairly familiar with how the collections work, what you might not realize is that this way of doing this created an extra intermediate array (with ), and an extra object for  (with , which returns an option).  It also creates two function objects (one for the filter and one for the flatMap), though that is rarely a major issue since function objects are small.So basically, the memory usage is, at a primitive level, the same.  But Scala's libraries have many powerful methods that let you create enormous numbers of (usually short-lived) objects very easily.  The garbage collector is usually pretty good with that kind of garbage, but if you go in completely oblivious to what memory is being used, you'll probably run into trouble sooner in Scala than Java.Note that the Computer Languages Benchmark Game Scala code is written in a rather Java-like style in order to get Java-like performance, and thus has Java-like memory usage.  You can do this in Scala: if you write your code to look like high-performance Java code, it will be high-performance Scala code.  (You  be able to write it in a more idiomatic Scala style and still get good performance, but it depends on the specifics.)I should add that per amount of time spent programming, my Scala code is usually  than my Java code since in Scala I can get the tedious not-performance-critical parts done with less effort, and spend more of my attention optimizing the algorithms and code for the performance-critical parts.I'm a new user, so I'm not able to add a comment to Rex Kerr's answer above (allowing new users to \"answer\" but not \"comment\" is a very odd rule btw).I signed up simply to respond to the \"phew, Java is so verbose and such hard work\" insinuation of Rex's popular answer above.  While you can of course write more concise Scala code, the Java example given is clearly bloated.  Most Java developers would code something like this:And of course, if we are going to pretend that Eclipse doesn't do most of the actual typing for you and that every character saved really makes you a better programmer, then you could code this:Now not only did I save the time it took me to type full variable names and curly braces (freeing me to spend 5 more seconds to think deep algorithmic thoughts), but I can also enter my code in obfuscation contests and potentially earn extra cash for the holidays.Write your Scala like Java, and you can expect almost identical bytecode to be emitted - with almost identical metrics.Write it more \"idiomatically\", with immutable objects and higher order functions, and it'll be a bit slower and a bit larger.  The one exception to this rule-of-thumb is when using generic objects in which the type params use the  annotation, this'll create even larger bytecode that can outpace Java's performance by avoiding boxing/unboxing.Also worth mentioning is the fact that more memory / less speed is an inevitable trade-off when writing code that can be run in parallel.  Idiomatic Scala code is far more declarative in nature than typical Java code, and is often a mere 4 characters () away from being fully parallel.So ifWould you then say that the Scala code is now comparatively 25% slower, or 3x faster?The correct answer depends on exactly how you define \"performance\" :)Computer Language Benchmarks Game: java/scala 1.71/2.25 java/scala 66.55/80.81So, this benchmarks say that java is 24% faster and scala uses 21% more memory.All-in-all it's no big deal and should not matter in real world apps, where most of the time is consumed by database and network. If Scala makes you and your team (and people taking project over when you leave) more productive, then you should go for it.Others have answered this question with respect to tight loops although there seems to be an obvious performance difference between Rex Kerr's examples that I have commented on.This answer is really targeted at people who might investigate a need for tight-loop optimisation as design flaw.I am relatively new to Scala (about a year or so) but the feel of it, thus far, is that it allows you to  many aspects of design, implementation and execution relatively easily (with enough background reading and experimentation :) (sorry, no links)These features, to me, are the ones that help us to tread the path to fast, tight applications.Rex Kerr's examples differ in what aspects of execution are deferred.  In the Java example, allocation of memory is deferred until it's size is calculated where the Scala example defers the mapping lookup. To me, they seem like completely different algorithms.Here's what I think is more of an apples to apples equivalent for his Java example:No intermediary collections, no  instances etc.\nThis also preserves the collection type so 's type is  - 's  implementation will probably be doing something along the lines of what Mr Kerr's Java code does.The deferred design features I listed above would also allow Scala's collection API developers to implement that fast Array-specific collect implementation in future releases without breaking the API. This is what I'm referring to with treading the path to speed.Also:The  method that I've used here instead of  fixes the intermediate collection problem but there is still the Option instance issue.One example of simple execution speed in Scala is with logging.In Java we might write something like:In Scala, this is just:because the message parameter to debug in Scala has the type \"\" which I think of as a parameter-less function that executes when it is evaluated, but which the documentation calls pass-by-name.EDIT {\nFunctions in Scala are objects so there is an extra object here. For my work, the weight of a trivial object is worth removing the possibility of a log message getting needlessly evaluated.\n}This doesn't make the code faster but it does make it more likely to be faster and we're less likely to have the experience of going through and cleaning up other people's code en masse.To me, this is a consistent theme within Scala.Hard code fails to capture why Scala is faster though it does hint a bit.I feel that it's a combination of code re-use and the ceiling of code quality in Scala.In Java, awesome code is often forced to become an incomprehensible mess and so isn't really viable within production quality APIs as most programmers wouldn't be able to use it.I have high hopes that Scala could allow the einsteins among us to implement far more competent APIs, potentially expressed through DSLs. The core APIs in Scala are already far along this path.\u00b4s presentation on the subject -  which does some Java/Scala comparisions. Tools: Great blogpost: Java and Scala both compile down to JVM bytecode, so the difference isn't that big. The best comparison you can get is probably on the , which essentially says that Java and Scala both have the same memory usage. Scala is only  slower than Java on some of the benchmarks listed, but that could simply be because the implementation of the programs are different.Really though, they're both so close it's not worth worrying about. The productivity increase you get by using a more expressive language like Scala is worth so much more than minimal (if any) performance hit.The Java example is really not an idiom for typical application programs.\nSuch optimized code might be found in a system library method. But then it would use an array of the right type, i.e. File[] and would not throw an IndexOutOfBoundsException. (Different filter conditions for counting and adding). \nMy version would be (always (!) with curly braces because I don't like to spend an hour searching a bug which was introduced by saving the 2 seconds to hit a single key in Eclipse):But I could bring you a lot of other ugly Java code examples from my current project. I tried to avoid the common copy&modify style of coding by factoring out common structures and behaviour. In my abstract DAO base class I have an abstract inner class for the common caching mechanism. For every concrete model object type there is a subclass of the abstract DAO base class, in which the inner class is subclassed to provide an implementation for the method which creates the business object when it is loaded from the database. (We can not use an ORM tool because we access another system via a proprietary API.)This subclassing and instantiation code is not at all clear in Java and would be very readable in Scala."},
{"body": "The Akka Streams library already comes with quite a . However, the main problem for me is that it provides too much material - I feel quite overwhelmed by the number of concepts that I have to learn. Lots of examples shown there feel very heavyweight and can't easily be translated to real world use cases and are therefore quite esoteric. I think it gives way too much details without explaining how to build all the building blocks together and how exactly it helps to solve specific problems.There are sources, sinks, flows, graph stages, partial graphs, materialization, a graph DSL and a lot more and I just don't know where to start. The  is meant to be a starting place but I don't understand it. It just throws in the concepts mentioned above without explaining them. Furthermore the code examples can't be executed - there are missing parts which makes it more or less impossible for me to follow the text.Can anyone explain the concepts sources, sinks, flows, graph stages, partial graphs, materialization and maybe some other things that I missed in simple words and with easy examples that don't explain every single detail (and which are probably not needed anyway at the beginning)?This answer is based on  version . The API can be slightly different in other versions. The dependency can be consumed by :Alright, lets get started. The API of Akka Streams consists of three main types. In contrast to , these types are a lot more powerful and therefore more complex. It is assumed that for all the code examples the following definitions already exist:The  statements are needed for the type declarations.  represents the actor system of Akka and  represents the evaluation context of the stream. In our case we use a , which means that the streams are evaluated on top of actors. Both values are marked as , which gives the Scala compiler the possibility to inject these two dependencies automatically whenever they are needed. We also import , which is a execution context for .Akka Streams have these key properties:In the following a deeper introduction in how to use the three main types shall be given.A  is a data creator, it serves as an input source to the stream. Each  has a single output channel and no input channel. All the data flows through the output channel to whatever is connected to the .A  can be created in multiple ways:In the above cases we fed the  with finite data, which means they will terminate eventually. One should not forget, that Reactive Streams are lazy and asynchronous by default. This means one explicitly has to request the evaluation of the stream. In Akka Streams this can be done through the  methods. The  would be no different to the well known  function - through the  addition it makes explicit that we ask for an evaluation of the stream. Since finite data is boring, we continue with infinite one:With the  method we can create an artificial stop point that prevents us from evaluating indefinitely. Since actor support is built-in, we can also easily feed the stream with messages that are sent to an actor:We can see that the  are executed asynchronously on different threads, which explains the result. In the above example a buffer for the incoming elements is not necessary and therefore with  we can configure that the stream should fail on a buffer overflow. Especially through this actor interface, we can feed the stream through any data source. It doesn't matter if the data is created by the same thread, by a different one, by another process or if they come from a remote system over the Internet.A  is basically the opposite of a . It is the endpoint of a stream and therefore consumes data. A  has a single input channel and no output channel.  are especially needed when we want to specify the behavior of the data collector in a reusable way and without evaluating the stream. The already known  methods do not allow us these properties, therefore it is preferred to use  instead.A short example of a  in action:Connecting a  to a  can be done with the  method. It returns a so called , which is as we will later see a special form of a  - a stream that can be executed by just calling its  method.It is of course possible to forward all values that arrive at a sink to an actor:Data sources and sinks are great if you need a connection between Akka streams and an existing system but one can not really do anything with them. Flows are the last missing piece in the Akka Streams base abstraction. They act as a connector between different streams and can be used to transform its elements.If a  is connected to a  a new  is the result. Likewise, a  connected to a  creates a new . And a  connected with both a  and a  results in a . Therefore, they sit between the input and the output channel but by themselves do not correspond to one of the flavors as long as they are not connected to either a  or a .In order to get a better understanding of , we will have a look at some examples:Via the  method we can connect a  with a . We need to specify the input type because the compiler can't infer it for us. As we can already see in this simple example, the flows  and  are completely independent from any data producers and consumers. They only transform the data and forward it to the output channel. This means that we can reuse a flow among multiple streams: and  represent completely new streams - they do not share any data through their building blocks.Before we move on we should first revisit some of the key aspects of Reactive Streams. An unbounded number of elements can arrive at any point and can put a stream in different states. Beside from a runnable stream, which is the usual state, a stream may get stopped either through an error or through a signal that denotes that no further data will arrive. A stream can be modeled in a graphical way by marking events on a timeline as it is the case here:We have already seen runnable flows in the examples of the previous section. We get a  whenever a stream can actually be materialized, which means that a  is connected to a . So far we always materialized to the value , which can be seen in the types:For  and  the second type parameter and for  the third type parameter denote the materialized value. Throughout this answer, the full meaning of materialization shall not be explained. However, further details about materialization can be found at the . For now the only thing we need to know is that the materialized value is what we get when we run a stream. Since we were only interested in side effects so far, we got  as the materialized value. The exception to this was a materialization of a sink, which resulted in a . It gave us back a , since this value can denote when the stream that is connected to the sink has been ended.  So far, the previous code examples were nice to explain the concept but they were also boring because we only dealt with finite streams or with very simple infinite ones. To make it more interesting, in the following a full asynchronous and unbounded stream shall be explained.As an example, we want to have a stream that captures click events. To make it more challenging, let's say we also want to group click events that happen in a short time after each other. This way we could easily discover double, triple or tenfold clicks. Furthermore, we want to filter out all single clicks. Take a deep breath and imagine how you would solve that problem in an imperative manner. I bet no one would be able to implement a solution that works correctly on the first try. In a reactive fashion this problem is trivial to solve. In fact, the solution is so simple and straightforward to implement that we can even express it in a diagram that directly describes the behavior of the code:The gray boxes are functions that describe how one stream is transformed into another. With the  function we accumulate clicks within 250 milliseconds, the  and  functions should be self-explanatory. The color orbs represent an event and the arrows depict how they flow through our functions. Later in the processing steps, we get less and less elements that flow through our stream, since we group them together and filter them out. The code for this image would look something like this:The whole logic can be represented in only four lines of code! In Scala, we could write it even shorter:The definition of  is a little bit more complex but this is only the case because the example program runs on the JVM, where capturing of click events is not easily possible. Another complication is that Akka by default doesn't provide the  function. Instead we had to write it by ourselves. Since this function is (as it is the case for the  or  functions) reusable across different use cases I don't count these lines to the number of lines we needed to implement the logic. In imperative languages however, it is normal that logic can't be reused that easily and that the different logical steps happen all at one place instead of being applied sequentially, which means that we probably would have misshaped our code with the throttling logic. The full code example is available as a  and shall not be discussed here any further.What should be discussed instead is another example. While the click stream is a nice example to let Akka Streams handle a real world example, it lacks the power to show parallel execution in action. The next example shall represent a small web server that can handle multiple requests in parallel. The web sever shall be able to accept incoming connections and receive byte sequences from them that represent printable ASCII signs. These byte sequences or strings should be split at all newline-characters into smaller parts. After that, the server shall respond to the client with each of the split lines. Alternatively, it could do something else with the lines and give a special answer token, but we want to keep it simple in this example and therefore don't introduce any fancy features. Remember, the server needs to be able to handle multiple requests at the same time, which basically means that no request is allowed to block any other request from further execution. Solving all of these requirements can be hard in an imperative way - with Akka Streams however, we shouldn't need more than a few lines to solve any of these. First, let's have an overview over the server itself:Basically, there are only three main building blocks. The first one needs to accept incoming connections. The second one needs to handle incoming requests and the third one needs to send a response. Implementing all of these three building blocks is only a little bit more complicated than implementing the click stream:The function  takes (besides from the address and the port of the server) also an actor system and a materializer as implicit parameters. The control flow of the server is represented by , which takes a source of incoming connections and forwards them to a sink of incoming connections. Inside of , which is our sink, we handle every connection by the flow , which will be described later.  returns a , which completes when the server has been started or the start failed, which could be the case when the port is already taken by another process. The code however, doesn't completely reflect the graphic as we can't see a building block that handles responses. The reason for this is that the connection already provides this logic by itself. It is a bidirectional flow and not just a unidirectional one as the flows we have seen in the previous examples. As it was the case for materialization, such complex flows shall not be explained here. The  has plenty of material to cover more complex flow graphs. For now it is enough to know that   represents a connection that knows how to receive requests and how to send responses. The part that is still missing is the  building block. It can look like this:Once again, we are able to split the logic in several simple building blocks that all together form the flow of our program. First we want to split our sequence of bytes in lines, which we have to do whenever we find a newline character. After that, the bytes of each line need to be converted to a string because working with raw bytes is cumbersome. Overall we could receive a binary stream of a complicated protocol, which would make working with the incoming raw data extremely challenging. Once we have a readable string, we can create an answer. For simplicity reasons the answer can be anything in our case. In the end, we have to convert back our answer to a sequence of bytes that can be sent over the wire. The code for the entire logic may look like this:We already know that  is a flow that takes a  and has to produce a . With  we can split a  in smaller parts - in our case it needs to happen whenever a newline character occurs.  is the flow that takes all of the split byte sequences and converts them to a string. This is of course a dangerous conversion, since only printable ASCII characters should be converted to a string but for our needs it is good enough.  is the last component and is responsible for creating an answer and converting the answer back to a sequence of bytes. As opposed to the graphic we didn't split this last component in two, since the logic is trivial. At the end, we connect all of the flows through the  function. At this point one may ask whether we took care of the multi-user property that was mentioned at the beginning. And indeed we did even though it may not be obvious immediately. By looking at this graphic it should get more clear:The  component is nothing but a flow that contains smaller flows. This component takes an input, which is a request, and produces an output, which is the response. Since flows can be constructed multiple times and they all work independently to each other, we achieve through this nesting our multi-user property. Every request is handled within its own request and therefore a short running request can overrun a previously started long running request. In case you wondered, the definition of  that was shown previously can of course be written a lot shorter by inlining most of its inner definitions:A test of the web server may look like this:In order for the above code example to function correctly, we first need to start the server, which is depicted by the  script:The full code example of this simple TCP server can be found . We are not only able to write a server with Akka Streams but also the client. It may look like this:The full code TCP client can be found . The code looks quite similar but in contrast to the server we don't have to manage the incoming connections anymore. In the previous sections we have seen how we can construct simple programs out of flows. However, in reality it is often not enough to just rely on already built-in functions to construct more complex streams. If we want to be able to use Akka Streams for arbitrary programs we need to know how to build our own custom control structures and combinable flows that allow us to tackle the complexity of our applications. The good news is that Akka Streams was designed to scale with the needs of the users and in order to give you a short introduction into the more complex parts of Akka Streams, we add some more features to our client/server example.One thing we can't do yet is closing a connection. At this point it starts to get a little bit more complicated because the stream API we have seen so far doesn't allow us to stop a stream at an arbitrary point. However, there is the  abstraction, which can be used to create arbitrary graph processing stages with any number of input or output ports. Let's first have a look at the server side, where we introduce a new component, called :This API looks a lot more cumbersome than the flow API. No wonder, we have to do a lot of imperative steps here. In exchange, we have more control over the behavior of our streams. In the above example, we only specify one input and one output port and make them available to the system by overriding the  value. Furthermore we defined a so called  and a , which are in this order responsible for receiving and emitting elements. If you looked closely to the full click stream example you should recognize these components already. In the  we grab an element and if it is a string with a single character , we want to close the stream. In order to give the client a chance to find out that the stream will get closed soon, we emit the string  and then we immediately close the stage afterwards. The  component can be combined with a stream via the  method, which was introduced in the section about flows.Beside from being able to close connections, it would also be nice if we could show a welcome message to a newly created connection. In order to do this we once again have to go a little bit further:The function   now takes the incoming connection as a parameter. Inside of its body we use a DSL that allows us to describe complex stream behavior. With  we create a stream that can only emit one element - the welcome message.  is what was described as  in the previous section. The only notable difference is that we added  to it. Now actually comes the interesting part of the DSL.  The  function makes a builder  available, which is used to express the stream as a graph. With the  function it is possible to connect input and output ports with each other. The  component that is used in the example can concatenate elements and is here used to prepend the welcome message in front of the other elements that come out of . In the last line, we only make the input port of the server logic and the output port of the concatenated stream available because all the other ports shall remain an implementation detail of the  component. For an in-depth introduction to the graph DSL of Akka Streams, visit the corresponding section in the . The full code example of the complex TCP server and of a client that can communicate with it can be found . Whenever you open a new connection from the client you should see a welcoming message and by typing  on the client you should see a message that tells you that the connection has been canceled.There are still some topics which weren't covered by this answer. Especially materialization may scare one reader or another but I'm sure with the material that is covered here everyone should be able to go the next steps by themselves. As already said, the  is a good place to continue learning about Akka Streams."},
{"body": "When should I use , , , ,  or ?I want an intuition/overview of their differences - possibly with some simple examples.In general, all 6 fold functions apply a binary operator to each element of a collection. The result of each step is passed on to the next step (as input to one of the binary operator's two arguments). This way we can  a result. and  cumulate a single result. and  cumulate a single result using a start value. and  cumulate a collection of intermediate cumulative results using a start value.With a collection of elements  and a binary operator  we can explore what the different fold functions do when going forwards from the LEFT element of the collection (from A to C):\nIf we start with the RIGHT element and go backwards (from C to A) we'll notice that now the  argument to our binary operator accumulates the result (the operator is the same, we just switched the argument names to make their roles clear): .If instead we were to  some result by subtraction starting from the LEFT element of a collection, we would cumulate the result through the first argument  of our binary operator :\nBut look out for the xRight variations now! Remember that the (de-)cumulated value in the xRight variations is passed to the  parameter  of our binary operator :The last List(-2, 3, -1, 4, 0) is maybe not what you would intuitively expect! As you see, you can check what your foldX is doing by simply running a scanX instead and debug the cumulated result at each step.Normally REDUCE,FOLD,SCAN method works by accumulating data on LEFT and keep on changing the RIGHT variable. Main difference between them is REDUCE,FOLD is:-Fold will always start with a  value i.e. user defined starting value. \nReduce will throw a exception if collection is empty where as fold gives back the seed value. Scan is used for some processing order of items from left or right hand side, then we can make use of previous result in subsequent calculation. That means we can scan items. A part of output for below mentioned code would be :- using  operation over a list of numbers (using  value ) using , operations over a list of Strings Code :"},
{"body": "One handy feature of Scala is , where the evaluation of a  is delayed until it's necessary (at first access).Of course, a  must have some overhead - somewhere Scala must keep track of whether the value has already been evaluated and the evaluation must be synchronized, because multiple threads might try to access the value for the first time at the same time.What exactly is the cost of a  - is there a hidden boolean flag associated with a  to keep track if it has been evaluated or not, what exactly is synchronized and are there any more costs?In addition, suppose I do this:Is this the same as having two separate s  and  or do I get the overhead only once, for the pair ?This is taken from the  and gives implementation details of  in terms of Java code (rather than bytecode):is compiled to something equivalent to the following Java code:It looks like the compiler arranges for a class-level bitmap int field to flag multiple lazy fields as initialized (or not) and initializes the target field in a synchronized block if the relevant xor of the bitmap indicates it is necessary.Using:produces sample bytecode:Values initialed in tuples like  have nested caching via the same mechanism.  The tuple result is lazily evaluated and cached, and an access of either x or y will trigger the tuple evaluation.  Extraction of the individual value from the tuple is done independently and lazily (and cached).  So the above double-instantiation code generates an , , and an  field of type .With Scala 2.10, a lazy value like:is compiled to byte code that resembles the following Java code:Note that the bitmap is represented by a . If you add another field, the compiler will increase the size of the field to being able to represent at least 2 values, i.e. as a . This just goes on for huge classes.But you might wonder why this works? The thread-local caches must be cleared when entering a synchronized block such that the non-volatile  value is flushed into memory. This blog article gives . proposes a new implementation of lazy val, which is more correct but ~25% slower than the \"current\" version.The  looks like:As of June 2013 this SIP hasn't been approved. I expect that it's likely to be approved and included in a future version of Scala based on the mailing list discussion.  Consequently, I think you'd be wise to heed :I've written a post with regard to this issue In nutshell, the penalty is so small that in practice you can ignore it.given the bycode generated by scala for lazy, it can suffer thread safety problem as mentioned in double check locking  "},
{"body": "I just got started with Scala/LiftWeb/Sbt developing, and I'd like to import a Sbt project in IntelliJ Idea.\nActually, I managed to import my project in two different ways:1) with Maven. I created a Maven project, and of top of that I created a Sbt project, which I then imported in IntelliJ. I could then easily start, stop the jetty server, and do other stuff.\nBut that's not what I want. I want to do the same stuff, just Maven-free.\nThat lead me to 2) with Eclipse. So, I created a new Sbt project (with a little script I wrote, configuring the Sbt project to be a WebProject). I used then the sbt-eclipsify plugin to 'convert' the project for Eclipse, which I then imported in IntelliJ (existing source -> eclipse).\nBut the problems started here: I cannot get the IntelliJ Sbt plugin to work. Can anyone help me with this?There are three basic ways how to create a project - modern versions of IntelliJ can import sbt project out of the box, otherwise you can either use sbt plugin to generate IntelliJ project, or use IntelliJ Scala plugin to create sbt project. Basic features work out of the box using both solutions, some complex builds can have problems, so try other tools to see if it works there.IntelliJ IDEA has become so much better these days. The current version (14.0.2) supports sbt projects out of the box with the Scala plugin. Just install the plugin and you should be able to open up Scala/sbt projects without any troubles.With the plugin, just point at a sbt project and IDEA is going to offer you a wizard to open that kind of project.IntelliJ plugin can be found here \n or can be installed directoly from within the IDE using Settings -> Plugins dialog. Afterwards one can just do File -> New Project -> Scala -> SBT based. IntelliJ will generate basic build.sbt, download necessary dependencies and open project. Sbt plugin that generate an idea project based on the sbt files can be found here: Simply add  to your ; no additional resolvers are needed.Create and add the following lines to  OR Use  in sbt to create IDEA project files.By default, classifiers (i.e. sources and javadocs) of sbt and library dependencies are loaded if found and references added to IDEA project files. If you don't want to download/reference them, use command .\n(according to the plugin author, 0.10.0  work!)Create and add the following lines to ~/.sbt/plugins/build.sbt:Use  sbt task to create IDEA project files.By default, classifiers (i.e. sources and javadocs) of sbt and library dependencies are loaded if found and references added to IDEA project files. If you don't want to download/reference them, use command .To use it, simply run this from your sbt shell, it will use the plugin as an external program:You can also add trait in your project definition, as you want:For now I do this by hand. It is quite simple.That's it from memory. It would be better if it were automated, but it's no big deal as it is now.One note of caution: The above approach doesn't work well with new-school sbt, i.e. versions 0.10 and newer, because it doesn't copy dependencies into lib_managed by default.  You can add to your build.sbt to make it copy the dependencies into lib_managed. and IntelliJ IDEA has become so much better these days. It's 2015 after all, isn't it?Having said that,  supports sbt projects out of the box with . Just install the plugin and you should be able to open up Scala/sbt projects without much troubles.I'm using the Early Access version of the plugin which is 1.2.67.6.EAP as of the time of the writing.With the plugin just point at a sbt project and IDEA is going to offer you a wizard to open that kind of project.For sbt  the system-wide plugin configuration file -  or  - should have the following lines:Run  to generate IDEA project files.Read  for more up-to-date information. You may also find my blog entry  useful.See .Clone and build Ismael's sbt-idea:Create an sbt plugin lib directory if you don't have one alreadyCopy the jar built in step one into hereRestart or reload sbt, then you can run  (or  if you want sources and javadoc in intelliJ too)Source: .You can open an SBT-based project in IDEA nowadays. It will create the necessary project and modules, and keep your dependencies up-to-date whenever you make changes to the build scripts.I just went through all this pain.  I spend days trying to get an acceptable environment up and have come to the conclusion that ENSIME, SBT and JRebel are going to be my development environment for some time.  Yes, it is going back to Emacs, but ENSIME turns it into a bit or an idea with refactoring, debugging support, navigation, etc.  It's not nowhere near as good as Eclipse (Java), but unless the scala plugins work better it's the best we have.Until the Scala development environments get up to snuff (Eclipse or IntelliJ) I'm not going to bother.  They're just way too buggy.See the discussion on the lift site.Within that thread, there is a link to a HOWTO for IntelliJ, but although it kinda works, there are many issues that render it a little less that useful.The answers are old for 2014.\nIn , the plugin Scala is ver 0.41.2 ( SBT is included).My  (terminal :   )Go to the project's root folder and enter in the terminalYou will see two new hidden folders   and .Then in IntelliJ, File > Open > select the project. \nIt should open the project without any problem.Before you start creating your SBT project, make sure that the Scala plugin is downloaded and enabled in IntelliJ IDEA.below link explains everything you need to know. "},
{"body": "What is the difference between:and Both can be called like .Method  evaluates on call and creates new function every time (new instance of ).With  you can get new function on every call: evaluates when defined,  - when called:Note that there is a third option: .It evaluates when called the first time:But returns the same result (in this case same instance of ) every time: evaluates when defined. evaluates on every call, so performance could be worse then with  for multiple calls. You'll get the same performance with a single call. And with no calls you'll get no overhead from , so you can define it even if you will not use it in some branches.With a  you'll get a lazy evaluation: you can define it even if you will not use it in some branches, and it evaluates once or never, but you'll get a little overhead from double check locking on every access to your .As @SargeBorsch noted you could define method, and this is the fastest option:But if you need a function (not method) for function composition or for higher order functions (like ) compiler will generate a function from your method every time you are using it as function, so performance could be slightly worse than with .Consider this: Do you see the difference? In short:: For every call to , it calls the body of the  method again. But with  i.e. , the function is initialized only once while declaration (and hence it prints  at line 4 and never again) and the same output is used each time it accessed. For example try doing this:When  is initialized, the value returned by  is set as the final value of . Next time  is used again, it will always return the same value.You can also lazily initialize . i.e. first time it is used it is initialized and not while declaration. For example:See this:Surprisingly, this will print 4 and not 9! val (even var) is evaluated immediately and assigned.\nNow change val to def.. it will print 9! Def is a function call.. it will evaluate each time it is called."},
{"body": "What is a good way to do logging in a Scala application? Something that is consistent with the language philosophy, does not clutter the code, and is low-maintenance and unobtrusive. Here's a basic requirement list:I know I can use the existing Java logging solutions, but they fail on at least two of the above, namely clutter and configuration.Thanks for your replies.Most of Scala's logging libraries have been some wrappers around a Java logging framework (slf4j, log4j etc), but as of March 2015, the surviving log libraries are all slf4j. These log libraries provide some sort of  object to which you can call , , etc. I'm not a big fan of slf4j, but it now seems to be the predominant logging framework. Here's the description of :The ability to change underlying log library at deployment time brings in unique characteristic to the entire slf4j family of loggers, which you need to be aware of:In a large project, it could actually be convenient to be able to control the logging behavior of transitive dependencies if everyone used slf4j. is written by Heiko Seeberger as a successor to his . It uses macro to expand calls into if expression to avoid potentially expensive log call.With Scala 2.10+ Consider ScalaLogging by Typesafe.  Uses macros to deliver a very clean APIQuoting from their wiki:After the macro has been applied, the code will have been transformed into the above described idiom.In addition ScalaLogging offers the trait  which conveniently provides a  instance initialized with the name of the class mixed into:Using slf4j and a wrapper is nice but the use of it's built in interpolation breaks down when you have more than two values to interpolate, since then you need to create an Array of values to interpolate.A more Scala like solution is to use a thunk or cluster to delay the concatenation of the error message.  A good example of this is Lift's logger\nWhich looks like this:Note that msg is a call-by-name and won't be evaluated unless isTraceEnabled is true so there's no cost in generating a nice message string.  This works around the slf4j's interpolation mechanism which requires parsing the error message.  With this model, you can interpolate any number of values into the error message.If you have a separate trait that mixes this Log4JLogger into your class, then you can doinstead ofI've actually followed the recommendation of Eugene and tried it and found out that it has a clumsy configuration and is subjected to bugs, which don't get fixed (such as ). It doesn't look to be well maintained and it . Here's what you need to be running it with Maven:This is how I got  working for me:Put this in your :Then, after doing an , this prints out a friendly log message:If you are using Play, you can of course simply  for writing log messages such as: .See the  for more info.I pulled a bit of work form the  trait of , and created a trait that also integrated a  library.Then stuff kind of looks like this:We like the approach so far.Implementation:I use SLF4J + Logback classic and apply it like this:Then you can use it whichever fits your style better:but this approach of course uses a logger instance per class instance. You should have a look at the scalax library :\n\nIn this library, there is a Logging trait, using sl4j as backend.\nBy using this trait, you can log quite easily (just use the logger field \nin the class inheriting the trait).,  and a  implementation.Haven't tried it yet, but Configgy looks promising for both configuration and logging: After using slf4s and logula for a while, I wrote , a simple logging trait wrapping slf4j. It offers an API similar to that of Python's logging library, which makes the common cases (basic string, simple formatting) trivial and avoids formatting boilerplate.I find very convenient using some kind of java logger, sl4j for example, with simple scala wrapper, which brings me such syntaxIn my opinion very usefull mixin of java proven logging frameworks and scala's fancy syntax.Quick and easy forms.Scala 2.10 and older:And build.sbt:Scala 2.11+ and newer:And build.sbt:"},
{"body": "For reading, there is the useful abstraction . How can I write lines to a text file?Edit (September 2011): since  asks about Scala2.9, and since  comments that  is pretty much non-existent since mid-2009... has changed place: see its , from  (also ):Original answer (January 2011), with the old place for scala-io:If you don't want to wait for Scala2.9, you can use the  library.\n(as mentioned in \"\")See This is one of the features missing from standard Scala that I have found so useful that I add it to my personal library.  (You probably should have a personal library, too.)  The code goes like so:and it's used like this:Similar to the answer by Rex Kerr, but more generic. First I use a helper function:Then I use this as:and  etc.A simple answer:Giving another answer, because my edits of other answers where rejected.  This is the  (similar to Garret Hall's)This is similar to Jus12, but without the verbosity and with correct Note you do NOT need the curly braces for , nor lambdas, and note usage of placeholder syntax. Also note better naming.One liners for saving/reading to/from , using .This isn't suitable for large files, but will do the job.Some links:\n\n\nHere is a concise one-liner using the Scala compiler library:Alternatively, if you want to use the Java libraries you can do this hack:From A micro library I wrote: orAfter reviewing all of these answers on how to easily write a file in Scala, and some of them are quite nice, I had three issues:Before starting, my goal isn't conciseness. It's to facilitate easier understanding for Scala/FP beginners, typically those coming from Java. At the very end, I will pull all the bits together, and then increase the conciseness.First, the  method needs to be updated to use  (again, conciseness is not the goal here). It will be renamed to :The beginning of the above  method might be confusing because it appears to have two parameter lists instead of the customary single parameter list. This is called currying. And I won't go into detail how currying works or where it is  useful. It turns out that for this particular problem space, it's the right tool for the job.Next, we need to create method, , which will create a (or overwrite an existing)  and write a . It uses a  which is encapsulated by a  which is in turn encapsulated by a . And to elevate performance, a default buffer size much larger than the default for  is defined, , and assigned the value 65536.Here's the code (and again, conciseness is not the goal here):The above  method is useful in that it takes a  as input and sends it to a . Let's now create a  method which takes a  and writes it to a .Here's the code (and I'll let you guess conciseness's priority here):Finally, it is useful to be able to fetch the contents of a  as a . While  provides a convenient method for easily obtaining the contents of a , the  method must be used on the  to release the underlying JVM and file system handles. If this isn't done, then the resource isn't released until the JVM GC (Garbage Collector) gets around to releasing the  instance itself. And even then, there is only a weak JVM guarantee the  method will be called by the GC to  the resource. This means that it is the client's responsibility to explicitly call the  method, just the same as it is the responsibility of a client to tall  on an instance of . For this, we need a second definition of the using method which handles .Here's the code for this (still not being concise):And here is an example usage of it in a super simple line streaming file reader (currently using to read tab delimited files from database output):An  has been provided as an answer to a .Now, bringing that all together with the imports extracted (making it much easier to paste into Scala Worksheet present in both Eclipse ScalaIDE and IntelliJ Scala plugin to make it easy to dump output to the desktop to be more easily examined with a text editor), this is what the code looks like (with increased conciseness):As a Scala/FP newbie, I've burned many hours (in mostly head scratching frustration) earning the above knowledge and solutions. I hope this helps other Scala/FP newbies get over this particular learning hump faster.Here's an example of writing some lines to a file using .To surpass samthebest and the contributors before him, I have improved the naming and conciseness:This line helps to write a file from an Array or String."},
{"body": "In shapeless, the Nat type represents a way to encode natural numbers at a type level. This is used for example for fixed size lists. You can even do calculations on type level, e.g. append a list of  elements to a list of  elements and get back a list that is known at compile time to have  elements.Is this representation capable of representing large numbers, e.g.  or 2, or will this cause the Scala compiler to give up?I will attempt one myself. I will gladly accept a better answer from Travis Brown or Miles Sabin.Nat can currently  be used to represent large numbersIn the current implementation of Nat, the value corresponds to the number of nested shapeless.Succ[] types:So to represent the number 1000000, you would have a type that is nested 1000000 levels deep, which would definitely blow up the scala compiler. The current limit seems to be about 400 from experimentation, but for reasonable compile times it would probably be best to stay below 50.However, there is a way to encode large integers or other values at type level, . The only thing you can do with those as far as I know is to check if they are equal or not. See below.This could be used to e.g. enforce same array size when doing bit operations on Array[Byte].Shapeless's  encodes natural numbers at the type level using Church encoding. An alternate method is to represent the naturals as a type level HList of bits. Check out  which implements this solution in a shapeless style. :)"},
{"body": "When I compile Scala code, by running ,  says:How do I do that? (From within SBT?)If you don't want to change your :Edit: Added  and  per comments.Add this setting to your build.sbt, and, if you have a multi-module project, add it to every project's settings.As times flows new solutions are emerged. So, now you could re-run the scala compiler without issuing entire project rebuild.You need to install : After that you could use the  task to compile single file. SBT allows per tasks settings configuration, so you could change for that tasks only:"},
{"body": "It sounds like a stupid question, but all I found on the internet was trash.\nI simply can't add an element of type T into a list List[T].\nI tried with  but it seems it creates a strange object and accessing to  always returns the first element that was put inside the list.Note that this operation has a complexity of O(n). If you need this operation frequently, or for long lists, consider using another data type (e.g. a ListBuffer).That's because you shouldn't do it (at least with an immutable list).\nIf you really really need to append an element to the end of a data structure and this data structure really really needs to be a list and this list really really has to be immutable then do eiher this:or that:Lists in Scala are not designed to be modified. In fact, you can't add elements to a Scala ; it's an , like a Java String. \nWhat you actually do when you \"add an element to a list\" in Scala is to create a . Instead of using lists for such use cases, I suggest to either use an  or a . Those datastructures are designed to have new elements added.Finally, after all your operations are done, the buffer then can be converted into a list. See the following REPL example:We can append or prepend two lists or list&array\n        "},
{"body": "Operator overloading in C++ is considered by many to be A Bad Thing(tm), and a mistake not to be repeated in newer languages. Certainly, it was one feature specifically dropped when designing Java.Now that I've started reading up on Scala, I find that it has what looks very much like operator overloading (although technically it doesn't have operator overloading because it doesn't have operators, only functions). However, it wouldn't seem to be qualitatively different to the operator overloading in C++, where as I recall operators are defined as special functions.So my question is what makes the idea of defining \"+\" in Scala a better idea than it was in C++?C++ inherits true blue operators from C.  By that I mean that the \"+\" in 6 + 4 is very special.  You can't, for instance, get a pointer to that + function.Scala on the other hand doesn't have operators in that way.  It just has great flexibility in defining method names plus a bit of built in precedence for non-word symbols.  So technically Scala doesn't have operator overloading.Whatever you want to call it, operator overloading isn't inherently bad, even in C++.   The problem is when bad programmers abuse it.  But frankly, I'm of the opinion that taking away programmers ability to abuse operator overloading doesn't put a drop in the bucket of fixing all the things that programmers can abuse.  The real answer is mentoring.  None-the-less, there are differences between C++'s operator overloading and Scala's flexible method naming which, IMHO, make Scala both less abusable and more abusable.In C++ the only way to get in-fix notation is using operators.  Otherwise you must use object.message(argument) or pointer->messsage(argument) or function(argument1, argument2).  So if you want a certain DSLish style to your code then there's pressure to use operators.In Scala you can get infix notation with any message send.  \"object message argument\" is perfectly ok, which means you don't need to use non-word symbols just to get infix notation.C++ operator overloading is limited to essentially the C operators.  Combined with the limitation that only operators may be used infix that puts pressure on people to try to map a wide range of unrelated concepts onto a relatively few symbols like \"+\" and \">>\"Scala allows a huge range of valid non-word symbols as method names.  For instance, I've got an embedded Prolog-ish DSL where you can write The :-, !, ?, and & symbols are defined as ordinary methods.   In C++ only & would be valid so an attempt to map this DSL into C++ would require some symbols that already evoke very different concepts.  Of course, this also opens up Scala to another kind of abuse.  In Scala you can name a method $!&^% if you want to. For other languages that, like Scala, are flexible in the use of non-word function and method names see Smalltalk where, like Scala, every \"operator\" is just another method and Haskell which allows the programmer to define precedence and fixity of flexibly named functions.Only by the ignorant. It is absolutely required in a language like C++, and it is noticeable that other languages that started off taking a \"purist\" view, have added it once their designers found out how necessary it is.Operator overloading was never universally thought to be a bad idea in C++ - just the abuse of operator overloading was thought to be a bad idea. One doesn't really need operator overloading in a language since they can be simulated with more verbose function calls anyway.  Avoiding operator overloading in Java made the implementation and specification of Java a little simpler and it forced programmers to not abuse operators.  There has been some debate in the Java community about introducing operator overloading.The advantages and disadvantages of operator overloading in Scala are the same as in C++  - you can write more natural code if you use operator overloading appropriately - and more cryptic, obfuscated code if you don't.FYI: Operators are not defined as special functions in C++, they behave just like any other function - although there are some differences in name lookup, whether they need to be member functions, and the fact that they can be called in two ways: 1) operator syntax, and 2) operator-function-id syntax.This article - \"\" - answers your question directly.Java mistakenly (according to the author) omitted operator overloading because it was complicated in C++, but forgot why (or didn't realize that it didn't apply to Java).Thankfully, higher level languages like Scala give developers options, while still running on the same JVM.There is nothing wrong with operator overloading.  In fact, there's something wrong with  having operator overloading for numeric types.  (Take a look at some Java code that uses BigInteger and BigDecimal.)C++ has a tradition of abusing the feature, though.  An often-cited example is that the bitshift operators are overloaded to do I/O.In general it is not a bad thing.\nNew languages such as C# also have operator overloading.It is the abuse of operator overloading that is a bad thing.But there are also problems with operator overloading as defined in C++. Because overloaded operators are just syntactic sugar for method calls they behave just like method. On the other hand normal built-in operators do not behave like methods. These inconsistency can be cause problems.Off the top of my head operators  and .\nThe built in versions of these are short-cut operators. This is not true for overloaded versions and has caused some problems.The fact that + - * / all return the same type that they operate on (after operator promotion)\nThe overloaded versions can return anything (This is where the abuse sets in, If your operators start to return some arbitrator type the user was not expecting things go down hill).Operator overloading is not something that you really \"need\" very often, but when using Java, if you hit a point where you genuinely need it, it'll make you want to rip your fingernails out just so you have an excuse to stop typing.That code which you've just found overflows a long? Yup, you're going to have to retype the whole lot to make it work with BigInteger. There is nothing more frustrating that having to reinvent the wheel just to change the type of a variable.Guy Steele argued that operator overloading should be in Java as well, in his keynote speech \"Growing a language\" - there's a video and a transcription of it, and it's really an amazing speech. You will wonder what he is talking about for the first couple of pages, but if you keep on reading, you will see the point and achieve enlightenment. And the very fact that he could do such a speech at all is also amazing.At the same time, this talk inspired a lot of fundamental research, probably including Scala - it's one of those papers that everybody should read to work in the field.Back to the point, his examples are mostly about numeric classes (like BigInteger, and some weirder stuff), but that's not essential.It is true, though, that misuse of operator overloading can lead to terrible results, and that even proper uses can complicate matters, if you try to read code without studying a bit the libraries it uses. But is that a good idea? OTOH, shouldn't such libraries try to include an operator cheat sheet for their operators?Operator overloading was not a C++ invention - it came from Algol IIRC and even Gosling does not claim it is a bad idea in general.I believe EVERY answer missed this. In C++ you can overload operators all you want, but you can't effect the precedence with which they're evaluated. Scala doesn't have this issue, IIRC.As for it being a bad idea, besides precedence issues, people come up with really daft meanings for operators, and it rarely aids readability. Scala libraries are especially bad for this, goofy symbols that you must memorize each time, with library maintainers sticking their heads in the sand saying, 'you only need to learn it once'. Great, now I need to learn some 'clever' author's cryptic syntax * the number of libraries I care to use. It wouldn't be so bad if there existed a convention of ALWAYS supplying a literate version of the operators.The only thing known wrong in C++ is the lack of the ability to overload []= as a separate operator. This could be hard to implement in a C++ compiler for what is probably not an obvious reason but plenty worth it.As the other answers have pointed out; operator overloading itself isn't necessarily bad. What is bad it when it is used in ways that make the resulting code un-obvious. Generally when using them you need to make them do the least surprising thing (having operator+ do division would cause trouble for a rational class's usage) or as Scott Meyers says:Now some people have taken operator overloading to the extreme with things like . At this level you have no idea how it is implemented but it makes an interesting syntax to get what you want done. I'm not sure if this is good or bad. It seems nice, but I haven't used it. I have never seen an article claiming that C++'s operator overloading is bad.User-definable operators permit an easier higher level of expressivity and usability for users of the language. AFAIK, There is nothing special in operator functions compared to \"normal\" member functions. Of course you only have a certain set of operators that you can overload, but that doesn't make them very special."},
{"body": "It seems that  was late to the Scala collections party, and all the influential blog posts had already left.In Java  is the default collection - I might use  but only when I've thought through an algorithm and care enough to optimise. In Scala should I be using  as my default , or trying to work out when  is actually more appropriate?As a general rule, default to using .  It\u2019s faster than  for  everything and more memory-efficient for larger-than-trivial sized sequences. See this  of the relative performance of Vector compared to the other collections. There are some downsides to going with .  Specifically:Another downside before Scala 2.10 was that pattern matching support was better for , but this was rectified in 2.10 with generalized  and  extractors.There is also a more abstract, algebraic way of approaching this question: what sort of sequence do you  have?  Also, what are you  doing with it?  If I see a function that returns an , I know that function has some holes in its domain (and is thus partial).  We can apply this same logic to collections.If I have a sequence of type , I am effectively asserting two things.  First, my algorithm (and data) is entirely stack-structured.  Second, I am asserting that the only things I\u2019m going to do with this collection are full, O(n) traversals.  These two really go hand-in-hand.  Conversely, if I have something of type , the  thing I am asserting is that my data has a well defined order and a finite length.  Thus, the assertions are weaker with , and this leads to its greater flexibility.Well, a  can be incredibly fast if the algorithm can be implemented solely with ,  and . I had an object lesson of that very recently, when I beat Java's  by generating a  instead of an , and couldn't beat that with anything else.However,  has a fundamental problem: it doesn't work with parallel algorithms. I cannot split a  into multiple segments, or concatenate it back, in an efficient manner.There are other kinds of collections that can handle parallelism much better -- and  is one of them.  also has great locality -- which  doesn't -- which can be a real plus for some algorithms.So, all things considered,  is the best choice  you have specific considerations that make one of the other collections preferable -- for example, you might choose  if you want lazy evaluation and caching ( is faster but doesn't cache), or  if the algorithm is naturally implemented with the operations I mentioned.By the way, it is preferable to use  or  unless you want a specific piece of API (such as 's ), or even  or  if your algorithm can be run in parallel.For immutable collections, if you want a sequence, your main decision is whether to use an  or a , which give different guarantees for performance. An IndexedSeq provides fast random-access of elements and a fast length operation. A LinearSeq provides fast access only to the first element via , but also has a fast  operation. (Taken from the Seq documentation.)For an  you would normally choose a . s and s are also IndexedSeqs.For a  you would normally choose a  or its lazy equivalent . Other examples are s and s.So in Java terms,  used similarly to Scala's , and  similarly to Scala's . But in Scala I would tend to use List more often than Vector, because Scala has much better support for functions that include traversal of the sequence, like mapping, folding, iterating etc. You will tend to use these functions to manipulate the list as a whole, rather than randomly accessing individual elements.Some of the statements here are confusing or even wrong, especially the idea that immutable.Vector in Scala is anything like an ArrayList.\nList and Vector are both immutable, persistent (i.e. \"cheap to get a modified copy\") data structures.\nThere is no reasonable default choice as their might be for mutable data structures, but it rather depends on what your algorithm is doing.\nList is a singly linked list, while Vector is a base-32 integer trie, i.e. it is a kind of search tree with nodes of degree 32.\nUsing this structure, Vector can provide most common operations reasonably fast, i.e. in O(log_32(n)). That works for prepend, append, update, random access, decomposition in head/tail. Iteration in sequential order is linear.\nList on the other hand just provides linear iteration and constant time prepend, decomposition in head/tail. Everything else takes in general linear time.This might look like as if Vector was a good replacement for List in almost all cases, but prepend, decomposition and iteration are often the crucial operations on sequences in a functional program, and the constants of these operations are (much) higher for vector due to its more complicated structure.\nI made a few measurements, so iteration is about twice as fast for list, prepend is about 100 times faster on lists, decomposition in head/tail is about 10 times faster on lists and generation from a traversable is about 2 times faster for vectors. (This is probably, because Vector can allocate arrays of 32 elements at once when you build it up using a builder instead of prepending or appending elements one by one).\nOf course all operations that take linear time on lists but effectively constant time on vectors (as random access or append) will be prohibitively slow on large lists.So which data structure should we use?\nBasically, there are four common cases:In situations which involve a lot random access and random mutation, a  (or \u2013 as the  say \u2013 a ) seems to be a good compromise. This is also what the  suggest.Also, the  class seems to play nicely in distributed environments without much data duplication because there is no need to do a copy-on-write for the complete object. (See: )If you're programming immutably and need random access, Seq is the way to go (unless you want a Set, which you often actually do). Otherwise List works well, except it's operations can't be parallelized. If you don't need immutable data structures, stick with ArrayBuffer since it's the Scala equivalent to ArrayList."},
{"body": "I know this question is a bit open but I have been looking at Scala/Lift as an alternative to Java/Spring and I wonder what are the real advantages that Scala/Lift has over it. From my perspective and experience, Java Annotations and Spring really minimizes the amount of coding that you have to do for an application. Does Scala/Lift improve upon that?Spring and Lift are almost diametrically opposed in terms of maturity and goals.In a sentence, Spring is heavyweight and Lift is lightweight.  With sufficient determination and resources you can turn that on its head, but you would need a  of both.Here are concrete differences that stuck in my mind after working with both frameworks.  This isn't an exhaustive list, which I can't compile anyhow.  Just what seemed most interesting to me...Both frameworks are compelling.  There's a broad range of apps where you can choose either and do well.I've gotta say that I strongly disagree with Dan LaRocque's answer.Lift is not monolithic.  It is composed on discrete elements.  It does not ignore J/EE elements, it support the likes of JNDI, JTA, JPA, etc.  The fact that you're not forced to uses these elements of J/EE is a strong indication of Lift's modular design.With the above being said, let me talk some about Lift's design philosophy.I wrote  before I started writing Lift.  To a great degree, and to a greater degree than is true for any other web framework that I know of, Lift meets these goals.Lift at its core seeks to abstract away the HTTP request/response cycle rather than placing object wrappers around the HTTP Request.  At the practical level, this means that most any action that a user can take (submitting form elements, doing Ajax, etc.) is represented by a GUID in the browser and a function on the server.  When the GUID is presented as part of the an HTTP request, the function is applied (called) with the supplied parameters.  Because the GUIDs are hard to predict and session-specific, replay attacks and many parameter tampering attacks are far more difficult with Lift than most other web frameworks, including Spring.  It also means that developers are more productive because they are focusing on user actions and the business logic associated with user actions rather than the plumbing of packing and unpacking an HTTP request.  For example, code for accepting or rejecting a FourSquare friend request:It's that simple.  Because the friendRequest is in the scope when the function is created, the function closes over the scope... there's no need to expose the primary key of the friend request or do anything else... just define the text of the button (it can be localized or it can be pulled from an XHTML template or it can be pulled from a localized template) and the function to execute when the button is pushed.  Lift takes care of assigning the GUID, setting up the Ajax call (via jQuery or YUI, and yes, you can add your own favorite JavaScript library), doing automatic retries with back-offs, avoiding connection starvation by queuing Ajax requests, etc.So, one big difference between Lift and Spring is that Lift's philosophy of GUID associated with function has the dual benefit of much better security and much better developer productivity.  The GUID -> Function association has proven very durable... the same construct works for normal forms, ajax, comet, multi-page wizards, etc.The next core piece of Lift is keeping the high level abstractions around for as long as possible.  On the page generation side, that means building the page as XHTML elements and keeping the page as XHTML until just before streaming the response.  The benefits are resistance to cross site scripting errors, the ability to move CSS tags to the head and scripts to the bottom of the page after the page has been composed, and the ability to rewrite the page based on the target browser.  On the input side, URLs can be re-written to extract parameters (both query and path parameters) in a type-safe manner, high level, security checked data is available for processing very early in the request cycle.  For example, here's how to define servicing of a REST request:Using Scala's built-in pattern matching, we match an incoming request, extract the third part of the path and get the User that corresponds to that value, and even apply access control checks (does the current session or request have permissions to access the given User record).  So, by the time the User instance hits the application logic, it's vetted.With these two core pieces, Lift has a tremendous advantage in terms of security.  To give you an idea of the magnitude of Lift's security that doesn't get in the way of features,  who did security for Yahoo! had this to say about FourSquare (one of the Lift poster-child sites):At the time, FourSquare had one engineer working on the code (not that @harryh isn't a super-genius) and his main focus was re-writing the PHP version of FourSquare while coping with weekly traffic doubling.The last part of Lift's security focus is SiteMap.  It's a unified access control, site navigation, and menu system.  The developer defines the access control rules for each page using Scala code (e.g.  or ) and those access control rules are applied before any page rendering starts.  This is much like Spring Security, except that it's baked in from the beginning of the project and the access control rules are unified with the rest of the application so you don't have to have process for updating the security rules in XML when the URLs change or the methods that calculate the access control change.To summarize so far, Lift's design philosophy gives you the benefits of baked in access control, resistance to the OWASP top 10 security vulnerabilities, much better Ajax support and much higher developer productivity than does Spring.But Lift also gives you the best Comet support of any web framework around.  That's why Novell chose Lift to power their  and here's what Novell has to say about Lift:So, Lift is not just another me-too MVC framework.  It's a framework that's got some core design principles behind it that have matured very well.  It's a framework that gives the dual advantages of security and developer productivity.  Lift is a framework that's built in layers and gives the developer the right choices based on their needs... choices for view generation, choices for persistence, etc.Scala and Lift give developers a much better experience than the melange of XML, annotations, and other idioms that make up Spring.I would recommend you to check play framework, it has some very interesting ideas and supports development in Java and ScalaJust for fun. And for the sake of learning new programming approaches. I strongly looked into using Lift for a recent web project, not being a big fan of Spring MVC. I have not used the latest versions, but the earlier versions of Spring MVC made you jump through a lot of hoops to get a web application running. I was almost sold on Lift until I saw that Lift can be very session dependent and would require 'sticky sessions' to work correctly. Excerpt from So once a Session is required, the user would have to be pin to that node. This creates the need for intelligent load balancing and affects scaling, which prevented Lift from being a solution in my case. I ended up selecting  and have been very pleased. Play has been stable and reliable so far and very easy to work with.I  come to Lift and Scala from a Java background, so this isn't from personal experience, but I know that many Lift developers find Scala to be a much more concise and efficient language than Java.Expanding your knowledge is always a worthwhile endeavor :) I just started learning Scala, it's affecting how I write normal Java and I can say it's been very beneficial so far. I hate to completely throw your world for a loop.  But you can you use Scala, Java, Lift, Spring in one application and have it not be a problem.In my humble opinion, imagination is what matters.Let's consider you want to write an app. If you're a decent developer, the app should already be build in your mind. The next step is to discover how it works through code. In order to do that, you need to pass the imagined app through a function that translates it to a real world app. That function is a programming language. So So the language choice is important. So is the framework. There are a ton of smart people here that will advise you on what to chose, but ultimately, the language / framework that best translates your imagination should be your choice. So prototype with both and make your choice.As for me, I'm slowly learning Scala and Lift and loving it.But the main problem is we can't compare spring with lift. Lift is basically use as UI framework and Spring is use as DI framework. \n\nIf you are developing web app that does have that much of backend sure you can use lift.\n\nbut if your developing web app that have some series backend and you definelty need to goto spring."},
{"body": "The question is best explained by an example:In Java for a JPA EntityManager, I can do the following(Account is my Entity class):In Scala, my naive attempt is:But when I try to use  in Scala, it seems to not like this.  How can I specify the java.lang.Class object for the Account class in Scala?According to \"\", However,  and  return slightly different values, reflecting the effect of type erasure on the JVM, in the case of getClass.That is why the :There is a .( reports that the ticket is \"now\", ie Nov. 2011, two years later, fixed.\nIn 2.9.1,  now does: )Back in 2009:Note: regarding , a possible workaround would be: in Scala is equivalent to  in Java."},
{"body": "I have seen many examples of ARM (automatic resource management) on the web for Scala. It seems to be a rite-of-passage to write one, though most look pretty much like one another. I  see a pretty cool example using continuations, though.At any rate, a lot of that code has flaws of one type or another, so I figured it would be a good idea to have a reference here on Stack Overflow, where we can vote up the most correct and appropriate versions.Daniel,I've just recently deployed the scala-arm library for automatic resource management.  You can find the documentation here: This library supports three styles of usage (currently):1) Imperative/for-expression:2) Monadic-style3)  Delimited Continuations-styleHere's an \"echo\" tcp server:The code makes uses of a Resource type-trait, so it's able to adapt to most resource types.   It has a fallback to use structural typing against classes with either a close or dispose method.   Please check out the documentation and let me know if you think of any handy features to add.Chris Hansen's  talks about about slide 21 of Martin Odersky's . This next block is taken straight from slide 21 (with permission): --end quote--Then we can call like this:What are the drawbacks of this approach? That pattern would seem to address 95% of where I would need automatic resource management... added code snippet extending the design pattern - taking inspiration from python  statement and addressing:This is with Scala 2.8.Here's  solution using continuations:Here are the solutions with and without continuations for comparison:And here's Tiark Rompf's suggestion of improvement:Daniel, good you asked this. I am myself intrigued after seeing James Iry's code. I see a gradual 4 step evolution for doing ARM in Scala:What I would really love to see is a presentation describing these. It will be very educational and should convince the begots that there is a world beyond Monads :)There is light-weight (10 lines of code) ARM included with better-files. See: Here is how it is implemented if you don't want the whole library:"},
{"body": "How to split a List of elements into lists with at most N items?ex: Given a list with 7 elements, create groups of 4, leaving the last group possibly with less elements.I think you're looking for . It returns an iterator, but you can convert the result to a list,Or if you want to make your own: Use:: upon reviewing this 2 years later, I wouldn't recommend this implementation since  is O(n), and hence this method is O(n^2), which would explain why the built-in method becomes faster for large lists, as noted in comments below. You could implement efficiently as follows:or even (slightly) more efficiently using :I am adding a tail recursive version of the split method since there was some discussion of tail-recursion versus recursion. I have used the  tailrec annotation to force the compiler to complain in case the implementation is not indeed tail-recusive. Tail-recursion I believe turns into a loop under the hood and thus will not cause problems even for a large list as the stack will not grow indefinitely. I think this is the implementation using splitAt instead of take/drop"},
{"body": "In the last version of scala (2.10.3) REPL, I can type  to quit from REPL.  However, in Scala 2.11.0 this doesn't work.  I ran into the same issue on upgrade, just use colon q.Additionally,  was deprecated in 2.10.x with  suggested instead, so this works as well:As a side note, I think they did this so you can distinguish between exiting the scala console in sbt and exiting sbt itself, though I could be wrong.You options to leave the REPL as stated in the answers before are:Use the end of file characters.  on linux,  on Windows.Using the  command on my 2.10.3 REPL gets me this hint: I don't know whether  is still there in 2.11.0 or not though.  When I use  in 2.10.4, I got a warning: You can use:all of them work in 2.11.x."},
{"body": "Learning Scala currently and needed to invert a Map to do some inverted value->key lookups.  I was looking for a simple way to do this, but came up with only:Anybody have a more elegant approach?Assuming values are unique, this works:On Scala 2.8, however, it's easier:Being able to do that is part of the reason why Scala 2.8 has a new collection library.Mathematically, the mapping might not be ivertible, e.g., from , you can't get , but rather you get , because there might be different keys associated with same values. So, if you are interested in knowing all the keys, here's the code: You can avoid the ._1 stuff while iterating in few ways.  Here's one way.  This uses a partial function that covers the one and only case that matters for the map: Here's another way:The map iteration calls a function with a two element tuple, and the anonymous function wants two parameters.  Function.tupled makes the translation.I came here looking for a way to invert a Map of type Map[A, Seq[B]] to Map[B, Seq[A]], where each B in the new map is associated with every A in the old map for which the B was contained in A's associated sequence. E.g.,\n\nwould invert to \nHere's my solution :where oldMap is of type  and newMap is of type The nested foldLefts make me cringe a little bit, but this is the most straightforward way I could find to accomplish this type of inversion. Anyone have a cleaner solution?In scala REPL:Note that duplicate values will be overwritten by the last addition to the map:You could invert a map using:The problem with this approach is that if your values, which have now become the hash keys in your map, are not unique you will drop the duplicate values. To illustrate:To avoid this you can convert your map to a list of tuples first, then invert, so that you don't drop any duplicate values:If it's a one-to-one map, you end up with singleton lists which can be trivially tested and transformed to a Map[B,A] rather than Map[B,List[A]]."},
{"body": "I've been curious about the impact of not having an explicit primary constructor in Scala, just the contents of the class body.In particular, I suspect that the private or protected constructor pattern, that is, controlling construction through the companion object or another class or object's methods might not have an obvious implementation. Am I wrong? If so, how is it done?You can declare the default constructor as private/protected by inserting the appropriate keyword between the class name and the parameter list, like this:'s answer is correct, but  offers an additional alternative:"},
{"body": "The question is in two parts. The first is conceptual. The next looks at the same question more concretely in Scala.I come from an imperative programming background (C++, Java). I have been exploring functional programming, specifically Scala.Some of the primary principles of pure functional programming:Even though modern  are extremely efficient with object creation and  is very inexpensive for short lived objects, it's probably still better to minimize object creation right? At least in a single-threaded application where concurrency and locking is not an issue. Since Scala is a hybrid paradigm, one can choose to write imperative code with mutable objects if necessary. But, as someone who has spent a lot of years trying to reuse objects and minimize allocation. I would like a good understanding of the school of thought that would not even allow that.As a specific case, I was a little surprised by this code snippet in   . It has a Java version of Quicksort followed by a neat looking Scala implementation of the same.Here is my attempt to benchmark the implementations. I haven't done detailed profiling. But, my guess is that the Scala version is slower because the number of objects allocated is linear (one per recursion call). Is there any way chance that tail call optimizations can come into play? If I am right, Scala supports tail call optimizations for self-recursive calls. So, it should only be helping it. I am using Scala 2.8.Time in milliseconds for five consecutive runsSince there are a few  flying around here, I\u2019d like to clarify some points.There are  of a functional quicksort implementation. In the following, let\u2019s consider this reference implementation in Haskell (I don\u2019t know Scala \u2026) from the :A third disadvantage is somewhat hidden: unlike the \u201cin-place\u201d variant, this implementation  for the cons cells of the list and potentially scatters memory all over the place. As a result, this algorithm has a very . I don\u2019t know whether smart allocators in modern functional programming languages can mitigate this \u2013 but on modern machines, cache misses have become a major performance killer. Unlike others, I wouldn\u2019t say that quicksort is inherently imperative and that\u2019s why it performs badly in an FP environment. Quite on the contrary, I would argue that quicksort is a perfect example of a functional algorithm: it translates seamlessly into an immutable environment, its asymptotic running time and space complexity are on par with the procedural implementation, and even its procedural implementation employs recursion.But this algorithm  performs worse when constrained to an immutable domain. The reason for this is that the algorithm has the peculiar property of benefitting from a lot of (sometimes low-level) fine-tuning that can only be efficiently performed on arrays. A naive description of the quicksort misses all these intricacies (both in the functional and in the procedural variant).After reading \u201cEngineering a sort function\u201d I can no longer consider quicksort an elegant algorithm. Implemented efficiently, it is a clunky mess, an engineer\u2019s piece of work, not an artist\u2019s (not to devalue engineering! this has its own aesthetic).But I would also like to point out that this point is particular to quicksort. Not every algorithm is amenable to the same sort of low-level tweaking. A lot of algorithms and data structures really  be expressed without performance loss in an immutable environment.And immutability can even  performance costs by removing the need of costly copies or cross-thread synchronizations.So, to answer the original question, \u201c\u201d \u2013 In the particular case of quicksort, there is a cost that is indeed a result of immutability. But in general, .There are a bunch of things wrong with this as a benchmark of functional programming.  Highlights include:So, this comparison is a great illustration that you must understand your language (and algorithm) in detail in order to write high-performance code.  But it's not a very good comparison of FP vs. non-FP.  If you want that, check out .  The take-home message there is that the penalty is typically not more than a factor of 2 or 3 or so, but it really depends.  (No promises that the Haskell folks have written the fastest algorithms possible either, but at least some of them probably tried!  Then again, some of the Haskell calls C libraries....)Now, suppose you do want a more reasonable benchmark of Quicksort, recognizing that this is probably one of the worst cases for FP vs. mutable algorithms, and ignoring the data-structure issue (i.e. pretending that we can have an immutable Array):Note the modification to the functional Quicksort so it only goes through the data once if at all possible, and the comparison to the built-in sort.  When we run it we get something like:So, aside from learning that trying to write your own sort is a bad idea, we find that there is a ~3x penalty for an immutable quicksort if the latter is implemented somewhat carefully.  (You could also write a trisect method that returns three arrays: those less than, those equal, and those greater than the pivot.  This might speed things up slightly more.)I don't think the Scala version is actually tail recursive, since you are using .Also, just because this is idiomatic Scala code, this doesn't mean it is the best way to do it.The best way to do this would be to use one of Scala's built-in sorting functions. That way you get the immutability guarantee and know you have a speedy algorithm.See Stack Overflow question  for an example.Sorting an array is, like, the most imperative task in the universe.  It is not surprising that many elegant 'immutable' strategies/implementations fail poorly on a 'sort an array' microbenchmark.  This does not imply that immutability is expensive \"in general\", though.  There are many tasks where immutable implementations will perform comparably to mutable ones, but array sorting often is not one of them.If you're simply rewriting your imperative algorithms and data structures into functional language it indeed will be expensive and useless. To make the things shine, you should use the features available only in functional programming: data stuctures persistence, lazy evaluations etc. The cost of immutability in Here is a version that is nearly as fast than the Java one. ;)This version makes a copy of the array, sorts it in place using the Java version and returns the copy. Scala does not force you to use immutable structure internally. So the benefit of Scala is that you can leverage mutability and immutability as you see fit. The disadvantage is that if you do that wrong you don't really get the benefits of immutability.Immutability is not expensive. It sure can be expensive if you measure a small subset of the tasks a program have to do, and pick a solution based on mutability to boot -- such as measuring quicksort.To put it simply, you don't quicksort when using purely functional languages.Let's consider this from another angle. Let's consider these two functions:Benchmark THAT, and you'll find that the code using mutable data structures has much worse performance, because it needs to copy the array, while the immutable code need not concern itself with that.When you program with immutable data structures, you structure your code to take advantage of its strengths. It is not simply the data type, or even individual algorithms. The program will be  in a different manner.Which is why benchmarking is usually meaningless. Either you choose algorithms that are natural to one style or another, and that style wins, or you benchmark the whole application, which is often impractical.QuickSort is known to be faster when done in-place, so this is hardly a fair comparison!Having said that... Array.concat?\nIf nothing else, you're showing how a collection type optimised for imperative programming is particularly slow when you try and use it in a functional algorithm; almost any other choice would be faster!Another  important point to consider, perhaps  most important issue when comparing the two approaches is: \"how well does this scale out to multiple nodes/cores?\"Chances are, if you're looking for an immutable quicksort then you're doing so because you actually want a parallel quicksort.  Wikipedia has some citations on this subject: The scala version can simply fork before the function recurses, allowing it to very quickly sort a list containing billions of entries if you have enough cores available.Right now, the GPU in my system has 128 cores available to me if I could just run the Scala code on it, and this is on a simple desktop system two years behind the current generation.  How would that stack up against the single-threaded imperative approach I wonder...Perhaps the more important question is therefore:It's been said that OO programming uses abstraction to hide complexity, and functional programming uses immutability to remove complexity. In the hybrid world of Scala we can use OO to hide the imperative code leaving application code none the wiser. Indeed the collections libraries use plenty of imperative code but that doesn't mean we shouldn't use them. As others have said, used with care, you really get the best of both worlds here."},
{"body": "What is so powerful about flatmap that it deserves such a place in the Scala folklore?The reasoning behind this phrase is that you can replace a lot of tedious if/then/else code you would write with calls to flatMap (and other higher order functions).This is especially true for Options (see ) But it applies to other monads as well (although I have to admit, I don't exactly understand the details yet myself)Imagine the situation where you have a collection for which you want to apply a function (or a series of functions) where each function might return null. When you actually use null you code will be riddled with null checks. But if you use Options instead of values, you can just flatmap the values with the desired functions, chaining the functions in the case of multiple functions and get a collection with just the results that aren't null, which in many cases is exactly what you want. Since that description is rather convoluted the shorter advice \"just flatmap that shit\" established itself.The story I heard was that two preeminent Scala programmers were pairing when one of them started writing some code like this:At which point the other said \"What is this? Amateur hour? Flat map that shit!\"As to what's so powerful about , well... First, it's the fundamental monadic operator. That means it is a common operation shared by, for example, containers (such as , collections, etc), continuations, state, etc. Second, while you can de-construct an , that, as opposed to , is not a monadic operation, so it cannot be as widely applied. Also, it requires too much knowledge about the data you are manipulating.The crucial thing about  is that it's Scala's representation of the monadic bind operation. There are numerous tutorials on the web explaining the purpose of monads and why exactly they're so useful; James Iry has one at  which goes into some detail.Runar Bjarnason is the person you're looking for for the origin.Realising why it's so powerful is something that can only come with time to be honest. The Option class is the best place to start for seeing how you would repeatedly flatMap a series of lookups (for example) into a final result."},
{"body": "When a case class has many fields and their names are long, it is often a good idea to write each field in each line like:This resembles C/C++  definition and totally readable even when the case class becomes bigger. But IntelliJ IDEA's default Scala plugin automatically changes its indentation:which looks weird to me, but the  doesn't mention anything about case class indentation.I couldn't find anything in the IDE settings that can change this behaviour. Is there an option to make the auto-indentation work like the way I described above or disable auto-indentation for case classes?Try File -> Settings... -> Code Style -> ScalaThere are lots of settings to customize your code formatting in there. In the \"Wrapping and Braces\" tab, under \"Method declaration parameters\":This will change it to the example you provided.If you want it to use the indenting in \"Continuation indent\" under \"Tabs and Indent\" you have to have both of the option above unchecked."},
{"body": "I've heard (and I know I've seen examples too, if only I can remember where) that  can obtain dependencies from a git repo.I am looking to obtain the dependency  from github. The repository does not provide any artifact JAR files, only a source tree which is set up to be built using . The process that I am imagining is that  will download the source repo, build it, and then use that as the dependency artifact.I may be imagining that  can in fact do something like this. Can it? And if so, how?Yes indeed. You can give your  a dependency with the  operator, and you can reference a Github project by its URI, for example . Alternatively, you can  the project, and then reference your local copy with . See  on the SBT wiki for details and examples.You can import unpackaged dependencies into your project from GitHub by treating them as project dependencies, using the  operator. (This is distinct from the way that precompiled library dependencies are included).Note that you can specify which branch to pull using  notation. Here's some Scala SBT code that is working well for me:Note that if you have multiple SBT projects dependending on the same external project, it's worth setting up a central  to avoid unnecessary recompilations (see ).Since I had problems getting the dependencies of my library resolved (using the suggested ) I'd like to point out to the object called . Thus, if one need to depend on a library residing in git, I suggest to do so as follows:"},
{"body": "I need to get the first date (as ) of a month and the last one. Getting the first is trivial, but getting the last seems to need some logic as months have different length and February length even varies over years. Is there a mechanism for this already built in to JodaTime or should I implement it myself?How about: returns a  which represents the \"day of month\" field in a way which knows the originating .As it happens, the  method is even  to recommend it for this particular task:Another simple method is this:An old question, but a top google result when I was looking for this.If someone needs the actual last day as an  instead using JodaTime you can do this:Using JodaTime,we can do this : As describe here in my small gist on github :  "},
{"body": "I'm new to Scala ,just started learning it today.I would like to know how to initialize an array in scala.What is the equivalent of the above code in Scala ? To initialize an array filled with zeros, you can use:This is equivalent to Java's .Can also do more dynamic inits with fill, e.g.==>Additional to Vasil's answer: If you have the values given as a Scala collection, you can writeBut usually the toArray method is more handy:If you know Array's length but you don't know its content, you can use If you want to have two dimensions array but you don't know its content, you can useOf course, you can change String to other type. If you already know its content, you can useAnother way of declaring multi-dimentional arrays:"},
{"body": "I am reading  and the concept of  makes perfect sense and easy to understand. But have two questions about :is easy to read but do we really need to write like that? I tried to implement it only with Future and without Promise like this:What is the difference between this and the given example and what makes a Promise necessary?The Promise and Future are complementary concepts. The Future is a value which will be retrieved, well, sometime in the future and you can do stuff with it when that event happens. It is, therefore, the read or out endpoint of a computation - it is something that you retrieve a value from.A Promise is, by analogy, the writing side of the computation. You create a promise which is the place where you'll put the result of the computation and from that promise you get a future that will be used to read the result that was put into the promise. When you'll complete a Promise, either by failure or success, you will trigger all the behavior which was attached to the associated Future.Regarding your first question, how can it be that for a promise p we have . You can imagine this like a single-item buffer - a container which is initially empty and you can afterwords store one value which will become its content forever. Now, depending on your point of view this is both a Promise and a Future. It is promise for someone who intends to write the value in the buffer. It is a future for someone who waits for that value to be put in the buffer.Specifically, for the Scala concurrent API, if you take a look at the Promise trait in  you can see how the methods from the Promise companion object are implemented :Now, those implementation of promises, DefaultPromise and KeptPromise can be found . They both extend a base little trait which happens to have the same name, but it is located in a different package:So you can see what they mean by . is the buffer I was referring above, while  is a buffer with the value put in from its very creation.Regarding your example, the future block you use there actually creates a promise behind the scenes. Let's look at the definition of  in  :By following the chain of methods you end up in the :So, as you can see, the result you get from your producer block gets poured into a promise.:Regarding the real-world use: Most of the time you won't deal with promises directly. If you'll use a library which performs asynchronous computation then you'll just work with the futures returned by the library's methods. Promises are, in this case, created by the library - you're just working with the reading end of what those methods do.But if you need to implement your own asynchronous API you'll have to start working with them.\nSuppose you need to implement an async HTTP client on top of, lets say, Netty. Then your code will look somewhat like this"},
{"body": "I'm doing matching against some case classes and would like to handle two of the cases in the same way. Something like this:But when I do this I get the error:I can get it working of I remove the parameters from the definition of B and C but how can I match with the params?Looks like you don't care about the values of the String parameters, and want to treat B and C the same, so:If you must, must, must extract the parameter and treat them in the same code block, you could:Though I feel it would be much cleaner to factor that out into a method:There are a couple of ways that I can see to achieve what you are after, if you have some commonality between case classes. The first is to have the case classes extend a trait which declares the commonality, the second is to use a structural type which removes the need to extend your case classes.The structural type method generates a warning about erasure which, at present I'm not sure how to eliminate. Well, it doesn't really make sense, does it? B and C are mutually exclusive, so either sb or sc get bound, but you don't know which, so you'd need further selection logic to decide which to use (given that they were bound to a Option[String], not a String). So there's nothing gained over this:Or this:"},
{"body": "I'm adding the Joda Time repository to SBT withThen I merrily use it like this:But, when I compile the project in SBT, I get a nasty:I tried the 2.0 version of joda-time, but get the same error.Add this dependency: It's an optional dependency of joda-time.\nI had to add it in my own project for the scala compiler to accept working with the joda-time jar.Your issue seems to be the same.Version is as at time of editing, latest versions can be found  I was running into a similar issue: Explicitly adding a dependency  resolved the issue."},
{"body": "Take the following function:This pattern matches nicely:What I would like to be able to do is the following:This gives off the following error: I guess this is because it thinks that target is actually a name you'd like to assign to whatever the input is. Two questions:What you're looking for is a . In Scala, these must either start with an uppercase letter, or be surrounded by backticks.Both of these would be solutions to your problem:To avoid accidentally referring to variables that already existed in the enclosing scope, I think it makes sense that the default behaviour is for lowercase patterns to be variables and not stable identifiers. Only when you see something beginning with upper case, or in back ticks, do you need to be aware that it comes from the surrounding scope."},
{"body": "I try to understand some Slick works and what it requires.Here it an example:Could somebody explain me what's the purpose of  method here, what is , why ? and what is Projection - method ' returns the instance of ? - Since no one else has answered, this might help to get you started. I don't know Slick very well.From the :In other words, slick needs to know how to deal with a row returned from the database. The method you defined uses their parser combinator functions to combine your column definitions into something that can be used on a row. "},
{"body": "I've been programming in Scala for a while and I like it but one thing I'm annoyed by is the time it takes to compile programs.  It's seems like a small thing but with Java I could make small changes to my program, click the run button in netbeans, and BOOM, it's running, and over time compiling in scala seems to consume a lot of time.  I hear that with many large projects a scripting language becomes very important because of the time compiling takes, a need that I didn't see arising when I was using Java.But I'm coming from Java which as I understand it, is faster than any other compiled language, and is fast because of the reasons I switched to Scala(It's a very simple language).  So I wanted to ask, can I make Scala compile faster and will scalac ever be as fast as javac.The Scala compiler is more sophisticated than Java's, providing type inference, implicit conversion, and a much more powerful type system.  These features don't come for free, so I wouldn't expect scalac to ever be as fast as javac.  This reflects a trade-off between the programmer doing the work and the compiler doing the work.That said, compile times have already improved noticeably going from Scala 2.7 to Scala 2.8, and I expect the improvements to continue now that the dust has settled on 2.8.  documents some of the ongoing efforts and ideas to improve the performance of the Scala compiler.There are two aspects to the (lack of) speed for the Scala compiler.You should be aware that Scala compilation takes at least an order of magnitude longer than Java to compile. The reasons for this are as follows:The best way to do Scala is with IDEA and SBT. Set up an elementary SBT project (which it'll do for you, if you like) and run it in automatic compile mode (command ) and when you save your project, SBT will recompile it.You can also use the SBT plug-in for IDEA and attach an SBT action to each of your Run Configurations. The SBT plug-in also gives you an interactive SBT console within IDEA.Either way (SBT running externally or SBT plug-in), SBT stays running and thus all the classes used in building your project get \"warmed up\" and JIT-ed and the start-up overhead is eliminated. Additionally, SBT compiles only source files that need it. It is by far the most efficient way to build Scala programs.The latest revisions of  (Eclipse) are much better atmanaging incremental compilation.See \"\" for more.The other solution is to integrate  - (as illustrated in this ) as a builder in your IDE.But not in  Eclipse though, as  mentions in the comments:Finally, as  reminds me in the comments: also include some kind of \"incremental\" compilation (through ), even though it , and enhanced incremental compilation is in the work for the upcoming 0.9 sbt version.Use  - it is a fast scala compiler that sits as a background task and does not need loading all the time. It can reuse previous compiler instance.I'm not sure if Netbeans scala plugin supports fsc (documentation says so), but I couldn't make it work. Try nightly builds of the plugin.You can use the JRebel plugin which is free for Scala. So you can kind of \"develop in the debugger\" and JRebel would always reload the changed class on the spot. I read some statement somewhere by Martin Odersky himself where he is saying that the searches for implicits (the compiler must make sure there is not more than one single implicit for the same conversion to rule out ambiguities) can keep the compiler busy. So it might be a good idea to handle implicits with care.If it doesn't have to be 100% Scala, but also something similar, you might give  a try.-- OliverI'm sure this will be down-voted, but extremely rapid turn-around is not always conducive to quality or productivity.Take time to think more carefully and execute fewer development micro-cycles. Good Scala code is denser and more essential (i.e., free from incidental details and complexity). It demands more thought and that takes time (at least at first). You can progress well with fewer code / test / debug cycles that are individually a little longer and still improve your productivity and the quality of your work.In short: Seek an optimum working pattern better suited to Scala."},
{"body": "What does Scala's @ operator do?For example, in the blog post  there is a something like thisIt enables one to bind a matched pattern to a variable. Consider the following, for instance:You can easily extract the content:But what if you wanted not the  of , but the option itself? That would be accomplished with this:Note that  can be used at  level, not just at the top level of the matching. can be used to bind a name to a successfully matched pattern, or subpattern. Patterns can be used in pattern matching, the left hand side of the  in for comprehensions, and in destructuring assigments.When pattern matching  binds  to the value matched by  if the pattern matches. In this case that means that the value of  will be  in that case-clause.Allows you to match the top-level of a pattern.  Example:It sets the value of  to the pattern which matches. In your example,  would therefore be  (as you could determine from a call to )"},
{"body": "In what cases I should use Array(Buffer) and List(Buffer). Only one difference that I know is that arrays are nonvariant and lists are covariant. But what about performance and some other characteristics? The Scala  is an immutable recursive data structure which is such a fundamental structure in Scala, that you should (probably) be using it much more than an  (which is actually  - the  of  is ).If you are coming from a Java background, then the obvious parallel is when to use  over . The former is generally used for lists which are only ever  (and whose size is not known upfront) whereas the latter should be used for lists which either have a known size (or maximum size) or for which  is important.  provides a constant-time conversion to a  which is reason alone to use  if such later conversion is required.A scala  should be implemented on the JVM by a Java array, and hence an  may be much more performant (as an ) than a  (which will box its contents, unless you are using the very latest versions of Scala which have the new  feature).However, I think that the use of s in Scala should be kept to a minimum because it feels like you really need to know what is going on under the hood to decide whether your array really will be backed by the required primitive type, or may be boxed as a wrapper type. In addition to the answers posted already, here are some specifics.While an  is literally a Java array, a  is an immutable data structure that is either  (the empty list) or consists of a pair .So unless you need rapid random access or need to count batches of elements, a  is better than an .An Array is mutable, meaning you can change the values of each index, while a List (by default) is immutable, meaning that a new list is created every time you do a modification.  In most cases it is a more \"functional\" style to work with immutable datatypes and you should probably try and use a List with constructs like , ,  and so forth.For performance characteristics, an Array is faster with random access to elements, whereas a List is faster when prepending (adding) new elements.  Iterating over them is comparable.  "},
{"body": "I am new to Scala and I could not really find a lot about the  keyword. I am trying to understand what the following expression may mean: is some kind of an alias, but what does it signify?Yes, the   is just a shorthand for Type aliases are often used to keep the rest of the code simple: you can now writewhich will be interpreted by the compiler asThis helps to avoid defining many custom types that are just tuples or functions defined on other types, for example.There are also several other interesting use cases for , as described for example in  of .Actually the  keyword in Scala can do much more than just aliasing a complicated type to a shorter name. It introduces .As you know, a class can have field members and method members. Well, Scala also allows a class to have type members.In your particular case  is, indeed, introducing an alias that allows you to write more concise code. The type system just replaces the alias with the actual type when type-checking is performed.But you can also have something like thisLike any other member of a class, type members can also be abstract (you just don't specify what their value actually is) and can be overridden in implementations.Type members can be viewed as dual of generics since much of the things you can implement with generics can be translated into abstract type members.So yes, they can be used for aliasing, but don't limit them to just this, since they are a powerful feature of Scala's type system.Please see this excellent answer for more details:Just an example to see how to use \"type\" as alias : The definition above defines Action to be an alias of the type of procedures(methodes) that take an empty parameter list and that return Unit."},
{"body": "What is a standard way of profiling Scala method calls?What I need are hooks around a method, using which I can use to start and stop Timers.In Java I use aspect programming, aspectJ, to define the methods to be profiled and inject bytecode to achieve the same.Is there a more natural way in Scala, where I can define a bunch of functions to be called before and after a function without losing any static typing in the process?Do you want to do this without changing the code that you want to measure timings for? If you don't mind changing the code, then you could do something like this:In addition to Jesper's answer, you can automatically wrap method invocations in the REPL:Now - let's wrap anything in thisOK - we need to be in power modeWrap awayI have no idea why that printed stuff out 5 timesUpdate as of 2.12.2:There are  that you can avail of. Since the URLs on the linked site are likely to change, I am pasting the relevant content below.This what I use: might be useful.I use a technique that's easy to move around in code blocks. The crux is that the same exact line starts and ends the timer - so it is really a simple copy and paste. The other nice thing is that you get to define what the timing means to you as a string, all in that same line.Example usage:The code:Pros: Cons: I like the simplicity of @wrick's answer, but also wanted:This is achieved here:For even more accuracy, a simple modification allows a JVM Hotspot warmup loop (not timed) for timing small snippets: is a nice library to perform benchmarking in ScalaBelow is a simple exampleIf you execute above code snippet in Scala Worksheet you get the running time in millisecondsWhile standing on the shoulders of giants...A solid 3rd-party library would be more ideal, but if you need something quick and std-library based, following variant provides:.Also worth noting you can use the  method to convert to the biggest time unit possible, although I am not sure how friendly this is with minor time difference between runs e.g.You can use : Usage:nanoTime will show you , so it will hard to see. So I suggest that you can use currentTimeMillis instead of it."},
{"body": "One of the new features of Scala 2.8 are context bounds. What is a context bound and where is it useful?Of course I searched first (and found for example ) but I couldn't find any really clear and detailed information.Did you find ?  It covers the new context bound feature, within the context of array improvements.  Generally, a type parameter with a  is of the form ; it is expanded to plain type parameter  together with an implicit parameter of type .  Consider the method  which forms an array from the results of applying\na given function f on a range of numbers from 0 until a given length. Up to Scala 2.7, tabulate could be\nwritten as follows:In Scala 2.8 this is no longer possible, because runtime information is necessary to create the right representation of . One needs to provide this information by passing a  into the method as an implicit parameter:As a shorthand form, a  can be used on the type parameter  instead, giving:Robert's answer covers the techinal details of Context Bounds. I'll give you my interpretation of their meaning.In Scala a View Bound () captures the concept of 'can be seen as' (whereas an upper bound  captures the concept of 'is a'). A context bound () says 'has a' about a type. You can read the examples about manifests as \" has a \". The example you linked to about  vs  illustrates the difference. A methodsays that the parameter can be seen as an . Compare withwhich says that the parameter has an associated .In terms of use, I don't think conventions are properly established (because context bounds are new).  One suggestion is that their use is preferred when you need to transfer an implicit definition from one scope to another without needing to refer to it directly (this is certainly the case for the   used to create an array).Another way of thinking about view bounds and context bounds is that the first transfers implicit conversions from the caller's scope. The second transfers implicit objects from the caller's scope.Context Bounds actually generalize View Bounds.So, given this code expressed with a View Bound:This could also be expressed with a Context Bound, with the help of a type alias representing functions from type  to type .A context bound must be used with a type constructor of kind . However the type constructor  is of kind . The use of the type alias partially applies second type parameter with the type , yielding a type constructor of the correct kind for use as a context bound.There is a proposal to allow you to directly express partially applied types in Scala, without the use of the type alias inside a trait. You could then write:As , a context bound represents a \"has-a\" constraint between a type parameter and a type class.  Put another way, it represents a constraint that an implicit value of a particular type class exists.When utilizing a context bound, one often needs to surface that implicit value. For example, given the constraint , one will often need the instance of  that satisfies the constraint.  , it's possible to access the implicit value by using the  method or a slightly more helpful  method:or "},
{"body": "One of the most powerful patterns available in Scala is the enrich-my-library* pattern, which uses implicit conversions to  to add methods to existing classes without requiring dynamic method resolution.  For example, if we wished that all strings had the method  that counted how many whitespace characters they had, we could:Unfortunately, this pattern runs into trouble when dealing with generic collections.  For example, a number of questions have been asked about .  There is nothing built in that works in one shot, so this seems an ideal candidate for the enrich-my-library pattern using a generic collection  and a generic element type :except, of course, it .  The REPL tells us:There are two problems: how do we get a  from an empty  list (or from thin air)?  And how do we get a  back from the  line instead of a ?*The key to understanding this problem is to realize that there are  in the collections library.  One is the public collections interface with all its nice methods.  The other, which is used extensively in  the collections library, but which are almost never used outside of it, is the builders.Our problem in enriching is exactly the same one that the collections library itself faces when trying to return collections of the same type.  That is, we want to build collections, but when working generically, we don't have a way to refer to \"the same type that the collection already is\".  So we need .Now the question is: where do we get our builders from?  The obvious place is from the collection itself.  .  We already decided, in moving to a generic collection, that we were going to forget the type of the collection.  So even though the collection could return a builder that would generate more collections of the type we want, it wouldn't know what the type was.Instead, we get our builders from  implicits that are floating around.  These exist specifically for the purpose of matching input and output types and giving you an appropriately typed builder.So, we have two conceptual leaps to make:Let's look at an example.Let's take this apart.  First, in order to build the collection-of-collections, we know we'll need to build two types of collections:  for each group, and  that gathers all the groups together.  Thus, we need two builders, one that takes s and builds s, and one that takes s and builds s.  Looking at the type signature of , we seewhich means that CanBuildFrom wants to know the type of collection we're starting with--in our case, it's , and then the elements of the generated collection and the type of that collection.  So we fill those in as implicit parameters  and .Having realized this, that's most of the work.  We can use our s to give us builders (all you need to do is apply them).  And one builder can build up a collection with , convert it to the collection it is supposed to ultimately be with , and empty itself and be ready to start again with .  The builders start off empty, which solves our first compile error, and since we're using builders instead of recursion, the second error also goes away.One last little detail--other than the algorithm  that actually does the work--is in the implicit conversion.  Note that we use  not .  This is because the class declaration was for  with one parameter, which it fills it itself with the  passed to it.  So we just hand it the type , and let it create  out of it.  Minor detail, but you'll get compile-time errors if you try another way.Here, I've made the method a little bit more generic than the \"equal elements\" collection--rather, the method cuts the original collection apart whenever its test of sequential elements fails.Let's see our method in action:It works!The only problem is that we don't in general have these methods available for arrays, since that would require two implicit conversions in a row.  There are several ways to get around this, including writing a separate implicit conversion for arrays, casting to , and so on.Edit: My favored approach for dealing with arrays and strings and such is to make the code even  generic and then use appropriate implicit conversions to make them more specific again in such a way that arrays work also.  In this particular case: Here we've added an implicit that gives us an  from --for most collections this will just be the identity (e.g.  already is an ), but for arrays it will be a real implicit conversion.  And, consequently, we've dropped the requirement that --we've basically just made the requirement for  explicit, so we can use it explicitly at will instead of having the compiler fill it in for us.  Also, we have relaxed the restriction that our collection-of-collections is --instead, it's any , which we will fill in later to be what we want.  Because we're going to fill this in later, we've pushed it up to the class level instead of the method level.  Otherwise, it's basically the same.Now the question is how to use this.  For regular collections, we can:where now we plug in  for  and  for .  Note that we do need the explicit generic types on the call to  so it can keep straight which types correspond to what.  Thanks to the , this automatically handles arrays.But wait, what if we want to use strings?  Now we're in trouble, because you can't have a \"string of strings\".  This is where the extra abstraction helps: we can call  something that's suitable to hold strings.  Let's pick , and do the following:We need a new  to handle the building of a vector of strings (but this is really easy, since we just need to call ), and then we need to fill in all the types so that the  is typed sensibly.  Note that we already have floating around a  CanBuildFrom, so strings can be made from collections of chars.Let's try it out:As of  it's a lot easier to \"enrich\" Scala collections than it was when Rex gave his excellent answer. For simple cases it might look like this,which adds a \"same result type\" respecting  operation to all s,And for the example from the question, the solution now looks like,Sample REPL session,Again, note that the same result type principle has been observed in exactly the same way that it would have been had  been directly defined on .As of  the magic incantation is slightly changed from what it was when Miles gave his excellent answer.The following works, but is it canonical?  I hope one of the canons will correct it.  (Or rather, cannons, one of the big guns.)  If the view bound is an upper bound, you lose application to Array and String.  It doesn't seem to matter if the bound is GenTraversableLike or TraversableLike; but IsTraversableLike gives you a GenTraversableLike.There's more than one way to skin a cat with nine lives.  This version says that once my source is converted to a GenTraversableLike, as long as I can build the result from GenTraversable, just do that. I'm not interested in my old Repr.This first attempt includes an ugly conversion of Repr to GenTraversableLike."},
{"body": "How does the following compile:What is actually going on here?There are a few things going on. First, Scala allows dots and parens to be omitted from many method calls, so  is equivalent to *.Second, an \"implicit conversion\" is applied. Since  is an  and  has no  method, the compiler searches for an implicit conversion that takes an  and returns something that  a  method, with the search constrained by the scope of your method call.You have imported  into your scope. Since  is an implicit class with an  parameter, its constructor defines an implicit  conversion.  has a  method, so it satisfies all the search criteria. Therefore, the compiler rewrites your call as **. *I mean this loosely.  is actually invalid because the  method has no parameter list and therefore the parens  be omitted on the method call.**Actually, this isn't quite true because  is a value class, so the compiler will avoid wrapping the integer if possible. The \"magic\" that's going on there is called \"implicit conversion\". You are importing the implicit conversions, and some of them handle the conversion between Int (and Double) to Duration. That's what's you are dealing with."},
{"body": "I know that artificial benchmarks are evil. They can show results only for very specific narrow situation. I don't assume that one language is better than the other because of the some stupid bench. However I wonder why results is so different. Please see my questions at the bottom.Benchmark is simple math calculations to find pairs of prime numbers which differs by 6 (so called )\nE.g. sexy primes below 100 would be: \nRunning: all except Factor was running in VirtualBox (Debian unstable amd64 guest, Windows 7 x64 host)\nCPU: AMD A4-3305M[*1] - I'm afraid to imagine how much time will it takeC:Ruby:Scala:Scala opimized  (the same idea like in Clojure optimization):Clojure:Clojure optimized :PythonFactorBash(zsh):Rough answers:Most important optimisation in the Clojure code would be to use typed primitive maths within , something like:With this improvement, I get Clojure completing 10k in 0.635 secs (i.e. the second fastest on your list, beating Scala) note that you have printing code inside your benchmark in some cases - not a good idea as it will distort the results, especially if using a function like  for the first time causes initialisation of IO subsystems or something like that!I'll answer just #2, since it's the only one I've got anything remotely intelligent to say, but for your Python code, you're creating an intermediate list in , whereas you're using  in your  in Ruby which is just iterating.If you change your  to:they're on par.I could optimize the Python further, but my Ruby isn't good enough to know when I've given more of an advantage (e.g., using  makes Python win on my machine, but I don't remember if the Ruby range you used creates an entire range in memory or not). Without being too silly, making the Python code look like:which doesn't change much more, puts it at 1.5s for me, and, with being extra silly, running it with PyPy puts it at .3s for 10K, and 21s for 100K.Here's a fast Clojure version, using the same basic algorithms:It runs about 20x faster than your original on my machine. And here's a version that leverages the new reducers library in 1.5 (requires Java 7 or JSR 166):This runs about 40x faster than your original. On my machine, that's 100k in 1.5 seconds.You can make the Scala a lot faster by modifying your  method toNot quite as concise but the program runs in 40% of the time! We cut out the superfluous  and anonymous  objects, the Scala compiler recognizes the tail-recursion and turns it into a while-loop, which the JVM can turn into more or less optimal machine code, so it shouldn't be too far off the C version.See also: Here is my scala version in both parallel and no-parallel, just for fun:\n(In my dual core compute, the parallel version takes 335ms while the no-parallel version takes 655ms)EDIT: According to 's suggestion, I have changed my code to avoid the effects of IO and jvm warmup:The result shows in my compute:Never mind the benchmarks; the problem got me interested and I made some fast tweaks.  This uses the  decorator, which memoizes a function; so when we call  we basically get that prime check for free.  This change cuts the work roughly in half.  Also, we can make the  calls step through just the odd numbers, cutting the work roughly in half again.This requires Python 3.2 or newer to get , but could work with an older Python if you install a Python recipe that provides .  If you are using Python 2.x you should really use  instead of .The above took only a very short time to edit.  I decided to take it one step further, and make the primes test only try prime divisors, and only up to the square root of the number being tested.  The way I did it only works if you check numbers in order, so it can accumulate all the primes as it goes; but this problem was already checking the numbers in order so that was fine.On my laptop (nothing special; processor is a 1.5 GHz AMD Turion II \"K625\") this version produced an answer for 100K in under 8 seconds.The above code is pretty easy to write in Python, Ruby, etc. but would be more of a pain in C.You can't compare the numbers on this version against the numbers from the other versions without rewriting the others to use similar tricks.  I'm not trying to prove anything here; I just thought the problem was fun and I wanted to see what sort of easy performance improvements I could glean.Don't forget Fortran!  (Mostly joking, but I would expect similar performance to C).  The statements with exclamation points are optional, but good style. ( is a comment character in fortran 90)I couldn't resist to do a few of the most obvious optimizations for the C version which made the 100k test now take 0.3s on my machine (5 times faster than the C version in the question, both compiled with MSVC 2010 /Ox).Here is the identical implemention in Java:With Java 1.7.0_04 this runs almost exactly as fast as the C version. Client or server VM doesn't show much difference, except that JIT training seems to help the server VM a bit (~3%) while it has almost no effect with the client VM. The output in Java seems to be slower than in C. If the output is replaced with a static counter in both versions, the Java version runs a little faster than the C version.These are my times for the 100k run:and the 1M run (16386 results):While this does not really answer your questions, it shows that small tweaks can have a noteworthy impact on performance. So to be able to really compare languages you should try to avoid all algorithmic differences as much as possible.It also gives a hint why Scala seems rather fast. It runs on the Java VM and thus benefits from its impressive performance.In Scala try using Tuple2 instead of List, it should go faster. Just remove the word 'List' since (x, y) is a Tuple2.Tuple2 is specialized for Int, Long and Double meaning it won't have to box/unbox those raw datatypes. . List isn't specialized. .Here's the code for the Go (golang.org) version:It ran just as fast as the C version.\nIntel Core 2 Duo T6500 2.1GHz, 2MB L2 cache, 800MHz FSB.\n4GB RAM The 100k version:  With 1000000 (1M instead of 100K):   But I think that it would be fair to use Go's built in multithreading capabilities and compare that version with the regular C version (without multithreading), just because it's almost too easy to do multithreading with Go.Update: I did a parallel version using Goroutines in Go:The parallelized version completed in 1.706 seconds. It used less than 1.5 Mb RAM. Fixed with a call to Update: I ran the paralellized version up to 1M numbers.  With 1000000 (1M instead of 100K):    The 100k version:   Based on , I wrote a scala version using recursion, and I improved on it by only going to the sqrt instead of x/2 for the prime check function. I get ~250ms for 100k, and ~600ms for 1M. I went ahead and went to 10M in ~6s.I also went back and wrote a CoffeeScript (V8 JavaScript) version, which gets ~15ms for 100k, 250ms for 1M, and 6s for 10M, by using a counter (ignoring I/O). If I turn on the output it takes ~150ms for 100k, 1s for 1M, and 12s for 10M. Couldn't use tail recursion here, unfortunately, so I had to convert it back into loops.Just for the fun of it, here is a parallel Ruby version. On my 1.8GHz Core i5 MacBook Air, the performance results are:It looks like the JVM's JIT is giving Ruby a nice performance boost in the default case, while true multithreading helps JRuby perform 50% faster in the threaded case. What's more interesting is that JRuby 1.7 improves the JRuby 1.6 score by a healthy 17%!The answer to your question #1 is that Yes, the JVM is incredably fast and yes static typing helps.The JVM should be faster than C in the long run, possibly even faster than \"Normal\" assembly language--Of course you can always hand optimize assembly to beat anything by doing manual runtime profiling and creating a separate version for each CPU, you just have to be amazingly good and knowledgable.The reasons for Java's speed are: The JVM can analyze your code while it runs and hand-optimize it--for instance, if you had a method that could be statically analyzed at compile time to be a true function and the JVM noticed that you were often calling it with the same parameters, it COULD actually eliminate the call completely and just inject the results from the last call (I'm not sure if Java actually does this exactly, but it doest a lot of stuff like this). Due to static typing, the JVM can know a lot about your code at compile time, this lets it pre-optimize quite a bit of stuff.  It also lets the compiler optimize each class individually without knowledge of how another class is planning to use it.  Also Java doesn't have arbitrary pointers to memory location, it KNOWS what values in memory may and may not be changed and can optimize accordingly.Heap allocation is MUCH more efficient than C, Java's heap allocation is more like C's stack allocation in speed--yet more versatile.  A lot of time has gone into the different algroithims used here, it's an art--for instance, all the objects with a short lifespan (like C's stack variables) are allocated to a \"known\" free location (no searching for a free spot with enough space) and are all freed together in a single step (like a stack pop). The JVM can know quirks about your CPU architecture and generate machine code specifically for a given CPU.The JVM can speed your code long after you shipped it.  Much like moving a program to a new CPU can speed it up, moving it to a new version of the JVM can also give you huge speed performances taylored to CPUs that didn't even exist when you initially compiled your code, something c physically cannot do without a recomiple.By the way, most of the bad rep for java speed comes from the long startup time to load the JVM (Someday someone will build the JVM into the OS and this will go away!) and the fact that many developers are really bad at writing GUI code (especially threaded) which caused Java GUIs to often become unresponsive and glitchy.  Simple to use languages like Java and VB have their faults amplified by the fact that the capibilities of the average programmer tends to be lower than more complicated languages."},
{"body": "I need some code samples (and I also really curious about them) of Scala and Java code which show that Scala code is more simple and concise then code written in Java (of course both samples should solve the same problem).If there is only Scala sample with comment like \"this is abstract factory in Scala, in Java it will look much more cumbersome\" then this is also acceptable.Thanks!I like most of all accepted and  answersLet's improve  and use Scala's :The above Scala class contains all features of the below Java class,  - for example it supports  (which Java doesn't have). Scala 2.8 adds named and default arguments, which are used to generate a  for case classes, which gives the same ability as the with* methods of the following Java class.Then, in usage we have (of course):AgainstI found this one impressiveJavaScalaAs well as these ones (sorry for not pasting, I didn't want to steal the code) Write a program to index a list of keywords (like books).You have a list  of objects of class  that has fields  and . Your task is to sort this list first by , and then by .Since I wrote this answer, there has been quite some progress. The lambdas (and method references)have finally landed in Java, and they are taking the Java world by storm.This is what the above code will look like with  (contributed by @fredoverflow):While this code is almost as short, it does not work quite as elegantly as the Scala one. In Scala solution, the  method accepts a function  where  is required to  an .  is a type-class. Think best of both worlds: Like , it's implicit for the type in question, but like , it's extensible and can be added retrospectively to types that did not have it. Since Java lacks type-classes, it has to duplicate every such method, once for , then for . For example, see  and  .The type-classes allow one to write rules such as \"If A has ordering and B has ordering, then their tuple (A, B) also has ordering\". In code, that is:That is how the  in our code can compare by name and then by age. Those semantics will be encoded with the above \"rule\". A Scala programmer would intuitively expect this to work this way. No special purpose methods like  had to be added to .Lambdas and method references are just a tip of an iceberg that is functional programming. :) You have got an XML file \"company.xml\" that looks like this:You have to read this file and print the  and  fields of all the employees.\n\n [ taken from  ]\n [ taken from , slide #19 ] Hmm, how to do it without replying in an unformatted reply section... Hmph.  I guess I'll edit your answer and let you delete it if it bugs you.This is how I would do it in Java with better libraries:This is just a quick hack involving no magic and all reusable components.  If I wanted to add some magic I could do something better than returning an array of string arrays, but even as is this GoodXMLLib would be completely reusable.  The first parameter of scanFor is the section, all future parameters would be the items to find which is limited, but the interface could be buffed slightly to add multiple levels of matching with no real problem.I will admit that Java has some pretty poor library support in general, but come on--to compare a horrible usage of Java's decade(?) old XML library to an implementation done based on being terse is just not fair--and is far from a comparison of the languages!A map of actions to perform depending on a string.Java 7:Scala:And it's all done in the best possible taste!Java 8:I liked this simple example of sorting and transformation, taken from David Pollak's 'Beginning Scala' book:In Scala:In Java:I'm writing a Blackjack game in Scala now. Here is how my dealerWins method would look in Java:Here's how it looks in Scala:Hooray for higher-order functions!Java 8 solution:I liked   so much I'm going to try to improve upon it. The code below is  a direct translation of the Java example, but it accomplishes the same task with the same API.I like much the method getOrElseUpdate, found in mutableMap and shown here, first Java, without: yes - a WordCount, and here in scala: And here it is in Java 8:And if you want to go 100% functional: and  have already been shown, but look how easy they are integrated with the map:This is a very simple example: Square integers and then add themIn scala:Compact map applies the function to all elements of the array, so:Fold left is will start with 0 as the accumulator (s) and apply  to all the elements (i) of the array, so that:Now this can be further compacted to:This one I will not try in Java (to much work), turn XML to a Map: Another one liner to get the map from the XML:How about Quicksort?The following is a java example found via a google search, the URL is A quick attempt at a Scala version. Open season for code improvers ;@) you need to design a method that will execute any given code asynchronously. \nSolution in :The same thing in  (using actors):The Circuit Breaker pattern from  in  \n() implementation looks like this in Scala: Which I think is super nice. It looks just as a pice of the language but it is a simple mixin in the  doing all work. Reference in other languages Google for \"Circuit breaker\" + your language. Why nobody posted this before:Java:116 characters.Scala:56 characters.I am preparing a document that gives several examples of Java and Scala code, utilising only the simple to understand features of Scala:If you would like me to add something to it, please reply in the comments.Lazily evaluated infinite streams are a good example:Here is a question addressing infinite streams in Java: Another good example are first class functions and closures:Java doesn't support first class functions, and mimicking closures with anonymous inner classes isn't very elegant.  Another thing this example shows that java can't do is running code from an interpreter/REPL. I find this immensely useful for quickly testing code snippets.This Scala code......would be completely unreadable in Java, if possible at all."},
{"body": "What is the best way to do an inverse sort in scala?  I imagine the following is somewhat slow.Is there a conveinient way of using sortBy but getting a reverse sort?  I would rather not need to use .There may be the obvious way of changing the sign, if you sort by some numeric valueMore generally, sorting may be done by method sorted with an implicit Ordering, which you may make explicit, and Ordering has a reverse (not the list reverse below)\nYou can do If the ordering you want to reverse is the implicit ordering, you can get it by implicitly[Ordering[A]] (A the type you're ordering on) or better Ordering[A]. That would besortBy is like using Ordering.by, so you can doMaybe not the shortest to write (compared to minus) but intent is clearThe last line does not work. To accept the  in , the compiler needs to know on which type we are ordering, so that it may type the . It may seems that would be the type of the element of the list, but this is not so, as the signature of sorted is\n. The ordering may be on , but also on any ancestor of  (you might use ). And indeed, the fact that list is covariant forces this signature. \nOne can dobut this is much less pleasant.maybe to shorten it a little more:Easy peasy (at least in case of ): has implicit parameter  which provides orderingso, we can define own  objectBoth  and  have a compact syntax:I find the one with  easier to understand.Another possibility in cases where you pass a function that you may not be able to modify directly to an Arraybuffer via sortWith for example:this is my code ;)val wordCounts = logData.flatMap(line => line.split(\" \")).map(word => (word, 1)).reduceByKey((a, b) => a + b)wordCounts.sortBy(- _._2).collect()"},
{"body": "There are path dependent types and I think it is possible to express almost all the features of such languages as Epigram or Agda in Scala, but I'm wondering why Scala does not support  more explicitly like it does very nicely in other areas (say, DSLs) ?\nAnything I'm missing like \"it is not necessary\" ?Syntactic convenience aside, the combination of singleton types, path-dependent types and implicit values means that Scala has surprisingly good support for dependent typing, as I've tried to demonstrate in .Scala's intrinsic support for dependent types is via . These allow a type to depend on a selector path through an object- (ie. value-) graph like so,In my view, the above should be enough to answer the question \"Is Scala a dependently typed language?\" in the positive: it's clear that here we have types which are distinguished by the values which are their prefixes.However, it's often objected that Scala isn't a \"fully\" dependently type language because it doesn't have  as found in Agda or Coq or Idris as intrinsics. I think this reflects a fixation on form over fundamentals to some extent, nevertheless, I'll try and show that Scala is a lot closer to these other languages than is typically acknowledged.Despite the terminology, dependent sum types (also known as Sigma types) are simply a pair of values where the type of the second value is dependent on the first value. This is directly representable in Scala,and in fact, this is a crucial part of the  in Scala prior to 2.10 (or earlier via the experimental -Ydependent-method types Scala compiler option).Dependent product types (aka Pi types) are essentially functions from values to types. They are key to the representation of  and the other poster children for dependently typed programming languages. We can encode Pi types in Scala using a combination of path dependent types, singleton types and implicit parameters. First we define a trait which is going to represent a function from a value of type T to a type U,We can than define a polymorphic method which uses this type,(note the use of the path-dependent type  in the result type ). Given a value of type T, this function will return a(n empty) list of values of the type corresponding to that particular T value.Now let's define some suitable values and implicit witnesses for the functional relationships we want to hold,And now here is our Pi-type-using function in action,(note that here we use Scala's  subtype-witnessing operator rather than  because  and  are singleton types and hence more precise than the types we are verifying on the RHS).In practice, however, in Scala we wouldn't start by encoding Sigma and Pi types and then proceeding from there as we would in Agda or Idris. Instead we would use path-dependent types, singleton types and implicits directly. You can find numerous examples of how this plays out in shapeless: , , , ,  etc. etc.The only remaining objection I can see is that in the above encoding of Pi types we require the singleton types of the depended-on values to be expressible. Unfortunately in Scala this is only possible for values of reference types and not for values of non-reference types (esp. eg. Int). This is a shame, but not an intrinsic difficulty: Scala's type checker represents the singleton types of non-reference values internally, and there have been a  of  in making them directly expressible. In practice we can work around the problem with a .In any case, I don't think this slight domain restriction can be used as an objection to Scala's status as a dependently typed language. If it is, then the same could be said for Dependent ML (which only allows dependencies on natural number values) which would be a bizarre conclusion.I would assume it is because (as I know from experience, having used dependent types in the Coq proof assistant, which fully supports them but still not in a very convenient way) dependent types are a very advanced programming language feature which is really hard to get right - and can cause an exponential blowup in complexity in practice. They're still a topic of computer science research.I believe that Scala's path-dependent types can only represent \u03a3-types, but not \u03a0-types. This:is not exactly a \u03a0-type. By definition, \u03a0-type, or dependent product, is a function which result type depends on argument value, representing universal quantifier, i.e. \u2200x: A, B(x). In the case above, however, it depends only on type T, but not on some value of this type. Pi trait itself is a \u03a3-type, an existential quantifier, i.e. \u2203x: A, B(x). Object's self-reference in this case is acting as quantified variable. When passed in as implicit parameter, however, it reduces to an ordinary type function, since it is resolved type-wise. Encoding for dependent product in Scala may look like the following:The missing piece here is an ability to statically constraint field x to expected value t, effectively forming an equation representing the property of all values inhabiting type T. Together with our \u03a3-types, used to express the existence of object with given property, the logic is formed, in which our equation is a theorem to be proven.On a side note, in real case theorem may be highly nontrivial, up to the point where it cannot be automatically derived from code or solved without significant amount of effort. One can even formulate Riemann Hypothesis this way, only to find the signature impossible to implement without actually proving it, looping forever or throwing an exception."},
{"body": "I have been trying:vs:and they seem to behave the same although I couldn't find anywhere saying that  expands to  so my question is, are these\nidentical/similar?On a side note, I have been trying to use  on these code pieces and they\nproduce the same code except for an extra line in the second one. How do I\nread that extra line?This is barely a constructor parameter. If this variable is not used anywhere except the constructor, it remains there. No field is generated. Otherwise  field is created and value of  parameter is assigned to it. No getter is created.Such declaration of parameter will create  field with private getter. This behavior is the same as above no matter if the parameter was used beside the constructor (e.g. in  or not).Same as above but Scala-like getter is public in case classesWhen case classes are involved, by default each parameter has  modifier.In the first case,  is only a constructor parameter. Since the main constructor is the content of the class itself, it is accessible in it, but only from this very instance. So it is almost equivalent to:On the other hand, in the second case  is a  private field, so it is accessible to this instance  other instances of .\nFor example, this compiles fine:And runs:But this doesn't:"},
{"body": "I would like to be able to find a match between the first letter of a word, and one of the letters in a group such as \"ABC\". In pseudocode, this might look something like:But how do I grab the first letter in Scala instead of Java? How do I express the regular expression properly? Is it possible to do this within a ?You can do this because regular expressions define extractors but you need to define the regex pattern first.  I don't have access to a Scala REPL to test this but something like this should work.Since version 2.10, one can use Scala's string interpolation feature:Even better one can bind regular expression groups:It is also possible to set more detailed binding mechanisms:An impressive example on what's possible with  is shown in the blog post :As delnan pointed out, the  keyword in Scala has nothing to do with regexes. To find out whether a string matches a regex, you can use the  method. To find out whether a string starts with an a, b or c in lower or upper case, the regex would look like this:You can read this regex as \"one of the characters a, b, c, A, B or C followed by anything\" ( means \"any character\" and  means \"zero or more times\", so \".*\" is any string).To expand a little on : The fact that regular expressions define extractors can be used to decompose the substrings matched by the regex very nicely using Scala's pattern matching, e.g.:Note that the approach from @AndrewMyers's answer matches the  string to the regular expression, with the effect of anchoring the regular expression at both ends of the string using  and . Example:And with no  at the end:String.matches is the way to do pattern matching in the regex sense.But as a handy aside, word.firstLetter in real Scala code looks like:Scala treats Strings as a sequence of Char's, so if for some reason you wanted to explicitly get the first character of the String and match it, you could use something like this:I'm not proposing this as the general way to do regex pattern matching, but it's in line with your proposed approach to first find the first character of a String and then match it against a regex.EDIT:\nTo be clear, the way I would do this is, as others have said:Just wanted to show an example as close as possible to your initial pseudocode. Cheers!First we should know that regular expression can separately be used. Here is an example:Second we should notice that combining regular expression with pattern matching would be very powerful. Here is a simple example.In fact, regular expression itself is already very powerful; the only thing we need to do is to make it more powerful by Scala. Here are more examples in Scala Document:  "},
{"body": "I am diving into Scala and noticed sbt. I have been quite happy with Gradle in java/groovy projects, and I know there's a scala plugin for Gradle.What could be good reasons to favour sbt over Gradle in a Scala project?Note that one key difference between SBT and Gradle is its : mentions that the all situation could evolve in the future:(both tools can )For me the key features of SBT are:The downsides are:sbt is a Scala DSL and for it Scala is a first class citizen, so in principal it seems to be a good fit.But sbt suffers from major incompatible changes between versions, which makes it hard to find the correct working plugin for a task and get it to work.I personally gave up on sbt, since it was causing more problems than it solved. I actually switched to gradle.Go figure.I'm fairly new to gradle, and very new to sbt - what I really like about sbt so far is the interactive console. It allows me to use commands like 'inspect' to get a better idea of what's going on. AFAIK gradle does not provide something like this atm.Sbt and gradle, both are based on statically typed languages....but sbt has few advantages:"},
{"body": "If I have an  and a corresponding  I can run them together:If the enumerator monad is \"bigger\" than the iteratee monad, I can use  or, more generally,  to \"lift\" the iteratee to match:But what do I do when the iteratee monad is \"bigger\" than the enumerator monad?There doesn't seem to be a  instance for , nor any obvious \"lift\" method."},
{"body": "All roads lead to Rome, and that is the same for SBT: To get started with  there is , , , etc, and then there are different ways to include and decide on repositories. I'm asking because sometimes I get a little lost. The SBT documentation is very thorough and complete, but I find myself not knowing when to use  or  or  or .Then it becomes fun, there is the  and  -  What comes first, the chicken or the egg? Do I just pull out a machette and start hacking my way forward? I quite often find projects that include everything and the kitchen sink, and then I realize - I'm not the only one who gets a little lost.As a simple example, right now, I'm starting a brand new project. I want to use the latest features of  and  and this will probably require the latest version of SBT.  In what file should I define it and how should it look? I know I can get this working, but I would really like an expert opinion on where everything should go (why it should go there will be a bonus).I've been using  for small projects for well over a year now. I used  and then  (as it made some headaches magically disappear), but I'm not sure why I should be using the one or the other. I'm just getting a little frustrated for not understanding how things fit together ( and repositories), and think it will save the next guy coming this way a lot of hardship if this could be explained in human terms.For Scala-based dependencies, I would go with what the authors recommend. For instance:  indicates to use:Or  has instructions on where to add:For Java-based dependencies, I use  to see what's out there, then click on the SBT tab. For instance  indicates to use:Then pull out the machette and start hacking your way forward. If you are lucky you don't end up using jars that depends on some of the same jars but with incompatible versions. Given the Java ecosystem, you often end up including everything and the kitchen sink and it takes some effort to eliminate dependencies or ensure you are not missing required dependencies. I think the sane point is to . Make sure you understand:Keep those 4 pages open at all times so that you can jump and look up various definitions and examples:Make maximum use of  and   to get familiar with actual values of settings, their dependencies, definitions and related settings. I don't believe the relationships you'll discover using  are documented anywhere. If there is a better way I want to know about it.The way I use sbt is:I don't bother checking in the IDE project files since they are generated by sbt, but there may be reasons you want to do that.You can see an example set up like this .Use Typesafe Activator, a fancy way of calling sbt, which comes with project templates and seeds : "},
{"body": "I started to learn Scala and almost in every tutorial I see a  file which describes project settings. But now I have installed  and created a project from template. And generated project from template missed  file, but it have  (which seems used for same purposes, but it is more flexible).   So what is the difference between  and ?\nWhich is more preferred and why?To give a brief example, this :is a shorthand notation roughly equivalent to this :The  file can also include s, s, and s (but not s and es).See , particularly the section \"Relating build.sbt to Build.scala\".Consider a  build definition if you're doing something complicated where you want the full expressiveness of Scala.Update July 2016 (3 years later) is officially deprecated in   implements that deprecation.\n\"\" has been updated.When s are being compiled, they are before that sort of merged with the  files inside  directory. They can't be used in recursive tasks, that is, you can't customize  from , for example. For more detailed information, consider reading related section is sbt documentation: "},
{"body": "While there might be valid cases where such method overloadings could become ambiguous, why does the compiler disallow code which is neither ambiguous at compile time nor at run time?Example:Are there any reasons why these restrictions can't be loosened a bit?Especially when converting heavily overloaded Java code to Scala default arguments are a very important and it isn't nice to find out after replacing plenty of Java methods by one Scala methods that the spec/compiler imposes arbitrary restrictions.I'd like to cite Lukas Rytz (from ):A solution for future Scala version could be to incorporate  of the non-default arguments (those at the beginning of a method, which disambiguate overloaded versions) into the naming schema, e.g. in this case:it would be something like:Someone willing to ?It would be very hard to get a readable and precise spec for the interactions of overloading resolution with default arguments. Of course, for many individual cases, like the one presented here, it's easy to say what should happen. But that is not enough. We'd need a spec that decides all possible corner cases. Overloading resolution is already very hard to specify. Adding default arguments in the mix would make it harder still. That's why we have opted to separate the two. I can't answer your question, but here is a workaround:If you have two very long arg lists which differ in only one arg, it might be worth the trouble...One of the possible scenario is The compiler will be confused about which one to call. In prevention of other possible dangers, the compiler would allow at most one overloaded method has default arguments.Just my guess:-)My understanding is that there can be name collisions in the compiled classes with default argument values. I've seen something along these lines mentioned in several threads.The named argument spec is here:\nIt states:So, for the time being at any rate, it's not going to work.You could do something like what you might do in Java, eg:If you called  which one should it invoke?"},
{"body": "I saw this quote on the question: Is this true?  If so, what is it about the JVM that creates this fundamental limitation?This post:  might help.In short, tail call optimization is hard to do in the JVM because of the security model and the need to always have a stack trace available. These requirements could in theory be supported, but it would probably require a new bytecode (see ).There is also more discussion in , where the evaluation (from 2002) ends:Currently, there is some work going on in the  project. The tail call subproject's status is listed as \"proto 80%\"; it is unlikely to make it into Java 7, but I think it has a very good chance at Java 8.The fundamental limitation is simply that the JVM does not provide tail calls in its byte code and, consequently, there is no direct way for a language built upon the JVM to provide tail calls itself. There are workarounds that can achieve a similar effect (e.g. trampolining) but they come at the grave cost of awful performance and obfuscating the generated intermediate code which makes a debugger useless.So the JVM cannot support any production-quality functional programming languages until Sun implement tail calls in the JVM itself. They have been discussing it for years but I doubt they will ever implement tail calls: it will be very difficult because they have prematurely optimized their VM before implementing such basic functionality, and Sun's effort is strongly focused on dynamic languages rather than functional languages.Hence there is a very strong argument that Scala is not a real functional programming language: these languages have regarded tail calls as an essential feature since Scheme was first introduced over 30 years ago.Scala 2.7.x supports tail-call optimization for self-recursion (a function calling itself) of final methods and local functions.Scala 2.8 might come with library support for trampoline too, which is a technique to optimize mutually recursive functions.A good deal of information about the state of Scala recursion can be found in .In addition to the paper linked in Lambda The Ultimate (from the link mmyers posted above), John Rose from Sun has some more to say about tail call optimization.I have heard that it might be implemented on the JVM someday. Tail call support amongst other things are being looked at on the Da Vinci Machine.All sources point to the JVM being unable to optimize in the case of tail recursion, but upon reading  (2003, O'reilly) I found the author claiming he can achieve greater recursion performance by implementing tail recursion.You can find his claim on page 212 (search for 'tail recursion' it should be the second result). What gives?"},
{"body": "If you're writing code that's using lots of beautiful, immutable data structures, case classes appear to be a godsend, giving you all of the following for free with just one keyword:But what are the disadvantages of defining an immutable data structure as a case class?What restrictions does it place on the class or its clients?Are there situations where you should prefer a non-case class?One big disadvantage: a case classes can't extend a case class. That's the restriction.Other advantages you missed, listed for completeness: compliant serialization/deserialization, no need to use \"new\" keyword to create.I prefer non-case classes for objects with mutable state, private state, or no state (e.g. most singleton components).  Case classes for pretty much everything else.First the good bits: This list should also include the uber-powerful copy method, one of the best things to come to Scala 2.8Then the bad, there are only a handful of real restrictions with case classes:In practice though, this is rarely a problem.  Changing behaviour of the generated apply method is guaranteed to surprise users and should be strongly discouraged, the only justification for doing so is to validate input parameters - a task best done in the main constructor body (which also makes the validation available when using )True, though it's still possible for a case class to itself be a descendant.  One common pattern is to build up a class hierarchy of traits, using case classes as the leaf nodes of the tree.It's also worth noting the  modifier.  Any subclass of a trait with this modifier  be declared in the same file.  When pattern-matching against instances of the trait, the compiler can then warn you if you haven't checked for all possible concrete subclasses.  When combined with case classes this can offer you a very high level level of confidence in your code if it compiles without warning.No real workaround, except to stop abusing classes with this many params :)One other restriction sometimes noted is that Scala doesn't (currently) support lazy params (like s, but as parameters).  The workaround to this is to use a by-name param and assign it to a lazy val in the constructor.  Unfortunately, by-name params don't mix with pattern matching, which prevents the technique being used with case classes as it breaks the compiler-generated extractor.This is relevant if you want to implement highly-functional lazy data structures, and will hopefully be resolved with the addition of lazy params to a future release of Scala.I think the TDD principle apply here: do not over-design. When you declare something to be a , you are declaring a lot of functionality. That will decrease the flexibility you have in changing the class in the future.For example, a  has an  method over the constructor parameters. You may not care about that when you first write your class, but, latter, may decide you want equality to ignore some of these parameters, or do something a bit different. However, client code may be written in the mean time that depends on  equality.Martin Odersky gives us a good starting point in his course  (Lecture 4.6 - Pattern Matching) that we could use when we must choose between class and case class.\nThe chapter 7 of  contains the same example.Furthermore, adding a new Prod class does not entail any changes to existing code:In contrast, add a new method requires modification of all existing classes.The same problem solved with case classes.Adding a new method is a local change.Adding a new Prod class requires potentially change all pattern matching.Transcript from the videolecture Remember: we must use this like a starting point and not like the only criteria."},
{"body": "Given three ways of expressing the same function :How do these definitions differ?  The REPL does not indicate any obvious differences: is a function that takes an integer and returns an integer. is a method with zero arity that returns a function that takes an integer and returns an integer. (When you type  at REPL later, it becomes a call to the method .) is same as . You're just not employing type inference there.Inside a class,  is evaluated on initialization while  is evaluated only when, , the function is called. In the code below you will see that x is evaluated the first time the object is used, but not again when the x member is accessed. In contrast, y is not evaluated when the object is instantiated, but is evaluated every time the member is accessed. Executing a definition such as  x = e will not evaluate the expression . Instead  is evaluated whenever  is used. Alternatively, Scala offers a value definition\n x = e, which does evaluate the right-hand-side  as part of the evaluation\nof the definition. If  is then used subsequently, it is immediately replaced by the\npre-computed value of , so that the expression need not be evaluated again."},
{"body": "In Java, reading environment variables is done with .Is there a way to do this in Scala?Since Scala 2.9 you can use  for the same effect:I think is nice to use the Scala API instead of Java. There are currently several project to compile Scala to other platforms than JVM (.NET, javascript, native, etc.) Reducing the dependencies on Java API, will make your code more portable.There is an object:this has a collection of methods that can be used to get environment info, includingSame way:If Lightbend's configuration library is used (by default in Play2 and Akka) then you can use syntax to override foo if an environment variable VAR_NAME exist. \nMore details in    To print  environment variables, you can use"},
{"body": "I have some big (more than 3 fields) Objects which can and should be immutable. Every time I run into that case i tend to create constructor abominations with long parameter lists. It doesn't feel right, is hard to use and readability suffers.It is even worse if the fields are some sort of collection type like lists. A simple  would ease the object creation so much but renders the object mutable.What do you guys use in such cases? I'm on Scala and Java, but i think the problem is language agnostic as long as the language is object oriented.Solutions I can think of:Thanks for your input!Well, you want both an easier to read and immutable object once created?I think a fluent interface  would help you.It would look like this (purely made up example):I wrote  in bold because most Java programmers get fluent interfaces wrong and pollute their object with the method necessary to build the object, which is of course completely wrong.The trick is that  (hence you Foo can be immutable).,  and  all create \"something else\".That something else may be a FooFactory, here's one way to do it....You FooFactory would look like this:In Scala 2.8, you could use named and default parameters as well as the  method on a case class. Here's some example code:Well, consider this on Scala 2.8:This does have its share of problems, of course. For instance, try making  and , and then getting two persons married to each other. I can't think of a way to solve that without resorting to either a  and/or a  constructor plus a factory.Here are a couple of more options:Make the implementation itself mutable, but separate the interfaces that it exposes to mutable and immutable. This is taken from the Swing library design.If your application contains a large but pre-defined set of immutable objects (e.g., configuration objects), you might consider using the  framework.You could also make the immutable objects expose methods that look like mutators (like addSibling) but let them return a new instance. That's what the immutable Scala collections do.The downside is that you might create more instances than necessary. It's also only applicable when there exist intermediate valid configurations (like some node without siblings which is ok in most cases) unless you don't want to deal with partially built objects.For example a graph edge which has no destination yet isn't a valid graph edge.Consider four possibilities:To me, each of 2, 3, and 4 is adapted to a difference situation. The first one is hard to love, for the reasons cited by the OP, and is generally a symptom of a design that has suffered some creep and needs some refactoring.What I'm listing as (2) is good when there is no state behind the 'factory', whereas (3) is the design of choice when there is state. I find myself using (2) rather than (3) when I don't want to worry about threads and synchronization, and I don't need to worry about amortizing some expensive setup over the production of many objects. (3), on the other hand, is called forth when real work goes into the construction of the factory (setting up from an SPI, reading configuration files, etc).Finally, someone else's answer mentioned option (4), where you have lots of little immutable objects and the preferable pattern is to get news ones from old ones.Note that I'm not a member of the 'pattern fan club' -- sure, some things are worth emulating, but it seems to me that they take on an unhelpful life of their own once people give them names and funny hats.Another potential option is to refactor to have fewer configurable fields.  If groups of fields only work (mostly) with each other, gather them up into their own small immutable object.  That \"small\" object's constructors/builders should be more manageable, as will the constructor/builder for this \"big\" object.It helps to remember there are . For your case, I think \"popsicle\" immutability will work really well:So you initialize your object, then set a \"freeze\" flag of some sort indicating that its no longer writable. Preferably, you'd hide the mutation behind a function so the function is still pure to clients consuming your API.I use C#, and these are my approaches. Consider: Constructor with optional parametersUsed as e.g. . Not for Java or C# versions before 4.0. but still worth showing, as it's an example how not all solutions are language agnostic. Constructor taking a single parameter objectUsage example: C# from 3.0 on makes this more elegant with object initializer syntax (semantically equivalent to the previous example):\nRedesign your class not to need such a huge number of parameters. You could split its repsonsibilities into multiple classes. Or pass parameters not to the constructor but only to specific methods, on demand. Not always viable, but when it is, it's worth doing."},
{"body": "I'm writing some Scala code which uses the  API. I would like to iterate over the rows contained in the  that I get from the Sheet class. I would like to use the iterator in a  style loop, so I have been trying to convert it to a native Scala collection but will no luck.I have looked at the Scala wrapper classes/traits, but I can not see how to use them correctly. How do I iterate over a Java collection in Scala without using the verbose  style of loop?Here's the code I wrote based on the correct answer:There is a wrapper class (). So if you define then it will act as a sub class of the Scala iterator so you can do .As of Scala 2.8, all you have to do is to import the JavaConversions object, which already declares the appropriate conversions.This won't work in previous versions though.The correct answer here is to define an implicit conversion from Java's  to some custom type. This type should implement a  method which delegates to the underlying . This will allow you to use a Scala -loop with any Java .For Scala 2.10:With Scala 2.10.4+ (and possibly earlier) it is possible to implicitly convert java.util.Iterator[A] to scala.collection.Iterator[A] by importing scala.collection.JavaConversions.asScalaIterator. Here is an example:You could convert the Java collection to an array and use that:Or go on and convert the array to a Scala list:If you would like to avoid the implicits in  you can use  to convert explicitly.Note the use of the  method to convert the Java  to a Scala .The JavaConverters have been available since Scala 2.8.1.Scala 2.12.0 deprecates , so since 2.12.0 one way of doing this would be something like:(notice the import, new is JavaConverters, deprecated is JavaConversions)If you are iterating through a large dataset, then you probably don't want to load whole collection into memory with  implicit conversion. In this case, a handy way approach is to implement  traitIt has similar concept but less verbose IMO :)"},
{"body": "What is the difference between  and  in Scala, and when to use which?Is the implementation same as in Java?EDIT: The related question talks about specific cases of . The more general case is .You normally use , it routes to , except that it treats s properly. Reference equality (rarely used) is .  is a final method, and calls , which is not final.This is radically different than Java, where  is an operator rather than a method and strictly compares reference equality for objects.NOTE: On the case of , just as in Java, it may not return the same result if you switch the arguments eg  will return  where the inverse will return . This is because of each implementation checking only specific types. Primitive numbers dont check if the second argument is of  nor  types but only of other primitive typesThe  method is the one overridden by subclasses. A method from the Java Specification that has come over to Scala too. If used on an unboxed instance, it is boxed to call this (though hidden in Scala; more obvious in Java with ->). The default implementation merely compares references (as in Java)The  method compares two objects and allows either argument to be null (as if calling a static method with two instances). It compares if both are , then it calls the  method on boxed instance.The  method compares  references, that is where the instance is located in memory. There is no implicit boxing for this method.There is an interesting difference between  and  for  and  types: They treat  differently: As was pointed out in a comment - \"this also happens in Java\" - depends on what exactly  is:This will printSo, the  yields  when compared for equality because this is how IEEE floating point numbers define it and this should really happen in every programming language (although it somehow messes with the notion of identity).The boxed NaN yields true for the comparison using  in Java as we are comparing object references.I do not have an explanation for the  case, IMHO it really should behave the same as  on unboxed double values, but it does not.Translated to Scala the matter is a little more complicated as Scala has unified primitive and object types into  and translates into the primitive double and the boxed Double as needed. Thus the scala  apparently boils down to a comparison of primitive  values but  uses the one defined on boxed Double values (there is a lot of implicit conversion magic going on and there is stuff pimped onto doubles by ).If you really need to find out if something is actually  use :In Scala  first check for  values and then calls  method on first object"},
{"body": "We are thinking on moving our Rest API Server (it is inside the web service, on Symfony PHP) to Scala for several reasons: speed, no overhead, less CPU, less code, scalability, etc. I didn't know Scala until several days ago but I've been enjoying what I've been learning these days with the Scala book and all the blog posts and questions (it's not so ugly!)I have the following options:Some things that I will have to use: HTTP requests, JSON output, MySQL (data), OAuth, Memcache (cache), Logs, File uploads, Stats (maybe Redis).What would you recommend?In no particular order: I'm going to recommend . It's an idiomatic Web framework that does things \"the Scala way\" and is very beautiful.Take a look at  (I'm its author), it provides everything you listed.  is quite extensive. From README:Xitrum is an async and clustered Scala web framework and web server on top of Netty and Hazelcast:I would add two more options: akka with built-in JAX-RS support, and simply using JAX-RS directly (probably the Jersey implementation).  While arguably less \"Scala-y\" than others (relying upon annotations to bind parameters and paths), JAX-RS is a joy to use, cleanly solving all of the problems of web service coding with minimal footprint.  I've not used it via akka, I would anticipate it being excellent there, getting impressive scalability via it's continuation-based implementation.  Take a look at , a Scala combinator library for building  HTTP services. Finch allows you to construct complex HTTP endpoints out of the number of predefined basic blocks. Similarly to parser combinators, Finch endpoints are easy to reuse, compose, test, and reason about.All good answers so far. One point in Lift's favor is its , which can make it quite easy to write short, elegant API methods. In addition, all the other things you want to do should be quite straight-forward to implement in Lift. That being said, Memcache might be not be necessary.A little late on the scene but I would definitely recommend using  framework for creation of REST API's. It small, to the point and automatic case class conversion support! "},
{"body": "In Scala, is there any difference at all between  and ?If not, which one is more idiomatic Scala style? Both for creating new empty lists and pattern matching on empty lists.Nil is more idiomatic and can be preferred in most cases.\nQuestions? has shown that the run time value of both  and  are the same. However, their static type is not:This is of particular importance when it is used to infer a type, such as in a fold's accumulator:As user unknown's answer shows, they are the same object.  Idiomatically Nil should be preferred because it is nice and short. There's an exception though: if an explicit type is needed for whatever reason I think is nicer than "},
{"body": "So, there is Is there an equivalent to write a string to file. a concise one ?Most languages support something like that. My fav is groovyThe use case being a one line to a small page of code. Having to carry your own library dosent make sense here. I expect a modern language to let me write a something to a file conveniently. There are posts similar to this. but, they dont answer my exact question or focused on older scala versions.\nthis  and this\nA concise one line:It is strange that no one had suggested NIO.2 operations (available since Java 7):I think this is by far the simplest and easiest and most idiomatic way, and it does not need any dependencies sans Java itself.Here is a concise one-liner using the Scala compiler library:Alternatively, if you want to use the Java libraries you can do this hack:If you like Groovy syntax, you can use the  design pattern to bring it to Scala:It will work as expected:You can easily use . Look at function . We use this library in our projects.One also has this format, which is both concise and the underlying library is beautifully written (see the source code):A micro library I wrote: orThis is concise enough, I guess:You can do this with a mix of Java and Scala libraries. You will have full control over the character encoding. But unfortunately, the file handles will not be closed properly. UPDATE: I have since created a more effective solution upon which I have elaborated here: I find myself working more and more in the Scala Worksheet within the Scala IDE for Eclipse (and I believe there is something equivalent in IntelliJ IDEA). Anyway, I need to be able to do a one-liner to output some of the contents as I get the \"Output exceeds cutoff limit.\" message if I am doing anything significant, especially with the Scala collections.I came up with a one-liner I insert into the top of each new Scala Worksheet to simplify this (and so I don't have to do the whole external library import exercise for a very simple need). If you are a stickler and notice that it is technically two lines, it's only to make it more readable in this forum. It is a single line in my Scala Worksheet.And the usage is simply:  This allows me to optionally provide the file name should I want to have additional files beyond the default (which completely overwrites the file each time the method is called).So, the second usage is simply:  Enjoy!I know it's not one line, but it solves the safety issues as far as I can tell;If you didn't care about the exception handling then you can writeThrough the magic of the semicolon, you can make anything you like a one-liner."},
{"body": "According to , Scala's type system is . What resources are available that enable a newcomer to take advantage of the power of type-level programming?Here are the resources I've found so far:These resources are great, but I feel like I'm missing the basics, and so do not have a solid foundation on which to build. For instance, where is there an introduction to type definitions? What operations can I perform on types?Are there any good introductory resources?Type-level programming has many similarities with traditional, value-level programming. However, unlike value-level programming, where the computation occurs at runtime, in type-level programming, the computation occurs at compile time. I will try to draw parallels between programming at the value-level and programming at the type-level.There are two main paradigms in type-level programming: \"object-oriented\" and \"functional\". Most examples linked to from here follow the object-oriented paradigm.A good, fairly simple example of type-level programming in the object-oriented paradigm can be found in apocalisp's , replicated here:As can be seen in the example, the object-oriented paradigm for type-level programming proceeds as follows:The Functional paradigm consists of defining lots of parameterized type constructors that are not grouped together in traits.The trick is to use implicit functions and values. The base case is usually an implicit value and the recursive case is usually an implicit function. Indeed, type-level programming makes heavy use of implicits.Consider this example ( and ):Here you have a peano encoding of the natural numbers. That is, you have a type for each non-negative integer: a special type for 0, namely ; and each integer greater than zero has a type of the form , where  is the type representing a smaller integer. For instance, the type representing 2 would be:  (successor applied twice to the type representing zero).We can alias various natural numbers for more convenient reference. Example:(This is a lot like defining a  to be the result of a function.)Now, suppose we want to define a value-level function  which takes in an argument value, , that conforms to  and returns an integer representing the natural number encoded in 's type. For example, if we have the value  ( of type ), we would want  to return .To implement , we're going to make use of the following class:As we will see below, there will be an object constructed from class  for each  from  up to (e.g.) , and each will store the value representation of the corresponding type (i.e.  will store the value ,  will store the value , etc.). Note,  is parameterized by two types:  and .  corresponds to the type we're trying to assign values to (in our example, ) and  corresponds to the type of value we're assigning to it (in our example, ).Now we make the following two implicit definitions:And we implement  as follows:To understand how  works, let's consider what it does on a couple of inputs:When we call , the compiler looks for an implicit argument  of type  (since  is of type ). It finds the object , it calls the  method of this object and gets back . The important point to note is that we did not specify to the program which object to use, the compiler found it implicitly.Now let's consider . This time, the compiler looks for an implicit argument  of type  (since  is of type ). It finds the function , which can return an object of the appropriate type () and evaluates it. This function itself takes an implicit argument () of type  (that is, a  where the first type parameter is has one fewer ). The compiler supplies  (as was done in the evaluation of  above), and  constructs a new  object with value . Again, it is important to note that the compiler is providing all of these values implicitly, since we do not have access to them explicitly. There are several ways to verify that your type-level computations are doing what you expect. Here are a few approaches. Make two types  and , that you want to verify are equal. Then check that the following compile:Alternatively, you can convert the type to a value (as shown above) and do a runtime check of the values. E.g. , where  is of type  and  is of type .The complete set of available constructs can be found in the types section of . has several academic papers about type constructors and related topics with examples from scala: is a blog with many examples of type-level programming in scala. is a very active project that is providing functionality that extends the Scala API using various type-level programming features. It is a very interesting project that has a big following. is a type-level library for Scala, including meta types for natural numbers, booleans, units, HList, etc. It is a project by .The  has some awesome examples of type-level programming in Scala (from other answer): has some relevant posts as well:(I've been doing some research on this subject and here's what I've learned. I'm still new to it, so please point out any inaccuracies in this answer.)In addition to the other links here, there are also my blog posts on type level meta programming in Scala:As suggested on Twitter:  by Miles Sabin. has source code, a wiki and examples."},
{"body": "How is pattern matching in Scala implemented at the bytecode level?Is it like a series of  constructs, or something else? What are its performance implications?For example, given the following code (from  pages 46-48), how would the equivalent Java code for the  method look like?P.S. I can read Java bytecode, so a bytecode representation would be good enough for me, but probably it would be better for the other readers to know how it would look like as Java code.P.P.S. Does the book  give an answer to this and similar questions about how Scala is implemented? I have ordered the book, but it has not yet arrived.The low level can be explored with a disassembler but the short answer is that it's a bunch of if/elses where the predicate depends on the patternThere's much more that you can do with patterns like or patterns and combinations like \"case Foo(45, x)\", but generally those are just logical extensions of what I just described.   Patterns can also have guards, which are additional constraints on the predicates.  There are also cases where the compiler can optimize pattern matching, e.g when there's some overlap between cases it might coalesce things a bit.  Advanced patterns and optimization are an active area of work in the compiler, so don't be surprised if the byte code improves substantially over these basic rules in current and future versions of Scala.  In addition to all that, you can write your own custom extractors in addition to or instead of the default ones Scala uses for case classes.  If you do, then the cost of the pattern match is the cost of whatever the extractor does.  A good overview is found in James (above) said it best. However, if you're curious it's always a good exercise to look at the disassembled bytecode. You can also invoke  with the  option, which will print your program with all Scala-specific features removed. It's basically Java in Scala's clothing. Here's the relevant  output for the code snippet you gave:Since version 2.8, Scala has had the  annotation. The goal is to ensure, that pattern matching will be compiled into  instead of series of conditional  statements."},
{"body": "I have a Java API that returns a List like:I am using the below scala code:Now if I try scala syntax sugar like:it does not work. I get the error:It seems I need to convert Java List to Scala List. How to do that in above context?Since  this conversion is now built into the language using:works.  did not workThere's a handy Scala object just for this - You can do the import and  afterwards as follows:This should give you Scala's  representation allowing you to accomplish .I was looking for an answer written in Java and surprisingly couldn't find any clean solutions here. After a while I was able to figure it out so I decided to add it here in case someone else is looking for the Java implementation (I guess it also works in Scala?):If you have to convert a Java  to a Scala , then you must do the following:1) Add2) Use methods ,  and then 3) Add the following to the  constructor that receives  as a parameter:"},
{"body": "There are a lot of functional idioms: monads, applicatives, arrows, etc. They are documented in different articles but unfortunately I don't know any book or article where they're summarized in one place (there is  but it has a lot of areas that aren't covered well). Can anyone recommend an article/book which covers them well in one place and which can be accessible to a programmer with intermediate skills in FP?My suggestion is, if you want to learn Scala, to read the book from Paul Chiusano and Tony Morris: Part II: Functional design and combinator libraries Part III: Functional design patterns Part IV: Breaking the rules: effects and I/O I'm sorry I don't know of articles or books which cover in detail the different usages for all of those constructs, but I can give you a few links to individual resources.A quite common pattern is to build  instead of simple monads (see also the link in the next paragraph). It basically means you build something that must be combined with other monads, resulting in a more complex one able to handle features of both of them.In  there are a few chapters about monads. In  the authors explain the basics and some common usages (maybe, list, state).  provides more explanations about how to effectively use them (it covers the reader monad as well). The following chapter explains how to use , but it may be more interesting to search for articles covering how it actually works: it should be a really good example of a well-organized use of monads for parsing. Fianlly,  introduces how monad transformers work and then shows how to build one, step by step. The considerations towards the final sections of the chapter are also interesting.I read once a really interesting question on SO about . The proposed links were awesome reads about the topic. With that spirit, I tried to ask the same for : I definitely got less answers than the one on monads, but interesting ones nevertheless.With respect to OOP patterns by the gang of four, there is a nice set of 3 articles by IBM about the topic in their series . The target functional language is Scala. They proceed by explaining usual design patterns in OOP and showing how they map into Scala.The most relevant article w.r.t. your question is for sure the first one, but the other two may be interesting related readings nevertheless.Jeremy Gibbons has a  which is destined eventually to become pretty much the book you're asking for. Of course, that's not yet in a condition to be as useful as you might want just now, but he deserves some encouragement!Meanwhile, I'll say +1 for Brent Yorgey's Typeclassopedia. It's really useful, and if there are later parts which confuse, this site is a good place to get to the bottom of them. I know Brent keeps it under review. If he's not reaching his readers, give him some help.Lot of FP stuff are published on Oleg's site:\n Presentation about FP patterns from Josh Suereth:\nHave you read the later chapters of ?It doesn't cover Arrows or Comonads, which are a more advanced topic in Haskell. To understand how and why to use Arrows or Comonads you should definitely have a firm grasp of Monads already, so I don't think this is a problem - LYAH is firmly aimed at the beginner end of the Haskell market."},
{"body": ": Re-written this question based on original answerThe  class is not covariant in its type parameter. Why is this?  is invariant in its type parameter because of the concept behind sets as functions.  The following signatures should clarify things slightly:If  were covariant in , the  method would be unable to take a parameter of type  due to the contravariance of functions.   could potentially be  in , but this too causes issues when you want to do things like this:In short, the best solution is to keep things invariant, even for the immutable data structure.  You'll notice that  is also invariant in one of its type parameters.at  Martin Odersky writes:So, it seems that all of our efforts to construct a principled reason for this were misguided :-): for anyone wondering why this answer seems slightly off-topic, this is because I (the questioner) have modified the question. Scala's type inference is good enough to figure out that you want CharSequences and not Strings in some situations. In particular, the following works for me in 2.7.3:As to how to create immutable.HashSets directly: don't. As an implementation optimization, immutable.HashSets of less than 5 elements are not actually instances of immutable.HashSet. They are either EmptySet, Set1, Set2, Set3, or Set4. These classes subclass immutable.Set, but not immutable.HashSet."},
{"body": "Are there any guidelines in Scala on when to use val with a mutable collection versus using var with an immutable collection? Or should you really aim for val with an immutable collection?The fact that there are both types of collection gives me a lot of choice, and often I don't\nknow how to make that choice.Pretty common question, this one. The hard thing is finding the duplicates.You should strive for . What that means is that, if I have an expression \"e\", I could make a , and replace  with . This is the property that mutability break. Whenever you need to make a design decision, maximize for referential transparency.As a practical matter, a method-local  is the safest  that exists, since it doesn't escape the method. If the method is short, even better. If it isn't, try to reduce it by extracting other methods.On the other hand, a mutable collection has the  to escape, even if it doesn't. When changing code, you might then want to pass it to other methods, or return it. That's the kind of thing that breaks referential transparency.On an object (a field), pretty much the same thing happens, but with more dire consequences. Either way the object will have state and, therefore, break referential transparency. But having a mutable collection means even the object itself might lose control of who's changing it.If you work with immutable collections and you need to \"modify\" them, for example, add elements to them in a loop, then you have to use s because you need to store the resulting collection somewhere. If you only read from immutable collections, then use s.In general, make sure that you don't confuse references and objects. s are immutable references (constant pointers in C). That is, when you use , you'll be able  that  points to, but you won't be able   points. The opposite holds if you use . Picking up my initial advice: if you don't need to change to which object a reference points, use s.The best way to answer this is with an example.  Suppose we have some process simply collecting numbers for some reason.  We wish to log these numbers, and will send the collection to  process to do this.Of course, we are still collecting numbers after we send the collection to the logger.  And let's say there is some overhead in the logging process that delays the actual logging.  Hopefully you can see where this is going. If we store this collection in a mutable , (mutable because we are continuously adding to it), this means that the process doing the logging will be looking at the  that's still being updated by our collection process.  That collection may be updated at any time, and so when it's time to log we may not actually be logging the collection we sent.If we use an immutable , we send an immutable data structure to the logger.  When we add more numbers to our collection, we will be  our  with a .  This doesn't mean collection sent to the logger is replaced!  It's still referencing the collection it was sent.  So our logger will indeed log the collection it received.I think the examples in this blog post will shed more light, as the question of which combo to use becomes even more important in concurrency scenarios: . And while we're at it, note the preferred use of synchronised vs @volatile vs something like AtomicReference: "},
{"body": "The pimp-my-library pattern allows me to seemingly add a method to a class by making available an implicit conversion from that class to one that implements the method.Scala does not allow two such implicit conversions taking place, however, so I cannot got from  to  using an implicit  to  and another implicit  to . Is there a way around this restriction?Scala has a restriction on automatic conversions to add a method, which is that it won't apply more than one conversion in trying to find methods. For example: (You can use type classes instead)However, if an implicit definition requires an implicit parameter itself(View bound), Scala  look for additional implicit values for as long as needed. Continue from the last example:\"Magic!\", you might say. Not so. Here is how the compiler would translate each one:So, while  is being used as an implicit conversion,  and  are being passed as , instead of being chained as implicit conversions.Related question of interest:Note that you can build circles with implicit parameters, too. Those are, however, detected by the compiler, as exhibited by this:The error(s) given to the user are not as clear as they could be, though; it just complains  for all three construction site. That might obscure the underlying problem in less obvious cases. a code that also accumulates the path."},
{"body": "What is the naming convention for Scala constants? A brief search on StackOverflow suggestions uppercase CamelCase (the first line below), but I wanted to double-check.Which is recommended Scala style?The officially recommended style (and I do mean officially) is the first style, camel case with first letter are upper case. It's laid down clearly by Odersky on Programming in Scala.The style is also followed by the standard library, and has some support in language semantics: identifiers starting with upper case are treated as constants in pattern matching.(Section 6.10, p. 107 in the second edition)(This is an addendum comment to Daniel's answer, but I'm posting it as an answer for the benefit of syntax highlighting and formatting.)Daniel's point about the style of using an initial capital letter being important in the language semantics is more subtle and important than I originally gave it credit for when I learned Scala.Consider the following code:Naively I would have expected that to reach all of the cases in the match.  Instead it prints:What's going on is that the  shadows the val  and creates a local variable of the same name which will be populated any time a  containing a string is evaluated.There are admittedly ways to work around it, but the simplest is to follow the style guide for constant naming.If you can't follow the naming convention, then as @reggoodwin points out in the comments below, you can put the variable name in ticks, like so "},
{"body": "Is there any List/Sequence built-in that behaves like  and provides the element's index as well?I believe you're looking for zipWithIndex?From: You also have variations like: or:Use . in .Result:The proposed solutions suffer from the fact that they create intermediate collections or introduce variables which are not strictly necessary. For ultimately all you need to do is to keep track of the number of steps of an iteration. This can be done using memoizing. The resulting code might look likeThe -Function wraps the interior function which receives both an index an the elements of . This might be familiar to you from JavaScript.Here is a way to achieve this purpose. Consider the following utility:This is already all you need. You can apply this for instance as follows:which results in the listThis way, you can use the usual Traversable-functions at the expense of wrapping your effective function. The overhead is the creation of the memoizing object and the counter therein. Otherwise this solution is as good (or bad) in terms of memory or performance as using unindexed . Enjoy!There is  in 2.7.x (which you can get from a normal iterator with .counted).  I believe it's been deprecated (or simply removed) in 2.8, but it's easy enough to roll your own.  You do need to be able to name the iterator:Or, assuming your collection has constant access time, you could map the list of indexes instead of the actual collection:"},
{"body": "I have looked at  but still don't understand the difference between Iterable and Traversable traits. Can someone explain ?To put it simply, iterators keep state, traversables don't.A  has one abstract method: . When you call ,  will feed the passed function all the elements it keeps, one after the other.On the other hand, an  has as abstract method , which returns an . You can call  on an  to get the next element at the time of your choosing. Until you do, it has to keep track of where it was in the collection, and what's next.Think of it as the difference between blowing and sucking.When you have call a s , or its derived methods, it will blow its values into your function one at a time - so it has control over the iteration.With the  returned by an  though, you suck the values out of it, controlling when to move to the next one yourself.  are  that can produce stateful First, know that  is subtrait of .Second, For example, the implemetation of  for  uses  (via a for comprehension) and throws a  exception to halt iteration once a satisfactory element has been found.In contrast, the  subtract overrides this implementation and calls  on the , which simply stops iterating once the element is found:It'd be nice not to throw exceptions for  iteration, but that's the only way to partially iterate when using just .From one perspective,  is the more demanding/powerful trait, as you can easily implement  using , but you can't really implement  using .In summary,  provides a way to pause, resume, or stop iteration via a stateful . With , it's all or nothing (sans exceptions for flow control).Most of the time it doesn't matter, and you'll want the more general interface. But if you ever need more customized control over iteration, you'll need an , which you can retrieve from an ."},
{"body": "I'll start with an example. Here's an equivalent of  for tuples as a macro in Scala 2.10:We can use this method as follows:This guy is a weird bird in a couple of respects. First, the  argument must be a literal integer, since we need to use it at compile time. In previous versions of Scala there was no way (as far as I know) for a method even to tell whether one of its arguments was a compile-time literal or not.Second, the  return type \u2014the static return type will include the specific arity and element type determined by the arguments, as shown above.So how would I document this thing? I'm not expecting Scaladoc support at this point, but I'd like to have a sense of conventions or best practices (beyond just making sure the compile-time error messages are clear) that would make running into a macro method\u2014with its potentially bizarre demands\u2014less surprising for users of a Scala 2.10 library.The most mature demonstrations of the new macro system (e.g., , , the others listed ) are still relatively undocumented at the method level. Any examples or pointers would be appreciated, including ones from other languages with similar macro systems.I think the best way to document these is with example code, as Miles has been doing in his experimental  of shapeless."},
{"body": "I'm developing several modules with dependencies among them, and would like to work with them all together in one IDEA project.  I'm using  to generate IDEA projects from the sbt build definitions, which works great for individual projects.  In the multiple-module case, however, the things I've tried so far don't quite work:; then create a master IDEA project from scratch an add those modules to it.  This makes the module sources all editable in the same window, but the dependencies among them are not tracked (so trying to navigate from some source within the  project to something in  takes me to the imported library version of , not the local sources)., where the parent project's Build.scala contains things like:This almost works, in that sbt-idea generates a master IDEA project with the dependencies among the subprojects tracked.  There are however two caveats: To summarize: I'd like to collect modules  into one big IDEA project with tracked dependencies for convenient editing.  How can I do it?  Thanks!With sbt 13.5 and intellij 13.x, you can specify inter-project dependency with relative path, using a .\nLet's say you have two projects, a core project  and another project , both living in a common directory The approach with multi-project build is the correct one. You can have a nested tree of subprojects of arbitrary length, but you cannot have a module belonging to multiple parent projects. This makes absolutely sense, and in Maven happens the same.The reason is that it would be hard to have the same module into multiple projects and keep the sources synchronized. A normal workflow is the following:If you want to load a module which does not belong to the current project inside Idea, this is however feasible as you can add this as an external module to the workspace:"},
{"body": "I am currently trying to get started with Akka and I am facing a weird problem. I've got the following code for my Actor:And this is how I start my workers:And this is how I shut everything down:Now what happens is if I send the workers messages with n > 0 (no exception is thrown), everything works fine and the application shuts down properly. However, as soon as I send it a single message which results in an exception, the application does not terminate because there is still an actor running, but I can't figure out where it comes from.In case it helps, this is the stack of the thread in question:PS: The thread which is not terminating isn't any of the worker threads, because I've added a postStop callback, every one of them stops properly.PPS:  workarounds the problem, but I think shutdownAll should only be used as a last resort, shouldn't it?The proper way to handle problems inside akka actors is not to throw an exception but rather to set supervisor hierarchies see  the above is true for old versions of Akka (1.2)\nIn newer versions (e.g. 2.2) you'd still set a supervisor hierarchy but it will trap Exceptions thrown by child processes. e.g.and in the supervisor:see Turning off the logging to make sure things terminate, as proposed by Viktor, is a bit strange. What you can do instead is:that cleanly shuts down all the (logger) listeners that keep the world running after the exception:Turn of the logger in the "},
{"body": "What are the differences using Scala Actors instead of JMS? For example from a performance and scalability perspective, what does the Scala Actor model add compared to JMS? In which cases does it make more sense to use Actors rather than JMS, i.e. what problems does Actors address that JMS cannot cover?JMS and Scala actors share a theoretical similarity but don't think of them as necessarily solving the same problems architecturally. Actors are meant to be a lightweight alternative to shared-memory concurrency where races and deadlocks are generally harder to accidentally create.  JMS is a sophisticated API that's meant to span direct messaging, publish/subscribe, transactions, EJB integration, etc.The closest JMS equivalent to an actor would be a message driven bean that is backed by a non-persistent, non-transactional, non-pub/sub queue.  I'll call that a \"simple JMS bean\".Now, to your questions.Performance is a hard thing to talk about since JMS is a specification rather than an implementation.  None-the-less when using simple JMS bean I'd expect performance to be roughly similar with perhaps a bit of an edge to the actor in time and memory.  As you add capabilities to JMS such as pub/sub, transactions, etc performance will naturally degrade even further but then you're trying to compare apples to oranges.As for scalability, simple JMS beans should scale pretty much exactly the same way as actors.  Adding transactions into the JMS mix will naturally hurt scalability by an amount depending on the scope of the transactions.The broader question of what do actors do that JMS can't.  Well, without built-in pub sub or transactions it would seem that actors subtract from JMS - and broadly that's true.  But here's the thing: actors require so little code that I can happily use them for very fine grained concurrency.  In ordinary Java code I might say \"I don't feel like screwing with JMS and its dependencies or the code it requires etc so I'll just spawn a thread, use a lock, and share a data structure.\"  With Scala actors I'm much more likely to say \"I'll just whip up an actor and move on.\"There's also a philosophical difference in design.  Actors have a simple, built in concept of supervisor hierarchies.  Actors are usually used in a \"let it crash\" design.  If an actor dies for some reason then another actor is responsible for deciding what to do about it such as restarting that actor, killing a bunch of actors and restarting all of them, or killing a bunch of actors and itself so that some otther actor can deal with the problem.  That kind of thing can appended onto JMS, but it's not core to the API and must be managed externally somehow.By the way, for a Scala actor library that moves more into the realms that JMS covers see .  Akka also brings a declarative approach to many common actor hierarchy strategies."},
{"body": "I am having a discussion around  in the Scala Style Guide I maintain. I've come to realize that there are two ways of , and I'm wondering what the use cases are:The style guide incorrectly implies these are the same, when they are clearly not. The guide is trying to make a point about created curried functions, and, while the second form is not \"by-the-book\" currying, it's still very similar to the first form (though arguably easier to use because you don't need the )From those that use these forms, what's the consensus on when to use one form over the other?Methods with multiple parameter sections can be used to assist local type inference, by using parameters in the first section to infer type arguments that will provide an expected type for an argument in the subsequent section.  in the standard library is the canonical example of this.If this were this written as:One would have to provide more explicit types:Another use for multiple parameter section methods is to create an API that looks like a language construct. The caller can use braces instead of parentheses.Application of N argument lists to method with M parameter sections, where N < M, can be converted to a function explicitly with a , or implicitly, with an expected type of . This is a safety feature, see the change notes for Scala 2.0, in the Scala References, for an background.Curried functions (or simply, functions that return functions) more easily be applied to N argument lists.This minor convenience is sometimes worthwhile. Note that functions can't be type parametric though, so in some cases a method is required.Your second example is a hybrid: a one parameter section method that returns a function.Where else are curried functions useful? Here's a pattern that comes up all the time:How can we share the result ? A common solution is to provide a vectorized version of :Ugly! We've entangled unrelated concerns -- calculating  and mapping over a sequence of .We could also use a method that returns a function. In this case its a bit more readable:But if we try to do the same with a method with multiple parameter sections, we get stuck:You can curry only functions, not methods.  is a method, so you need the  to force its conversion to a function.  returns a function, so the  is not only unnecessary but makes no sense here. Considering how different methods and functions are (e.g. from the perspective of the JVM), Scala does a pretty good job blurring the line between them and doing \"The Right Thing\" in most cases, but there  a difference, and sometimes you just need to know about it.I think it helps to grasp the differences if I add that with  you pretty much just define a method with  parameters, only those two parameters are grouped into two parameter lists (see the consequences of that in other comments). In fact, that method is just  as far as Java (not Scala!) is concerned. When you write , that's just a function literal, a shorter form of . On the other hand, with  you define a method that has only one parameter, and for Java it will be . When you write  in Scala it's just a plain method call (as opposed to a function literal).Also note that  has (potentially) less overhead than  has if you immediately provide all parameters. Like  just translates to  on the JVM level, no  object is created. On the other hand,  will first create a  object that encloses , and then call  on that."},
{"body": "I heard that with  it is somehow possible to do dynamic typing in Scala. But I can't imagine how that might look like or how it works.I found out that one can inherit from trait The  says that one can use it like this:But when I try it out it doesn't work:This is completely logical, because after looking to the , it turned out that this trait is completely empty. There is no method  defined and I can't imagine how to implement it by myself.Can someone show me what I need to do to make it to work?Scalas type  allows you to call methods on objects that don't exist or in other words it is a replica of \"method missing\" in dynamic languages.It is correct,  doesn't have any members, it is just a marker interface - the concrete implementation is filled-in by the compiler. As for Scalas  feature there are well defined rules describing the generated implementation. In fact, one can implement four different methods:To use one of these methods it is enough to write a class that extends  and to implement the methods there:Furthermore one need to add aor set the compiler option  because the feature is hidden by default. is the easiest one to implement. The compiler translates a call of  to , thus it is required that this method has an argument list expecting a :As one can see, it is also possible to call the dynamic methods explicitly.Because  is used to update a value this method needs to return . Furthermore, the name of the field to update and its value are passed to different argument lists by the compiler:The code works as expected - it is possible to add methods at runtime to the code. On the other side, the code isn't typesafe anymore and if a method is called that doesn't exist this must be handled at runtime as well. In addition this code is not as useful as in dynamic languages because it is not possible to create the methods that should be called at runtime. This means that we can't do something likewhere  would be transformed to  at runtime. But this is not that bad because even in dynamic languages this is a dangerous feature.Another thing to note here, is that  needs to be implemented together with . If we don't do this we will get a compile error - this rule is similar to the implementation of a Setter, which only works if there is a Getter with the same name.The ability to call methods with arguments is provided by :The name of the method and its arguments again are separated to different parameter lists. We can call arbitrary methods with an arbitrary number of arguments if we want but if we want to call a method without any parentheses we need to implement .Hint: It is also possible to use apply-syntax with :The last available method allows us to name our arguments if we want:The difference in the method signature is that  expects tuples of the form  where  is an arbitrary type.All of the above methods have in common that their parameters can be parameterized:Luckily, it is also possible to add implicit arguments - if we add a  context bound we can easily check the types of the arguments. And the best thing is that even the return type is correct - even though we had to add some casts.But Scala would not be Scala when there is no way to find a way around such flaws. In our case we can use type classes to avoid the casts:While the implementation doesn't look that nice, its power can't be questioned:At the top of all, it is also possible to combine  with macros:Macros give us back all compile time guarantees and while it is not that useful in the above case, maybe it can be very useful for some Scala DSLs.If you want to get even more information about  there are some more resources:"},
{"body": "In Scala I see such feature as object-private variable. From my not very rich Java background I learnt to close everything (make it private) and open (provide accessors) if necessary. Scala introduces even more strict access modifier. Should I always use it by default? Or should I use it only in some specific cases where I need to explicitly restrict changing field value even for objects of the same class? In other words how should I choose betweenThe second is more strict and I like it but should I always use it or only if I have a strong reason?EDITED: As I see  private[this] is just some subcase and instead of  I can use other modifiers: \"package, class or singleton object\". So I'll leave it for some special case.I don't think it matters too much, since any changes will only touch one class either way. So the most important reason to prefer  over  over  doesn't apply.Use  where performance really matters (since you'll get direct field access instead of methods this way). Otherwise, just settle on one style so people don't need to figure out why  property is  and  one is .There is a case where  is required to make code compile. This has to do with an interaction of variance notation and mutable variables. Consider the following (useless) class:So this class is designed to hold an optional value, return it as an option and enable the user to call  to clear the value (hence the var). As stated, this is useless except to demonstrate the point.If you try compiling this code with  instead of  it will fail with the following error message:This error occurs because value is a mutable variable on the covariant type T (+T) which is normally a problem unless marked as private to the instance with . The compiler has special handling in its variance checking to handle this special case.So it's esoteric but there is a case where  is required over .so you can do private[this] every time you want but you can have some problem if you need refer it  is accessible from any method of the  (and its companion ). is accessible from methods of  object only, not from other objects of .This was tested using scala 2.11.5. Consider the code belowit will compile and work as this java (1.8) codehowever if you use '[this]' modifier the code below won't compileThis is because in the first case 'x' is accessible on class level, whereas in the second case is it more strict instance level. It means that 'x' can be accessed only from the instance to which it belongs. So 'this.x' is fine but 'other.x' is not.You can refer to section 13.5 of \"Programming in Scala: A Comprehensive Step-By-Step Guide\" book for more details about access modifiers.When adding the scope to the  modifier (), it effectively behaves as a \u201cup to\u201d X, where X designates some enclosing package, class or singleton object.For example, , where  is a package means that every instance of every class belonging to package  can access whichever member the modifier is restricting.In the case of , it means that the member is only accessible for each instance.\nThis becomes more clear in the following example:As you can see, the second Foo doesn\u2019t have any problem since any instance can access the private val i. However for the first Foo there\u2019s an error since each instance cannot see other instance\u2019s i.It\u2019s a good practice to write private[this] since it imposes a bigger restriction.To elaborate on the performance issue Alexey Romanov has mentioned, here are some of my guesses.\nQuotes from book \"Programming in Scala: A Comprehensive Step-by-Step Guide, 2nd Edition\" Section 18.2:To test it out, this code will cause compilation error:Scala complains about . Adding override keyword to  won't help should prove that the method is generated by the compiler. Adding  keyword to variable  will still cause this compilation error. However, the following code compiles fine:So, I guess  will prevent scala from generating getter and setter methods. Thus, accessing such variable will save the overhead of calling the getter and setter method.It's better to use  if you plan to synchronize the variable. Here a good example from the :In most OOP programming language like java, private fields/methods mean that these private fields/methods are not accessible outside from the class. However, instances/objects of same class can have access to the private fields of objects using assignment operator or by means of copy constructor. In Scala,private[this] is object private,which makes sure that any other object of same class is unable to access private[this] members.1.Without private[this] 2.Using private[this] Hence private[this] makes sure that _password field is only accessible with this."},
{"body": "Why do Scala and frameworks like Spark and Scalding have both  and ? So then what's the difference between  and ?A big big difference, not mentioned in any other stackoverflow answer relating to this topic clearly, is that  should be given a , i.e. an operation that is both commutative and associative.  This means the operation can be parallelized.This distinction is very important for Big Data / MPP / distributed computing, and the entire reason why  even exists.  The collection can be chopped up and the  can operate on each chunk, then the  can operate on the results of each chunk - in fact the level of chunking need not stop one level deep.  We could chop up each chunk too.  This is why summing integers in a list is O(log N) if given an infinite number of CPUs.If you just look at the signatures there is no reason for  to exist because you can achieve everything you can with  with a .  The functionality of  is a greater than the functionality of . you cannot parallelize a , so its runtime is always O(N) (even if you feed in a commutative monoid). This is because it's assumed the operation is  a commutative monoid and so the cumulated value will be computed by a series of sequential aggregations.   does not assume commutativity nor associativity.  It's associativity that gives the ability to chop up the collection, and it's commutativity that makes cumulating easy because order is not important (so it doesn't matter which order to aggregate each of the results from each of the chunks). Strictly speaking commutativity is not necessary for parallelization, for example distributed sorting algorithms, it just makes the logic easier because you don't need to give your chunks an ordering.If you have a look at the Spark documentation for  it specifically says \"... commutative and associative binary operator\"Here is proof that  is NOT just a special case of Now this is where it gets a little closer to the FP / mathematical roots, and a little trickier to explain.  Reduce is defined formally as part of the MapReduce paradigm, which deals with orderless collections (multisets), Fold is formally defined in terms of recursion (see catomorphism) and thus assumes a structure / sequence to the collections.There is no  method in Scalding because under the (strict) Map Reduce programming model we cannot define  because chunks do not have an ordering and  only requires associativity, not commutativity.Put simply,  works without an order of cumulation,  requires an order of cumulation and it is that order of cumulation that necessitates a zero value NOT the existence of the zero value that distinguishes them.  Strictly speaking   work on an empty collection, because its zero value can by deduced by taking an arbitrary value  and then solving , but that doesn't work with a non-commutative operation as there can exist a left and right zero value that are distinct (i.e. ).  Of course Scala doesn't bother to work out what this zero value is as that would require doing some mathematics (which are probably uncomputable), so just throws an exception.It seems (as is often the case in etymology) that this original mathematical meaning has been lost, since the only obvious difference in programming is the signature. The result is that  has become a synonym for , rather than preserving it's original meaning from MapReduce. Now these terms are often used interchangeably and behave the same in most implementations (ignoring empty collections). Weirdness is exacerbated by peculiarities, like in Spark, that we shall now address.So Spark  have a , but the order in which sub results (one for each partition) are combined (at the time of writing) is the same order in which tasks are completed - and thus non-deterministic. Thanks to @CafeFeed for pointing out that  uses , which after reading through the code I realised that it's non-deterministic. Further confusion is created by Spark having a  but no .There is a difference between  and  even when applied to non-empty sequences. The former is defined as part of the MapReduce programming paradigm on collections with arbitrary order () and one ought to assume operators are commutative in addition to being associative to give deterministic results. The latter is defined in terms of catomorphisms and requires that the collections have a notion of sequence (or are defined recursively, like linked lists), thus do not require commutative operators.In practice due to the unmathematical nature of programming,  and  tend to behave in the same way, either correctly (like in Scala) or incorrectly (like in Spark).My opinion is that confusion would be avoided if use of the term  was completely dropped in Spark.  At least spark does have a note in their documentation:If I am not mistaken, even though the Spark API does not require it, fold also requires for the f to be commutative. Because the order in which the partitions will be aggregated is not assured.\nFor example in the following code only the first print out is sorted:  Print out:abcdefghijklmnopqrstuvwxyzabcghituvjklmwxyzqrsdefnopdefghinopjklmqrstuvabcwxyzOne other difference for Scalding is the use of combiners in Hadoop. Imagine your operation is commutative monoid, with  it will be applied on the map side also instead of shuffling/sorting all data to reducers. With  this is not the case. It is always good practice to define your operations as monoid in Scalding. in Apache Spark is not the same as  on not-distributed collections. In fact  to produce deterministic results:This  by  and suggested by  in . that observed behavior is related to  when in fact  doesn't shuffle and doesn't use .Explained: for RDDis the same  for RDD:where  is performed with disregard of partition order and results in need of commutative function. and  are equivalent in terms of order of processing and effectively (by inheritance and delegation) implemented by  and  on . Conclusion:  on RDD cannot depend on order of chunks and needs ."},
{"body": "I have scala map:When I try to iterate over map like;the above does not work. In each iteration I must know what is the key and what is the value. What is the proper way to iterate over scala map using scala syntactic sugar? method receives  as argument, not 2 arguments. So you can either use it like tuple:or you can make pattern match:Three options:The trick is that iteration gives you key-value pairs, which you can't split up into a key and value identifier name without either using  or ."},
{"body": "I've already read the question and answers to .Usually, we use messaging solutions which have existed for years already: either a JMS implementation such as WebSphere MQ or Apache ActiveMQ is used for Point-To-Point communication, or Tibco Rendevous for Multicast messaging.They are very stable, proven and offer high availability and performance. Nevertheless, configuration and setup seem much more complex than in Akka.When and why should I use Akka for some use cases where the aforementioned products - WebSphere MQ or ActiveMQ - have been used successfully so far? Why should I consider using Akka instead of WebSphere MQ or Tibco RV in my future project? And when should I avoid Akka? Does it offer the same high availability and performance as the other solutions? Or is it a bad idea to even compare Akka to the other messaging middlewares?Maybe there also is another messaging solution in the JVM environment which I should consider besides JMS (Point-to-Point), TibcoRV (Multicast) and Akka?First off the \"older\" message systems (MQ) are older in implementation but they are a newer in engineering idea of: . Scala Actors and Akka maybe a newer implementation but are built on an older concurrency model of Actors.The two models however end up being very similar in practice because they both are event message based: See my answer to .If you're going to code only for the JVM then Akka is probably a good choice. Otherwise I would use RabbitMQ.Also if you're a Scala developer, then Akka should be a no-brainer. However Akka's Java bindings are not very Java-ish and require casting due to Scala's type system. Also in Java people don't typically make immutable objects which I recommend you do for messaging. Consequently its very easy in Java to accidentally do something using Akka that will not scale (using mutable objects for messages, relying on weird closure callback state). With MQ this is not a problem because the messages are always serialized at the cost of speed. With Akka they are generally not.Akka also scales better with large amount of consumers than most MQ. This is because for most MQ (JMS, AMQP) clients  every queue connection requires a thread... thus lots of queues == lots of permanently running threads. This is mainly a client issue though. I think ActiveMQ Apollo has a non-blocking dispatcher that purportedly fixes that issue for AMQP. The RabbitMQ client has channels that allow you to combine multiple consumers but there are still issues with large number of consumers potentially causing deadlocks or connections to die so generally more threads are added to avoid this issue. That being said  is rather new and probably still doesn't offer all the reliable message guarantees and QoS that traditional message queues provide (but that is changing everyday). Its also generally peer-to-peer but does I think support server-to-peer which is generally what most MQ systems do (ie single point of failure) but there are MQ systems that are peer-to-peer (RabbitMQ is server-to-peer). You can use Akka as a wrapper to RabbitMQ particularly since RabbitMQ does not help you with handling the consumption of messages and routing the messages locally (in a single JVM).I made an assumption that the OP was concerned with distributed processing which both  and Message Queues can handle. That is I assumed he was talking about . . I say most because you can apply the message queue model locally as a concurrency model (ie topic, queues, exchanges) which both the  library and  do.Picking the right concurrency model/library is very important for low latency applications. A distributed processing solution such as a message queue is generally not ideal because the routing is almost always done over the wire which is obviously slower than within application and thus Akka would be a superior choice. However I believe some proprietary MQ technologies allow for local routing. Also as I mentioned earlier most MQ clients are pretty stupid about threading and do not rely on non-blocking IO and have a thread per connection/queue/channel... ironically non-blocking io is not always low latency but is generally more resource efficient.As you can see the topic of distributed programming and concurrent programming is rather large and changing everyday so my original intention was not confuse but rather focus on one particular area of distributed message processing which is what I though the OP was concerned with. In terms of concurrency one might want to focus their searches on \"reactive\" programming (RFP / streams) which is a \"newer\" but similar model to the actor model and message queue model of which all of these models can be generally combined because they are event based.I'm not an expert in messaging systems, but you can combine them with Akka in your apps, getting the best of both worlds. Here's an example that you might find useful for experimenting with Akka and messaging systems, in this case ZeroMQ:Akka-Camel would be a better example than ZeroMQ - ZeroMQ is a direct tcp to tcp communication (hence zero - there is no message queue).With AkkaCamel you can abstract away the queue and produce/consume messages direct from an actor without any code to deal with the message queue message pushing/pulling.You can forego akka-zeromq and use Akka directly with remoting.\nI think akka-zeromq is being removed from the core library but we built a good zeromq library for akka called scala-zeromq ()Akka has a couple key core use cases:1) Mutable stateIt's easier to handle shared state by hiding it in an actor. As actors handle messages synchronously, you can hold state in an actor and expose that field with high consistency via the actor API2) DistributionConcurrency is free in akka so you say it's really about solving distribution problems. Distribution across machines and cores. Akka has build in \"location transparency\" for sending messages over the wire. It has clustering and patters associated for scaling up a single service as well. This makes it a very good solution for distribution (eg micro-service architecture)Here is an example of using Akka with ActiveMQ with Akka-Camel (using Java8)"},
{"body": "With the release of Scala 2.9.0, the Typesafe Stack  was also announced, which combines the Scala language with the Akka framework. Now, though Scala has actors in its standard library, Akka uses its own implementation. And, if we look for other implementations, we'll also find that Lift and Scalaz have implementations too!So, what is the difference between these implementations?This answer isn't really mine.  by Viktor Klang (of Akka fame) with the help of David Pollak (of Lift fame), Jason Zaugg (of Scalaz fame), Philipp Haller (of Scala Actors fame).All I'm doing here is formatting it (which would be easier if Stack Overflow supported tables).There are a few places I'll fill later when I have more time.If user defines public methods on\ntheir Actors, are they callable from\nthe outside?(1) Any function f becomes such an actor: (2) Any function f becomes such an actor:(3) Contravariant functor: . Also Kleisli composition in .TBDSupports nested receives?TBDTBD &  for average latency and throughput on JVM 1.8.0_x."},
{"body": "Following on from , what are some good resources which explain how the new   collections library has been structured. I'm interested to find some information on how the following fit together:There's a  by Martin Odersky which should probably be your first reference. It has been supplemented as well with , which will be of particular interest to those who want to design their own collections. The rest of this answer was written way before any such thing existed (in fact, before 2.8.0 itself was released).You can find a paper about it as . Other papers in that area should be interesting as well to people interested in the differences between Scala 2.7 and 2.8.I'll quote from the paper, selectively, and complement with some thoughts of mine. There are also some images, generated by Matthias at decodified.com, and the original SVG files can be found .There are actually three hierarchies of traits for the collections: one for mutable collections, one for immutable collections, and one which doesn't make any assumptions about the collections.There's also a distinction between parallel, serial and maybe-parallel collections, which was introduced with Scala 2.9. I'll talk about them in the next section. The hierarchy described in this section refers .The following image shows the non-specific hierarchy introduced with Scala 2.8:\nAll elements shown are traits. In the other two hierarchies there are also classes directly inheriting the traits as well as classes which can be  belonging in that hierarchy through implicit conversion to wrapper classes. The legend for these graphs can be found after them.Graph for immutable hierarchy:\nGraph for mutable hierarchy:\nLegend:Here's an abbreviated ASCII depiction of the collection hierarchy, for those who can't see the images.When Scala 2.9 introduced parallel collections, one of the design goals was to make their use as seamless as possible. In the simplest terms, one can replace a non-parallel (serial) collection with a parallel one, and instantly reap the benefits.However, since all collections until then were serial, many algorithms using them assumed and depended on the fact that they  serial. Parallel collections fed to the methods with such assumptions would fail. For this reason, all the hierarchy described in the previous section .Two new hierarchies were created to support the parallel collections.The parallel collections hierarchy has the same names for traits, but preceded with : , ,  and . Note that there is no , since any collection supporting parallel access is capable of supporting the stronger  trait. It doesn't have some of the more specialized traits present in the serial hierarchy either. This whole hierarchy is found under the directory .The classes implementing parallel collections also differ, with  and  for both mutable and immutable parallel collections, plus  and  implementing  and  implementing .Another hierarchy also exists that mirrors the traits of serial and parallel collections, but with a prefix : , , ,  and . These traits are  to both parallel and serial collections. This means that a method taking a  cannot receive a parallel collection, but a method taking a  is expected to work with both serial and parallel collections.Given the way these hierarchies were structured, code written for Scala 2.8 was fully compatible with Scala 2.9, and demanded serial behavior. Without being rewritten, it cannot take advantage of parallel collections, but the changes required are very small.Any collection can be converted into a parallel one by calling the method  on it. Likewise, any collection can be converted into a serial one by calling the method  on it.If the collection was already of the type requested (parallel or serial), no conversion will take place. If one calls  on a parallel collection or  on a serial collection, however, a new collection with the requested characteristic will be generated.Do not confuse , which turns a collection into a non-parallel collection, with , which returns a  created from the elements of the collection. Calling  on a parallel collection will return a , not a serial collection.While there are many implementing classes and subtraits, there are some basic traits in the hierarchy, each of which providing more methods or more specific guarantees, but reducing the number of classes that could implement them.In the following subsections, I'll give a brief description of the main traits and the idea behind them.This trait is pretty much like trait  described below, but with the limitation that you can only use it . That is, any methods called on a   render it unusable.This limitation makes it possible for the same methods to be shared between the collections and . This makes it possible for a method that works with an  but not using -specific methods to actually be able to work with any collection at all, plus iterators, if rewritten to accept .Because  unifies collections and iterators, it does not appear in the previous graphs, which concern themselves only with collections.At the top of the  hierarchy is trait . Its only abstract operation isThe operation is meant to traverse all elements of the collection, and apply the given operation f to each\nelement. The application is done for its side effect only; in fact any function result of f is discarded by\nforeach.Traversible objects can be finite or infinite. An example of an infinite traversable object is the stream\nof natural numbers . The method  indicates whether a collection is possibly\ninfinite. If  returns true, the collection is certainly finite. If it returns false, the\ncollection has not been not fully elaborated yet, so it might be infinite or finite.This class defines methods which can be efficiently implemented in terms of  (over 40 of them).This trait declares an abstract method  that returns an iterator that yields all the collection\u2019s elements one by one. The  method in  is implemented in terms of . Subclasses of  often override foreach with a direct implementation for efficiency.Class  also adds some less-often used methods to , which can be implemented efficiently only if an  is available. They are summarized below.After  there come three base traits which inherit from it: , , and . All three have an  method and all three implement the  trait, but the meaning of  is different in each case.I trust the meaning of ,  and  is intuitive. After them, the classes break up in specific implementations that offer particular guarantees with regards to performance, and the methods it makes available as a result of it. Also available are some traits with further refinements, such as ,  and .This was done to achieve maximum code reuse. The concrete  implementation for classes with a certain structure (a traversable, a map, etc) is done in the Like classes. The classes intended for general consumption, then, override selected methods that can be optmized.The builder for the classes, ie, the object which knows how to create instances of that class in a way that can be used by methods like , is created by a method in the companion object. So, in order to build an object of type X, I need to get that builder from the companion object of X. Unfortunately, there is no way, in Scala, to get from class X to object X. Because of that, there is a method defined in each instance of X, , which returns the companion object of class X.While there might be some use for such method in normal programs, its target is enabling code reuse in the collection library.You aren't supposed to care about that. They are implicit precisely so that you don't need to figure out how to make it work.These implicits exists to enable the methods on the collections to be defined on parent classes but still return a collection of the same type. For example, the  method is defined on , but if you used on a  you'll get a  back."},
{"body": "In Scala, if I define a method called  in a class or a top-level object, that method will be called whenever I append a pair a parentheses to an instance of that class, and put the appropriate arguments for  in between them. For example:So basically,  is just syntactic sugar for .How does Scala do this conversion?Is there a globally defined implicit conversion going on here, similar to the implicit type conversions in the Predef object (but different in kind)? Or is it some deeper magic? I ask because it seems like Scala strongly favors consistent application of a smaller set of rules, rather than many rules with many exceptions. This initially seems like an exception to me.I don't think there's anything deeper going on than what you have originally said: it's just syntactic sugar whereby the compiler converts  into  as a special syntax case.This might seem like a specific rule, but only a few of these (for example, with ) allows for -like constructs and libraries.It is actually the other way around, an object or class with an apply method is the normal case and a function is way to construct implicitly an object of the same name with an apply method. Actually every function you define is an subobject of the Function trait (n is the number of arguments).Refer to section  on page 77 of the  for more information of the topic.Yes. And this rule belongs to this smaller set."},
{"body": "I'm using IntelliJ IDEA 13 (Community Edition) with the Scala plugin.My initial import of an existing Scala project with a  worked fine.  The library dependencies were picked up by IDEA. Additional dependencies added after the initial import were not picked up, although I had checked the  option.How can I force IDEA to reload the dependencies from ?You can force SBT to reload changes:\nOpen SBT toolwindow (on the right side of IDE) and press refresh button.If you use auto-import feature you need to save your file to force auto-refresh.As of 7 Dec, 2013, the issue is solved.  (this applies to both community and ultimate editions).This is the link to the issue opened on JetBrains developer community web site:\nIF plugin gets 'stuck' it helps to invalidate cache (under file menu) and restart intellij.This may be due to a broken publication of  that occurred tonight, about 4 or 5 hours ago.  Given that your post is 4 hours old ...You can see the ivy activity in  ( or  on OSX). I have opened a ticket at JetBrains.Ticket for broken plublication of sbt.jetbrains is resolved. Updating scala plugin will fix this issue\nTicket : "},
{"body": "How do I get an instance of  in Scala? In Java, I can do this:What would be the equivalent in Scala?There is a method  in  that retrieves the runtime representation of a class type.You can use the  method to get the class object of an instance at runtime in the same manner as Java"},
{"body": "I come from a Python background, where at any point in my code I can addand at runtime I'll be dropped into an interactive interpreter at that spot. Is there an equivalent for scala, or is this not possible at runtime?Yes, you can, on Scala 2.8. Note that, for this to work, you have to include the scala-compiler.jar in your classpath. If you invoke your scala program with , it will be done automatically (or so it seems in the tests I made).You can then use it like this:You may pass multiple  arguments. When the REPL comes up, the value on the right will be bound to a val whose name you provided on the left. For instance, if I change that line like this:Then the execution will happen like this:You continue the execution by typing .You may also unconditionally drop into REPL by invoking , which receives a  of  instead of a vararg. Here's a full example, code and execution:And then:To add to Daniel's answer, as of Scala 2.9, the  and  methods are contained in . Also,  is now .As of Scala 2.10 both  and  have been removed from .To break into interpreter you will have to work with  directly.First add  library. For Eclipse Scala, right click on project =>  =>  => .And then you can use the following where you want to start the interpreter:In Eclipse Scala the interpreter can be used from the  view:"},
{"body": "Is there a good \"scala-esque\" (I guess I mean functional) way of recursively listing files in a directory? What about matching a particular pattern?For example recursively all files matching  in .Scala code typically uses Java classes for dealing with I/O, including reading directories.  So you have to do something like:You could collect all the files and then filter using a regex:Or you could incorporate the regex into the recursive search:I would prefer solution with Streams becouse you can iterate over infinite file system(Streams are lazy evaluated collections)Example for searchingScala is a multi-paradigm language. A good \"scala-esque\" way of iterating a directory would be to reuse an existing code!I'd consider  a perfectly scala-esque way of iterating a directory. You can use some implicit conversions to make it easier. LikeI like yura's stream solution, but it (and the others) recurses into hidden directories. We can also simplify by making use of the fact that  returns null for a non-directory.Now we can list filesor realise the whole stream for later processingApache Commons Io's  fits on one line, and is quite readable:As of Java 1.7 you all should be using java.nio. It offers close-to-native performance (java.io is very slow) and has some useful helpersBut Java 1.8 introduces exactly what you are looking for:You also asked for file matching. Try  and also See documentation here: Take a look at scala.tools.nsc.ioThere are some very useful utilities there including deep listing functionality on the Directory class.If I remember correctly this was highlighted (possibly contributed) by retronym and were seen as a stopgap before io gets a fresh and more complete implementation in the standard library.And here's a mixture of the stream solution from @DuncanMcGregor with the filter from @Rick-777:This gives you a Stream[File] instead of a (potentially huge and very slow) List[File] while letting you decide which sorts of directories to recurse into with the descendCheck() function.I personally like the elegancy and simplicity of @Rex Kerr's proposed solution. But here is what a tail recursive version might look like:How aboutScala has library 'scala.reflect.io' which considered experimental but does the workNo-one has mentioned yet Here's a similar solution to Rex Kerr's, but incorporating a file filter:The method returns a List[File], which is slightly more convenient than Array[File].  It also ignores all directories that are hidden (ie. beginning with '.').It's partially applied using a file filter of your choosing, for example:The simplest Scala-only solution (if you don't mind requiring the Scala compiler library):Otherwise, @Renaud's solution is short and sweet (if you don't mind pulling in Apache Commons FileUtils):Where  is a java.io.File:It seems nobody mentions the  library from scala-incubrator...Or with Or if you want  explicitly...Documentation is available here: This incantation works for me:You can use tail recursion for it:Why are you using Java's File instead of Scala's AbstractFile?With Scala's AbstractFile, the iterator support allows writing a more concise version of James Moore's solution:"},
{"body": "I have noticed a strange behavior of my scala compiler. It occasionally throws an OutOfMemoryError when compiling a class. Here's the error message:It only happens once in a while and the error is usually not thrown on the subsequent compile run. I use Scala 2.9.0 and compile via SBT.Does anybody have a clue as to what might be the cause for this error? Thanks in advance for your insights.The cause for  is that it doesn't have enough  :) If you are using Oracle JVM, you need to add the  (or some other amount of space) argument to your  script. For other JVMs, look at their documentation.I use HomeBrew to install sbt on OS X. It supports a  argument which can be put in  file with .I assumed you're using  0.13.6 or higher. Create  file in your  project's root with the following content: is for Java 8 whereas  is for Java 7. They are critical to prevent out of memory errors related either to  or  exhaustion. Of course, consider adapting flag values or adding any other flags required.More details and alternative approaches can be found in this .I had this issue, played around with it for 10 minutes looking at sites trying to change the memory size.Turns out i resolved it by,Then,This cleared it up for me.It looks like a memory leak in SBT for me as in my case the program compiles and runs successfully for about 3-5 times before hitting the exception which is fixed by SBT restart.The most adequate solution indeed seems to be  JVM parameter as Alexey Romanov suggests or to restart SBT periodically if it helps.But there is another interesting way: try switching to . AFAIK it doesn't use PermGen any more and is probably immune to this exception this way.I still hope SBT authors will address this issue in future versions.I am building with the Jenkins sbt plugin and had the same problems. They were resolved after copying the SBT_OPTS from the sbt file to the Jenkins job config's JVM flags. Originally using a command like:I got first OutOfMemoryError:  which I solved using , and then OutOfMemoryError: , to which  was the remedy. So in my case, a command like this worked:"},
{"body": "Can someone please explain traits in Scala? What are the advantages of traits over extending an abstract class?The short answer is that you can use multiple traits -- they are \"stackable\". Also, traits cannot have constructor parameters.Here's how traits are stacked. Notice that the ordering of the traits are important. They will call each other from right to left.This  gives a good example of trait usage.  One big advantage of traits is that you can extend multiple traits but only one abstract class. Traits solve many of the problems with multiple inheritance but allow code reuse. If you know ruby, traits are similar to mix-insThis is the best example I've seenScala in practice: Composing Traits \u2013 Lego style:\nTraits are useful for mixing functionality into a class.  Take a look at .  Note how you can mix in various domain-specific languages (DSL) into a test class.  look at the quick start guide to look at some of the DSL's supported by Scalatest (  )Similar to interfaces in Java, traits are used to define object types by specifying the signature of the supported methods. Unlike Java, Scala allows traits to be partially implemented; i.e. it is possible to define default implementations for some methods. In contrast to classes, traits may not have constructor parameters.\nTraits are like classes, but which define an interface of functions and fields that classes can supply concrete values and implementations.Traits can inherit from other traits or from classes.   I am quoting from the website of the book  and more specifically the section called \"\" from Chapter 12.There is a bit more information in the above link regarding traits and I suggest you read the full section. I hope this helps."},
{"body": "What are the precise rules for when you can omit (omit) parentheses, dots, braces, = (functions), etc.?For example, (service.findAllPresentations.get.first.votes.size) must be equalTo(2).Why can't I go:?The compiler error is:Why does it think I'm trying to pass in a parameter? Why must I use dots for every method call?Why must  be equalTo(2) result in:Yet, the \"must be equalTo 2\" of\n must be equalTo 2, that is, method chaining works fine? - object chain chain chain param.I've looked through the Scala book and website and can't really find a comprehensive explanation.Is it in fact, as Rob H explains in Stack Overflow question , that the only valid use-case for omitting the '.' is for \"operand operator operand\" style operations, and not for method chaining?You seem to have stumbled upon the answer. Anyway, I'll try to make it clear.You can omit dot when using the prefix, infix and postfix notations -- the so called . While using the operator notation, and only then, you can omit the parenthesis if there is less than two parameters passed to the method.Now, the operator notation is a notation for , which means it can't be used in the absence of the object which is being called.I'll briefly detail the notations.Only , ,  and  can be used in prefix notation. This is the notation you are using when you write  or .That's the notation where the method appears between an object and it's parameters. The arithmetic operators all fit here.That notation is used when the method follows an object . For example, you can write , and that's postfix notation.You can chain infix notation calls without problem, as long as no method is curried. For example, I like to use the following style:That's the same thing as:Now, why am I using parenthesis here, if filter and map take a single parameter? It's because I'm passing anonymous functions to them. I can't mix anonymous functions definitions with infix style because I need a boundary for the end of my anonymous function. Also, the parameter definition of the anonymous function might be interpreted as the last parameter to the infix method.You can use infix with multiple parameters:Curried functions are hard to use with infix notation. The folding functions are a clear example of that:You need to use parenthesis outside the infix call. I'm not sure the exact rules at play here.Now, let's talk about postfix. Postfix can be hard to use, because it . For example, you can't do the following:Because tail does not appear at the end of the expression. You can't do this either:You could use infix notation by using parenthesis to mark end of expressions:Note that postfix notation is discouraged because .I hope this has cleared all the doubts. If not, just drop a comment and I'll see what I can do to improve it. or  can be omitted from class parameters which will make the parameter private.Adding var or val will cause it to be public (that is, method accessors and mutators are generated). can be omitted if the class has no body, that is,Generic parameters can be omitted if they can be inferred by the compiler. However note, if your types don't match, then the type parameter is always infered so that it matches. So without specifying the type, you may not get what you expect - that is, givenThis will give you a type error Whereas this works fine:Because the type parameter, T, is inferred as the least common supertype of the two - Any. can be dropped if the function returns Unit (nothing). for the function body can be dropped if the function is a single statement, but only if the statement returns a value (you need the  sign), that is,but this doesn't work:The return type of the function can be omitted if it can be inferred (a recursive method must have its return type specified). can be dropped if the function doesn't take any arguments, that is,which by convention is reserved for methods which have no side effects - more on that later. isn't actually dropped per se when defining a  paramenter, but it is actually a quite semantically different notation, that is,Says myOp takes a pass-by-name parameter, which results in a String (that is, it can be a code block which returns a string) as opposed to function parameters,which says  takes a function which has zero parameters and returns a String.(Mind you, pass-by-name parameters get compiled into functions; it just makes the syntax nicer.) can be dropped in the function parameter definition if the function only takes one argument, for example:But if it takes more than one argument, you must include the (): can be dropped to use operator notation, which can  be used for infix operators (operators of methods that take arguments). See  for more information.Because this notation is reserved by convention for methods that have no side effects, like List#tail (that is, the invocation of a function with no side effects means that the  function has no observable effect, except for its return value).When calling a function which takes a function, you cannot omit the () from the inner function definition, for example:When calling a function that takes a by-name parameter, you cannot specify the argument as a parameter-less anonymous function. For example, given:You must call it as:orbut not:IMO, overuse of dropping return types can be harmful for code to be re-used. Just look at specification for a good example of reduced readability due to lack of explicit information in the code. The number of levels of indirection to actually figure out what the type of a variable is can be nuts. Hopefully better tools can avert this problem and keep our code concise.A collection of quotes giving insight into the various conditions...Personally, I thought there'd be more in the specification. I'm sure there must be, I'm just not searching for the right words...There are a couple of sources however, and I've collected them together, but nothing really complete / comprehensive / understandable / that explains the above problems to me...:From :From :From blog post :From the language specification:From :But what also confuses me is this quote:Because as far as I can see, there  an object to receive the call...Actually, on second reading, maybe this is the key:As mentioned on the blog post:  .So perhaps this is actually a very strict \"syntax sugar\" which  works where you are effectively . e.g.And nothing else.This would explain my examples in the question.But as I said, if someone could point out to be exactly where in the language spec this is specified, would be great appreciated.Ok, some nice fellow (paulp_ from #scala) has pointed out where in the language spec this information is:Hmm - to me it doesn't mesh with what I'm seeing or I just don't understand it ;)There aren't any. You will likely receive advice around whether or not the function has side-effects. This is bogus. The correction is to not use side-effects to the reasonable extent permitted by Scala. To the extent that it cannot, then all bets are off.  bets. Using parentheses is an element of the set \"all\" and is superfluous. It does not provide any value once all bets are off.This advice is essentially an attempt at an  that fails (not to be confused with: is less useful than other effect systems).Try not to side-effect. After that, accept that all bets are off. Hiding behind a de facto syntactic notation for an effect system can and does, only cause harm.I find it easier to follow this rule of thumb: in expressions spaces alternate between methods and parameters. In your example,  parses as . Note that the parentheses around the 2 have a higher associativity than the spaces. Dots also have higher associativity, so would parse as . parses as . "},
{"body": "In Scala 2.8 is there a way to overload constructors of a case class?If yes, please put a snippet to explain, if not, please explain why?Overloading constructors isn't special for case classes:However, you may like to also overload the  method in the companion object, which is called when you omit .In Scala 2.8, named and default parameters can often be used instead of overloading.You can define an overloaded constructor the usual way, but to invoke it you have to use the \"new\" keyword."},
{"body": "The Eclipse scala plugin has a nice feature which shows you the type of a variable when you hover the mouse over it.  How do I see the same information with the IntelliJ plugin?Select expression and type  + .If you want to change the shortcut go to  and enter \"Type Info\" in the search field. In older versions, it's  +  +  + .CTRL / Command key and hover over the variable or method.You can hit  +  on a variable or method signature to view its type.There is also a neat trick to view type of any expression: select the expression and type  +  + . This will show  dialog with expression type to be extracted shown for convenience.On OS X, it's  +  +  for me. This shows the exact type with resolved generics.Under MacOS with  keybindings, it is supposed to be +, however that does not seem to work at all (for me).In the settings ( and enter \"Type Info\" in the search field), \nI added the shortcut ++ to the action. This shortcut works fine.You can set IntelliJ up sort of like eclipse. Go to:and then tick \"Show type info on mouse hover after, ms\" and set your preferred timeout.This works well except that other messages seem to take preference. The most common being \"Declaration is never used\" which is quite a lot of the time if you have just written a  and want to see it's type. Then you have to resort to ( + ) for PC or ( +  + ) forMac.Hold down the Command key as you mouseover.  Note: I tested this on Mac OS X with standard keybindings.  Different OSes or non-standard keybindings may of course be different, but the functionality is certainly there.There's good tips gathered around here, but since the mappings vary by the keyboard bindings, here's a summary for the  bindings.Note: The  bindings are  not the default in IntelliJ 13.1, but they really should be. They have a more native OS X feel than the old  bindings. You can activate them via  (in the search field) > The real usability problem with these (as of IntelliJ 13.1) is that one needs to hold the key,  move the cursor. It should ideally also work when you have the cursor already on the expression, then start keeping ctrl/alt/cmd pressed in. I'll request them for this.ps. I know it's OS X, officially, not Mac OS X. IntelliJ IDEA still uses the old naming at least for the keycaps."},
{"body": "I have a handy function that I've used in Java for converting an InputStream to a String.  Here is a direct translation to Scala:Is there an idiomatic way to do this in scala?does pretty much the same thing.  Not sure why you want to get lines and then glue them all back together, though.  If you can assume the stream's nonblocking, you could just use , read the whole thing into a byte array, and create a string from that directly.Edit: in 2.11, a performance bug with  on  has been fixed, so now it's just\nwill also do the deed.....Faster way to do this:"},
{"body": "In Scala, we can use at least two methods to retrofit existing or new types. Suppose we want to express that something can be quantified using an . We can define the following trait.And then we can use implicit conversions to quantify e.g. Strings and Lists.After importing these, we can call the method  on strings and lists. Note that the quantifiable list stores its length, so it avoids the expensive traversal of the list on subsequent calls to .An alternative is to define a \"witness\"  that states, that some type  can be quantified.We then provide instances of this type class for  and  somewhere.And if we then write a method that needs to quantify its arguments, we write:Or using the context bound syntax:Now comes the question. How can I decide between those two concepts?What I have noticed so far.Present one (or more) use case(s) where the difference between both concepts matters and explain why I would prefer one over the other. Also explaining the essence of the two concepts and their relation to each other would be nice, even without example.While I don't want to duplicate my material from ,  I think it's worth noting that type classes / type traits are infinitely more flexible.has the ability to search its local environment for a default type class.   However, I can override default behavior at any time by one of two ways:Here's an example:This makes type classes infinitely more flexible.   Another thing is that type classes / traits support implicit  better.In your first example, if you use an implicit view, the compiler will do an implicit lookup for:Which will look at 's companion object and the  companion object.Notice that  is  in the implicit lookup.   This means you have to place the implicit view in a package object  import it into scope.   It's more work to remember what's going on.On the other hand, a type class is .  You see what it's looking for in the method signature.   You also have an implicit lookup ofwhich will look in 's companion object  's companion object.   Meaning that you can provide defaults  new types (like a  class) can provide a default in their companion object and it will be implicitly searched.In general, I use type classes.   They are infinitely more flexible for the initial example.   The only place I use implicit conversions is when using an API layer between a Scala wrapper and a Java library, and even this can be 'dangerous' if you're not careful.One criterion that can come into play is how you want the new feature to \"feel\" like; using implicit conversions, you can make it look like it is just another method:...while using type classes it will always look like it you are calling an external function:One thing that you can achieve with type classes and not with implicit conversions is adding properties to a , rather than to an instance of a type. You can then access these properties even when you do not have an instance of the type available. A canonical example would be:This example also shows how the concepts are tightly related: type classes would not be nearly as useful if there were no mechanism to produce infinitely many of their instances; without the  method (not a conversion, admittedly), I could only have finitely many types have the  property.You can think of the difference between the two techniques by analogy to function application, just with a named wrapper.  For example:An instance of the former encapsulates a function of type , whereas an instance of the latter has already been applied to an .  You could continue the pattern...thus you could think of  sort of like the partial application of  to some  instance.  A great example of this was written up by Miles Sabin as .So really my point is that, in principle:"},
{"body": "I just finished , and I've been looking into the changes between Scala 2.7 and 2.8. The one that seems to be the most important is the continuations plugin, but I don't understand what it's useful for or how it works. I've seen that it's good for asynchronous I/O, but I haven't been able to find out why. Some of the more popular resources on the subject are these:And this question on Stack\u00a0Overflow:Unfortunately, none of these references try to define what continuations are for or what the shift/reset functions are supposed to do, and I haven't found any references that do.  I haven't been able to guess how any of the examples in the linked articles work (or what they do), so one way to help me out could be to go line-by-line through one of those samples. Even this simple one from the third article:Why is the result 8? That would probably help me to get started.My  does explain what  and  do, so you may want to read that again.Another good source, which I also point in my blog, is the Wikipedia entry on . That one is, by far, the most clear on the subject, though it does not use Scala syntax, and the continuation is explicitly passed.The paper on delimited continuations, which I link to in my blog but seems to have become broken, gives many examples of usage.But I think the best example of the  of delimited continuations is Scala Swarm. In it, the library  the execution of your code at one point, and the remaining computation becomes the continuation. The library then does something -- in this case, transferring the computation to another host, and returns the result (the value of the variable which was accessed) to the computation that was stopped.Now, you don't understand even the simple example on the Scala page, so  read my blog. In it I'm  concerned with explaining these basics, of why the result is .I found the existing explanations to be less effective at explaining the concept than I would hope.  I hope this one is clear (and correct.)  I have not used continuations yet.When a continuation function  is called:So in this example, follow the letters from A to ZThis prints:Continuation capture the state of a computation, to be invoked later.Think of the computation between leaving the shift expression and leaving the reset expression as a function. Inside the shift expression this function is called k, it is the continuation. You can pass it around, invoke it later, even more than once.I think the value returned by the reset expression is the value of the expression inside the shift expression after the =>, but about this I'm not quite sure.So with continuations you can wrap up a rather arbitrary and non-local piece of code in a function. This can be used to implement non-standard control flow, such as coroutining or backtracking.So continuations should be used on a system level. Sprinkling them through your application code would be a sure recipe for nightmares, much worse than the worst spaghetti code using goto could ever be. I have no in depth understanding of continuations in Scala, I just inferred it from looking at the examples and knowing continuations from Scheme.Given the canonical example from the  for Scala's delimited continuations, modified slightly so the function input to  is given the name  and thus is no longer anonymous.The Scala plugin transforms this example such that the computation (within the input argument of ) starting from each  to the invocation of   is  with the function (e.g. ) input to .The replaced computation is  (i.e. moved) into a function . The function  inputs the function , where   the replaced computation,  inputs , and the computation in  replaces  with .Which has the same effect as:Note the type  of the input parameter  (i.e. the type signature of ) was given by the type signature of the input parameter of .Another  example with the conceptually equivalent abstraction, i.e.  is the function input to :I believe this would be translated to the logical equivalent of:I hope this elucidates the coherent common abstraction which was somewhat obfuscated by prior presentation of these two examples. For example, the canonical first example was presented in the  as an anonymous function, instead of my named , thus it was not immediately clear to some readers that it was abstractly analogous to the  in the  second example.Thus delimited continuations create the illusion of an inversion-of-control from \"you call me from outside of \" to \"I call you inside \".Note the return type of  is, but  is not, required to be the same as the return type of , i.e.  has the freedom to declare any return type for  as long as  returns the same type as . Ditto for  and  (see also  below).Delimited continuations do not implicitly invert the control of state, e.g.  and  are not pure functions. Thus the caller can not create referentially transparent expressions and thus does not have .We can explicitly achieve pure functions with delimited continuations.I believe this would be translated to the logical equivalent of:This is getting noisy, because of the explicit environment.Tangentially note, Scala does not have Haskell's global type inference and thus as far as I know couldn't support implicit lifting to a state monad's  (as one possible strategy for hiding the explicit environment), because Haskell's global (Hindley-Milner) type inference depends on .From my point of view, the best explanation was given here: One of examples:Another (more recent -- May 2016) article on Scala continuations is:\n\"\" by \n.\nIt also refers to 's  mentioned in 's .But before that, it describes Continuations like so:That being said, as announced in "},
{"body": "I'm looking for a good open source library for scala for math and statistics. Hopefully something like Apache Math or Colt, but implemented in Scala. Can anyone point me in the right direction? Yes, there are some:by twitter for graph processing:Abstract algebra library from twitter:! has experimental status !Figaro is a Scala library for Probabilistic Programming. You could find more information about Figaro here Figaro is available for download from The author of this library is currently writing a book on Probabilistic Programming using Figaro. Here is the link to the book page: "},
{"body": "With equals sign:Without equals sign:Both of the above programs execute the same way. In the blog post  I read that when the equals sign are missing, the method will return  (same as Java's ), so methods that return a value must use the equals sign. But methods that don't return a value can be written either way.What is the best practice for using the equals sign in Scala methods that don't return a value?I actually disagree pretty strongly with Daniel. I think the non-equal syntax should  be used. If your method is being exposed as an API and you're worried about accidentally returning the wrong type, add an explicit type annotation:The non-equal syntax is shorter and might look \"cleaner\", but I think it just adds the possibility of confusion. I have sometimes forgotten to add an equal sign and believed my method was returning a value when actually it was returning Unit. Because the non-equal and equal-with-inferred-type syntaxes are so visually similar, it's easy to miss this problem.Even though it costs me a little more work, I prefer the explicit type annotations when they matter (namely, exposed interfaces).UPDATE: as of Scala-2.10, using equals sign is preferred. Old answer:Methods which return  should  use the non-equals syntax.  This avoids potential mistakes in implementation carrying over into the API.  For example, you could have accidentally done something like this:Trivial example of course, but you can see how this might be a problem.  Because the last expression does  return , the method itself will have a return type other than .  This is exposed in the public API, and might cause other problems down the road.  With the non-equals syntax, it doesn't matter what the last expression is, Scala fixes the return type as .It's also two characters cleaner.  :-)  I also tend to think that the non-equals syntax makes the code just a little easier to read.  It is more obvious that the method in question returns  rather than some useful value.On a related note, there is an analogous syntax for abstract methods:The method  has signature .  Scala does this when you omit the type annotation on an abstract member.  Once again, this is cleaner, and (I think) easier to read.You  use equals sign in call declarations except the definitions returning Unit.In this latter case, you  forgo the equals sign. This syntax may be deprecated, though, so it's best avoided. Using equals sign and declaring the return type will always work.For methods,  recommends the equals syntax as opposed to procedure syntaxOne thing : imagine the last statement of a method that should return Unit does not return Unit. Using the non-equal syntax is then very convenient, I hope this will not be deprecated as I see several use case for itAs time as progressed the default style has changed, and this has been mentioned in many of the comments to answers, it is recommended in the  to use the  syntax for function declarations.for method that dont have a return value,  a way to express such methods is leaving out the result type and the\nequals sign, following the method with a block enclosed in curly braces. In\nthis form, the method looks like a procedure, a method that is executed only\nfor its side effects. "},
{"body": "Is there a case where a companion object (singleton) for a class is needed? Why would I want to create a class, say  and also create a companion object for it?The companion object basically provides a place where one can put \"static-like\" methods. Furthermore, a companion object, or companion module, has full access to the class members, including private ones.Companion objects are great for encapsulating things like factory methods. Instead of having to have, for example,  and  everywhere, you can have a class with a companion object take on the factory responsibilities.Companion objects are useful for storing state and methods that are common to all instances of a class  methods or fields. They use regular virtual methods which can be overridden through inheritance. Scala truly has nothing static. There are lots of ways you can use this but here's a simple example.Which produces this output:...and it's a good place to store static factory methods (not that DP) for accompanied classes. If you name those overloaded factory methods apply(//) you will be able to create/initialize you class Example code:I wouldn't call the object/base class AbstractXxxxx because it doesn't looks bad: like creating something abstract. Give those names a real meaning.\nConsider using immutable, method less, case classes and seal the abstract base class. In addition to the things Saem said in , the Scala compiler also looks for  of types in the corresponding companion objects (of either the source or the target), so the conversions don't need to be imported.About the reason for singleton objects in general  says:I always see companion objects as a  to write both functional and object oriented code in Scala. Many times we just need pure functions which take some input and provide a processing result. Putting those relevant functions in the companion object makes it easy to look up and use, for myself as well as some one building on top of my code. Moreover, it is a language provided feature to write the singleton pattern without doing anything. This is especially useful when you need a singleton to encapsulate a delegator for the life of JVM. For example, writing a simple HTTP client library in Scala where you can encapsulate an underlying Java implementation based delegator and let consumers of your API live in pure world."},
{"body": "How do I split a sequence into two lists by a predicate?Alternative: I can use  and , or write my own method, but isn't there a better more general (built-in) method ?By using  method: Good that  was the thing you wanted -- there's another method that also uses a predicate to split a list in two: .The first one,  will put all \"true\" elements in one list, and the others in the second list. will put all elements in one list until an element is \"false\" (in terms of the predicate). From that point forward, it will put the elements in the second list.You might want to take a look at  - it allows you to search the scala standard library for functions by their signature. For example, type the following:You would see .You can also use foldLeft if you need something a little extra.  I just wrote some code like this when partition didn't cut it:If you want to split a list into more than 2 pieces, and ignore the bounds, you could use something like this (modify if you need to search for ints)"},
{"body": "I'm looking for a way to convert an arbitrary length list of Futures to a Future of List. I'm using Playframework, so ultimately, what I really want is a , but to make things simpler, let's just say  The normal way to do this would be to use  but there's a twist... The list I'm given usually has around 10-20 futures in it, and it's not uncommon for one of those futures to fail (they are making external web service requests). Instead of having to retry all of them in the event that one of them fails, I'd like to be able to get at the ones that succeeded and return those.   For example, doing the following doesn't workInstead of getting the only the exception, I'd like to be able to pull the 1 and 3 out of there. I tried using , but that apparently just calls  behind the scenes. Thanks in advance for the help!The trick is to first make sure that none of the futures has failed.   is your friend here, you can combine it with  to convert all the  results to  instances, all of which are certain to be successful futures.Then use  as before, to give you a Then filter:You can even pull out the specific failures, if you need them:I tried Kevin's answer, and I ran into a glitch on my version of Scala (2.11.5)...  I corrected that, and wrote a few additional tests if anyone is interested... here is my version >  I just came across this question and have another solution to offer:The idea here is that within the fold you are waiting for the next element in the list to complete (using the for-comprehension syntax) and if the next one fails you just fallback to what you already have."},
{"body": "I am not able to understand the point of  class in Scala. I mean, I am not able to see any advanages of  over .For example, consider the code:Now suppose, the method  returns , then the call made to  on first line of  is bound to fail with . Similarly if  returns , the  call will again fail with some similar error.If so, then why does Scala complicate things by introducing a new value wrapper () instead of following a simple approach used in Java?I have edited my code as per 's suggestion. I am still not able to see any particular advantage of . I have to test for the exceptional  or  in both cases. :(If I have understood correctly from , is the only advantage of  is that it explicitly tells the programmer that ? Is this the only reason behind this design choice?You'll get the point of  better if you force yourself to never, ever, use . That's because  is the equivalent of \"ok, send me back to null-land\".So, take that example of yours. How would you call  without using ? Here are some alternatives:None of this alternatives will let you call  on something that does not exist.As for why  exists, Scala doesn't tell you how your code should be written. It may gently prod you, but if you want to fall back to no safety net, it's your choice.You nailed it here:Except for the \"only\". But let me restate that in another way: the  advantage of  over  is type safety. It ensures you won't be sending a  method to an object that may not exist, as the compiler won't let you.You said you have to test for nullability in both cases, but if you forget -- or don't know -- you have to check for null, will the compiler tell you? Or will your users?Of course, because of its interoperability with Java, Scala allows nulls just as Java does. So if you use Java libraries, if you use badly written Scala libraries, or if you use badly written  Scala libraries, you'll still have to deal with null pointers.Other two important advantages of  I can think of are:The latter one takes much longer to fully appreciate, and it's not well suited to simple examples, as it only shows its strength on complex code. So, I'll give an example below, but I'm well aware it will hardly mean anything except for the people who get it already.Compare:with:The monadic property , which appears in Scala as the  function, allows us to chain operations on objects without worrying about whether they are 'null' or not.Take this simple example a little further. Say we wanted to find all the favourite colours of a list of people.Or perhaps we would like to find the name of a person's father's mother's sister:I hope this sheds some light on how options can make life a little easier.The difference is subtle. Keep in mind to be truly a function it  return a value - null is not really considered to be a \"normal return value\" in that sense, more a /nothing. But, in a practical sense, when you call a function that optionally returns something, you would do: Granted, you can do something similar with null - but this makes the semantics of calling  obvious by virtue of the fact it returns  (a nice practical thing, other than relying on someone reading the doc and getting an NPE because they don't read the doc). I will try and dig up a functional programmer who can give a stricter answer than I can.For me options are really interesting when handled with for comprehension syntax. Taking  preceding example:If any of the assignation are , the  will be  but no  will be raised. You can then safely pass to a function taking Option parameters without worrying. so you don't check for null and you don't care of exceptions. Compare this to the java version presented in  example.You have pretty powerful composition capabilities with Option:Maybe someone else pointed this out, but I didn't see it:One advantage of pattern-matching with Option[T] vs. null checking is that Option is a sealed class, so the Scala compiler will issue a warning if you neglect to code either the Some or the None case. There is a compiler flag to the compiler that will turn warnings into errors. So it's possible to prevent the failure to handle the \"doesn't exist\" case at compile time rather than at runtime. This is an enormous advantage over the use of the null value.It's not there to help avoid a null check, it's there to force a null check.  The point becomes clear when your class has 10 fields, two of which could be null.  And your system has 50 other similar classes.  In the Java world, you try to prevent NPEs on those fields using some combination of mental horesepower, naming convention, or maybe even annotations.  And every Java dev fails at this to a significant degree.  The Option class not only makes \"nullable\" values visually clear to any developers trying to understand the code, but allows the compiler to enforce this previously unspoken contract. [ copied from  by  ]One point that nobody else here seems to have raised is that while you can have a null reference, there is a distinction introduced by Option.That is you can have , which would be inhabited by ,  and  where  is one of the usual inhabitants of . This means that if you have some kind of container, and want to be able to store null pointers in it, and get them out, you need to pass back some extra boolean value to know if you actually got a value out. Warts like this  in the java containers APIs and some lock-free variants can't even provide them. is a one-off construction, it doesn't compose with itself, it is only available for reference types, and it forces you to reason in a non-total fashion.For instance, when you checkyou have to carry around in your head throughout the  branch that  and that this has already been checked. However, when using something like optionyou  y is not by construction -- and you'd know it wasn't  either, if it weren't for Hoare's .Option[T] is a monad, which is really useful when you using high-order functions to manipulate values.I'll suggest you read articles listed below, they are really good articles that show you why Option[T] is useful and how can it be used in functional way.Adding on to Randall's , understanding why the potential absence of a value is represented by  requires understanding what  shares with many other types in Scala\u2014specifically, types modeling monads. If one represents the absence of a value with null, that absence-presence distinction can't participate in the contracts shared by the other monadic types.If you don't know what monads are, or if you don't notice how they're represented in Scala's library, you won't see what  plays along with, and you can't see what you're missing out on. There are many benefits to using  instead of null that would be noteworthy even in the absence of any monad concept (I discuss some of them in the \"Cost of Option / Some vs null\"  mailing list thread ), but talking about it isolation is kind of like talking about a particular linked list implementation's iterator type, wondering why it's necessary, all the while missing out on the more general container/iterator/algorithm interface. There's a broader interface at work here too, and  provides a presence-and-absence model of that interface.I think the key is found in Synesso's answer: Option is  primarily useful as a cumbersome alias for null, but as a full-fledged object that can then help you out with your logic.The problem with null is that it is the  of an object.  It has no methods that might help you deal with it (though as a language designer you can add increasingly long lists of features to your language that emulate an object if you really feel like it).One thing Option can do, as you've demonstrated, is to emulate null; you then have to test for the extraordinary value \"None\" instead of the extraordinary value \"null\".  If you forget, in either case, bad things will happen.  Option does make it less likely to happen by accident, since you have to type \"get\" (which should remind you that it  null, er, I mean None), but this is a small benefit in exchange for an extra wrapper object.Where Option really starts to show its power is helping you deal with the concept of I-wanted-something-but-I-don't-actually-have-one.Let's consider some things you might want to do with things that might be null.Maybe you want to set a default value if you have a null.  Let's compare Java and Scala:In place of a somewhat cumbersome ?: construct we have a method that deals with the idea of \"use a default value if I'm null\".  This cleans up your code a little bit.Maybe you want to create a new object only if you have a real value.  Compare:Scala is slightly shorter and again avoids sources of error.  Then consider the cumulative benefit when you need to chain things together as shown in the examples by Synesso, Daniel, and paradigmatic.It isn't a  improvement, but if you add everything up, it's well worth it everywhere save very high-performance code (where you want to avoid even the tiny overhead of creating the Some(x) wrapper object).The match usage isn't really that helpful on its own except as a device to alert you about the null/None case.  When it is really helpful is when you start chaining it, e.g., if you have a list of options:Now you get to fold the None cases and the List-is-empty cases all together in one handy statement that pulls out exactly the value you want.It is really a programming style question. Using Functional Java, or by writing your own helper methods, you could have your Option functionality but not abandon the Java language:Just because Scala includes it by default doesn't make it special. Most aspects of functional languages are available in that library and it can coexist nicely with other Java code.  Just as you can choose to program Scala with nulls you can choose to program Java without them.Null return values are only present for compatibility with Java. You should not use them otherwise.Admitting in advance that it is a glib answer, Option is a monad.Actually I share the doubt with you. About Option it really bothers me that 1) there is a performance overhead, as there is a lor of \"Some\" wrappers created everywehre. 2) I have to use a lot of Some and Option in my code. So to see advantages and disadvantages of this language design decision we should take into consideration alternatives. As Java just ignores the problem of nullability, it's not an alternative. The actual alternative provides Fantom programming language. There are nullable and non-nullable types there and ?. ?: operators instead of Scala's map/flatMap/getOrElse. I see the following bullets in the comparison:Option's advantage:Nullable's advantage:So there is no obvious winner here. And one more note. There is no principal syntactic advantage for using Option. You can define something like:Or use some implicit conversions to get pritty syntax with dots.The real advantage of having explicit option types is that you are able to  use them in 98% of all places, and thus statically preclude null exceptions. (And in the other 2% the type system reminds you to check properly when you actually access them.)Another situation where Option works, is in situations where types are not able to have a null value. It is not possible to store null in an Int, Float, Double, etc. value, but with an Option you can use the None.In Java, you would need to use the boxed versions (Integer, ...) of those types."},
{"body": "What's the easiest way to debug Scala code managed by sbt using IntelliJ's built-in debugger? The documentation from  lists commands for running the main class for a project or the tests, but there seem to be no commands for debugging.Follow-up question: what's the easiest way to attach IntelliJ's debugger to Jetty when using sbt's jetty-run command?For ordinary debugging in IntelliJ, you can use an Application run/debug configuration in the usual way, regardless of whether you're using sbt to compile your code.To connect to your application running in Jetty, you'll need to create a Remote debug configuration.  When you do so, IntelliJ will give you a set of command line arguments for running the remote JVM -- something likeLaunch sbt with these arguments and then execute . Finally, launch your remote debug configuration in IntelliJ.  might be useful.There's a very convenient  flag in the official  for Mac, Linux & . You can use the flag to specify the debug port:, this starts the JVM for SBT with the typical verbose debugging incantation:You now can run your code as normal, for example with the sbt  command.Now you connect IntelliJ to your running process using a . Note that the upper 3 fields in this form, while scary, are just for you to copy text  of, rather than into (they're giving the verbose debugging incantation specified above, which  already takes care of for you) - the only configuration you can change is in the section halfway down:I had some trouble with this too, so at the risk of being overly detailed, here's what I did:I am adding another answer here, because I found this question when looking up a related problem: Debugging test classes using breakpoints.I am using ScalaTest, and I typically run a suite using sbt's 'test-only' command. Now when I want to use the interactive debugger, I can do the following:Create a new Run/Debug Configuration of type 'ScalaTest', put the main \"Test Class:\" name, and choose \"Before launch: Run SBT Action 'test-compile'\". That's it, you can place breakpoints now in the test sources, and run this configuration.None of these answers or provided links worked for me, so once I figured this out, I figured I'd share...including the background understanding I didn't have when I started...This is based mostly on the instructions  just with additional explanation that carried me through it.My Environment:\nScala 2.10.2, SBT 0.13, and IntelliJ 13.1Background:What is Debugging?What is Remote Debugging?Referencing the link above, the following explanations/modifications are useful:This one works for me every time, and the only thing you need to set up is remote debugging in IntelliJ; I start up SBT with JVM parameters from the terminal in IntelliJ:After that I can start remote debugging on localhost:5005Using Scala 2.10 and  SBT 0.12, I got it working like this: I've been struggling with debugging too on Windows with a Spray-can / Akka / Scala app built via SBT, using Intellij.  Piecing together various suggestions, the simplest approach for me was:In particular use the suspend=y option.  This will hold the app until you connect a remote debugger from IntellijFor what it's worth Windows folk, edit  and locate the following lines of code:then replace them with this code:Best I can do to get same behaviour for  when seen in the Bash script launcherNB. I don't think  actually exists outside this script, unless you created explicitly in you environment, but anyway you get the point :DFile->Settings->Other Settings->SBT \n VM parameters\n    -Xmx512M -XX:MaxPermSize=256M -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=5005Run->Edit Configurations\n      Press + and then select remote\n      Press ApplyNow in the SBT console (Started inside by intelliJ) when you execute the command 'run'  You will see \"Listening for transport dt_socket at address: 5005\"Now press Run->Debug. You will see the debug menus below activated. It has two tabs Debugger and Console.Use F7 to from next line to nextI chose suspend to be n. With it being y when I ran the run command it was stuckI also got the same problem, I like to share how I resolved. By the way I am using Ubuntu 14.04 and IntelliJ 15. you should see this line in terminal: \"Listening for transport dt_socket at address: 5005\"\n3. Edit Configurations -> Click + -> Select 'Remote' You should see Debug View opened and in Debug Console You should able to see \"Connected to the target VM, address: , transport: 'socket'\"ex: com.myproject.module.AddServiceTest[Spec]I hope It will help for you "},
{"body": "In Scala, you often use an iterator to do a  loop in an increasing order like:How would you do it so it goes from 10 to 1? I guess  gives an empty iterator (like usual range mathematics)?I made a Scala script which solves it by calling reverse on the iterator, but it's not nice in my opinion, is the following the way to go?The answer from @Randall is good as gold, but for sake of completion I wanted to add a couple of variations: Having programmed in Pascal, I find this definition nice to use:Used this way:Scala provides many ways to work on downwards in loop. 1st Solution: with \"to\" and \"by\" 2nd Solution:  With \"to\" and \"reverse\" 3rd Solution: with \"to\" only"},
{"body": "I prefer Python over Scala. But, as Spark is natively written in Scala, I was expecting my code to run faster in the Scala than the Python version for obvious reasons.With that assumption, I thought to learn & write the Scala version of some very common preprocessing code for some 1\u00a0GB of data. Data is picked from the SpringLeaf competition on . Just to give an overview of the data (it contains 1936 dimensions and 145232 rows). Data is composed of various types e.g. int, float, string, boolean. I am using 6 cores out of 8 for Spark processing; that's why I used minPartitions=6 so that every core has something to process.\nStage 0 (38 mins), Stage 1 (18 sec)\n\nStage 0 (11 mins), Stage 1 (7 sec)\nBoth produces different DAG visualisation graphs (due to which both pics show different stage 0 functions for Scala (map) and Python (reduceByKey))But, essentially both code tries to transform data into (dimension_id, string of list of values) RDD and save to disk. The output will be used to compute various statistics for each dimension.Performance wise, Scala code for this real data like this seems to run  than the Python version.\nGood news for me is that it gave me good motivation to stay with Python. Bad news is I didn't quite understand why?The original answer discussing the code can be found below.First of all, you have to distinguish between different types of API, each with its own performance considerations.This is the component which will be most affected by the performance of the Python code and the details of PySpark implementation. While Python performance is rather unlikely to be a problem, there at least few factors you have to consider: Basic considerations are pretty much the same as before with a few additional issues. While basic structures used with MLlib are plain Python RDD objects, all algorithms are executed directly using Scala.It means an additional cost of converting Python objects to Scala objects and the other way around, increased memory usage and some additional limitations we'll cover later.As of now (Spark 2.x), the RDD-based API is in a maintenance mode and is .These are probably the best choice for standard data processing tasks. Since Python code is mostly limited to high-level logical operations on the driver, there should be no performance difference between Python and Scala.A single exception is Python UDFs which significantly less efficient than their Scala equivalents. While there is some chance for improvements (there has been substantial development in Spark 2.0.0), the biggest limitation is full roundtrip between internal representation (JVM) and Python interpreter. If possible, you should favor a composition of built-in expressions (. Python UDF behavior has been improved in Spark 2.0.0, but it is still suboptimal compared to native execution.Also be sure to avoid unnecessary passing data between  and . This requires expensive serialization and deserialization, not to mention data transfer to and from Python interpreter.It is worth noting that Py4J calls have pretty high latency. This includes simple calls like:Usually, it shouldn't matter (overhead is constant and doesn't depend on the amount of data) but in the case of soft real-time applications, you may consider caching/reusing Java wrappers. As for now (Spark  2.1) neither one provides PySpark API so you can say that PySpark is infinitely worse than Scala.In practice, GraphX development stopped almost completely and the project is currently in the maintenance mode with .  library provides an alternative graph processing library with Python bindings.Subjectively speaking there is not much place for statically typed  in Python and even if there was the current Scala implementation is too simplistic and doesn't provide the same performance benefits as .From what I've seen so far, I would strongly recommend using Scala over Python. It may change in the future if PySpark gets support for structured streams but right now Scala API seems to be much more robust, comprehensive and efficient. My experience is quite limited.Structured streaming in Spark 2.x seem to reduce the gap between languages but for now it is still in its early days. Nevertheless, RDD based API is already referenced as \"legacy streaming\" in the  (date of access 2017-03-03)) so it reasonable to expect further unification efforts.Not all Spark features are exposed through PySpark API. Be sure to check if the parts you need are already implemented and try to understand possible limitations. It is particularly important when you use MLlib and similar mixed contexts (see ). To be fair some parts of the PySpark API, like , provides a more comprehensive set of methods than Scala.The PySpark API closely reflects its Scala counterpart and as such is not exactly Pythonic. It means that it is pretty easy to map between languages but at the same time, Python code can be significantly harder to understand.PySpark data flow is relatively complex compared to pure JVM execution. It is much harder to reason about PySpark programs or debug. Moreover at least basic understanding of Scala and JVM in general is pretty much a must have.The Spark DataFrame (SQL, Dataset) API provides an elegant way to integrate Scala/Java code in PySpark application. You can use  to expose data to a native JVM code and read back the results. I've explained some options  and you can find a working example of Python-Scala roundtrip in .It can be further augmented by introducing User Defined Types (see ).First of all, there is one part in your code which doesn't make sense at all. If you already have  pairs created using  or  what is the point in creating string just to split it right afterwards?  doesn't work recursively so you can simply yield tuples and skip following  whatsoever.Another part I find problematic is . Generally speaking,  is useful if applying aggregate function can reduce the amount of data that has to be shuffled. Since you simply concatenate strings there is nothing to gain here. Ignoring low-level stuff, like the number of references, the amount of data you have to transfer is exactly the same as for .Normally I wouldn't dwell on that, but as far as I can tell it is a bottleneck in your Scala code. Joining strings on JVM is a rather expensive operation (see for example: ). It means that something like this   which is equivalent to  in your code is not a good idea.If you want to avoid  you can try to use  with . Something similar to this should do the trick:but I doubt it is worth all the fuss.Keeping the above in mind, I've rewritten your code as follows:::In  mode (Intel(R) Xeon(R) CPU E3-1245 V2 @ 3.40GHz) with 4GB memory per executor it takes (n = 3):I am pretty sure that most of that time is spent on shuffling, serializing, deserializing and other secondary tasks. Just for fun, here's naive single-threaded code in Python that performs the same task on this machine in less than a minute:"},
{"body": "I have already read various accounts of Clojure vs. Scala and while I realize that both have their place.  There are a few considerations that I haven't acquired a complete explanation on when it comes to comparing both Clojure with Scala:1.) Which of the two languages is generally ?  I realize that this will vary from one language feature to another but an general assessment of performance would be helpful.  For example: I know that Python dictionaries are really fast.  But as a whole, it is a  slower language than Java.  I don't want to go with Clojure and run into this problem down the road.2.) How is interoperability with Java?  All I have read so far is that Scala has native collections types that make it a bit clumsy to integrate with a large Java code-base, whereas Clojure follows a simple Iterable/Iterator-centric way to inter-operate with Java classes.  Any more thoughts/details on this?Ultimately, if it is a close enough draw between clojure and scala, I might try them both.  One thing about Clojure is the language seems  simple.  But then again, Scala has a very flexible type system.  But, I know that Scala is fast (based on multiple personal accounts).  So, if Clojure is significantly slower: I'd like to know sooner rather than later.I think either language will be fast enough for you. When comparing Python and Java, it seems a bit unreasonable to blame the language for the speed difference. Java is compiled JIT (except on mobile devices*) whereas Python is interpreted. Just because both use a bytecode does not mean the implementations will have even remotely comparable performance. But both Scala and Clojure are JVM languages so they should have similar performance.Scala has a few implementation advantages over Clojure and I would expect somewhat higher performance. Although Scala's static typing would normally translate into a speed advantage over Clojure's duck typing, Clojure  support type hinting which can speed up code considerably. Possibly, ordinary Scala is faster than ordinary Clojure, but you only need to optimize the bottlenecks. Most of a program's run time is generated by a small amount of the actual code.Regarding interop w/ Java, Scala is closer to Java but I'm sure both languages interoperate well. In  Stuart Halloway writes: \"[you can access] \".And since Scala author Martin Odersky  Sun's Java compiler, I kinda think no balls have been dropped on the Scala side, either. :-)You would be hard-pressed to pick two better languages, though I like Ruby also. Why are you worried about which one to try?  Why not try them both? Scala is more likely to be \"the next Java\", while it's hard to imagine that Lisp will finally take off after not doing so for over 50 years. But it's clear that Lisp is on its own unique level of abstraction, and Clojure is fairly simple, so Scala + Clojure won't be that much harder than just (the rather complex) Scala and I'm sure you will be glad you did it. And for that matter they interoperate...* dalvik (android's JVM) got a JIT compiler in 2.2 version in 2010 With the present JVM Scala has an advantage on the account of being statically typed, as JVM support for dynamic typing -- reflection -- is slow. In fact, one Scala feature which must be implemented through the same techniques, structural types, is often warned against for this very reason.Also, Scala accepts mutable objects just fine, and some algorithms are just faster to implement with mutability.As both Scala and Java are essentially class-based languages, they interoperate more easily. Or, perhaps, more seamlessly. A Java class is a class to Scala, and a Scala class is a class to Java. Problems might arise when it comes to Scala's singletons or Java's static members, particularly when there's a framework involved expecting things to work in a certain way.So I'd go with Scala on both  accounts. Clojure is, in many ways, a better , and it certainly has very interesting features not present (so far) on Scala, but you reap such benefits by going fully functional. If you intend to do that, then Clojure is very likely better. If you don't, then you should probably stay with Scala.Note that Clojure and Scala are two totally different types of programming languages - Clojure is a functional Lisp-like language, it is  object oriented. Scala is an object oriented language which has functional programming features.In my opinion, the features and concepts of a language (functional, OO, ...) are much more important criteria for choosing a language than the performance (of a particular implementation of that language) - altough I understand that you don't want to get trapped into a language for which there is no well-performing implementation available.I'd go for Scala, because it is object oriented but also allows you to learn functional programming (if you're interested in that). On the other hand, if you don't care about OO and you want to learn \"pure\" functional programming, try Clojure.The stats produced by the \"\" are about the best you're probably going to find.They are in-depth and you can compare many languages.  The problem is that they don't cover Clojure :(That said, it's pretty easy to submit anything--it's all open source.The stats do say that Scala is pretty damn quick.If your code is time-critical or space-critical throughout, stick to Java. But it isn't, even if you think it is. The  sheds little light on Clojure's true resource costs. No Clojure data structures are employed. Functional and sequence abstractions do not appear. Clojure may appear to be simple. It isn't, but it is expressive. It may run five times slower than Java, but  is five times smaller (YMMV). For most of most applications, this is a big win. But for some, and for some parts of many others, it's a devastating loss. With experience of the Clojure language, I believe it is possible to tell in advance whether your problem will cleave cleanly into a part that can be succinctly and adequately (in performance terms) expressed in Clojure and a part that needs doing in Java. Scala has been said to be . Clojure is nothing like Java. You might say that it is  - a bold, some would say preposterous, claim - which may turn out to be true. On interoperability, I can't speak for Clojure, but I would expect it to be in a similar situation as Scala.It is trivially easy to call Java from Scala.It is easy to call Scala from Java as long as you conform your external API to the common points between Scala and Java. For example, a Scala object is used in some ways like static methods in Java, but it's not the same thing. Scala classes may compile to a number of classes with names that look funny in Java.You will not want to mix and match much. Building component in Scala or Clojure that uses lots of Java libraries is very feasible. You can of course call into this component from Java, but what you are not going to want to do is try to consume a Scala API intended for use by Scala programs from Java.SVN claims to be \"CVS done right\". In my view, Scala is Java done right.The  discusses Clojure-Java interoperability. Calling Java methods is straightforward, but extending Java classes/interfaces is quite different.Scala on the other hand is much closer to Java. Scala-Java interoperability is elaborated at Calling Java code and extending Java classes/interfaces works the same way as calling Scala code. Some pain points might be some edge cases of dealing with Java's generics, because Scala's type system is  than Java's. Creating getters and setters following the Java Bean convention .Calling Scala from Java is most of the time straightforward, but for example Scala's companion objects requires knowing how they are compiled to bytecode. Also using traits with non-abstract methods from Java should be complicated, and calling methods with special characters would require knowing how they are encoded in the bytecode.It's now (as of May 2010) worth loking at the latest 1.2 branch of Clojure - this includes a lot of additional support for primitive types and static typing (through various type hints and protocols).My understanding is that you can use these features when you need it to get speed equivalent to writing exactly the same code in pure Java."},
{"body": "I see this code in this blog: :There is an operator  in the code  which I've never seen. Since it's difficult to search it(ignored by search engines), who can tell me what does it mean?To explain it, we first have to explain nested classes in Scala. Consider this simple example:Now let's try something with it:When you declare a class inside another class in Scala, you are saying that  of that class has such a subclass. In other words, there's no  class, but there are  and  classes, and they are  classes, as the error message is telling us above.If you did not understand that, look up path dependent types.Now,  makes it possible for you to refer to such nested classes without restricting it to a particular instance. In other words, there's no , but there's , which means a  nested class of  instance of .We can see this in work by changing the code above:And trying it out:It's known as type projection, and is used to access type members.Basically, it's a way of referring to classes within other classes.  (search for \"pound\")Here's a couple resources for searching on \"symbolic operators\" (which are really methods), but I haven't figured out how to escape \"#\" to search on in scalex)"},
{"body": "What's the Scala recipe for reading line by line from the standard input ? Something like the equivalent java code : The most straight-forward looking approach will just use  which is part of . however that is rather ugly as you need to check for eventual null value:this is so verbose, you'd rather use  instead.I think a more pretty approach will use :For the console you can use . You can write (if you want to stop on an empty line):If you cat a file to generate the input you may need to stop on either null or empty using: Can you not useAs available here : A recursive version (the compiler detects a tail recursion for improved heap usage),Note the use of  from Scala 2.11 . Also note with this approach we can accumulate user input in a collection that is eventually returned -- in addition to be printed out. Namely,"},
{"body": "When a resilient distributed dataset (RDD) is created from a text file or collection (or from another RDD), do we need to call \"cache\" or \"persist\" explicitly to store the RDD data into memory? Or is the RDD data stored in a distributed way in the memory by default?As per my understanding, after the above step, textFile is a RDD and is available in all/some of the node's memory.If so, why do we need to call \"cache\" or \"persist\" on textFile RDD then?Most RDD operations are lazy. Think of an RDD as a description of a series of operations. An RDD is not data. So this line:It does nothing. It creates an RDD that says \"we will need to load this file\". The file is not loaded at this point.RDD operations that require observing the contents of the data cannot be lazy. (These are called .) An example is  \u2014 to tell you the number of lines in the file, the file needs to be read. So if you write , at this point the file will be read, the lines will be counted, and the count will be returned.What if you call  again? The same thing: the file will be read and counted again. Nothing is stored. An RDD is not data.So what does  do? If you add  to the above code:It does nothing.  is also a lazy operation. The file is still not read. But now the RDD says \"read this file and then cache the contents\". If you then run  the first time, the file will be loaded, cached, and counted. If you call  a second time, the operation will use the cache. It will just take the data from the cache and count the lines.The cache behavior depends on the available memory. If the file does not fit in the memory, for example, then  will fall back to the usual behavior and re-read the file.I think the question would be better formulated as:Spark processes are lazy, that is, nothing will happen until it's required.\nTo quick answer the question, after  is issued, nothing happens to the data, only a  is constructed, using the file as source.Let's say we transform that data a bit:Again, nothing happens to the data. Now there's a new RDD  that contains a reference to  and a function to be applied when needed.Only when an action is called upon an RDD, like , the RDD chain, called  will be executed. That is, the data, broken down in partitions, will be loaded by the Spark cluster's executors, the  function will be applied and the result will be calculated.On a linear lineage, like the one in this example,  is not needed. The data will be loaded to the executors, all the transformations will be applied and finally the  will be computed, all in memory - if the data fits in memory. is useful when the lineage of the RDD branches out. Let's say you want to filter the words of the previous example into a count for positive and negative words. You could do this like that:Here, each branch issues a reload of the data. Adding an explicit  statement will ensure that processing done previously is preserved and reused. The job will look like this:For that reason,  is said to 'break the lineage' as it creates a checkpoint that can be reused for further processing.Rule of thumb: Use  when the lineage of your RDD  or when an RDD is used multiple times like in a loop.Yes, only if needed.No! And these are the reasons why :For more details please check the .Adding another reason to add (or temporarily add)  method call. with  method, spark will give debugging informations regarding the size of the RDD. so in the spark integrated UI, you will get RDD memory consumption info. and this proved very helpful diagnosing memory issues. "},
{"body": "I am using the build in JSON class in Scala 2.8 to parse JSON code. I don't want to use the Liftweb one or any other due to minimizing dependencies.The way I am doing it seems too imperative, is there a better way to do it?This is a solution based on extractors which will do the class cast:At the start of the for loop I artificially wrap the result in a list so that it yields a list at the end. Then in the rest of the for loop I use the fact that generators (using ) and value definitions (using ) will make use of the unapply methods.(Older answer edited away - check edit history if you're curious)This is the way I do the pattern match:I like @huynhjl's answer, it led me down the right path. However, it isn't great at handling error conditions. If the desired node does not exist, you get a cast exception. I've adapted this slightly to make use of  to better handle this.Of course, this doesn't handle errors so much as avoid them. This will yield an empty list if any of the json nodes are missing. You can use a  to check for the presence of a node before acting...I tried a few things, favouring pattern matching as a way of avoiding casting but ran into trouble with type erasure on the collection types. The main problem seems to be that the complete type of the parse result mirrors the structure of the JSON data and is either cumbersome or impossible to fully state. I guess that is why  is used to truncate the type definitions. Using  leads to the need for casting.I've hacked something below which is concise but is extremely specific to the JSON data implied by the code in the question. Something more general would be more satisfactory but I'm not sure if it would be very elegant."},
{"body": "Just going through the sample Scala code on Scala website, but encountered an annoying error when trying to run it.Here's the code: . On running it on Eclipse, I got this message 'Editor does not contain a main type' that prevents it from running.Is there anything I need to do...i.e break that file into multiple files, or what?I have this problem a lot with Eclipse and Scala. It helps if you clean your workspace and rebuild your Project.Sometimes Eclipse doesn't recognize correctly which files it has to recompile :(Edit:\nThe Code runs fine in EclipseIn Eclipse, make sure you add your source folder in the project properties -> java build path -> source.  Otherwise, the main() function may not be included in your project.You have to make sure that your .java files  .src folder in eclipse. I had the same exact problem until I got it figured out.A simplier way is to close your project and reopen it !Just make sure that the folder you work in is added to the built path: your folder, -->  --> and it should now find  thereinhope this helpsI had the same problem. I tried all sorts of things. And I came to know thatThings I did:Project properties >> Java Build Path >> Source This solved the error.What you should do is, create a Java Project, but make sure you put this file in the package file of that project, otherwise you'll encounter same error.That code is valid. Have you tried to compile it by hand using scalac? Also, have you called your file \"addressbook\", all lowercase, like the name of the object?Also, I found that Eclipse, for some reason, set the main class to be \".addressbook\" instead of \"addressbook\".A quick solution: First, exclude the package:\nRight click on the source package >> Build Path >> ExcludeThen include it back:\nRight click on the source package >> Build Path >> Includeyou should create your file by selecting on right side you will find your file name,under that will find src folder their you right click select -->class optiontheir your file should be createdYou can try to run the main function from the outline side bar of eclipse.*** your project in eclipse. Sometime there are linkage problems. This solved my problemI just had this problem too. The solution is to make sure eclipse created the project as Java project. Just create a new Java project and copy your class into the src folder (and import the eventual dependencies). This should fix the problem.The correct answer is: the Scala library needs to before the JRE library in the buildpath.  Go to  and move Scala library to the topI had this problem with a Java project that I imported from the file system (under Eclipse Helios).  Here's a hint: the src code didn't seem to be compiled at all, as no \"bin\" directory showed up.I had to create a Java project from scratch (using the wizard), then compare the  files of the non-working and working projects.The project giving \"Editor does not contain a main type\" had this as the \"buildSpec\" in the .project file:But the working project had this as the \"buildSpec\":I copied this in, and the imported project worked.I know my answer is for Java, but the same might be the issue for your Scala project.May be the file you have created is outside the src(source) folder. Trying to call the class object(from the file located in the src folder) from the .java file outside the source folder results in the same error. Copy .java file to the source folder, then build it. The error will be gone.Make sure that your .java file is present either in the str package, or in some other package. If the java file with the main function is outside all packages, this error is thrown.I had the same problem. I had the main class out of the src package, in other folder. I move it in and correct folder and solvedrun \"eclipse -clean -refresh\" from command line.  This fixed the issue for me when all other solutions failed.  This could be the issue with the Java Build path.\nTry below steps :This should resolve the issue.For me, in Eclipse 3.6,  this problem occurs when my main method is not public.  I caused the problem by having a main method like this:The dubugger was unable to detect this by itself.  I am pretty suprised Eclipse overlooked this.In the worst case - create the project once again with all the imports from the beginning. In my case none of the other options worked. This type of error hints that there is an error in the project settings. I once managed to solve it, but once further developments were done, the error came back. Recreating everything from the beginning helped me understand and optimize some links, and now I am confident it works correctly.Follow the below steps:File >> Import >> Existing Projects into Workspace >> Select Archive Filed >> Browse and locate file >> Finish. If its already imported some other way delete it and try it that way. I was having the same problem until i tried that. One more thing to check: make sure that your source file contains the correct package declaration corresponding to the subdirectory it's in.  The error mentioned by the OP can be seen when trying to run a \"main type\" declared in a file in a subdirectory but missing the package statement.I have this problem too after I changed the source folder. The solution that worked for is just editing the file and save it. Try 'Update Project'. Once I did this, The Run as Java Application option appeared.In my particular 'Hello World' case the cause for this problem was the fact, that my  method was inside the Scala .That is because Scala  in Java terms is the entity with only static members and methods inside.That is why Java's  in Scala must be placed under .(Scala  may not contain static's inside)On reason for an Error of: \"Editor does not contain a main type\"Error encountered in: Eclipse NeonOperating System: Windows 10 ProWhen you copy your source folders over from a thumb-drive and leave out the Eclipse_Projects.metadata folder.  Other than a fresh install, you will have to make sure you merge the files from (Thrumb-drive)F:Eclipse_Projects.metadata.plugins  . These plug-ins are the bits and pieces of library code taken from the SDK when a class is created.  I really all depends on what you-----import javax.swing.*;-----  into your file.  Because your transferring it over make sure to merge the ------Eclipse_Projects.metadata.plugins------ manually with a simple copy and paste, while accepting the skip feature for already present plugins in your Folder.For windows 10: you can find your working folders following a similar pattern of file hierarchy. C:Users>Mikes Laptop> workspace > .metadata > .plugins <---merge plugins hereIf it is maven project please check the java file is created under src/main/javaIf you are not getting please change the JRE path and create the java files in above folder structure"},
{"body": "Related to Stack Overflow question , how do I convert a Java collection ( say) into a Scala collection ?I am actually trying to convert a Java API call to  , which returns a , into a Scala immutable . So for example:This seems to work. Criticism is welcome!Your last suggestion works, but you can also avoid using :Note that  is made available by default thanks to .For future reference: With Scala 2.8, it could be done like this: is a  after this.Also see  for a more explicit way (using JavaConverters), which seems to be recommended now.If you want to be more explicit than the JavaConversions demonstrated in , you can use JavaConverters: () and  () have been deprecated with .Instead of  use:as suggested by .Instead of  use:For both there is also the possibility to only import the conversions/converters to Java or Scala respectively, e.g.:\nThe statement above that  and  were deprecated seems to be wrong. There were some deprecated properties in Scala 2.10, which resulted in deprecation warnings when importing them. So the alternate imports here seem to be only aliases. Though I still prefer them, as IMHO the names are more appropriate.You may also want to explore this excellent library:  that has two-way conversion between Java and Scala collections. In your case, to convert a java.util.List to Scala List you can do this:You can add the type information in the toArray call to make the Set be parameterized:This might be preferable as the collections package is going through a major rework for Scala 2.8 and the scala.collection.jcl package is Another simple way to solve this problem:You could convert the Java collection to an array and then create a Scala list from that:"},
{"body": "I used to think that  and  are same, until I saw section 4.1 in Scala Reference:And I have written a test: output:The byte code is just as Scala Reference said:  is not .Why doesn't  just treat  as ? Is there any underlying reason?So, this is just a guess, but it was a perennial annoyance in Java that final static variables with a literal on the right-hand side get inlined into bytecode as constants. That engenders a performance benefit sure, but it causes binary compatibility of the definition to break if the \"constant\" ever changed. When defining a final static variable whose value might need to change, Java programmers have to resort to hacks like initializing the value with a method or constructor.A val in Scala is already final in the Java sense. It looks like Scala's designers are using the redundant modifier final to mean \"permission to inline the constant value\". So Scala programmers have complete control over this behavior without resorting to hacks: if they want an inlined constant, a value that should never change but is fast, they write \"final val\". if they want flexibility to change the value without breaking binary compatibility, just \"val\".I think the confusion here arises from conflating immutability with the semantics of final.  s can be overridden in child classes and therefore can't be treated as final unless marked as such explicitly.@Brian The REPL provides class scope at the line level.  See:"},
{"body": "How do you terminate a run in SBT without exiting?I'm trying CTRL+C but it exits SBT. Is there a way to only exit the running application while keeping SBT open?In the default configuration, your runs happen in the same JVM that sbt is running, so you can't easily kill them separately.If you do your run in a separate, forked JVM, as described at , then you can kill that JVM (by any means your operating system offers) without affecting sbt's JVM:From sbt version 0.13.5 you can add to your build.sbtIt is defined as \"Enables (true) or disables (false) the ability to interrupt task execution with CTRL+C.\" in the There are some bugs reported:I've found the following useful when I have control over the main loop of the application being run from sbt.I tell sbt to fork when running the application (in build.sbt):I also tell sbt to forward stdin from the sbt shell to the application (in build.sbt):Finally, in the main thread of the application, I wait for end-of-file on stdin and then shutdown the JVM:Of course, you can use any thread to read stdin and shutdown, not just the main thread.Finally, start sbt, optionally switch to the subproject you want to run, run.Now, when you want to stop the process, close its stdin by typing CTRL-D in the sbt shell."},
{"body": "Mostly, Android applications are written in Java. But i heard that its also possible to use Scala or some other languages. And I also read that it's possible to include native C/C++ code.Is there a refernce/list available that shows which languages can be used?Scala is . See .Support for other languages is :As stated above, many languages are available for developing in Android. Java, C, Scala, C++, several scripting languages etc. Thanks to Mono you are also able to develop using C# and the .Net framework. Here you have some speedcomparisions: See:  can be used, but it's slow.See also: , and a .I made good experiences with Scala. I use the simple build tool (sbt: ) with the Android-Plugin ()Java and C:You may find more information in Android developers site."},
{"body": "Let's say I'm getting a (potentially big) list of images to download from some URLs. I'm using Scala, so what I would do is :I'm a bit new to Scala, so this still looks a little like magic to me :Thanks for your advice!Use Futures in Scala 2.10. They were joint work between the Scala team, the Akka team, and Twitter to reach a more standardized future API and implementation for use across frameworks. We just published a guide at: Beyond being completely non-blocking (by default, though we provide the ability to do managed blocking operations) and composable, Scala's 2.10 futures come with an implicit thread pool to execute your tasks on, as well as some utilities to manage time outs.Above, we first import a number of things: remains largely the same as what you originally did- the only difference here is that we use managed blocking- . It notifies the thread pool that the block of code you pass to it contains long-running or blocking operations. This allows the pool to temporarily spawn new workers to make sure that it never happens that all of the workers are blocked. This is done to prevent starvation (locking up the thread pool) in blocking applications. Note that the thread pool also knows when the code in a managed blocking block is complete- so it will remove the spare worker thread at that point, which means that the pool will shrink back down to its expected size.(If you want to absolutely prevent additional threads from ever being created, then you ought to use an AsyncIO library, such as Java's NIO library.)Then we use the collection methods of the Future companion object to convert  from   to a .The  object is how we can ensure that  is executed on the calling thread--  simply forces the current thread to wait until the future that it is passed is completed. (This uses managed blocking internally.)"},
{"body": "Suppose I haveIs there a list operation that returns \"a\", \"b\", \"c\"Have a look at the ScalaDoc for ,. Others have suggested using  rather than . That's fine, but be aware that by default, the  interface doesn't preserve element order. You may want to use a Set implementation that explicitly  preserve order, such as .Before using Kitpon's solution, think about using a  rather than a , it ensures each element is unique.As most list operations (, , , ...) are the same for sets and lists, changing collection could be very easy in the code. now has a  method.So calling  is now possible without converting to a  or .Using Set in the first place is the right way to do it, of course, but:Works. Or just  as it supports the   interface. inArr.distinct foreach println _ The algorithmic way..."},
{"body": " allows you to search many standard Haskell libraries by either function name, or by approximate type signature. I find it very useful. Is there anything like Hoogle for Scala? Search in  only finds types and packages by name.There are plans to make the Hoogle interface work with multiple languages: In the past few months I've worked on a new search engine for Scala APIs: The functionality is quite similar to Hoogle and also searches like  provide meaningful results.It's a pity that it does not even have an index like javadoc. Hoogle is nicer, though. I use a personal search engine from Google to search the Scaladocs. A search provider (searching with site: ) in Firefox is another way to search the docs. Both do not work well for the Scala 2.8. release \u2013 it's not indexed well enough to be useful \u2013 and works not with all operators. For example a search for Cons  returns only nonsense."},
{"body": "I am aware of following projects (mostly from ):implemented in Java:implemented in Scala: What are the respective performance characteristics, pitfalls, quirks? Which ones support the Github extensions? Which one would you recommend for a Play! / Scala application?The  created by MarkdownPapers' author compares the performance for the following implementations:TxtMark far exceeds the performance of the other tools, as shown in the following graph:PegDown, which supports many GitHub extensions and others, is far more fully featured. Custom plugins or extensions are also possible.My use case involved processing markdown from an administrative screen where performance was less impactful than a rich feature set, which meant PegDown won over TxtMark."},
{"body": "I've just read: As far as I understand,  is a trait and its only instance is .When a method takes a Null argument, then we can only pass it a  reference or  directly, but not any other reference, even if it is null ( for example).I just wonder in which cases using this  trait could be useful.\nThere is also the Nothing trait for which I don't really see any more examples.I don't really understand either what is the difference between using Nothing and Unit as a return type, since both doesn't return any result, how to know which one to use when I have a method that performs logging for example?Do you have usages of Unit / Null / Nothing as something else than a return type?You only use Nothing if the method never returns (meaning it cannot complete normally by returning, it could throw an exception). Nothing is never instantiated and is there for the benefit of the type system (to quote James Iry: ). From the article you linked to:Your logging method would return Unit. There is a value Unit so it can actually be returned. From the :The article you quote can be misleading. The  type is there for compatibility with the , and Java in particular.We must consider that :thus it becomes necessary to define a type for the  value, which is the  trait, and has  as its only instance.There is nothing especially useful in the  type unless you're the type-system or you're developing on the compiler. In particular I can't see any sensible reason to define a  type parameter for a method, since you can't pass anything but  can be used like this:This allows you to pass in an arbitrary block of code to be executed. might be used as a bottom type for any value that is nullable. An example is this: is used in the definition of This allows you to assign a  to any type of  because  'extends' everything.I've never actually used the  type, but you use , where you would on java use .  is a special type, because as Nathan already mentioned, there can be no instance of .  is a so called bottom-type, which means, that it is a sub-type of any other type. This (and the contravariant type parameter) is why you can prepend any value to  - which is a  - and the list will then be of this elements type.  also if of type . Every attempt to access the values inside such a container will throw an exception, because that it the only valid way to return from a method of type .if you use , there is no things to do (include print console)\nif you do something, use output type ... then how to use ?"},
{"body": "What is the motivation for Scala assignment evaluating to Unit rather than the value assigned?A common pattern in I/O programming is to do things like this:But this is not possible in Scala because..... returns Unit, not the new value of bytesRead.Seems like an interesting thing to leave out of a functional language.\nI am wondering why it was done so?I advocated for having assignments return the value assigned rather than unit.  Martin and I went back and forth on it, but his argument was that putting a value on the stack just to pop it off 95% of the time was a waste of byte-codes and have a negative impact on performance.I'm not privy to inside information on the actual reasons, but my suspicion is very simple. Scala makes side-effectful loops awkward to use so that programmers will naturally prefer for-comprehensions.It does this in many ways. For instance, you don't have a  loop where you declare and mutate a variable. You can't (easily) mutate state on a  loop at the same time you test the condition, which means you often have to repeat the mutation just before it, and at the end of it. Variables declared inside a  block are not visible from the  test condition, which makes  much less useful. And so on.Workaround:For whatever it is worth.As an alternate explanation, perhaps Martin Odersky had to face a few very ugly bugs deriving from such usage, and decided to outlaw it from his language. has  with some actual facts, which are clearly endorsed by the fact that  himself commented his answer, giving credence to the performance-related issues argument put forth by Pollack.This happened as part of Scala having a more \"formally correct\" type system.  Formally-speaking, assignment is a purely side-effecting statement and therefore should return .  This does have some nice consequences; for example:The  method returns  (as would be expected for a setter) precisely because assignment returns .I agree that for C-style patterns like copying a stream or similar, this particular design decision can be a bit troublesome.  However, it's actually relatively unproblematic in general and really contributes to the overall consistency of the type system.I'd guess this is in order to keep the program / the language free of side effects.What you describe is the intentional use of a side effect which in the general case is considered a bad thing.Perhaps this is due to the  principle?CQS tends to be popular at the intersection of OO and functional programming styles, as it creates an obvious distinction between object methods that do or do not have side-effects (i.e., that alter the object). Applying CQS to variable assignments is taking it further than usual, but the same idea applies.A short illustration of why CQS is useful: Consider a hypothetical hybrid F/OO language with a  class that has methods , , , and . In imperative OO style, one might want to write a function like this:Whereas in more functional style, one would more likely write something like this:These seem to be  to do the same thing, but obviously one of the two is incorrect, and without knowing more about the behavior of the methods, we can't tell which one. Using CQS, however, we would insist that if  and  alter the list, they must return the unit type, thus preventing us from creating bugs by using the second form when we shouldn't. The presence of side effects therefore also becomes implicit in the method signature.It is not the best style to use an assignment as a boolean expression. You perform two things at the same time which leads often to errors. And the accidential use of \"=\" instead of \"==\" is avoided with Scalas restriction.By the way: I find the initial while-trick stupid, even in Java. Why not somethign like this?Granted, the assignment appears twice, but at least bytesRead is in the scope it belongs to, and I'm not playing with funny assignment tricks...You can have a workaround for this as long as you have a reference type for indirection. In a na\u00efve implementation, you can use the following for arbitrary types.Then, under the constraint that you\u2019ll have to use  to access the reference afterwards, you can write your  predicate asand you can do the checking against  in a more implicit manner without having to type it."},
{"body": "Split on an empty string returns an array of size 1 :Consider that this returns empty array:Please explain :)For the same reason thatandwill return an array of size 2. Everything before the first match is returned as the first element.If you split an orange zero times, you have exactly one piece - the orange.Splitting an empty string returns the empty string as the first element. If no delimiter is found in the target string, you will get an array of size 1 that is holding the original string, even if it is empty. -> \ntherefore\n -> The Java and Scala split methods operate in two steps like this:According to this, the result of  should be an empty array because of the second step, right? And that is bad, but at least it  in , if you remember to take a look at the documentation:So, I advise you to always pass  as the second parameter (this will skip step two above), unless you specifically know what you want to achieve / you are sure that the empty string is not something that your program would get as an input. Splitting the empty string is an artificially introduced corner case and the documentation warns you about that. Always pass -1 as the second parameter to avoid bugs, unless you have a good reason.In all programming languages I know a blank string is still a valid String. So doing a split using any delimiter will always return a single element array where that element is the blank String. If it was a null (not blank) String then that would be a different issue."},
{"body": "Why does Overloading makes it a little harder to lift a method to a function:You cannot selectively import one of a set of overloaded methods.There is a greater chance that ambiguity will arise when trying to apply implicit views to adapt the arguments to the parameter types:It can quietly render default parameters unusable:Individually, these reasons don't compel you to completely shun overloading. I feel like I'm missing some bigger problems.The evidence is stacking up.The reasons that Gilad and Jason (retronym) give are all very good reasons to avoid overloading if possible. Gilad's reasons focus on why overloading is problematic in general, whereas Jason's reasons focus on why it's problematic in the context of other Scala features.To Jason's list, I would add that overloading interacts poorly with type inference. Consider:A change in the inferred type of  could alter which  method gets called. The  of  need not change, just the inferred type of , which could happen for all sorts of reasons.For all of the reasons given (and a few more I'm sure I'm forgetting), I think method overloading should be used as sparingly as possible.I think the advice is not meant for scala especially, but for OO in general (so far I know scala is supposed to be a best-of-breed between OO and functional). is fine, it's the heart of polymorphism and is central to OO design.  on the other hand is more problematic. With method overloading it's hard to discern which method will be really invoked and it's indeed a frequently a source of confusion. There is also rarely a justification why overloading is really necessary. The problem can most of the time be solved another way and I agree that overloading is a smell.Here is  that explain nicely what I mean with \"overloading is a source of confusion\", which I think is the prime reason why it's discouraged. It's for java but I think it applies to scala as well."},
{"body": "What is the conceptual difference between abstract classes and traits?A class can only , and thus, only one abstract class. If you want to compose several classes the Scala way is to use : you combine an (optional) superclass, your own member definitions and one or more . A trait is restricted in comparison to classes in that it cannot have constructor parameters (compare the ). The restrictions of traits in comparison to classes are introduced to avoid typical problems with multiple inheritance. There are more or less complicated rules with respect to the inheritance hierarchy; it might be best to avoid a hierarchy where this actually matters. ;-)  As far as I understand, it can only matter if you inherit two methods with the same signature / two variables with the same name from two different traits.One aspect of traits is that they are a . Allowing a constrainted form of AOP (around advice).The resolution of  reflects the linearization of the inheritance hierarchy.Conceptually, a trait is a component of a class, not a class by itself. As such, it typically does not have constructors, and it is not meant to \"stand by itself\".I suggest using an abstract class when it has an independent meaning, and traits when you just want to add functionality in an object-oriented manner. , you might find that if all your methods revolve around doing a single thing, you probably want a trait.For a (non-language-specific) example, if your Employee should extend both \"Person\" and \"Cloneable\", make Person a base class and Cloneable a trait.Traits(in Scala at least) system has an explicit way of declaring parent priority in subclass to avoid typical problems associated with multiple inheritance ie. conflicts with inherited methods having same signature. Traits are akin to Java interfaces, but are allowed to have method implementations. "},
{"body": "Suppose I have several futures and need to wait until  any of them fails  all of them succeed. For example: Let there are 3 futures: , , . How would you implement it?You could use a for-comprehension as follows instead:In this example, futures 1, 2 and 3 are kicked off in parallel.  Then, in the for comprehension, we wait until the results 1 and then 2 and then 3 are available.  If either 1 or 2 fails, we will not wait for 3 anymore.  If all 3 succeed, then the  val will hold a tuple with 3 slots, corresponding to the results of the 3 futures.  Now if you need the behavior where you want to stop waiting if say fut2 fails first, things get a little trickier.  In the above example, you would have to wait for fut1 to complete before realizing fut2 failed.  To solve that, you could try something like this:Now this works correctly, but the issue comes from knowing which  to remove from the  when one has been successfully completed.  As long as you have some way to properly correlate a result with the Future that spawned that result, then something like this works.  It just recursively keeps removing completed Futures from the Map and then calling  on the remaining  until there are none left, collecting the results along the way.  It's not pretty, but if you really need the behavior you are talking about, then this, or something similar could work.You can use a promise, and send to it either the first failure, or the final completed aggregated success:Then you can  on that resulting  if you want to block, or just  it into something else.The difference with for comprehension is that here you get the error of the first to fail, whereas with for comprehension you get the first error in traversal order of the input collection (even if another one failed first). For example:And:Here is a solution without using actors.You can do this with futures alone.  Here's one implementation.  Note that it won't terminate execution early!  In that case you need to do something more sophisticated (and probably implement the interruption yourself).  But if you just don't want to keep waiting for something that isn't going to work, the key is to keep waiting for the first thing to finish, and stop when either nothing is left or you hit an exception:Here's an example of it in action when everything works okay:But when something goes wrong:This question has been answered but I am posting my value class solution (value classes were added in 2.10) since there isn't one here. Please feel free to criticize.ConcurrentFuture is a no overhead Future wrapper that changes the default Future map/flatMap from do-this-then-that to combine-all-and-fail-if-any-fail. Usage:In the example above, f1,f2 and f3 will run concurrently and if any fail in any order the future of the tuple will fail immediately.You can use this:For this purpose I would use an Akka actor. Unlike the for-comprehension, it fails as soon as any of the futures fail, so it's a bit more efficient in that sense.Then, create the actor, send a message to it (so that it will know where to send its reply to) and wait for a reply.You might want to checkout Twitter's Future API. Notably the Future.collect method. It does exactly what you want: The source code Future.scala is available here:\n"},
{"body": "how to sort a list in Scala by two fields, in this example I will sort by lastName and firstName?I try things like this but it doesn't work. So I be curious for a good and easy solution.Thanks in advance!PongoIf you want to sort by the merged names, as in your question, or if you first want to sort by lastName, then firstName; relevant for longer names (Wild, Wilder, Wilderman).If you writewith 2 underlines, the method expects two parameters:In general, if you use a stable sorting algorithm, you can just sort by one key, then the next.The final result will be sorted by lastname, then where that is equal, by firstname.Perhaps this works only for a List of Tuples, butappears to work and be a simple way to express it."},
{"body": "I can see there's a sorting object, , with a  method, , on it.What would be a code example of using it, sorting an array of object of arbitrary type? It looks like I need to pass in an implementation of the  trait, but I am unsure of the syntax.Also, I would prefer answers doing this the 'Scala way'. I know I can just use a Java library.Sorting.quickSort declares functions for taking an Array of numbers or Strings, but I'm assuming you mean you want to sort a list of objects of your own classes?The function I think you're looking at isWhich, if I'm reading this right, means that the objects in the Array must have the  trait. So your class must extend  (or must mix it in), and therefore must implement the  method of that trait.So to rip off an example from the book:So given an Array[MyClass], then Sorting.quickSort should work.With Scala 2.8 or later it is possible to do:that uses , an implementation of quicksort.Nowadays this one works too: If you just want to sort things, but aren't married to the Sorting object in particular, you can use the sort method of List.  It takes a comparison function as an argument, so you can use it on whatever types you'd like:Lists probably qualify as \"more scalaish\" than arrays.From the scala api :Scala's \"default\" array is a mutable data structure, very close to Java's Array. Generally speaking, that means an \"array\" is not very Scala-ish, even as mutable data structures go. It serves a purpose, though. If array is the right data type for your need, then that is how you sort it. There are other sorting methods on object Sorting, by the way.I think I just realized what your question is... you don't need to pass any implicit parameter (it's implicit, after all). That parameter exists to say that there must be some way to convert the type K into an Ordered[K]. These definitions already exist for Scala's classes, so you don't need them.For an arbitrary class you can define it this way:Now, if Person was Ordered to begin with, this wouldn't be a problem:While the accepted answer isn't wrong, the quicksort method provides more flexibility than that. I wrote this example for you.This shows how implicit and explicit conversions from Foo to some class extending Ordered[Foo] can be used to get different sort orders."},
{"body": "In Scala, algebraic data types are encoded as  one-level type hierarchies. Example:With es and s, Scala generates a bunch of things like , ,  (used by pattern matching) etc that brings us many of the key properties and features of traditional ADTs.There is one key difference though \u2013 . Compare the following two for example (Copied from the respective REPLs).I have always considered the Scala variation to be on the advantageous side.After all, .  for instance is a subtype of .In fact, . (Could we call this a limited version of dependent typing?) This can be put to good use \u2013 Once you know what data constructor was used to create a value, the corresponding type can be propagated through rest of the flow to add more type safety. For example, Play JSON, which uses this Scala encoding, will only allow you to extract  from , not from any arbitrary .In Haskell,  would probably have type . Which means it will fail at runtime for a  etc. This problem also manifests in the form of well known partial record accessors.  \u2013 on Twitter, mailing lists, IRC, SO etc. Unfortunately I don't have links to any of those, except for a couple -  by Travis Brown, and , a purely functional JSON library for Scala.Argonaut  takes the Haskell approach (by ing case classes, and providing data constructors manually). You can see that the problem I mentioned with Haskell encoding exists with Argonaut as well. (Except it uses  to indicate partiality.)I have been pondering this for quite some time, but still do not understand what makes Scala's encoding wrong. Sure it hampers type inference at times, but that does not seem like a strong enough reason to decree it wrong. What am I missing?To the best of my knowledge, there are two reasons why Scala's idiomatic encoding of case classes can be bad: type inference, and type specificity.  The former is a matter of syntactic convenience, while the latter is a matter of increased scope of reasoning.The subtyping issue is relatively easy to illustrate:The type of  turns out to be , which is probably not what you wanted.  You can generate similar issues in other, more problematic areas:The type of  is .  This is basically  to be not what you want.  In order to get around this issue, containers like  need to be covariant in their type parameter.  Unfortunately, covariance introduces a whole bucket of issues, and in fact degrades the soundness of certain constructs (e.g. Scalaz compromises on its  type and several monad transformers by allowing covariant containers, despite the fact that it is unsound to do so).So, encoding ADTs in this fashion has a somewhat viral effect on your code.  Not only do you need to deal with subtyping in the ADT itself, but  container you ever write needs to take into account the fact that you're landing on subtypes of your ADT at inopportune moments.The second reason not to encode your ADTs using public case classes is to avoid cluttering up your type space with \"non-types\".  From a certain perspective, ADT cases are not really types: they are data.  If you reason about ADTs in this fashion (which is not wrong!), then having first-class types for each of your ADT cases increases the set of things you need to carry in your mind to reason about your code.For example, consider the  algebra from above.  If you want to reason about code which uses this ADT, you need to be constantly thinking about \"well, what if this type is ?\"  That just not a question anyone really  to ask, since  is data.  It's a tag for a particular coproduct case.  That's all.Personally, I don't care much about any of the above.  I mean, the unsoundness issues with covariance are real, but I generally just prefer to make my containers invariant and instruct my users to \"suck it up and annotate your types\".  It's inconvenient and it's dumb, but I find it preferable to the alternative, which is a lot of boilerplate folds and \"lower-case\" data constructors.As a wildcard, a third potential disadvantage to this sort of type specificity is it encourages (or rather, allows) a more \"object-oriented\" style where you put case-specific functions on the individual ADT types.  I think there is very little question that mixing your metaphors (case classes vs subtype polymorphism) in this way is a recipe for bad.  However, whether or not this outcome is the fault of typed cases is sort of an open question."},
{"body": "In java world (more precisely if you have no multiple inheritance/mixins) the rule of thumb is quite simple: \"Favor object composition over class inheritance\".  I'd like to know if/how it is changed if you also consider mixins, especially in scala?\nAre mixins considered a way of multiple inheritance, or more class composition?\nIs there also a \"Favor object composition over class composition\" (or the other way around) guideline?I've seen quite some examples when people use (or abuse) mixins when object composition could also do the job and I'm not always sure which one is better. It seems to me that you can achieve quite similar things with them, but there are some differences also, some examples: I know the short answer is \"It depends\", but probably there are some typical situation when this or that is better.Some examples of guidelines I could come up with so far (assuming I have two traits A and B and A wants to use some methods from B):In many cases mixins seem to be easier (and/or less verbose), but I'm quite sure they also have some pitfalls, like the \"God class\" and others described in two artima articles: ,  (BTW it seems to me that most of the other problems are not relevant/not so serious for scala).Do you have more hints like these?A lot of the problems that people have with mix-ins can be averted in Scala if you only mix-in abstract traits into your class definitions, and then mix in the corresponding concrete traits at object instantiation time.  For instanceThis construct has several things to recommend it.  First, it prevents there from being an explosion of classes as different combinations of trait functionalities are needed.  Second, it allows for easy testing, as one can create and mix-in \"do-nothing\" concrete traits, similar to mock objects.  Finally, we've completely hidden the locking trait used, and even that locking is going on, from consumers of our service.    Since we've gotten past most of the claimed drawbacks of mix-ins, we're still left with a tradeoff\nbetween mix-in and composition.  For myself, I normally make the decision based on whether a hypothetical delegate object would be entirely encapsulated by the containing object, or whether it could potentially be shared and have a lifecycle of it's own.  Locking provides a good example of entirely encapsulated delegates.  If your class uses a lock object to manage concurrent access to its internal state, that lock is entirely controlled by the containing object, and neither it nor its operations are advertised as part of the classes public interface. For entirely encapsulated functionality like this, I go with mix-ins.  For something shared, like a datasource, use composition.Other differences you haven't mentioned: ()Hence the :That last part, regarding the  of an object, has often helped decide between class (and class composition) and trait (and mixins) for a given concept."},
{"body": "I am trying to get a subarray in scala, and I am a little confused on what the proper way of doing it is.  What I would like the most would be something like how you can do it in python:but I am fairly certain you cannot do this.The most obvious way to do it would be to use the Java Arrays util library.But it always makes me feel a little dirty to use Java libraries in Scala.  The most \"scalaic\" way I found to do it would bebut is there a better way?You can call the slice method:It works like in python."},
{"body": "I just had a look at the new  and  packages to see if there is something helpful here. However, I am at a complete loss.Has anybody got an example on how to actually start a process?And, which is most interesting for me: Can you detach processes?A detached process will continue to run when the parent process ends and is one of the weak spots of Ant.There seem to be some confusion what detach is. Have a real live example from my current project. Once with z-Shell and once with TakeCommand:In both cases it is fire and forget, the emulator is started and will continue to run even after the script has ended. Of course having to write the scripts twice is a waste. So I look into Scala now for unified process handling without cygwin or xml syntax.First import:then create a ProcessBuilderThen you have two options:If you want to handle the input and output of the process you can use :I'm pretty sure detached processes work just fine, considering that you have to explicitly wait for it to exit, and you need to use threads to babysit the stdout and stderr.  This is pretty basic, but it's what I've been using:Process was imported from SBT. Here's a thorough guide on how to use the process library .Most of what you'll do is related to , the trait. Get to know that.There are implicits that make usage less verbose, and they are available through the package object . Import its contents, like shown in the examples. Also, take a look at its scaladoc as well.The following function will allow easy use if here documents:Sadly I was not able to (did not have the time to) to make it an infix operation. Suggested calling convention is therefore:It would be call if anybody could give me a hint on how to make it a more shell like call:Documenting process a little better was second on my list for probably two months.  You can infer my list from the fact that I never got to it.  Unlike most things I don't do, this is something I said I'd do, so I greatly regret that it remains as undocumented as it was when it arrived.  Sword, ready yourself! I fall upon thee!If I understand the dialog so far, one aspect of the original question is not yet answered:The primary difficulty is that all of the classes involved in spawning a process must run on the JVM, and they are unavoidably terminated when the JVM exits.   However, a workaround is to indirectly achieve the goal by leveraging the shell to do the \"detach\" on your behalf.  The following scala script, which launches the gvim editor, appears to work as desired:It assumes that scala is in the PATH, and it does (unavoidably) leave a JVM parent process running as well."},
{"body": "I can find tons of examples but they seem to either rely mostly on Java libraries or just read characters/lines/etc.I just want to read in some file and get a byte array with scala libraries - can someone help me with that?Java 7:I believe this is the simplest way possible. Just leveraging existing tools here. NIO.2 is wonderful.This should work (Scala 2.8): The library  is problematic, DON'T USE IT in reading binary files.The error can be reproduced as instructed here: In the file , it contains the hexidecimal , which is  in binary and should be converted to  in decimal.The  file contain two ways to read the file:When I run , the program outputs follows:The Java library generates correct output, while the Scala library not.You might also consider using :"},
{"body": "So, for example, in Python the 2nd item of a tuple would be accessed via .In Scala, access is only possible via strange names .So the question is, why can't I access data in tuples as Sequence or List if it is by definition? Is there some sort of idea or just yet not inspected?Scala knows the arity of the tuples and is thus able to provide accessors like , , etc., and produce a compile-time error if you select  on a pair, for instance. Moreover, the type of those fields is exactly what the type used as parameter for  (e.g.  on a  will return a ).If you want to access the nth element, you can write , but the return type of this can only be , so you lose the type information.I believe the following excerpt from \"Programming in Scala: A Comprehensive Step-by-Step Guide\" (Martin Odersky, Lex Spoon and Bill Venners) directly addresses both of your questions:Scala tuples get very little preferential treatment as far as the language syntax is concerned, apart from expressions  being treated by the compiler as an alias for scala.Tuplen() class instantiation. Otherwise tuples do behave as any other Scala objects, in fact they are written in Scala as . Tuple2 and Tuple3 are also known under the aliases of Pair and Triple respectively:One big difference between ,  or any collection and tuple is that in tuple each element has it's own type where in List all elements have the same type.And as consequence, in Scala you will find classes like  or , so for each element you also have type parameter. Collections accept only 1 type parameter: . Syntax like  is just syntactic sugar for . And , , etc. are just fields on tuple that return correspondent element.  You can easily achive that with :A lot of methods available for standard collection are also available for tuples this way (, , , ,  and  for concatenation,  and  for adding elements, , , , , , , , , , ...)With normal index access, any expression can be used, and it would take some serious effort to check at compiletime if the result of the index expression it is guaranteed to be in range. Make it an attribute, and a compile-time error for  follows \"for free\". Things like allowing only integer constants inside item access on tuples would be a very special case (ugly and unneeded, some would say ridiculous) and again some work to implement in the compiler.Python, for instance, can get away with that because it wouldn't (couldn't) check (at compiletime, that is) if the index is in range anyway.I think it's for type checking. As delnan says, if you have a tuple  and an index  (an arbitrary expression),  would give the compiler no information about which element is being accessed (or even if it's a valid element for a tuple of that size). When you access elements by field name ( is a valid identifier, it's not special syntax), the compiler knows which field you're accessing and what type it has. Languages like Python don't really have types, so this is not necessary for them.Apart from the benefits Jean-Philippe Pellet already mentioned this notation is also very common in mathematics (see ).  A lot of lecturers append indexes to tuple variables if they want to referring to the elements of a tuple.  And the common (LaTeX) notation for writing \"with index \" (referring to the -th element of the tuple) is .  So I find it actually very intuitive."},
{"body": "I know there have been quite a few questions on this, but I've created a simple example that I thought should work,but still does not and I'm not sure I understand whyCan someone explain why the second statement does not compile?It expands to:You want:The placeholder syntax for anonymous functions replaces the smallest possible containing expression with a function."},
{"body": "It seems that there has been a recent rising interest in STM (software transactional memory) frameworks and language extensions.   in particular has an excellent implementation which uses  rather than a rolling commit log.  GHC Haskell also has  which also allows transaction composition.  Finally, so as to toot my own horn just a bit, I've recently implemented an  which statically enforces reference restrictions.All of these are interesting experiments, but they seem to be confined to that sphere alone (experimentation).  So my question is: have any of you seen or used STM in the real world?  If so, why?  What sort of benefits did it bring?  What about performance?  (there seems to be a great deal of conflicting information on this point)  Would you use STM again or would you prefer to use some other concurrency abstraction like actors?I participated in the hobbyist development of the BitTorrent client in Haskell (named conjure). It uses STM quite heavily to coordinate different threads (1 per peer + 1 for storage management + 1 for overall management).Benefits: less locks, readable code.Speed was not an issue, at least not due to STM usage.Hope this helpsThe article \"Software Transactional Memory: why is it only a research toy?\" fails to look at the Haskell implementation, which is a really big omission.  The problem for STM, as the article points out, is that implementations must chose between either making all variable accesses transactional unless the compiler can prove them safe (which kills performance) or letting the programmer indicate which ones are to be transactional (which kills simplicity and reliability).  However the Haskell implementation uses the purity of Haskell to avoid the need to make most variable uses transactional, while the type system provides a simple model together with effective enforcement for the transactional mutation operations.  Thus a Haskell program can use STM for those variables that are truly shared between threads whilst guaranteeing that non-transactional memory use is kept safe.We use it pretty routinely for high concurrency apps at Galois (in Haskell). It works, its used widely in the Haskell world, and it doesn't deadlock (though of course you can have too much contention). Sometimes we rewrite things to use MVars, if we've got the design right -- as they're faster.Just use it. It's no big deal. As far as I'm concerned, STM in Haskell is \"solved\". There's no further work to do. So we use it.We, , are using Haskell STM with GHC in production.  Our server receives a stream of messages about new and modified \"objects\" from a clincal \"data server\", it transforms this event stream on the fly (by generating new objects, modifying objects, aggregating things, etc) and calculates which of these new objects should be synchronized to connected iPads.  It also receives form inputs from iPads which are processed, merged with the \"main stream\" and also synchronized to the other iPads.  We're using STM for all channels and mutable data structures that need to be shared between threads.  Threads are very lightweight in Haskell so we can have lots of them without impacting performance (at the moment 5 per iPad connection). Building a large application is always a challenge and there were many lessons to be learned but we never had any problems with STM.  It always worked as you'd naively expect.  We had to do some serious performance tuning but STM was never a problem. (80% of the time we were trying to reduce short-lived allocations and overall memory usage.)STM is one area where Haskell and the GHC runtime really shines.  It's not just an experiment and not for toy programs only.We're building a different component of our clincal system in Scala and have been using Actors so far, but we're really missing STM.  If anybody has experience of what it's like to use one of the Scala STM implementations in production I'd love to hear from you. :-)You may want to read this article:\nWe have implemented our entire  (in-memory database and runtime) on top of our own STM implementation in C. Prior to this, we had some log and lock based mechanism to deal with concurrency, but this was a pain to maintain. We are very happy with STM since we can treat every operation the same way. Almost all locks could be removed. We use STM now for almost anything at any size, we even have a memory manager implement on top. The performance is fine but to speed things up we now developed a custom  in collaboration with ETH Zurich. The system natively supports transactional memory.But there are some challenges caused by STM as well. Especially with larger transactions and hotspots that cause unnecessary transaction conflicts. If for example two transactions put an item into a linked list, an unnecessary conflict will occur that could have been avoided using a lock free data structure.I'm currently using Akka in some PGAS systems research.  is a Scala library for developing scalable concurrent systems using Actors, STM, and built-in fault tolerance capabilities modeled after Erlang's \"Let It Fail/Crash/Crater/ROFL\" philosophy. Akka's STM implementation is supposedly built around a Scala port of Clojure's STM implementation. An overview of Akka's STM module can be found ."},
{"body": "Let's say I want to handle multiple return values from a remote service using the same code. I don't know how to express this in Scala:I know I can use Extract Method and call that, but there's still repetition in the call. If I were using Ruby, I'd write it like this:Note that I simplified the example, thus I don't want to pattern match on regular expressions or some such. The match values are actually complex values.You can do:Note that you cannot bind parts of the pattern to names - you can't do this currently:"},
{"body": "If I have a  in Scala, what is the idiomatic way to filter out the  values?One way is to use the following:Is there a more \"idiomatic\" way? This does seem pretty simple.If you want to get rid of the options at the same time, you can use :someList.filter(_.isDefined) if you want to keep the result type as List[Option[A]]"},
{"body": "I've written a rather large program in Scala 2.7.5, and now I'm looking forward to version 2.8. But I'm curious about how this big leap in the evolution of Scala will affect me.What will be the biggest differences between these two versions of Scala? And perhaps most importantly:You can find here a  (April 2009), completed with recent  (June 2009)\"Rewriting code\" is not an obligation (except for using some of the improved Collections), but some features like  (: an abstract representation of the control state, or the \"rest of computation\" or \"rest of code to be executed\") can give you some new ideas. A good introduction is , written by  (who has also posted a much  in this thread).Note:  seems to work with some 2.8 nightly-build (vs. the )When you migrate, the compiler can provide you with some safety nets.There are many other new features that can be safely ignored as you start migrating, for example  and Continuations.VonC's answer is hard to improve on, so I won't even try to. I'll cover some other stuff not mentioned by him.First, some deprecated stuff will go. If you have deprecation warnings in your code, it's likely it won't compile anymore.Next, Scala's library is being expanded. Mostly, common little patterns such as catching exceptions into  or , or converting an AnyRef into an Option with  mapped into . These things can mostly pass unnoticed, but I'm getting tired of posting something on the blog and later having someone tell me it's already on Scala 2.8. Well, actually, I'm not getting  of it, but, rather, and happily, used to it. And I'm not talking here about the Collections, which  getting a major revision.Now, it  be nice if people posted actual examples of such library improvements as answers. I'd happily upvote all such answers.REPL is not getting just command-completion. It's getting a lot of stuff, including the ability to examine the AST for an object, or the ability to insert break points into code that fall into REPL.Also, Scala's compiler is being modified to be able to provide fast partial compilation to IDEs, which means we can expect them to become much more \"knowledgable\" about Scala -- by querying the Scala compiler itself about the code.One big change is likely to pass unnoticed by many, though it will decrease problems for library writers and users alike. Right now, if you write the following:You are importing not Java's  library, but 's  library, as , ,  and  all got within scope, and  can be found inside . With Scala 2.8, only  gets scoped. Since, sometimes, you want some of the rest to be in Scope, an alternative  syntax is now allowed:which is equivalent to:And happens to get both  and  into scope.needs to becomeI had to visit the IRC channel for that one, but then realized I should have started here.Here's a checklist from Eric Willigers, who has been using Scala since 2.2. Some of this stuff will seem dated to more recent users.>\n>\n>"},
{"body": "I am trying to issue a simple POST request to a webservice which returns some XML in Scala.It seems that  is the standard library used for this task, but I cannot find documentation for it. The main site, which I link above, explains at length what is a promise and how to do asynchronous programming, but does not actually document the API. There is a  - which looks a bit scary - but it only seems useful to people who already know what to do and only need a reminder for the cryptic syntax.It also seems that , but I cannot find any documentation for it either.I use the following: .Here's a simple GET request:and an example of a POST:Scalaj HTTP is available through SBT:You could use .  The documentation is lacking (it took me some digging to find out ) but it's a great option if you are already using spray.  And the documentation is better than dispatch.We're using it at  over  because the operators are less symbolic and we're already using spray/actors.I'm using dispatch: They've just released a new version (0.9.0) with a complete new api that I really like. And it is async.Example from project page:edit: This might help you Another option is Typesafe's play-ws, which is the Play Framework WS library broken out as a standalone lib:I wouldn't necessarily offer this as the best option, but worth mentioning.Why not use  ? Here's the , which covers a wide range of scenarios.If I can make a shameless plug, I have an API called  which is simply a wrapper in Scala for Java's HttpUrlConnection.I had to do the same to test one end point (in Integration test). So following is the code to fetch response from GET request in Scala.\nI am making use of scala.io.Source to read from endpoint and ObjectMapper for json to object conversion.And here is my test method for the same"},
{"body": "How to create a list with the same element n-times ?Manually implementnation:Is there also a built-in way to do the same ?See  that collection data structures, like , ,  and so on, extend: It's not available in Scala 2.7.Using  like this,I have another answer which emulates flatMap I think (found out that this solution returns Unit when applying duplicateN)}but this is rather for a predetermined List and you want to duplicate n times each elementWorks like a charm."},
{"body": "Does anyone know if there is a nice way I can convert a Scala case class instance, e.g.Into a mapping of some kind, e.g.Which works for any case class, not just predefined ones. I've found you can pull the case class name out by writing a method that interrogates the underlying Product class, e.g.So I'm looking for a similar solution but for the case class fields. I'd imagine a solution might have to use Java reflection, but I'd hate to write something that might break in a future release of Scala if the underlying implementation of case classes changes.Currently I'm working on a Scala server and defining the protocol and all its messages and exceptions using case classes, as they are such a beautiful, concise construct for this. But I then need to translate them into a Java map to send over the messaging layer for any client implementation to use. My current implementation just defines a translation for each case class separately, but it would be nice to find a generalised solution.This should work:Because case classes extend  one can simply use  to get field values:Or alternatively:One advantage of Product is that you don't need to call  on the field to read its value. Another is that productIterator doesn't use reflection.Note that this example works with simple case classes that don't extend other classes and don't declare fields outside the constructor.If anybody looks for a recursive version, here is the modification of @Andrejs's solution:It also expands the nested case-classes into maps at any level of nesting.Solution with  from interpreter package:Here's a simple variation if you don't care about making it a generic function:I don't know about nice... but this seems to work, at least for this very very basic example. It probably needs some work but might be enough to get you started? Basically it filters out all \"known\" methods from a case class (or any other class :/ )Details: If you happen to be using Json4s, you could do the following:You could use shapeless.LetDefine a LabelledGeneric representationDefine two typeclasses to provide the toMap methodsThen you can use it like this.which prints For nested case classes, (thus nested maps)\ncheck  "},
{"body": "since concurrent programming becomes constantly more important, I was wondering\nwhat you think about Erlang vs. Scala in that respect. It seems to me that Scala\nhas a larger user base and potentially a brighter future than Erlang. Furthermore,\nScala is kind of java.I know these questions are alway a bit subjective, but what would be the better\nfuture investment: Erlang or Scala. Or even another language?Erlang has been designed for concurrent, fault-tolerant communication systems. You can easily write servers that handle large number of network connections and (thanks to one garbage collector per Erlang process) the servers can retain soft real-time characteristics (i.e., the whole server is not paused until GC finishes). You can also hot-swap Erlang code, distribute it across several nodes, etc. That's why (arguably) the most-scalable XMPP server (ejabberd) is written in Erlang. Yaws (a web server) is another example where Erlang excels, see: . Riak/Couch are examples of NoSQL DB build with Erlang. These are the problems where Erlang is a great choice.But Erlang VM is not as fast as JVM in terms of raw computations, so as soon as you need to do something computationally intensive (e.g. financial modeling) JVM will be your preferred platform. Moreover, Erlang's concurrency model (actors) is baked in the language. If that doesn't fit the problem you're trying to solve, then you won't be happy with Erlang.Scala is more 'general' language in a sense that concurrency, horizontal scalability, or fault-tolerance is not part of the language. It is solved at the level of libraries (that's why there are at least 3 implementations of actors in Scala). The good thing is that you can pick concurrency model that fits your domain. For example if you need software transactional memory (STM), just pick Akka and you're good to go ().Plus there is the whole argument that with Scala you can leverage your \"JVM investments\" and multitude of JVM libs.You didn't give any info on what kind of software you want to write with either of those languages so it's hard to give you a definitive answer. Having said that, given all the above, Scala may be \"safer\" investment than Erlang (not bashing Erlang/OTP at all, it's a fine language/platform).BTW. If a single-machine concurrency is important to you Clojure () should not be overlooked (also JVM language).UPDATE1: If you like what Erlang offers but not its syntax, take a look at  Just pick one and stick with it for a while.  Learn some stuff, make some cool things and either keep going with that language or move on to another one.With respect to learning concurrent programming, either will be fine.  The key here is that you will be learning something new and unless there's a job opening that you are trying to get hired for that uses Erlang specifically, it really doesn't matter.  Plus, even if that opening did require Erlang, you would still likely have a good chance if you knew Scala really well.Just think, all of the time you have spent trying to pick a new language could have been better spent if you just picked one and already started learning it by now.Both languages, at the core, are not that hard to learn, and also to learn the concurrency features they provide. In fact, Scala actors are influenced by Erlang actors. I would go to both of them, take your time looking at their construct, do some tests in concurrency problems, etc.If you know Java, Scala will be more natural, as Erlang is more like prolog. If, on the contrary, you're more oriented towards mathematical or logical type of languages, start with Erlang."},
{"body": "What is the syntax for adding an element to a scala.collection.mutable.Map ?Here are some failed attempts:The point is that the first line of your codes is not what you expected. You should use orinstead.As always, you should question whether you truly need a mutable map.Immutable maps are trivial to build:Mutable maps are no different when first being built:In both of these cases, inference will be used to determine the correct type parameters for the Map instance.You can also hold an immutable map in a , the variable will then be updated with a new immutable map instance every time you perform an \"update\"If you don't have any initial values, you can use Map.empty:When you sayyou are not creating a map instance, but instead aliasing the Map type.Try instead the following:Create a new immutable map:Add a new key/value pair to the above map (and create a new map, since they're both immutable):Create a mutable map without initial value:Create a mutable map with initial values:Update existing key-value:Add new key-value:Check the updated map:"},
{"body": "I am a Scala programmer, learning Haskell now. It's easy to find practical use cases and real world examples for OO concepts, such as decorators, strategy pattern etc. Books and interwebs are filled with it. I came to the realization that this somehow is not the case for functional concepts. Case in point: .  I am struggling to find practical use cases for applicatives. Almost all of the tutorials and books I have come across so far provide the examples of  and . I expected applicatives to be more applicable than that, seeing all the attention they get in the FP community.  I think I understand the conceptual basis for  (maybe I am wrong), and I have waited long for my moment of enlightenment. But it doesn't seem to be happening. Never while programming, have I had a moment when I would shout with a joy, \"Eureka! I can use applicative here!\" (except again, for  and ).  Can someone please guide me how applicatives can be used in a day-to-day programming? How do I start spotting the pattern? Thanks!Well, how often in your day-to-day Haskell programming do you create new data types? Sounds like you want to know when to make your own Applicative instance, and in all honesty unless you are rolling your own parser, you probably won't need to do it very much.  applicative instances, on the other hand, you should learn to do frequently.Applicative is not a \"design pattern\" like decorators or strategies. It is an abstraction, which makes it much more pervasive and generally useful, but much less tangible. The reason you have a hard time finding \"practical uses\" is because the example uses for it are almost too simple. You use decorators to put scrollbars on windows. You use strategies to unify the interface for both aggressive and defensive moves for your chess bot. But what are applicatives for? Well, they're a lot more generalized, so it's hard to say what they are for, and that's OK. Applicatives are handy as parsing combinators; the Yesod web framework uses Applicative to help set up and extract information from forms. If you look, you'll find a million and one uses for Applicative; it's all over the place. But since it's so abstract, you just need to get the feel for it in order to recognize the many places where it can help make your life easier.Applicatives are great when you've got a plain old function of several variables, and you have the arguments but they're wrapped up in some kind of context. For instance, you have the plain old concatenate function  but you want to apply it to 2 strings which were acquired through I/O. Then the fact that  is an applicative functor comes to the rescue:Even though you explicitly asked for non- examples, it seems like a great use case to me, so I'll give an example. You have a regular function of several variables, but you don't know if you have all the values you need (some of them may have failed to compute, yielding ). So essentially because you have \"partial values\", you want to turn your function into a partial function, which is undefined if any of its inputs is undefined. Thenbutwhich is exactly what you want.The basic idea is that you're \"lifting\" a regular function into a context where it can be applied to as many arguments as you like. The extra power of  over just a basic  is that it can lift functions of arbitrary arity, whereas  can only lift a unary function.Since many applicatives are also monads, I feel there's really two sides to this question.  This is mostly a matter of style. Although monads have the syntactic sugar of -notation, using applicative style frequently leads to more compact code.In this example, we have a type  and we want to construct random values of this type. Using the monad instance for , we might writeThe applicative variant is quite a bit shorter.Of course, we could use  to get similar brevity, however the applicative style is neater than having to rely on arity-specific lifting functions.In practice, I mostly find myself using applicatives much in the same way like I use point-free style: To avoid naming intermediate values when an operation is more clearly expressed as a composition of other operations.Since applicatives are more restricted than monads, this means that you can extract more useful static information about them.An example of this is applicative parsers. Whereas monadic parsers support sequential composition using , applicative parsers only use . The types make the difference obvious: In monadic parsers the grammar can change depending on the input, whereas in an applicative parser the grammar is fixed.By limiting the interface in this way, we can for example determine whether a parser will accept the empty string . We can also determine the first and follow sets, which can be used for optimization, or, as I've been playing with recently, constructing parsers that support better error recovery. I think of Functor, Applicative and Monad as design patterns. Imagine you want to write a Future[T] class. That is, a class that holds values that are to be calculated.In a Java mindset, you might create it likeWhere 'get' blocks until the value is available. You might realize this, and rewrite it to take a callback:But then what happens if there are two uses for the future? It means you need to keep a list of callbacks. Also, what happens if a method receives a Future[Int] and needs to return a calculation based on the Int inside? Or what do you do if you have two futures and you need to calculate something based on the values they will provide?But if you know of FP concepts, you know that instead of working directly on T, you can manipulate the Future instance.Now your application changes so that each time you need to work on the contained value, you just return a new Future.Once you start in this path, you can't stop there. You realize that in order to manipulate two futures, you just need to model as an applicative, in order to create futures, you need a monad definition for future, etc.UPDATE: As suggested by @Eric, I've written a blog post: I finally understood how applicatives can help in day-to-day programming with that presentation:The autor shows how applicatives can help for combining validations and handling failures. The presentation is in Scala, but the author also provides the full code example for Haskell, Java and C#.I think Applicatives ease the general usage of monadic code. How many times have you had the situation that you wanted to apply a function but the function was not monadic and the value you want to apply it to is monadic? For me: quite a lot of times!\nHere is an example that I just wrote yesterday:in comparison to this using Applicative:This form looks \"more natural\" (at least to my eyes :)Coming at Applicative from \"Functor\" it generalizes \"fmap\" to easily express acting on several arguments (liftA2) or a sequence of arguments (using <*>).Coming at Applicative from \"Monad\" it does not let the computation depend on the value that is computed.  Specifically you cannot pattern match and branch on a returned value, typically all you can do is pass it to another constructor or function.Thus I see Applicative as sandwiched in between Functor and Monad.  Recognizing when you are not branching on the values from a monadic computation is one way to see when to switch to Applicative.Here is an example taken from the aeson package:There are some ADTs like ZipList that can have applicative instances, but not monadic instances.  This was a very helpful example for me when understanding the difference between applicatives and monads.  Since so many applicatives are also monads, it's easy to not see the difference between the two without a concrete example like ZipList.I think it might be worthwhile to browse the sources of packages on Hackage, and see first-handedly how applicative functors and the like are used in existing Haskell code.I described an example of practical use of the applicative functor in a discussion, which I quote below.Note the code examples are pseudo-code for my hypothetical language which would hide the type classes in a conceptual form of subtyping, so if you see a method call for  just translate into your type class model, e.g.  in Scalaz or Haskell.It is useful to read , because I can't copy it all here. I expect that url to not break, given who the owner of that blog is. For example, I quote from further down the discussion.I think what I presented  serves as a good example of an use case for  spotted in the wild."},
{"body": "I am struggling to find any decent links to design patterns, best practice or good, basic architectural principles that should be used in building Actor-based apps. Those few that I know of are:This is related to a , if not  the same! It's not such a simple question because the  of concurrency allows for many different types of applications to be built, from a stateful single-VM application (with a few separate actor classes) to a stateless cluster of thousands of instances of an actor class. The core principles are the same however:I posted a blog on  in Scala a few weeks ago.  It's a post of a best practices and things to avoid based on a few years of experience with the paradigm.The book 'Reactive Design Patterns' is in the making at Manning.See: "},
{"body": "A friend of mine posed a seemingly innocuous Scala language question last week that I didn't have a good answer to: whether there's an easy way to declare a collection of things belonging to some common typeclass.  Of course there's no first-class notion of \"typeclass\" in Scala, so we have to think of this in terms of traits and context bounds (i.e. implicits).Concretely, given some trait  representing a typeclass, and types ,  and , with corresponding implicits in scope ,  and , we want to declare something like a , into which we can throw instances of ,  and  with impunity.  This of course doesn't exist in Scala; a  discusses this in more depth.The natural follow-up question is \"how does Haskell do it?\"  Well, GHC in particular has a type system extension called , described in the  paper.  In brief, given a typeclass  one can legally construct a list .  Given a declaration of this form, the compiler does some dictionary-passing magic that lets us retain the typeclass instances corresponding to the types of each value in the list at runtime.Thing is, \"dictionary-passing magic\" sounds a lot like \"vtables.\"  In an object-oriented language like Scala, subtyping is a much more simple, natural mechanism than the \"Boxy Types\" approach.  If our ,  and  all extend trait , then we can simply declare  and be happy.  Likewise, as Miles notes in a comment below, if they all extend traits ,  and  then I can use  as an equivalent to the impredicative Haskell .However, the main, well-known disadvantage with subtyping compared to typeclasses is tight coupling: my ,  and  types have to have their  behavior baked in.  Let's assume this is a major dealbreaker, and I  use subtyping.  So the middle ground in Scala is pimps^H^H^H^H^Himplicit conversions: given some ,  and  in implicit scope, I can again quite happily populate a  with my ,  and  values...... Until we want .  At that point, even if we have implicit conversions ,  and , we can't put an  into the list.  We could restructure our implicit conversions to literally provide , but I've never seen anybody do that before, and it seems like yet another form of tight coupling.Okay, so my question finally is, I suppose, a combination of a couple questions that were previously asked here:  and  ... is there some unifying theory that says impredicative polymorphism and subtype polymorphism are one and the same?  Are implicit conversions somehow the secret love-child of the two?  And can somebody articulate a good, clean pattern for expressing multiple bounds (as in the last example above) in Scala?You're confusing impredicative types with existential types. Impredicative types allow you to put  values in a data structure, not arbitrary concrete ones. In other words  means that you have a list where each element works as any numeric type, so you can't put e.g.  and  in a list of type , but you can put something like  in it. Impredicative types is not what you want here.What you want is existential types, i.e.  (not real Haskell syntax), which says that each element is some unknown numeric type. To write this in Haskell, however, we need to introduce a wrapper data type:Note the change from  to . That's because we're describing the . We can put any numeric type , but then the type system \"forgets\" which type it was. Once we take it back out (by pattern matching), all we know is that it's some numeric type. What's happening under the hood, is that the  type contains a hidden field which stores the type class dictionary (aka. vtable/implicit), which is why we need the wrapper type.Now we can use the type  for a list of arbitrary numbers, but we need to wrap each number on the way in, e.g. . The correct dictionary for each type is looked up and stored in the hidden field automatically at the point where we wrap each number. The combination of existential types and type classes is in some ways similar to subtyping, since the main difference between type classes and interfaces is that with type classes the vtable travels separately from the objects, and existential types packages objects and vtables back together again.However, unlike with traditional subtyping, you're not forced to pair them one to one, so we can write things like this which packages one vtable with two values of the same type.or even fancier things. Once we pattern match on the wrapper, we're back in the land of type classes. Although we don't know which type  and  are, we know that they're the same, and we have the correct dictionary available to perform numeric operations on them. Everything above works similarly with multiple type classes. The compiler will simply generate hidden fields in the wrapper type for each vtable and bring them all into scope when we pattern match.As I'm very much a beginner when it comes to Scala, I'm not sure I can help with the final part of your question, but I hope this has at least cleared up some of the confusion and given you some ideas on how to proceed.@hammar's answer is perfectly right. Here is the scala way of doint it. For the example i'll take  as the type class and the values  and  to pack in a list :What we want is to be able run the following codeBuilding the list is easy but the list won't \"remember\" the exact type of each of its elements. Instead it will upcast each element to a common super type . The more precise super super type between  and  being , the type of the list is .The problem is: what to forget and what to remember? We want to forget the exact type of the elements BUT we want to remember that they are all instances of . The following class does exactly thatThis is an encoding of the existential :It pack a value with the corresponding type class instance.Finally we can add an instance for The  is required to bring the instance into scope. Thanks to the magic of implicits:which is very close to the expected code."},
{"body": "Scala has a language feature to support disjunctions in pattern matching ('Pattern Alternatives'):However, I often need to trigger an action if the scrutiny satisfies PatternA  PatternB (conjunction.)I created a pattern combinator '&&' that adds this capability. Three little lines that remind me why I love Scala!\nI've just been asked how the compiler interprets . These are infix operator patterns (Section 8.1.9 of the Scala Reference). You could also express this with standard extract patterns (8.1.7) as Boolean#&&val b = true && false && true`.I really like this trick. I do not know of any existing way to do this, and I don't foresee any problem with it -- which doesn't mean much, though. I can't think of any way to create a .As for adding it to the standard library... perhaps. But I think it's a bit hard. On the other hand, how about talking Scalaz people into including it? It looks much more like their own bailiwick.A possible problem with this is the bloated translation that the pattern matcher generates. \nHere is the  of the sample program, generated with . Even  fails to simplify the  expressions.Large pattern matches already generate more bytecode than is legal for a single method, and use of this combinator may amplify that problem.If  was built into the language, perhaps the translation could be smarter. Alternatively, small improvements to  could help."},
{"body": "In Scala, a class's primary constructor has no explicit body, but is defined implicitly from the class body. How, then, does one distinguish between fields and local values (i.e. values local to the constructor method)?For example, take the following code snippet, a modified form of some sample code from \"Programming in Scala\":My understanding is that this will generate a class with three fields: a private \"g\", and public \"x\" and \"y\". However, the g value is used only for calculation of the x and y fields, and has no meaning beyond the constructor scope.So in this (admittedly artificial) example, how do you go about defining local values for this constructor?E.g.There are a few ways to do that. You can declare such temporary variables inside private definitions, to be used during construction time. You can use temporary variables inside blocks which return expressions (such as in Alaz's answer). Or, finally, you can use such variables inside alternate constructors.In a manner similar to the alternate constructors, you could also define them inside the object-companion's \"apply\" method.What you  do is declare a field to be \"temporary\".Note also that any parameter received by the primary constructor is a field also.  If you don't want such parameters to become fields, and don't want to expose the actual fields in a constructor, the usual solution is to make the primary constructor private, with the actual fields, and use either an alternate constructor or an object-companion's apply() as the effective \"primary\" constructor.Another option we have is to make the primary object constructor private and use a companion object's apply method as a builder. If we apply (pun is not intended) this approach to your example it will look like this:To create an R instance instead of:write: This is a little bit verbose, but it could be worse, I think :). Let's hope that private[this] vals will be treated as temporary variables in future releases of Scala. Martin himself hinted that.Some discussion on this topic, including Martin Odersky's comments, is "},
{"body": "Coming back to Scala after a spell writing Haskell, I've started using the type keyword to make my class definitions a bit more readable, eg:The trouble I've run into is that these type definitions need to live inside a class or object - they're not \"first class citizens\" like they are in Haskell. If I try to define a type outside of a class or object, I get a compiler .My problem then is how to use these types across multiple classes and objects in a package? What I've done now seems quite ugly:And then in another class file:Is there a nicer way of doing this - and incidentally do the Scala gurus think it's a good thing or a bad thing that types can only be defined inside classes/objects?Will  work for you?From the article:The  has many types and values already, so I think you can use the same technique for your own types.I'm not sure what constitutes niceness in this case, but here are two options:Using a trait: ()Or just import the object's members, effectively using it like a package:Youn can use a package object and put the type declaration in there. When you import that package the type definition will be available. An example : Notice that the package object has the same name as the package in order to be usable without a object prefix i.e myType.mine\nIf you name it with a different name for example :it can be accessed with mainType.mine when you import \"package\"for more info :\n"},
{"body": "Can someone explain to me in simple terms what the Shapeless library is for?Scala has generics and inheritance functionality so I'm a bit confused what Shapeless is for.Maybe a use case to clarify things would be helpful.It's a little hard to explain, as shapeless has a wide range of features;  I'd probably find it easier to \"explain, in simple terms, what variables are for\".  You definitely want to start with the .Broadly speaking, shapeless is about programming with types.  Doing things at compile-time that would more commonly be done at runtime, keeping precise track of the type of each element in a list, being able to translate from tuples to HLists to case classes, creating polymorphic functions (as opposed to methods), etc.A typical usage scenario would go something like:For reference, an  will have a precise type, such as  (yes, that really  a single type) where everything is pinned down and the size is fixed.  So you either need to know at compile time exactly what will be going into your HList, or you need the type-safe cast.If you take the  of such an HList, you get a , and a compile-time guarantee that the head of this will be a .  Prepending a value to the head will similarly preserve all types involved.Shapeless also comes with the  type class, allowing you to use HList operations on tuples and case classes as well.The other features I tend to use are:Looking at an  is something that might seem baffling until you try to work with types and delegate or switch on types. Take a look at the following:What is the type of  here? If you were to inspect it, you'd see it was of type . That's not very helpful. What's even less helpful is if I tried to use the following  to  over it:At runtime, this might throw a  because I haven't actually told you what type  is. It could be of type .With an  you can know right at compile time if you've failed to capture one of the types of that list. In the above, if I had defined  when I accessed the 3rd element, it's type would be  and this would be known at compile time.As @KevinWright states, there's more to it than that with Shapeless but  is one of the defining features of the library. Everything in Shapeless has two things in common:First, it isn't in the Scala standard library, but arguably should be. Therefore, asking what Shapeless is for is a bit like asking with the Scala standard library is for! It's . It's a grab bag.(but it isn't a totally arbitrary grab bag, because:)Second, everything in Shapeless provides increased checking and safety . Nothing in Shapeless (that I can think of?) actually \u201cdoes\u201d anything at runtime. All of the interesting action happens when your code is compiled. The goal is always increased confidence that if your code compiles at all, it won't crash or do the wrong thing at runtime. (Hence this notable quip: )There is a nice introduction to what type-level programming is all about, with links to further resources, at ."},
{"body": "I've recently started learning scala, and I've come across the  (cons) function, which prepends to a list.\nIn the book \"Programming in Scala\" it states that there is no append function because appending to a list has performance o(n) whereas prepending has a performance of o(1)Something just strikes me as wrong about that statement.Isn't performance dependent on implementation?  Isn't it possible to simply implement the list with both forward and backward links and store the first and last element in the container?The second question I suppose is what I'm supposed to do when I have a list, say 1,2,3 and I want to add 4 to the end of it?The key is that  does not mutate , but instead creates a new list, which contains x followed by all elements of . This can be done in O(1) time because you only need to set  as the successor of  in the newly created, singly linked list. If doubly linked lists were used instead,  would also have to be set as the predecessor of 's head, which would modify . So if we want to be able to do  in O(1) without modifying the original list, we can only use singly linked lists.Regarding the second question: You can use  to concatenate a single-element list to the end of your list. This is an O(n) operation.Other answers have given good explanations for this phenomenon. If you are appending many items to a list in a subroutine, or if you are creating a list by appending elements, a functional idiom is to build up the list in reverse order, cons'ing the items on the  of the list, then reverse it at the end. This gives you O(n) performance instead of O(n\u00b2).Prepending is faster because it only requires two operations:Appending requires more operations because you have to traverse to the end of the list since you only have a pointer to the head.I've never programmed in Scala before, but you could try a In today's Scala, you can simply use  to append an item at the end of any sequential collection.  (There is also  to prepend. The mnemonic for many of Scala's 2.8+ collection operations is that the on goes next to the lection.)This will be O() with the default linked implementation of  or , but if you use  or , this will be effectively constant time.  Scala's  is probably Scala's most useful list-like collection\u2014unlike Java's  which is mostly useless these days.If you are working in Scala 2.8 or higher, the  is an absolute must read.Most functional languages prominently figure a singly-linked-list data structure, as it's a handy immutable collection type.  When you say \"list\" in a functional language, that's typically what you mean (a singly-linked list, usually immutable).  For such a type, append is O(n) whereas cons is O(1)."},
{"body": "There is a relatively-new lightweight JVM called  that can produce executables for iOS targets.There isn't too much documentation on the website (and not much can be found searching with Google). I was wondering if anybody was aware of a step-by-step tutorial on how to get a basic Scala program running on iOS, using .Another alternative JVM to iOS compiler is . Although it is at an early stage, it looks quite promising, with examples on how to compile Scala for iOS. This was an old answer, valid at that time, but, as @JamesMoore points out, RoboVM is no more. What looks very promising now, and may well be the way to run Scala code in iOS in the near future is Compiled Scala sources are completely standard class files. You should be able to follow the  (look for \u201cEmbedding\u201d) on the website without large changes, just treat  as a dependency of your code.I managed to bootstrap the complete compiler and the standard library running on Avian a few days ago.Some parts might still be a bit rough around the edges, e. g. there is  which will be part of the next release of Scala (2.10.1) but is not in 2.10.0. If you want to play with it right now, you need to use a nightly build until 2.10.1 is released.If you encounter any additional issues, please report them!I may not need it anymore, now that !EDIT: Oracle updated the article to announce that they will not release a JVM, so it looks like JavaFX+Avian may be the way to go.Running Java byte code on iOS (not-rooted) is not only running that or those JVM. As far as I understand iOS memory management doesn't allow executable memory pages to be writable in user mode. That basically prohibits any JIT compilation.\nSo even if it's possible to run some compiled (either from Java or Scala) classes on specific Java VM I would carefully check how this VM supports Ahead-Of-Time compilation in order to be runnable on iOS.\nAs I have seen Avian AOT works well on a desktop. For iOS you will have to check it yourself, although the project looks promising in AOT area."},
{"body": "I have an application where I would like to have mixed Java and Scala source (actually its migrating a java app to scala - but a bit at a time). I can make this work in IDEs just fine, very nice. But I am not sure how to do this with maven - scalac can compile java and scala intertwined, but how to I set up maven for the module? Also, does my scala source have to be a different folder to the java? Using the maven scala plugin, a config like the one below will work for a project that mixes java and scala source (scala source of course goes in the /scala directory, as mentioned by someone else). You can run run mvn compile, test etc... and it will all work as normal. Very nice (it will run scalac first automatically). For a great IDE, IntelliJ 8 works nicely: add in the scala plug in, then add a scala facet, and then adjust the compile setting for scala to run scalac first (critical if you have circular dependencies with scala and java source). Yeah, the scala part has to be in a separate module and in  directory. Maven regards mixed source like this as heresy. You enable scala compilation by importing the . The  page as a good example.I once asked a  about how to include non-Java code in a Maven project. The gist of the answer was to have under src a different directory for each programming language, and to find/write a Maven plugin that would know what to do with each. Eg:Look at Sonatype Maven Cookbook  I solved this some time ago by having one Maven module written in Scala and the other in Java. But since Scala and Java can cross depend on one another (Java -> Scala -> Java or the other way around), then this is something very desirable without multi module projects.There is work underway in solving this, you can read about it  and a new version of the maven-scala-plugin will be released soon.Here's a small project link() that I did using scala, java and mongo db. Note in the pom.xml of the project one can specify the source folder of scala files it can be same as that of java or a different folder like src/main/scala. There is no mandate for scala classes to be on a separate src folders. I have tried both of them and both of them work great. The snipped of the pom that I am referring to is You need to combine several approaches in order to mix scala and java classes freely. This solution worked for me:  "},
{"body": "I'm using  with a  2.1 and I have some troubles.Given the following entity......I have to import a package for a  database driver, but I want to use  for  and  in . How should I proceed?I was able to workaround this by  the driver settings in my unit test:I don't like this solution and I'm wondering if there is an elegant way to write DB-agnostic code so there are two different database engines used - one in testing and another in production?I don't want to use evolution, either, and prefer to let Slick create the database tables for me:The first time I start the application, everything works fine... then, of course, the second time I start the application it crashes because the tables already exist in the PostgreSQL database.That said, my last two questions are:You find an example on how to use the cake pattern / dependency injection to decouple the Slick driver from the database access layer here: .A few days ago I wrote a Slick integration library for play, which moves the driver dependency to the application.conf of the Play project: .With the help of this library your example would be implemented as follows:Add snapshot repositoryOr local repository, if slick-integration is published locallyIn the case of slick-integration, it is assumed that you use primary keys of type Long which are auto incremented. The pk name is 'id'. The Table/Mapper implementation has default methods (delete, findAll, findById, insert, update). Your entities have to implement 'withId' which is needed by the 'insert' method.This is the Data Access Layer (DAL) which is used by the controllers to access the database. Transactions are handled by the Table/Mapper implementation within the corresponding Component.I cannot give you a sufficient answer to this question...... but perhaps this is not really s.th you want to do. What if you add an attribute to an table, say ? If you want to safe the data currently stored within your tables, then an alter script would do the job. Currently, such an alter script has to be written by hand. The  could be used to retrieve the create statements. They should be sorted to be better comparable with previous versions. Then a diff (with previous version) is used to manually create the alter script. Here, evolutions are used to alter the db schema.Here's an example on how to generate (the first) evolution:I was also trying to address this problem: the ability to switch databases between test and production. The idea of wrapping each table object in a trait was unappealing. I am not trying to discuss the pros and cons of the cake pattern here, but I found another solution, for those who are interested.Basically, make an object like this:Obviously, you can do any decision logic you like here. It does not have to be based on system properties.Now, instead of:You can sayUPDATE: A Slick 3.0 Version, courtesy of trent-ahrens:The  does exactly the same as what is proposed in the other answers, and it seems to be under the umbrella of Play/Typesafe.You just can import \nand it will choose the appropriate driver according to .It also offers some more things like connection pooling, DDL generation...If, like me, you're not using Play! for the project, a solution is provided by Nishruu  "},
{"body": "Does anyone know how to show a custom failure message in ScalaTest?For example:Shows the following message when it fails:But i want more descriptive message like:You're the first to ask for such a feature. One way to achieve this is with withClue. Something like:That should get you this error message:NumberOfElements: 10 was not equal to 5If you want to control the message completely you can write a custom matcher. Or you could use an assertion, like this:Can you elaborate on what your use case is? Why is it that 10 did not equal 5 is not up to snuff, and how often have you had this need? Here's the kind of thing you're requesting:So this way you can write:"},
{"body": "As prompted by a comment on my For example:Looking in the scaladoc, all of these use the  operation inherited from , so how come it's always able to return the most specific valid collection? Even , which provides  via an implicit conversion.Scala collections are clever things...Internals of the collection library is one of the more advanced topics in the land of Scala. It involves higher-kinded types, inference, variance, implicits, and the  mechanism - all to make it incredibly generic, easy to use, and powerful from a user-facing perspective. Understanding it from the point-of-view of an API designer is not a light-hearted task to be taken on by a beginner.On the other hand, it's incredibly rare that you'll ever actually need to work with collections at this depth.So let us begin...With the release of Scala 2.8, the collection library was completely rewritten to remove duplication, a great many methods were moved to just one place so that ongoing maintenance and the addition of new collection methods would be far easier, but it also makes the hierarchy harder to understand.Take  for example, this inherits from (in turn)That's quite a handful! So why this deep hierarchy? Ignoring the  traits briefly, each tier in that hierarchy adds a little bit of functionality, or provides a more optimised version of inherited functionality (for example, fetching an element by index on a  requires a combination of  and  operations, grossly inefficient on an indexed sequence).  Where possible, all functionality is pushed as far up the hierarchy as it can possibly go, maximising the number of subclasses that can use it and removing duplication. is just one such example.  The method is implemented in  (Though the  traits only really exist for library designers, so it's generally considered to be a method on  for most intents and purposes - I'll come to that part shortly), and is widely inherited.  It's possible to define an optimised version in some subclass, but it must still conform to the same signature.  Consider the following uses of  (as also mentioned in the question):In each case, the output is of the same type as the input wherever possible.  When it's not possible, superclasses of the input type are checked until one is found that  offer a valid return type. Getting this right took a lot of work, especially when you consider that  isn't even a collection, it's just implicitly convertible to one.So how is it done?One half of the puzzle is the  traits (I  say I'd get to them...), whose main function is to take a  type param (short for \"Representation\") so that they'll know the true subclass actually being operated on.  So e.g.  is the same as , but abstracted over the  type param.  This param is then used by the second half of the puzzle; the  type class that captures source collection type, target element type and target collection type to be used by collection-transforming operations.It's easier to explain with an example!BitSet defines an implicit instance of  like this:When compiling , the compiler will attempt an implicit lookup of This is the clever part... There's only one implicit in scope that matches the first two type parameters. The first parameter is , as captured by the  trait, and the second is the element type, as captured by the current collection trait (e.g. ). The  operation is then also parameterised with a type, this type  is inferred based on the third type parameter to the  instance that was implicitly located.  in this case.So the first two type parameters to  are inputs, to be used for implicit lookup, and the third parameter is an output, to be used for inference. in  therefore matches the two types  and , so the lookup will succeed, and inferred return type will also be .When compiling , the compiler will attempt an implicit lookup of .  This will fail for the implicit in BitSet, so the compiler will next try its superclass -  - This contains the implicit:Which matches, because Coll is a type alias that's initialised to be  when  derives from . The  will match anything, as  is parameterised with the type , in this case it's inferred to be ... Thus yielding a return type of .So to correctly implement a collection type, you not only need to provide a correct implicit of type , but you also need to ensure that the concrete type of that of that collection is supplied as the  param to the correct parent traits (for example, this would be  in the case of subclassing ). is a little more complicated as it provides  by an implicit conversion. The implicit conversion is to , which subclasses , which ultimately derives  -  being the  type param.There's also a  in scope so that the compiler knows that when mapping the elements of a  to s, then the return type should also be a string. From this point onwards, the same mechanism is used.The  online pages have a detailed explanation geared towards the practical aspects of creating new collections based on the 2.8 collection design.Quote:It uses as example a collection for encoding RNA sequences and one for Patricia trie. Look for the  section for the explanation of what to do to return the appropriate collection type."},
{"body": "What are all the instances of syntactic sugar in Scala?They are hard to search for since most/all of them are purely symbols and are thus hard to search for without knowing the name of the concept.TODO:  In addition to Jaxkson's answer:For example:For exampleAs mentioned by , tuples and symbols get a slightly special syntax.For example, the following two are equivalent.There are two methods used for extractors,  and . These are used in multiple variable assignments and pattern matching.Honestly, I don't really get the purpose of the above syntax since it can be done almost just as easily by just putting the code in the  statements. Of course if you have a better example, leave a comment below  do all those / definitions for you (as well as other stuff) so use them whenver possible to save time and reduce code.As a quick example,Context bounds desugar into  parameters, e.g. consider a function that leverages the  type class:where the  part is a context bound, gets translated to:therefore the following compiles, too:Anonymous functions: is short for "},
{"body": "How can I convert immutable.Map to mutable.Map in Scala so I can update the values in Map?The cleanest way would be to use the  varargs factory.  Unlike the  approach, this uses the  mechanism, and so has the potential to be more efficient if library code was written to take advantage of this:This works because a  can also be viewed as a sequence of Pairs.How about using collection.breakOut?There is a variant to create an empty mutable  that has default values taken from the immutable . You may store a value and override the default at any time: (see the comment by Rex Kerr): You will not be able to remove the elements coming from the immutable map:"},
{"body": "I am getting introduced to Functional Programming [FP] (using Scala). One thing that is coming out from my initial learnings is that FPs rely heavily on recursion. And also it seems like, in  FPs the only way to do iterative stuff is by writing recursive functions.And because of the heavy usage of recursion seems the next thing that FPs had to worry about were  typically due to long winding recursive calls. This was tackled by introducing some optimizations (tail recursion related optimizations in maintenance of stackframes and  annotation from Scala v2.8 onwards) Can someone please enlighten me why recursion is so important to functional programming paradigm? Is there something in the specifications of functional programming languages which gets \"violated\" if we do stuff iteratively? If yes, then I am keen to know that as well.PS: Note that I am newbie to functional programming so feel free to point me to existing resources if they explain/answer my question. Also I do understand that Scala in particular provides support for doing iterative stuff as well. highlights the equivalence between different computability models. Using recursion we don't need a  while solving  problem, and this make possible to specify a semantic in simpler terms. Thus solutions can be simpler, in a formal sense.I think that Prolog shows better than functional languages the effectiveness of recursion (it doesn't have iteration), and the practical limits we encounter when using it.Pure functional programming means programming without side effects. Which means, if you write a loop for instance, the body of your loop can't produce side effects. Thus, if you want your loop to do something, it has to reuse the result of the previous iteration and produce something for the next iteration. Thus, the body of your loop is a function, taking as parameter the result of previous execution and calling itself for the next iteration with its own result. This does not have a huge advantage over directly writing a recursive function for the loop.A program which doesn't do something trivial will have to iterate over something at some point. For functional programming this means the program has to use recursive functions.The feature that brings about the  that you do things recursively is immutable variables.Consider a simple function for calculating the sum of a list (in pseudocode):Now, the  in each iteration of the list is different, but we can rewrite this to use a  function with a lambda argument to get rid of this problem:Still, the value of the  variable has to be altered in each run of the lambda. This is illegal in a language with immutable variables, so you have to rewrite it in a way that doesn't mutate state:Now, this implementation will require pushing to and popping from the call stack a lot, and a program where all small operations would do this would not run very quickly. Therefore, we rewrite it to be tail recursive, so the compiler can do tail call optimization:Of course, if you want to loop indefinitely, you absolutely need a tail recursive call, or else it would stack overflow.The  annotation in Scala is a tool to help you analyse which functions are tail recursive. You claim \"This function is tail recursive\" and then the compiler can tell you if you are mistaken. This is especially important in Scala as compared to other functional languages because the machine it runs on, the JVM, does not support tail call optimization well, so it is not possible to get tail call optimization in Scala in all the same circumstances you would get it in other functional languages.: recursion is used to handle inductively defined data, which are Recursion is natural, when you operate on higher levels of abstraction. Functional programming is not just about coding with functions; it is about operating on higher levels of abstraction, where you naturally use functions. Using functions, it is only natural to reuse the same function (to call it again), from whatever context where it makes sense. The world is built by repetition of similar/same building blocks. If you cut a piece of fabric in two, you have two pieces of fabric. Mathematical induction is at the heart of maths. We, humans, count (as in, ). Any  (like, , and ) is natural to handle/analyze by a recursive function, according to same cases by which that thing is defined/constructed.Recursion is everywhere. Any iterative loop is a recursion in disguise anyway, because when you reenter that loop, you reenter  loop again (just with maybe different loop variables). So it's not like  new concepts about computing, it's more like  the foundations, and making it .So, recursion is natural. We just write down some laws about our problem, some equations  involving the function we're defining which preserve some invariant (under the assumption that the function is coherently defined), re-specifying the problem in simplified terms, and voila! We have the solution.An example, a function to calculate the length of list (an inductively defined recursive data type). Assume it is defined, and returns the list's length, non-surprisingly. What are the laws it must obey? What invariant is preserved under what simplification of a problem?The most immediate is taking the list apart to its head element, and the rest - a.k.a. the list's tail (according to how a list is defined/constructed). The law is,D'uh! But what about the empty list? It must be thatSo how do we write such a function?... Wait... We've written it already! (In Haskell, if you were wondering, where function application is expressed by juxtaposition, parentheses are used just for grouping, and  is a list with  its first element, and  the rest of them).All we need of a language to allow for such style of programming is that it has  (and perhaps, a bit luxuriously,  ), so there's no stack blow-up, and we're set.Another thing is purity - immutability of code variables and/or data structure (records' fields etc). What that does, besides freeing our minds from having to track what is changing when, is it makes time explicitly apparent in our code, instead of hiding in our \"changing\" mutable variables/data. We can only \"change\" in the imperative code the value of a variable  - we can't very well change its value in the past, can we?And so we end up with lists of recorded change history, with change explicitly apparent in the code: instead of  we write . It makes reasoning about code so much easier.To address immutability in the context of tail recursion with , consider this tail recursive re-write of the above function  under accumulator argument paradigm:Here TCO means call frame reuse, in addition to the direct jump, and so the call chain for  can be seen as actually mutating the call stack frame's entries corresponding to the function's parameters:In a pure language, without any value-mutating primitives, the only way to express change is to pass updated values as arguments to , to be processed further. If the further processing is the same as before, than naturally we have to invoke the same function for that, passing the updated values to it as arguments. And that's recursion.And the following makes the whole history of calculating the length of an argument list  (and available for reuse, if need be):In Haskell this is variably known as guarded recursion, or corecursion (at least I think it is). There is nothing 'special' in recursion. It is widespread tool in programming and mathematics and nothing more. However, functional languages are usually minimalistic. They do introduce many fancy concepts like pattern matching, type system, list comprehension and so on, but it is nothing more then syntactic sugar for very general and very powerful, but simple and primitive tools. This tools are: function abstraction and function application. This is conscious choice, as simplicity of the language core makes reasoning about it much easier. It also makes writing compilers easier. The only way to describe a loop in terms of this tools is to use recursion, so imperative programmers may think, that functional programming is about recursion. It is not, it is simply required to imitate that fancy loops for poor ones that cannot drop this syntactic sugar over  statement and so it is one of the first things they stuck into. Another point where (may be indirect) recursion required is processing of recursively defined data structures. Most common example is  ADT. In FP it is usually defined like this  . Since definition of the ADT here is recursive, the processing function for it should be recursive too. Again, recursion here is not anyway special: processing of such ADT in recursive manner looks naturally in both imperative and functional languages. Well, In case of list-like ADT imperative loops still can be adopted, but in case of different tree-like structures they can't.So there is no anything special in recursion. It is simply another type of function application. However, because of limitations of modern computing systems (that comes from poorly made design decisions in C language, which is de-facto standard cross-platform assembler) function calls cannot be nested infinitely even if they are tail calls. Because of it, designers of functional programming languages have to either limit allowed tail-calls to tail recursion (scala) or use complicated techniques like trampoling (old ghc codegen) or compile directly to asm (modern ghc codegen).TL;DR: There is no anything special in recursion in FP, no more than in IP at least, however, tail recursion is the only type of tail calls allowed in scala because of limitations of JVM.Avoiding side effects is one of the pillars of functional programming (the other is using higher order functions).Imagine how you might make use of imperative flow control  relying on mutation. Is it possible?Of course  depends on mutation (). In fact, any conditional loop construct does. In  the  will depend on some mutable state. Sure,  doesn't use mutation. But if you ever want out of that loop you'll need an . Really, try to think of a (non-infinite) looping mechanism other than recursion that doesn't rely on mutation.What about ? Now we're getting closer to functional programming. The  can be thought of as a  to the body of the loop. Reusing the name isn't the same as reassigning a value. Perhaps in the body of the loop you're ing values as an expression in terms of  (no mutation).Exactly equivalently, you could move the body of the  loop into the body of a function  and then map that over  using a higher order function - replacing your loop construct with something like .But then how is the library function  implemented without mutation? Well, using recursion of course! It's done for you. It becomes less common to have to implement recursive functions yourself once you begin making use of the second pilar of higher order functions to replace your looping constructs.Additionally, with tail call optimization a recursive definition is equivalent to an iterative process. You might also enjoy this blog post: There are two properties that I consider essential to functional programming:Now if you program in an imperative style you have to use assignment.Consider a for loop. It has an index and on each iteration the index has a different value. So you could define a function that returns this index. If you call that function twice you might  get different results. Thus breaking the principle no 2.If you break principle no 2. passing around functions (principle no 1) becomes something extremely dangerous, because now the result of the function might depend on when and how often a function gets called.Last time I used a Functional language (Clojure) I was never even tempted to use recursion.  Everything could be dealt with as a set of things, to which a function was applied to get part products, to which another function was applied, until the final result was reached.Recursion is only one way, and not necessarily the clearest way, to handle the multiple items you usually have to handle to deal with any gFor the sake of new FP learners I would like to add my 2 cents.As mentioned in some of the answers, recursion is their to make use of immutable variables but why we need to do that ? its because it makes it easy to run the program on multiple cores in parallel, but why we want that ? Can't we run it in single core and be happy as always we have been ? No because content to process is increasing day by day and CPU Clock cycle can't be increased so significantly than adding more cores. From past one decade the clock speed has been around upto 2.7 ghz to 3.0 ghz for consumer computers and chip designers are having issues in fitting more and more transistors in their.Also FP has been their from very long time, but didn't pick up as it used recursion and memory was very expensive in those days but as clock speeds were soaring year after year so community decided to keep on going with OOP\nEdit: it was quite fast, I had only couple of minutes"},
{"body": "I'm attempting to print the contents of a collection to the Spark console.I have a type:And I use the command:But this is printed : How can I write the RDD to console or save it to disk so I can view its contents?If you want to view the content of a RDD, one way is to use :That's not a good idea, though, when the RDD has billions of lines. Use  to take just a few to print out:The  function is a , which means that Spark will not actually evaluate your RDD until you run an  on it. To print it, you can use  (which is an action):To write it to disk you can use one of the  functions (still actions) from the If you're running this on a cluster then  won't print back to your context. You need to bring the  data to your session. To do this you can force it to local array and then print it out:You can convert your  to a  then  it.This will show the top 20 lines of your data, so the size of your data should not be an issue.In python This will printout all the contents of the RDDYou can also save as a file: There are probably many architectural differences between  and  (not only 'collect', but also other actions). One the differences I saw is when doing , the output will be in a random order. For ex: if my rdd is coming from a text file where each line has a number, the output will have a different order. But when I did , order remains just like the text file.Instead of typing each time, you can; [1] Create a generic print method inside Spark Shell.[2] Or even better, using implicits, you can add the function to RDD class to print its contents.Example usage:Output:PS. These only makes sense if you are working in local mode and with a small amount of data set. Otherwise, you either will not be able to see the results on the client or run out of memory because of the big dataset result."},
{"body": "I was thinking about a nice way to convert a List of tuple with duplicate key  into map . Normally (in python), I'd create an empty map and for-loop over the list and check for duplicate key. But I am looking for something more scala-ish and clever solution here.btw, actual type of key-value I use here is  and I want to turn into a map of Group and then project:More scalish way to use fold, in the way like  (skip  step).For Googlers that don't expect duplicates or are fine with the :As of 2.12, the default policy reads:Here's another alternative:For Googlers that do care about duplicates:Here is a more Scala idiomatic way to convert a list of tuples to a map handling duplicate keys. You want to use a fold.You can try this"},
{"body": "How in Scala to find unique items in List?The most efficient order-preserving way of doing this would be to use a  as an ancillary data structure:We can wrap this in some nicer syntax using an implicit conversion:In 2.8, it's:Roll your own uniq filter with order retention:If you refer to the Since  is immutable, you wont modify the initial  by calling Warning: as mentioned by (!), this does not preserve the order:For a , that method should be available in Scala2.8, according to .\nIn the meantime, you will need to define an ad-hoc static method as the  Imho, all the interpretations of the question are false:Given this list:the only unique item in the list is . The other items aren't unique. will find it.A simple ad-hoc method is just to add the List to a Set, and use from there:Produces:This works for a Seq (or any Iterable), but is not necessary in 2.8, where the removeDuplicates method will probably be more readable.  Also, not sure about the runtime performance vs a more thought-out conversion.Also, note the lost ordering.list.toSet will do it since Set by definition only contains unique elements"},
{"body": "What are the most commonly held misconceptions about the Scala language, and what counter-examples exist to these?UPDATEI was thinking more about various claims I've seen, such as \"Scala is dynamically typed\" and \"Scala is a scripting language\".I accept that \"Scala is [Simple/Complex]\" might be considered a myth, but it's also a viewpoint that's very dependent on context.  My personal belief is that it's the  that can make Scala appear either simple or complex depending oh who's using them.  Ultimately, the language just offers abstractions, and it's the way that these are used that shapes perceptions.Not only that, but it has a certain tendency to inflame arguments, and I've not yet seen anyone change a strongly-held viewpoint on the topic...Myth: Scala supports operator overloading.Actually, Scala just has very flexible method naming rules and infix syntax for method invocation, with special rules for determining method precedence when the infix syntax is used with 'operators'. This subtle distinction has critical implications for the utility and potential for abuse of this language feature compared to true operator overloading (a la C++), as explained more thoroughly in James Iry's answer to .Myth: That . :-)Debunked:  by James Iry.I disagree with the argument that Scala is hard because you can use very advanced features to do hard stuff with it. The scalability of Scala means that you can write DSL abstractions and high-level APIs in Scala itself that otherwise would need a language extension. So to be fair you need to compare Scala libraries to other languages compilers. People don't say that C# is hard because (I assume, don't have first hand knowledge on this) the C# compiler is pretty impenetrable. For Scala it's all out in the open. But we need to get to a point where we make clear that most people don't need to write code on this level, nor should they do it.I think a common misconception amongst many scala developers, those at EPFL (and yourself, Kevin) is that . The argument usually goes something like this:My personal opinion is that . Scala's type system taken together with implicits allows one to write . Any suggestion otherwise is just preposterous, regardless of what the above \"metrics\" might lead you to think. ().The obvious arguments against this are (of course):Of these, it seems to me that only #2 is valid. Whether or not you write code quite as complex as , I think it's just silly to use the language (and continue to use it) with no real understanding of the type system. How else can one get the best out of the language?Myth: methods and functions are the same thing.In fact, a function is a value (an instance of one of the  classes), while a method is not.  . The most important practical distinctions are:There is a myth that Scala is difficult because Scala is a complex language.This is false--by a variety of metrics, Scala is no more complex than Java.  (Size of grammar, lines of code or number of classes or number of methods in the standard API, etc..)But it is undeniably the case that Scala code can be ferociously difficult to understand.  How can this be, if Scala is not a complex language?The answer is that Scala is a  language.  Unlike Java, which has many special constructs (like enums) that accomplish one particular thing--and requires you to learn specialized syntax that applies just to that one thing, Scala has a variety of very general constructs.  By mixing and matching these constructs, one can express very complex ideas with very little code.  And, unsurprisingly, if someone comes along who has not had the same complex idea and tries to figure out what you're doing with this very compact code, they may find it daunting--more daunting, even, than if they saw a couple of pages of code to do the same thing, since then at least they'd realize how much conceptual stuff there was to understand!There is also an issue of whether things are more complex than they really need to be.  For example, some of the type gymnastics present in the collections library make the collections a joy to use but perplexing to implement or extend.  The  here are not particularly complicated (e.g. subclasses should return their own types), but the  required (higher-kinded types, implicit builders, etc.) are complex.  (So complex, in fact, that Java just gives up and doesn't try, rather than doing it \"properly\" as in Scala.  Also, in principle, there is hope that this will improve in the future, since the method can evolve to more closely match the goal.)  In other cases, the  are complex;  is a bit of a mess, but if you really want to take all numbers less than 5, and then take every 2nd number out of 10 in the remaining list, well, that's just a somewhat complicated idea, and the code pretty much says what it does if you know the basic collections operations.Summary: Scala is not complex, but it allows you to compactly express complex ideas.  Compact expression of complex ideas can be daunting.There is a myth that Scala is non-deployable, whereas a wide range of third-party Java libraries can be deployed without a second thought.To the extent that this myth exists, I suspect it exists among people who are not accustomed to separating a virtual machine and API from a language and compiler.  If java == javac == Java API in your mind, you might get a little nervous if someone suggests using scalac instead of javac, because you see how nicely your JVM runs.Scala ends up as JVM bytecode, plus its own custom library.  There's no reason to be any more worried about deploying Scala on a small scale or as part of some other large project as there is in deploying any other library that may or may not stay compatible with whichever JVM you prefer.  Granted, the Scala development team is not backed by quite as much force as the Google collections, or Apache Commons, but its got at least as much weight behind it as things like the Java Advanced Imaging project.Some common misconceptions related to Actors library:Myth:andis the same.It is not; you can call , but  tries to call the apply method of StringLike with no arguments (results in an error).Myth: You can replace a fold with a reduce when computing something like a sum from zero.This is a common mistake/misconception among new users of Scala, particularly those without prior functional programming experience.  The following expressions are  equivalent:The two expressions differ in how they handle the empty sequence: the fold produces a valid result (0), while the reduce throws an exception.Myth: .Debunked  by Martin Odersky himself. (Also see this paper -  - by Odersky et al.)Myth:  refers to the same type represented by .As an example of this misconception, one might assume that in the following code the type of  is :In reality,  refers to the type whose only instance is . In general,  is the singleton type whose only instance is . So in the example above, the type of  is .  The following session demonstrates the principle:Scala has type inference and refinement types (structural types), whereas Java does not.The myth is  by James Iry.Myth: that Scala is highly scalable, without qualifying what forms of scalability.Scala may indeed be highly scalable in terms of the ability to express higher-level denotational semantics, and this makes it a very good language for experimentation and even for scaling production at the project-level scale of  compositionality.However, every referentially opaque language (i.e. allows mutable data structures), is imperative (and not declarative) and will not scale to WAN  compositionality and security. In other words, imperative languages are compositional (and security) spaghetti w.r.t. uncoordinated development of modules. I realize such uncoordinated development is perhaps currently considered by most to be a \"pipe dream\" and thus perhaps not a high priority. And this is not to disparage the benefit to compositionality (i.e. eliminating corner cases) that higher-level semantic unification can provide, e.g. .There will possibly be significant cognitive dissonance for many readers, and they are likely to prematurely press the \"down\" vote, before fully understanding the issue I raise, especially since there are popular misconceptions about imperative vs. declarative (i.e. mutable vs. immutable), (and ,) e.g. the monadic semantic is never inherently imperative . Yes in Haskell the IO monad is imperative, but it being imperative has nothing to with it being a monad.I explained this in more detail in the \"Copute Tutorial\" and \"Purity\" sections, which is either at the home page or temporarily at .My point is I am very grateful Scala exists, but I want to clarify what Scala scales and what is does not. I need Scala for what it does well, i.e. for me it is the ideal platform to prototype a new declarative language, but Scala itself is not exclusively declarative and afaik referential transparency can't be enforced by the Scala compiler, other than remembering to use  everywhere.I think my point applies to the complexity debate about Scala. I have found (so far and mostly conceptually, since so far limited in actual experience with my new language) that removing mutability and loops, while retaining  subtyping (which Haskell doesn't have), radically simplifies the language. For example, the  fiction disappears, and afaics, a slew of other issues and constructs become unnecessary, e.g. non-category theory standard library, for comprehensions, etc.."},
{"body": "I've got an sbt (Scala) project that currently pulls artifacts from the web. We'd like to move towards a corporate-standardized Nexus repository that would cache artifacts. From the Nexus documentation, I understand how to do that for Maven projects. But sbt obviously uses a different approach. (I understand Ivy is involved somehow, but I've never used it and don't understand how it works.)How do I tell sbt and/or the underlying Ivy to use the corporate Nexus repository system for all dependencies? I'd like the answer to use some sort of project-level configuration file, so that new clones of our source repository will automatically use the proxy. (I.e., mucking about with per-user config files in a dot-directory is not viable.) Thanks!Step 1: Follow the instructions  at , which I have summarised and added to below:: Even though, in the sbt 0.12.0 launcher jar at least, the boot properties files for older sbt versions don't contain the required line (the one that mentions ), it will still work for those versions of sbt if you edit those files to add the required line, and repackage them into the sbt 0.12.0 launcher jar! This is because the feature is implemented in the launcher, not in sbt itself. And the sbt 0.12.0 launcher is claimed to be able to launch all versions of sbt, right back to 0.7!Step 2: To make sure external repositories are not being used, remove the default repositories from your resolvers. This can be done in one of two ways:OK, with some help from Mark Harrah on the sbt mailing list, I have an answer that works.My build class now looks like the following (plus some other repos):Now, if I delete the Squeryl tree from my machine's  directory, sbt tries to grab it from the Nexus tree with the appropriate URL. Problem solved!All you need is to define a property file  which will allow you to:Note: this  file is inspired from:I have commented any  Maven repository definition, and added a reference to my own Nexus Maven repo.Define a sbt.bat wrapper (in order to be sure to specify   ) like:And your sbt will download artifacts  from:Just tested at home with an old Nexus opensource 1.6 I had running, java 1.6, sbt07.4That gives:If I try a funny value in the sbt.boot.properties file:So it does limit itself to the two repo I defined:(I commented everything else: , , ...)If I comment  repositories and put a funny value (2.7.9) for the scala version in the , I do get (like the OP did)If I put 2.7.7 (while still having  repo commented), yes, it won't generate an error:\nIf I remove that library from my  directory, then it will throw an Exception:Well this has bugged me for a while so I found a guy that has written an SBT plugin for maven out on github called  so all you have to do is include it in your plugins project and make your project mixin with  maven.MavenDependencies and all your operations like update and publish-local work with your local maven. The nice thing about that is if you are like me, your org is all maven.  So, all you libs are in you local maven repo but if for some reason you build with sbt first, then you start getting a bunch or jars in ivy too.  What a waste of space, and time since you will still need to get them for your maven builds.That said, I wish this were built into sbt so I would not need to add it to every project.  Maybe as a processor at least.  He mentioned in one thing I read that he would like to add it to 0.9 but I have not been able to find it.edit the config file in sbt_home/conf \"sbtconfig.txt\"add two line the repo.properties content is"},
{"body": "I'm still at the beginning in learning scala in addition to java and i didn't get it how is one supposed to do DI there? can or should i use an existing DI library, should it be done manually or is there another way?Standard Java DI frameworks will usually work with Scala, but you can also use language constructs to  without external dependencies.A new dependency injection library specifically for Scala is Dick Wall's .Whereas the Jonas Bon\u00e9r article referenced in Dan Story's answer emphasizes compile-time bound instances and static injection (via mix-ins), SubCut is based on runtime initialization of immutable modules, and dynamic injection by querying the bound modules by type, string names, or scala.Symbol names.You can read more about the comparison with the Cake pattern in the  document.Dependency Injection itself can be done without any tool, framework or container support. You only need to remove s from your code and move them to constructors. The one tedious part that remains is wiring the objects at \"the end of the world\", where containers help a lot.Though with Scala's 2.10 macros, you can generate the wiring code at compile-time and have auto-wiring and type-safety.See the I haven't done so myself, but most DI frameworks work at the bytecode level (AFAIK), so it should be possible to use them with any JVM language.A recent project illustrates a DI based purely on constructor injection: In addition to the answer of Dan Story, I blogged about a DI variant that also uses language constructs only but is not mentioned in Jonas's post: .\nThis pattern is working very well for me.I have shown how I created a very simple functional DI container in scala using 2.10 Previous posts covered the techniques. I wanted to add a link to Martin Odersky's May 2014 talk on the Scala language objectives. He identifies languages that \"require\" a DI container to inject dependencies as poorly implemented. I agree with this personally, but it is only an opinion. It does seem to indicate that including a DI dependency in your Scala project is non-idiomatic, but again this is opinion. Practically speaking, even with a language designed to inject dependencies natively, there is a certain amount of consistency gained by using a container. It is worth considering both points of view for your purposes."},
{"body": "Scala has symbols - names that start with a single quote ' and which are a kind of string constants.I know symbols from Ruby (where they start with a colon). In Ruby they are used for some meta-programming tasks, like generating getters and setters for member variables (for example  to generate a getter for ).I haven't seen a lot of use of symbols in Scala code yet. What are practical uses for symbols in Scala?Do symbols really fit into Scala?In the wonderful land of Lisp, code is represented as nested lists of literal objects that denote themselves (strings, numbers, and so on), and symbols, which are used as identifiers for things like classes, functions, and variables.  As Lisp code has a very simple structure, Lisp allows the programmer to manipulate it (both at compile-time and run-time).  Clearly, when doing this, the programmer will inevitably encounter symbols as data objects.So symbols are (and need to be) objects in Lisp in any case, so why not use them as hash table keys or as enums as well?  It's the natural way of doing things, and it keeps the language simple, as you don't have to define a special enumeration type.To summarise, symbols are naturally used for code manipulation, enumeration, and keying.  But Java people don't use identity as the equivalence relation between hash keys by default (which your traditional Lisp does), so they can just use strings as their keys.  Enum types are defined separately in Scala.  And finally, code as data isn't supported by the language at all.So no, my impression is that symbols don't belong in the Scala language.  That said, I'm going to keep an eye on the replies to this question.  They might still demonstrate a genuine use of symbols in Scala that I can't think of right now.( Depending on the Lisp dialect, Lisp symbols may also be namespace-qualified, which is, of course, an immensely useful feature when manipulating code, and a feature that strings don't have.)Searching a bit around the web it seems that the sense of symbols (symbol literals) in (a language like) Scala in comparision with Strings is a matter of semantics, and thus possibly even compiler awareness.'String' is a datatype, consisting of a sequence of characters. You can operate on strings and so manipulate them. Strings can semantically be any text data, from a filename to a message to be printed on screen, a line in a CSV file, or whatever.For the compiler -and thus the IDE- strings are values of a data type String, like numbers (sequences of digits) are values of a data type say: Integer .\nThere is on the program level no difference between \"foo\" and \"bar\".OTOH Symbols are identifiers, i.e. semantically identifying an item in the program.\nIn this matter they are like class names, method names or attribute names. \nBut while a class name identifies the class -i.e. the set of properties declaring the class' structure and behaviour- and a method name identifies the method -i.e. the parameters and statements- , a symbol name identifies the symbol -i.e. itsself, nothing more- .So the compiler can explicitly distinguish between the symbols 'foo and 'bar, like he distinguishes between the classes  Foo and Bar. As part of the compiler's symbol table, you can apply the same mechanisms in an IDE e.g. to search for the usage of 'foo (i.e. the references to this symbol) like you search for the usage of class Foo.In comparision, searching for a string \"foo\" would require different approaches, like a full text scan. It follows the same semantics as searching for all occurrences of 4711 in the program code.That's how I understand it, someone may correct me if I'm wrong.According to the Scala book, Symbols are interned: \"\"In contrast, s are only interned if they appear in literal form (at least in Java they are, not entirely sure about Scala). So I guess if you do a lot of serialization of s that are then put into collections, you might use symbols instead and save yourself some memory.But I agree with skaffman, I'm not totally convinced of their use.(In Ruby, s are, apart from the meta-programming example you give, often used as keys in es. In Ruby this is useful because there, s are never interned: every  allocates new memory. In Scala it might be useful, as I mentioned, if you combine it with a lot of (de)serialization so the Java s don't get interned as well.)I guess Scala added them because functional languages use them.They forgot, though, to add the ability of referencing an identifier through a symbol, which is kind of the central point of their existence. There's an experimental feature in Scala 2.8 that gives some of that. I'll quote the relevant part of the API documentation in full:A more convenient syntax for reflective invocation.\nExample usage: You can call it reflectively one of two ways: If you call the  method and do not give the type inferencer enough help, it will most likely infer , which will result in a . Author\nPaul PhillipsI believe comparisons between symbols is faster. If you've used Erlang, symbols are used a tonne when passing around messages and you want something cheap, and fast, that works well ACROSS machine boundaries. I'm not sure in what state remote actors are in Scala, IIRC, they were rather dodgy, but in the future when them in place, symbols could well be very useful in much the same way as they are in Erlang. Also case classes, some of the benefits aren't as apparent, then again, symbols are still cheaper.I suppose you would use them when you want to refer to the name of a thing that isn't an existing identifier in the code. The Scala book gives the example of referring to the name of a database column - it isn't an arbitrary string, it's actually the name of a thing.It's a bit tenuous, though, I'm not altogether convinced.I can name one case when symbols are really used in Scala.\nPlay 2.2 uses  to access database.\nHere goes a code sample for the simple add entity method:so you can see the usage of symbols in the .on(bla bla bla)\nIt is also absolutely valid to use String literals instead of symbols and some guys are doing so, but in the anorm source code the corresponding method signature really use Symbol paremeter type.As already noted, symbols carry-over from other (more) functional languages. Something others have not mentioned is that not only do they fill the role of symbols, but they are also the closest equivalent of keywords (minus the performance advantage maybe). In my opinion, they are more useful as keywords, meaning explicit identifiers.Below I will include a court description from  of keywords and symbols.Scala symbols are not as powerful as symbols in some languages. Therefore, they are not as useful either. However, I don't see why they couldn't offer the same meta-programming and performance advantages as keywords. At the very least, they can make your code easier to read."},
{"body": "I want to be able to declare something like this:That it, the type  should be both a subtype of   . Is this possible?Use Compound Type:"},
{"body": "How to explain Scala's type system to a Haskell expert?\nWhat examples show Scala's advantages?How to explain Haskell's type system to an advanced Scala practitioner?\nWhat can be done in Haskell that can't be done in Scala?Scala is a strict and impure language with first-class modules. New types are declared as modules (\"classes\"/\"traits\" with subtle differences). Modules can be type constructors taking universally quantified type parameters. Modules have members which consist of values, mutable variables, and functions (called \"methods\", to which the module is implicitly passed as a variable called ). Modules may have type members which can also take parameters. Type members are existentially quantified and type parameters can be higher-kinded. Because types can be members of first-class values, Scala provides a flavour of dependent typing called .First-class functions are also modules. A function is a module with a method named . A method is not first-class, but a syntax is provided to wrap a method in a first-class function. Unfortunately, a module requires all of its type parameters up front, hence a partially applied first-class function is not allowed to be universally quantified. More generally, Scala completely lacks a direct mechanism for types of rank higher than 1, but modules parameterized on higher-kinded types can be exploited to simulate rank-n types.Instead of type classes with global scope, Scala lets you declare an implicit value of any given type. This includes function types, which provides implicit conversion, and therefore type extension. In addition to implicit conversions, type extension is provided by the \"extends\" mechanism which lets you declare a subtype/supertype relation among modules. This mechanism can be used to simulate algebraic datatypes where the supertype can be seen as the type on the left-hand side of a data declaration, and its subtypes as the value constructors on the right-hand side. This also means that ADTs are not really closed. Scala has extensive pattern-matching capabilities using a virtualized pattern matcher with first-class patterns.Scala supports subtyping, and this limits type inference considerably. But type inference has improved over time. Inference of higher kinded types is supported. However, Scala lacks any meaningful kind system, and therefore has no kind inference and no kind unification. If a type variable is introduced, it is of kind  unless annotated otherwise. Certain types like  (the supertype of all types) and  (a subtype of every type) are technically of  although they cannot be applied to type arguments.Haskell is a purely functional language. This means that functions are not allowed to have any side-effects at all. For example, a Haskell program doesn't print to the screen as such, but is a function that returns a value of the  datatype which describes a sequence of actions for the IO subsystem to perform.Whereas Scala is strict by default and provides \"by-name\" annotation for nonstrict function arguments, Haskell is lazy by default using \"by-need\" semantics, and provides annotation for strict arguments.Type inference in Haskell is more complete than in Scala, having full inference, even for partially applied polymorphic type constructors. This means that type annotation is almost never necessary.Recent extensions to the GHC compiler allow advanced type system features that have no equivalent in Scala, such as rank-n types, type families, and polykinds.In Haskell, a module is a collection of types and functions, but modules are not first-class entities. Implicits are provided by type classes, but these are globally scoped once declared, and they cannot be passed explicitly as in Scala. Multiple instances of a given type class for a given type are resolved by the  mechanism whereas in Scala this would be solved simply by scoping or by passing instances explicitly.Since Haskell isn't \"object-oriented\", there's no method/function dichotomy. Every function is first class and every function is curried by default (no Function1, Function2, etc).I don't believe anyone has systematically compared Haskell (as exemplified by GHC's type system) with Scalas. The main points of difference are the degree of type inference, and support for higher-rank types. But a full treatment of the differences would be a publishable result."},
{"body": "Assume I have a Regex pattern I want to match many Strings to.I just want to check whether a given String fully matches the Regex. What is a good and idiomatic way to do this in Scala?I know that I can pattern match on Regexes, but this is syntactically not very pleasing in this case, because I have no groups to extract:Or I could fall back to the underlying Java pattern:which is not elegant, either.Is there a better solution?Answering my own question I'll use the \"pimp my library pattern\"and use it like thisunless someone comes up with a better (standard) solution.I don't know Scala all that well, but it looks like you can just do:For the full match you may use . This method tries to match target (whole match) and returns the matches.The answer is in the regex:Then use the one of the existing methods."},
{"body": "I have a very simple question - when should we apply the new keyword when creating objects in Scala? Is it when we try to instantiate Java objects only?Use the  keyword when you want to refer to a 's own constructor:Omit  if you are referring to the companion object's  method:If you've made a case class:Scala secretly creates a companion object for you, turning it into this:So you can doLastly, keep in mind that there's no rule that says that the companion \nmethod has to be a proxy for the constructor:And, since you mentioned Java classes: yes -- Java classes rarely have\ncompanion objects with an  method, so you must use  and the actual\nclass's constructor.Not at all. There is two general cases when you ommit  in scala.\nWith singleton objects (that are oftenly used to store static functions and as a kind of factory similar to what you may seen in java):With a  (actually, underneath there are class + object =  pattern, e.g. having class and object with the same name):So when you work with a simple classes, rules are the same as with Java.Bonus, there is a anonymous classes in scala, which can be constructed like this: "},
{"body": "I have an application based on Squeryl. I define my models as case classes, mostly since I find convenient to have copy methods.I have two models that are strictly related. The fields are the same, many operations are in common, and they are to be stored in the same DB table.  there is some behaviour that only makes sense in one of the two cases, or that makes sense in both cases but is different.Until now I only have used a single case class, with a flag that distinguishes the type of the model, and all methods that differ based on the type of the model start with an if. This is annoying and not quite type safe.What I would like to do is factor the common behaviour and fields in an ancestor case class and have the two actual models inherit from it. But, as far as I understand, inheriting from case classes is frowned upon in Scala, and is even prohibited if the subclass is itself a case class (not my case).My preferred way of avoiding case class inheritance without code duplication is somewhat obvious: create a common (abstract) base class:\nIf you want to be more fine-grained, group the properties into individual traits:Since this is an interesting topic to many, let me shed some light here.You could go with the following approach:Yes, you have to duplicate the fields. If you don't, it simply would not be possible to implement correct equality .However, you don't need to duplicate methods/functions.If the duplication of a few properties is that much of an importance to you, then use regular classes, but remember that they don't fit FP well.Alternatively, you could use composition instead of inheritance:Composition is a valid and a sound strategy that you should consider as well.And in case you wonder what a sealed trait means \u2014 it is something that can be extended only in the same file. That is, the two case classes above have to be in the same file. This allows for exhaustive compiler checks:Gives an error:Which is really useful. Now you won't forget to deal with the other types of s (people). This is essentially what the  class in Scala does.If that does not matter to you, then you could make it non-sealed and throw the case classes into their own files. And perhaps go with composition.case classes are perfect for value objects, i.e. objects that don't change any properties and can be compared with equals. But implementing equals in the presence of inheritance is rather complicated. Consider a two classes: and So according to the definition the ColorPoint(1,4,red) should be equal to the Point(1,4) they are the same Point after all. So ColorPoint(1,4,blue) should also be equal to Point(1,4), right? But of course ColorPoint(1,4,red) should not equal ColorPoint(1,4,blue), because they have different colors. There you go, one basic property of the equality relation is broken.You can use inheritance from traits solving lots of problems as described in another answer. An even more flexible alternative is often to use type classes. See  or In these situations I tend to use composition instead of inheritance i.e.Obviously you can use a more sophisticated hierarchy and matches but hopefully this gives you an idea. The key is to take advantage of the nested extractors that case classes provide"},
{"body": "Say I have an  likeNow I would like to append an element to the array, say the value , as in the following example: I can of course use  and do this on my own, but there must be a Scala library function for this, which I simply could not find. Thanks for any pointers!Notes:Thanks for the answers pointing to the  operator method. This is what I was looking for. Unfortunately, it is rather slower than a custom append() method implementation using  -- about two to three times slower. Looking at the implementation in , a builder is created, then the array is added to it, then the append is done via the builder, then the builder is rendered. Not a good implementation for arrays. I did a quick benchmark comparing the two methods, looking at the fastest time out of ten cycles. Doing 10 million repetitions of a single-item append to an 8-element array instance of some class  takes 3.1 sec with  and 1.7 sec with a simple  method that uses  doing 10 million single-item append repetitions on 8-element arrays of Long takes 2.1 sec with  and 0.78 sec with the simple  method. Wonder if this couldn't be fixed in the library with a custom implementation for ?For what it's worth, I filed a ticket:\nYou can use  to append element to array and  to prepend it:should produce:It's the same as with any other Scala collection. Works also \"reversed\":There is also an \"in-place\" version:The easiest might be:Actually, Array can be implcitly transformed in a "},
{"body": "I'm starting to explore Scala, and one of the things I'm intrigued by is the  type and the promise of being able to eliminate  related errors.However I haven't been able to work out how to transform a list (or other collection) of, say, , to a collection of  (obviously filtering out any values that are ).In other words, how do I get from this:... to this:I'm using Scala 2.8 if that has any impact on the answer.For educational purposes, you might like some alternatives:With Scalaz, you can perform a slightly different operation called , that returns ."},
{"body": "Is there a function that can truncate or round a Double?  At one point in my code I would like a number like:  to be rounded to You can use :There are a number of other , which unfortunately aren't very well documented at present (although ).Here's another solution without BigDecimalsTruncate:Round: Or for any double n and precision p:Similar can be done for the rounding function, this time using currying:which is more reusable, e.g. when rounding money amounts the following could be used:Since no-one mentioned the  operator yet, here comes. It only does truncation, and you cannot rely on the return value not to have floating point inaccuracies, but sometimes it's handy:Edit: fixed the problem that @ryryguy pointed out.  (Thanks!)If you want it to be fast, Kaito has the right idea.   is slow, though.  For any standard use you're better off with a recursive function:This is about 10x faster if you're within the range of  (i.e 18 digits), so you can round at anywhere between 10^18 and 10^-18.How about : Recently, I faced similar problem and I solved it using following approachI used  SO issue. I have couple of overloaded functions for both Float\\Double and implicit\\explicit options. Note that, you need to explicitly mention the return type in case of overloaded functions.For those how are interested, here are some times for the suggested solutions...You may use implicit classes:I wouldn't use BigDecimal if you care about performance. BigDecimal converts numbers to string and then parses it back again:I'm going to stick to math manipulations as  suggested."},
{"body": " I've been looking for a good solution to developing a service in Scala that will sit between mobile devices and existing web services.The current list of viable options are:There are probably more options out there. How does one decide which one to use? What are the traits (excuse the pun ;-) of a good Scala middleware choice. On the one side, I would like to go for Akka, because it is part of the TypeSafe Scala stack, but on the other, something like Finagle has a rich set of libraries and makes plumbing so easy. Spray looks nice and simple to use.Any advice, insights or experience would be greatly appreciated. I'm sure someone out there must have some experience with some of these that they won't mind sharing.I would love for this question to be reopened. A good answer to this question will help new Scalateers to avoid related pitfalls.These are my own experiences since asking this question: - I used Finagle for a project and it's rock solid. - In my latest project I'm using Spray and I'm extremely happy. The latest releases are built on Akka 2 and you can run it directly with the Spray-can library which removes the need for a web server. Spray is a set of libraries, rather than a framework and is very modular. The  gives a great overview, and  shows a really nice development approach and architecture.Life moves pretty fast. If you don't stop and look around once in a while, you could miss it. - These days the choice has become simpler. In my humble opinion Spray has won the battle. It is being  to become the next Akka HTTP. I have been using Spray now on multiple projects and can honestly say that it's fantastic and best supported software I have ever encountered.This does not answer the initial question, but at least gives some indication on why Spray seems like the best choice in most cases. It is extremely flexible, non-blocking and very stable. It has both client-side and server-side libraries and a great testkit. Also, have a look at these stats to get an idea on performance: I personally started with spray a long time ago and tried everything else there was out there for Scala. While Scala, spray, akka, shapeless, and scalaz certainly have a bit of a learning curve, once you start digging in and really learning how you are supposed to use the technologies, they make sense and I immediately saw the benefits especially for the kind of work I'm doing right now.Personally I think nothing really stands up to spray for building both servers, rest apis, http clients, and whatever else you want. What I love about spray is that they built with akka in mind. It may have been a really early project when I first started using it, but the architecture made sense. Those guys knew what they were doing in terms of exploiting the benefits of using an actor model and not having any blocking operations.While actors might take a bit getting used to, I do like them. They have made my systems very scalable and cheap to run because I don't need as beefy hardware as in the past. Plus, spray has that spray-routing DSL so making a rest api is relatively simple as long as you follow the rules ... don't block. That of course means don't go and pull in apache commons http client to make client requests from the api or actors because you will be going back to blocking models.So far I am very happy with spray, typesafe, and akka. Their models just naturally lend themselves to building very resilient systems that come back up on their own if anything should happen and you take a fail-fast approach. The one beef that I have with spray (and it's not spray's fault) is the damn IDE support for the routing DSL. I absolutely despise Eclipse and have always been an IDEA user. When I started using the Scala plugin, everything seemed ok. Then my routing dsl naturally evolved into way bigger beasts. Something about the way IDEA parses that code makes it shit its pants anytime it encounters anything with spray-routing or shapeless. It's to the point where it's unusable (I type 2-3 letters and have to wait 5 minutes to regain control).So, for any spray-routing or heavy shapeless code, I fire up emacs with ensime, ensime-sbt, and scala-mode2. Now if I could only get a Cassandra library with the quality of astyanax and built using a more non-blocking architecture. you can find a great list of scala resources with a brief description of all the alternatives you listed. From my own experience, I use Scalatra and it is tiny, simple and effective for things like uri mapping and calling web services."},
{"body": "I found the following code snippet:The  looks like a method, but my friend told me it's a keyword.I googled it, but found few documents about . What does it mean, and where can I get some documents about it?The  keyword is used to define existential types in Scala. There's this Scala  page explaining what they are. I couldn't find a place in the Scala docs explaining them in detail, so  is a blog article I found on Google explaining how they are useful.Update: you can find a precise definition of existential types in the  but it is quite dense.To summarize some of the posts I linked to, existential types are useful when you want to operate on something but don't care about the details of the type in it. For example, you want to operate on arrays but don't care  of array:which you could also do with a type variable on the method:but you may not want to add the type variable in some cases. You can also add a bound to the type variable:Also see  which is where I got this example from.I don't know Scala, but your question picked up my interest and started Googling.I found that in :"},
{"body": "How does  relate to ? The Cats project mentions it is descended from scalaz.  I would like to keep this from getting too political*, but cats is for all intents and purposes scalaz. It has not reached full parity as of yet, but keep in mind it was only created a few months ago. The goal is for it to be a more pragmatic approach and more democratic when it comes to its evolution. So, naming of operators and classes is hopefully going to be a little more straightforward, as well as it has no qualms with using mutable data within a method if it means better performance. Last, they are HOPING to have better documentation....all of this means that it may end up becoming a replacement for scalaz with a better beginner's approach for those not embroiled in the math world. If you want a fuller answer, then maybe head over to their gitter board and Erik (non) could answer it himself :)*The gist is that scalaz has some social baggage with it that causes a number of big names to shy away from using and/or contributing."},
{"body": "By suitable, I mean:In no particular order.Which GUI toolkits satisfy which conditions?Scala has its own Swing library. This will use the standard Swing library under the hood but it has some nice additions and most notably, because it's made for Scala, it can be used in a much more concise way than the standard Swing library:See how nice and easy it is to set widget properties?Except for the nicer syntax and the small additions (like the  class), I think the advantages and disadvantages are pretty much the same as with vanilla Swing.Here's an .ScalaFX is a mature wrapper around JavaFX 2. It is superficially similar to Scala Swing in some regards, but has some features that make it stand out:I use Java Swing. Here's how it stacks up against your requirements:Now that some time has passed, it's worth noting that ScalaJS is also an option to consider. Against your requirements:The downside is that it takes your GUI off the JVM and into the browser, so you can't use all those Java libraries.I'd say use .Net its mature and used extensively. But ...I assume that you require to use Scala. For that I'd stick to Java as it has better tool support like in IDEA or NetBeans. You can of course mix Java and Scala. So your GUI can be done in Java that calls Scala backend. Also take a look at JGoodies they have lot of nice stuff to build UI in Swing. At this moment there is no tool support for Scala GUI applications (and for non GUI it is still behind Java)."},
{"body": "What's the best way to terminate a fold early? As a simplified example, imagine I want to sum up the numbers in an , but if I encounter something I'm not expecting (say an odd number) I might want to terminate. This is a first approximationHowever, this solution is pretty ugly (as in, if I did a .foreach and a return -- it'd be much cleaner and clearer) and worst of all, it traverses the entire iterable even if it encounters a non-even number.So what would be the best way to write a fold like this, that terminates early? Should I just go and write this recursively, or is there a more accepted way?My first choice would usually be to use recursion.  It is only moderately less compact, is potentially faster (certainly no slower), and in early termination can make the logic more clear.  In this case you need nested defs which is a little awkward:My second choice would be to use , as it keeps everything else intact and you only need to wrap the fold in a  so you have something to return from--in this case, you already have a method, so:which in this particular case is a lot more compact than recursion (though we got especially unlucky with recursion since we had to do an iterable/iterator transformation).  The jumpy control flow is something to avoid when all else is equal, but here it's not.  No harm in using it in cases where it's valuable.If I was doing this often and wanted it within the middle of a method somewhere (so I couldn't just use return), I would probably use exception-handling to generate non-local control flow.  That is, after all, what it is good at, and error handling is not the only time it's useful.  The only trick is to avoid generating a stack trace (which is really slow), and that's easy because the trait  and its child trait  already do that for you.  Scala already uses this internally (in fact, that's how it implements the return from inside the fold!).  Let's make our own (can't be nested, though one could fix that):Here of course using  is better, but note that you could put  anywhere, not just wrapping an entire method.Next in line for me would be to re-implement fold (either myself or to find a library that does it) so that it could signal early termination.  The two natural ways of doing this are to not propagate the value but an  containing the value, where  signifies termination; or to use a second indicator function that signals completion.  The Scalaz lazy fold shown by Kim Stebel already covers the first case, so I'll show the second (with a mutable implementation):(Whether you implement the termination by recursion, return, laziness, etc. is up to you.)I think that covers the main reasonable variants; there are some other options also, but I'm not sure why one would use them in this case.  ( itself would work well if it had a , but it doesn't, and the extra work it takes to do that by hand makes it a silly option to use here.)The scenario you describe (exit upon some unwanted condition) seems like a good use case for the  method. It is essentially , but should end upon encountering an element that doesn't meet the condition.For example:This will work just fine for s/s too. The solution I suggest for your \"sum of even numbers, but break on odd\" is:And just to prove that it's not wasting your time once it hits an odd number...You can do what you want in a functional style using the lazy version of foldRight in scalaz. For a more in depth explanation, see . While this solution uses a , you can convert an  into a  efficiently with .This only printswhich clearly shows that the anonymous function is only called twice (i.e. until it encounters the odd number). That is due to the definition of foldr, whose signature (in case of ) is . Note that the anonymous function takes a by name parameter as its second argument, so it need no be evaluated.Btw, you can still write this with the OP's pattern matching solution, but I find if/else and map more elegant.Well, Scala does allow non local returns. There are differing opinions on whether or not this is a good style.In this particular case, as @Arjan suggested, you can also do:@Rex Kerr your answer helped me, but I needed to tweak it to use EitherYou could try using a temporary var and using takeWhile. Here is a version. The  should be  in this case. You can throw a well-chosen exception upon encountering your termination criterion, handling it in the calling code.A more beutiful solution would be using span:... but it traverses the list two times if all the numbers are evenJust for an \"academic\" reasons (:Takes twice then it should but it is a nice one liner.\nIf \"Close\" not found it will return Another (better) is this one:"},
{"body": "I need a mature HTTP client library that is idiomatic to scala, concise in usage, simple semantics. I looked at the Apache HTTP and the Scala Dispatch and numerous new libraries that promise an idiomatic Scala wrapping. Apache HTTP client sure demands verbosity, while Dispatch was easily confusing.What is a suitable HTTP client for Scala usage?I've recently started using , a bit arcane (great general intro, serious lack of detailed scenario/use-case based docs).  Dispatch 0.9.1 is a Scala wrapper around Ning's ; to fully understand what going on requires introducing one's self to that library.  In practice, the only thing I really had to look at was the  - everything else falling nicely into my understanding of HTTP.I give the 0.9 release a solid thumbs up (so far!) on getting the job done very simply.. once you get past that initial learning curve.Dispatch's Http \"builder\" is immutable, and seems to work well in a threaded environment.  Though I can't find anything in docs to state that it is thread-safe; general reading of source suggests that it is.Do be aware that the 's are mutable, and therefore are NOT thread-safe.Here are some additional links I've found helpful:A little late to the party here, but I've been impressed with .It's got a nice DSL for building requests, supports both sync and async execution, as well as a variety of (un)marshalling types (JSON, XML, forms). It plays very nicely with , too.I did a Dispatch, and a few others libraries, .\nThe only serious ones currently are  and . is a bit arcane in its syntax.  is quite easy to use :(build.sbt)(basic usage)Having had some unhappy experiences with the Apache client, I set about writing my own. The built-in HttpURLConnection is widely asserted to be buggy. But that's not my experience of it. In fact, the reverse has been so, the Apache client having a somewhat problematic threading model. Since Java6 (or 5?), HttpURLConnection has provided efficient HTTP1.1 connections with essentials like keep-alive being built in, and it handles concurrent usage without fuss.So, to compensate for the inconvenient API offered by HttpURLConnection, I set about writing a new API in Scala, as an open-source project. It's just a wrapper for HttpURLConnection, but unlike HttpURLConnection, it aims to be easy to use. Unlike Apache client, it should fit easily into an existing project. Unlike Dispatch, it should be easy to learn.It's called Bee ClientMy apologies for the shameless plug. :)Besides Dispatch there is not much out there.  had a attempt at building a functional http client. But it is outdated for a while an no version of it exists in the scalaz7 branch. Additionally there is a useful  of the ning async-http-client within the playframework. There your can do calls like:You can use this API as inspiration if you don't use play! in your project and dislike the Dispatch API. Two years after originally responding to this post, I would have a different answer.I'd start by taking a hard look at , which a collaboration between the spray and akka teams.  As of today, it hasn't reached it 1.0 release; it's coming soon, is backed by typesafe, and is looking good.You really should consider using . In my opinion it have a bit tricky syntax, but it is still pretty usable is you aimed to build high-perfomance http client. Main advantage of using spray is that it based on  actor library which is really extremly scalable and powerfull (you can scale out your http client to several machines only by changing conf files). One more option is play2 ws lib usage (). As far as I know it is still not separated from play distribution, but due it's extremely simplicity it worth it to spent some time for attaching whole play framework for the only part. There are some issues with providing configuration to it, so this is not too good for drop-and-use, but we have used it in few not play-based projects and everything was fine.ScalaJ-Http is a very simple synchronous http client I'd recommend it if you need a no-ceremony barebones Scala client. I've used Dispatch, Spray Client and the Play WS Client Library...None of them were simply to use or configure. So I created a simpler HTTP Client library which lets you perform all the classic HTTP requests in simple one-liners.See an example:... produces ...The library is called Cirrus and is available via Maven CentralThe documentation is available on GitHubSurprised that no one mentioned finagle here. It is super simple to use:See quick start guid for more detail: "},
{"body": "I haven't seen many examples of the scalaz state monad. There is  but it is hard to understand and there is only one  on stack overflow it seems.I'm going to post a few examples I've played with but I would welcome additional ones. Also if somebody can provide example on why , ,  and  are used for that would be great.I assume,  and the following imports (look at answer history for ):The state type is defined as  where  is type of the state and  is the type of the value being decorated. The basic syntax to create a state value makes use of the  function:To run the state computation on a initial value:The state can be threaded through function calls. To do this instead of , define . Use the  function...Then the  syntax can be used to compose functions:Here is another example. Fill a list with  state computations.Use sequence to get a . We can provide a type alias.Or we can use  which will infer the types:Another example with  to compute frequency of sums on the list above.  computes the sum of the throws and counts frequencies.Now use traverse to apply  over .  is equivalent to .Or more succinctly by using  to infer the types:Note that because  is a type alias for , tenDoubleThrows2 ends up being typed as . I use  to turn it back into a  type. In short, it seems the key to use state is to have functions returning a function modifying the state and the actual result value desired... I gave up on trying using  may be someone else can show if  or  can be augmented to perform the combined computation. What I found instead is that the composition of the two state transformers can be combined like this:It's predicated on  being a one parameter function taking the result of the first state transformer and returning a state transformer. Then the following would work:I stumbled on an interesting blog post  from sigfp that has an example of applying two state monads through a monad transformer. Here is a scalaz translation. The  shows a  monad:So I have here an example of using  and . After playing with it a bit,  turns out to be really convenient to generate a  value, but the other thing it allows is to access the state inside the for comprehension.  is a convenient way to transform the state inside the for comprehension. So the example above can be read as:The for comprehension syntax already makes it trivial to work on the  side in . , ,  and  provide some tools to work on the  side in .The  in the blog post translates to:Very much the same explanation as .The  is more tricky and I hope there is something simpler that I have yet to discover. In that code,  takes care of the transformation of both states (increment and suffix with ) as well as pulling out the  state.  allows us to add state transformation on an arbitrary monad . In this case the state is an  that is incremented. If we called  we would end up with . In our example,  is , so we'll end up with  which is .   The tricky part here is that we want to pull out the  state value out from . This is what  is for. It just creates an object that gives access to the state in a way we can flatMap with . Edit: Turns out all of that awkwardness can be avoided if we truly reused  and  which conveniently store the wanted states in the  element of their returned tuples:Here is a very small example on how  can be used:Let's define a small \"game\" where some game units are fighting the boss (who is also a game unit).When the play is on we want to keep track of the game state, so let's define our \"actions\" in terms of a state monad:Let's hit the boss hard so he loses 10 from his : And the boss can strike back! When he does everyone in a party loses 5 . Now we can  these actions into :Of course in the real life the play will be more dynamic, but it is food enough for my small example :)We can run it now to see the final state of the game:So we barely hit the boss and one of the units have died, RIP.The point here is the . \n (which is just a function ) allows you to define actions that produce results and as well manipulate some state without knowing too much where the state is coming from.\nThe  part gives you composition so your actions can be composed:and so on. As for differences between ,  and : can be seen as  and  together:or simplySo when you use  you conceptually use  and , or you can just use them alone."},
{"body": "I've started using Spark SQL and DataFrames in Spark 1.4.0.  I'm wanting to define a custom partitioner on DataFrames, in Scala, but not seeing how to do this.One of the data tables I'm working with contains a list of transactions, by account, silimar to the following example.At least initially, most of the calculations will occur between the transactions within an account.  So I would want to have the data partitioned so that all of the transactions for an account are in the same Spark partition.But I'm not seeing a way to define this.  The DataFrame class has a method called 'repartition(Int)', where you can specify the number of partitions to create.  But I'm not seeing any method available to define a custom partitioner for a DataFrame, such as can be specified for an RDD.The source data is stored in Parquet.  I did see that when writing a DataFrame to Parquet, you can specify a column to partition by, so presumably I could tell Parquet to  partition it's data by the 'Account' column.  But there could be millions of accounts, and if I'm understanding Parquet correctly, it would create a distinct directory for each Account, so that didn't sound like a reasonable solution.Is there a way to get Spark to partition this DataFrame so that all data for an Account is in the same partition?In Spark >= 1.6 it is possible to use partitioning by column for query and caching. See:  and  using  method:Unlike  Spark  (including  a.k.a ) cannot use custom partitioner as for now. You can typically address that by creating an artificial partitioning column but it won't give you the same flexibility. :One thing you can do is to pre-partition input data before you create a Since  creation from an  requires only a simple map phase existing partition layout should be preserved*:The same way you can repartition existing :So it looks like it is not impossible. The question remains if it make sense at all. I will argue that most of the time it doesn't::JDBC data sources support . It can be used as follows:It creates a single JDBC partition per predicate. Keep in mind that if sets created using individual predicates are not disjoint you'll see duplicates in the resulting table.:Spark  provides  method which can be used to \"partition\" data on write. It separates data on write using provided set of columnsThis enables predicate push down on read for queries based on key:but it is not equivalent to . In particular aggregations like:will still require : (Spark >= 2.0): has similar applications as  but it is available only for tables (). As of today (Spark 2.1.0) it doesn't look like there are any execution plan optimizations applied on bucketed tables.* By  I mean only a data distribution.  RDD has no longer a partitioner.\n** Assuming no early projection. If aggregation covers only small subset of columns there is probably no gain whatsoever.In Spark < 1.6 If you create a , not the plain old  you can use the   (ensures each of N reducers gets non-overlapping ranges of x) &  (shortcut for Distribute By and Sort By) for example; Not sure how this fits in with Spark DF api. These keywords aren't supported in the normal SqlContext (note you dont need to have a hive meta store to use the HiveContext) Spark 1.6+ now has this in the native DataFrame APIUse the DataFrame returned by:There is no explicit way to use partitionBy on a DataFrame, only on a PairRDD, but when you sort a DataFrame, it will use that in it's LogicalPlan and that will help when you need to make calculations on each Account.I just stumbled upon the same exact issue, with a dataframe that I want to partition by account.\nI assume that when you say \"want to have the data partitioned so that all of the transactions for an account are in the same Spark partition\", you want it for scale and performance, but your code doesn't depend on it (like using mapPartitions() etc), right?I was able to do this using RDD. But I don't know if this is an acceptable solution for you.\nOnce you have the DF available as an RDD, you can apply  to perform custom repartitioning of data. Here is a sample I used:So to start with some kind of answer : ) - You can'tI am not an expert, but as far as I understand DataFrames, they are not equal to rdd and DataFrame has no such thing as Partitioner.Generally DataFrame's idea is to provide another level of abstraction that handles such problems itself. The queries on DataFrame are translated into logical plan that is further translated to operations on RDDs. The partitioning you suggested will probably be applied automatically or at least should be.If you don't trust SparkSQL that it will provide some kind of optimal job, you can always transform DataFrame to RDD[Row] as suggested in of the comments."},
{"body": "Multiple parameters lists, e.g.  and multiple parameters per list, e.g.  are semantically equivalent so far as I can tell, and most functional languages have only one way of declaring multiple parameters, e.g. F#.The only reason I can figure out for supporting both these styles of function definitions is to allow syntax-like language extensions using a parameter list that has only one parameter in it.can now be called with the syntax-lookingHowever, there could be other ways of supporting the use of curly braces without having multiple parameter lists.A related question: why is the use of multiple parameter lists in Scala called \"currying\"? Currying is usually defined as a technique for making an n-ary function unary for the sake of supporting partial application. However, in Scala one can partially apply a function without making a \"curried\" (multiple parameter lists with one param each) version of the function.It makes you able to do e.g.:As well as allowing you to write methods that look like part of the language (which you already spotted), it's worth noting that the type inferencer will work with one block at a time.So in this: will first be inferred as , which will then be used as the type of the two underscores in the closure.\nThis is how the compiler then knows, with complete type safety, that the + operation is valid.To answer your \"related question,\" currying is simply a way of turning a function of multiple arguments, for example , into a function which takes one argument and returns a function, e.g.  (parentheses shown but not necessary).The tuple-ized form and the curried form are isomorphic, and we may translate freely between them. All of these are equivalent, but have different syntactic implications:When you declare separate parameter groups, this is the kind of currying you're doing. The multi-parameter-group method is a method which returns a function... you can see this in the REPL:Back references in default arguments:I know one of the motivations was implicit parameter lists.  \"implicit\" is a property of the list, not the parameter.  Another was probably case classes: only the first parameter list become case fields."},
{"body": "I have a List, which may contain elements that will compare as equal. I would like a similar List, but with one element removed. So from (A, B, C, B, D) I would like to be able to \"remove\" just one B to get e.g. (A, C, B, D). The order of the elements in the result does not matter.I have working code, written in a Lisp-inspired way in Scala. Is there a more idiomatic way\nto do this?The context is a card game where two decks of standard cards are in play, so there may\nbe duplicate cards but still played one at a time.I haven't seen this possibility in the answers above, so:Edit:Like a charm :-).You could use the filterNot method.You could try this:And as method:Unfortunately, the collections hierarchy got itself into a bit of a mess with  on .  For  it works just like you might hope:but, sadly,  ended up with a -style implementation and thus does the \"wrong thing\"  throws a deprecation warning at you (sensible enough, since it is actually ing):So arguably the easiest thing to do is convert  into a collection that does this right, and then convert back again:Alternatively, you could keep the logic of the code you've got but make the style more idiomatic:As one possible solutions you can find index of the first suitable element and then remove element at this index:How aboutIf you see , there's something wrong.Just another thought on how to do this using a fold:"},
{"body": "I have Intellij ultimate 11.1, with the scala plugin installed.I installed scala 2.9.2 using homebrew, which put scala in:Creating a new java module project, I get this:Can someone help me fix these issues, why didn't the scala plugin make this work out of the box? :)You need to specify  directory on your first screenshot. This is Mac  specific, the same case is valid for  installed via .Standard  has different layout which IDEA understands correctly,  is creating  directory where it writes  and  directories expected by IDEA.Here's a step by step for any of those who are trying to get Scala in IntelliJ up and running. IntelliJ > Preferences > Plugins > Browse Repositories... > Scala\nClick Ok and allow IntelliJ to restartIn Terminal enterHere you are going to create a Java project from scratch as you normally would.  However, in the last step, you will select Scala in the \"Technologies\" page.  In the  enter  In the  dropdown select Click Ok and you are ready to create your first Scala objectFor reference ->\nThe latest Brew has already realized this problem and created automatic symlinks upon installing Scala.Firstly, install Scala with DocumentationSecondly, provide the following path in IntelliJRather than downloading the docs separately you can use the  option when installing scala with homebrew:Then in IntelliJ point your JavaDocs path in the scala-library to .I'm just found another way to configure scala with intelJIdea.\nIf scala installed with brew then just write in terminal:you will see smth like that:Then just include  into your idea config panel.I am using Yosemite, and the following worked for me.Then I tested IntelliJ by creating a new project, using the Java SDK, and for compiler putting in:I tested compiling a Hello World program via  and it compiled correctly to the console.Note, I ignored what @Zagorulkin said above even though it seems logical that if Homebrew is saying so my compile path should be different to the above.Change  to  and you are good to go."},
{"body": "I found there is also an Akka actor model, so I am wondering what's the difference between the Akka's Actor and Scala's Actor model?Well, there isn't. There is just Actor model, and Akka actors and Scala actors are two  of that model.All Actor model says that your concurrency primitives are actors, which can:and provides certain guarantees, e.g.:There is no difference between Scala and Akka actors on this level.For differences in what they can do, see . The biggest one, for me, is that Akka supports supervisors and ActorRegistry.There is also a historical answer. The creators of Scala thought there should be an actor framework. Jonas Bon\u00e9r tried it out, but was not completely satisfied so he started working on a new - which evolved into Akka. However, the Scala people thought it was better than their own - so at Jfokus 2011 they announced that Akka was to become the standard actor framework of Scala. However, that migration will take some time.This depends a little on what you mean with \"model\" - you can either refer to the \"execution model\" or the \"programming model\" (and perhaps other models as well).For execution models, there are basically two: thread-based or event-based. The Scala standard actor library contains both. The thread-based uses one thread for each actor, whereas the event-based uses a thread-pool. The former is more intuitive to understand, the latter is more efficient. Akka is built on the event-based model.For programming model, there is a big difference between the scala standard library and Akka. In the scala standard library, you basically implement the \"run\" method - and if you want to wait for an incoming message, you get yourself into a waiting state (by calling \"receive\" or \"react\"). So, the programming model follow the \"thread-metaphor\". However, in Akka, the programming metaphor is that you implement a few life-cycle methods - but the \"run\"-method is written inside the framework. It actually turns out that this programming model works a lot better with the event-based execution model as well.If you are interested in the different execution models and programming models of scala standard actors I have written    on the issue. "},
{"body": "Where can I find a list of Scala's \"magic\" functions, such as , , , , etc.?By magic-functions I mean functions which are used by some syntactic sugar of the compiler, for exampleI googled for some combination of   and synonyms of , but I didn't find anything.I'm not interested with the  of magic functions in the standard library, but in which magic functions exists.As far as I know:Getters/setters related:Pattern matching:For-comprehensions:Prefixed operators:Beyond that, any implicit from A to B. Scala will also convert  into , if the former operator isn't defined, \"op\" is not alphanumeric, and  isn't , ,  or .And I don't believe there's any single place where all of Scala's syntactic sugars are listed.In addition to  and , there are also a number of unary operators which (I believe) qualify as magical:Add to that the regular infix/suffix operators (which can be almost anything) and you've got yourself the complete package.You really should take a look at the Scala Language Specification.  It is the only authoritative source on this stuff.  It's not that hard to read (as long as you're comfortable with context-free grammars), and very easily searchable.  The only thing it doesn't specify well is the XML support.Sorry if it's not exactly answering your question, but my favorite WTF moment so far is @ as assignment operator inside pattern match. Thanks to soft copy of \"Programming in Scala\" I found out what it was pretty quickly.Using @ we can bind any part of a pattern to a variable, and if the pattern match succeeds, the variable will capture the value of the sub-pattern. Here's the example from  (Section 15.2 - Variable Binding):And 's what  says about it.They are defined in the Scala Language Specification.\nAs far as I know, there are just three \"magic\" functions as you mentioned.Scalas Getter and Setter may also relate to your \"magic\":I'll also add  for  likeAnd  rule, from Odersky-Spoon-Venners book:Maybe we should also mention syntactic desugaring of  which can be found And (of course!), (as correctly pointed out, this one is just an  done through a library, so it's probably not eligible, but I find it's a common puzzler for newcomers)I'd like to add that there is also a \"magic\" trait - :So you can do stuff like"},
{"body": "I have heard this time and again, and I am trying to understand and validate the idea that FP and OO are orthogonal.First of all, what does it mean for 2 concepts to be orthogonal?FP encourages immutability and purity as much as possible, while OO seems built for state and mutation \u2013 a slightly organized version of imperative programming?  I realize that objects can be immutable, but OO seems to imply state/change to me.They seem like opposites. How does that affect their orthogonality?A language like Scala makes it easy to do OO and FP both, does this affect the orthogonality of the two methods? implies that two things are unrelated. It comes from mathematics where it means . In common usage it can mean two decisions are unrelated or that one subject is irrelevant when considering another subject. As used here, orthogonal means that one concept doesn't either imply or exclude the other.The two concepts  and  are not incompatible with each other. Object orientedness does not imply mutability. Many people who are introduced to object oriented programs the traditional way often first use C++, Java, C# or similar languages where mutability is common and even encouraged (standard libraries provide a varierty of mutable classes for people to use). Therefore it is understandable that many people associate object oriented programming with imperative programming and mutability, as this is how they have learned it.However object oriented programming covers topics like:None of this implies mutability, and none of it excludes functional programming. So yes they are  in that they are different concepts. They are not opposites - you can use one, or the other, or both (or even neither). Languages like Scala and F# attempt to combine both paradigms into a single language:It means they don't affect each other. I.e. a functional language isn't less functional because it's also object oriented.If they were opposites (i.e. a purely functional language could not possibly be object oriented), they would by definition not be orthogonal. However I do not believe that this is the case.While this is true for most mainstream OO languages, there is no reason that an OO language needs to have mutable state.If a language has objects, methods, virtual inheritance and ad-hoc polymorphism, it's an object oriented language - whether it also has mutable state or not.It means that the two concepts do not have contrasting ideas or are not incompatible with each other.OO is about encapsulation, object composition, data abstraction, polymorphism via subtyping, and  (immutability is encouraged in OO as well). FP is about function composition, control abstraction, and constrained polymorphism (aka parametric polymorphism). Thus the two ideas are not contradictory. They both provide you with different kinds of powers and abstraction mechanisms, which are certainly possible to have in one language. In fact, this is the thesis on which  was built!In his  talk at Google, Martin Odersky explains it very well how he believes the two concepts - OO and FP - are orthogonal to each other and how Scala unifies the two paradigms elegantly and seamlessly into a new paradigm popularly known in Scala community as object-functional paradigm. Must watch talk for you. :-)Other examples of object-functional languages: , , .For two concepts to be orthogonal means that they can be independently realized to any degree in any given manifestation. Considering music, for instance, you can classify a musical piece as to how harmonic it is and how rhythmic it is. The two concepts \"harmonic\" and \"rhythmic\" are orthogonal in the sense that there are harmonic and rhythmic pieces, disharmonic and arrythmic pieces, but also disharmonic and rhythmic pieces as well as harmonic and arrhythmic pieces.Applied to original question this means that there are purely functional, non-object oriented  programming lanuages such as Haskell, purely object-oriented, \"non-functional\" languages such as Eiffel, but also languages which are neither such as C and languages which are both such as Scala.Simply speaking, Scala being object-oriented means that you can define data structures (\"classes\" and \"traits\") which encapsulate data with the methods that manipulate this data, guaranteeing that instances of these structures (\"objects\") are always in a defined state (the object's contract laid out in its class).On the other hand, Scala being a functional language means that it favors immutable over mutable state and that functions are first class objects, which can be used just like any other object as local variables, fields or parameters to other functions. In addition to this, almost every statement in Scala has a value, which encourages you to use a functional programming style.Orthogonality of object-orientated programming and functional programming in Scala additionaly means that you as a programmer are free to choose any mixture of these two concepts you see fit for your purpose. You can write your programs in a purely imperative style, using mutable objects only and not using functions as objects at all, on the other hand you can also write purely functional programs in Scala not using any of its object-oriented features. Scala really does not require you to use one style or the other. It lets you choose the best of both worlds to solve your problem.Like all classifications, the division of programming languages into functional, object-oriented, procedural, etc. is fictional. But we do need classifications, and in programming languages we classify by a set of language features and the philosophical approach of those who use the language (where the later is influenced by the former). So sometimes \"object-oriented\" languages can have success adopting the features and philosophies of \"functional\" programming languages and vice-versa. But certainly not all programming language features and philosophies are compatible. For example, a functional language like OCaml accomplishes encapsulation through lexical scoping and closures, whereas a object-oriented languages use public/private access modifiers. These are not incompatible mechanisms per-se, but they are redundant, and a language like F# (a mostly functional language which seeks to live in harmony with the decidedly object-oriented .NET library and language stack) has to go to lengths to bridge the gap.As another example, OCaml uses a structural type system for object-orientation, whereas most object-oriented languages use a nominal type system. These are pretty-much incompatible, and interestingly represent incompatibility within the realm of object-oriented languages. The idea of objects can be implemented in an immutable fashion. An example is the book \"\", by Abadi and Cardelli, that aims at formalizing these ideas, and where objects are first given immutable semantics because that makes reasoning about object-oriented programs simpler.In this case, a method that would traditionally have modified the object in-place instead returns a new object, while the previous object persists.You can implement functions as objects and objects as collections of functions, so there is clearly some relationship between the two concepts.You are talking about  functional programming.There is no requirement for objects to be mutable. I would say that objects and mutation were orthogonal concepts. For example, the OCaml programming language provides a syntax for purely functional object update.Not really. The lack of tail call optimization means that the majority of idiomatic purely functional code will stack overflow in Scala because it leaks stack frames. For example,  (CPS) and all of the techniques described in the paper  by Bruce McAdam. There is no easy way to fix that because the JVM itself is incapable of tail call optimization.Regarding the orthogonality of purely functional programming and object oriented programming, I would say that they are at least close to being orthogonal simply because purely functional programming deals only with programs in the small (e.g. higher order functions) whereas object oriented programming deals with the large-scale structuring of programs. This is why functional programming languages usually provide some other mechanism for large-scale structuring, e.g. the higher-order module systems of Standard ML and OCaml, or CLOS for Common Lisp or typeclasses for Haskell.One thing that helped me understand the relationship between FP and OO was the SICP book, particularly the section  If you are thinking about these issues and you have a spare weekend, it might be worth reading through the first three chapters, its pretty eye opening.Orthogonal. It sounds good. If you got an education you can band it about a bit and pretend. Its a bit like paradigm. It all depends in which circles you travel in and what each type of programming technique will give you. I have read a few posts on SS and most who come from a functional programming language usual persist on the fact that you can only go functional and anything else goes against the thinking and the mind set. Object Oriented programming is mainly about capturing state and keeping this state as localised as possible as not to be affected by anything that is not part of the object that you manage the state with. On the other hand, functional programming looks at the problem of state from a different perspective and tries to separate state from the system and reduce it down to functions. Yes you can use both techniques in your code but they both are looking at the design of software from different angles. There has been a great deal of interest in the techniques of Functional Programming, mainly because of the management required of state when dealing with multi-core chips and parallel programming. It seems at this point in time that functional programming does have the upper hand in dealing with this however you can achieve the same effect using Objects. You just think of the problem differently. Instead of scratching you head, trying to get rid of state as much as possible, you look at objects in the design and see how you can pair them down to what the core of what they are expected to do, using design patterns, CRC and Object Analysis. Where Objects do come into there own though, and where functional programming is a lot more difficult is in analysing the real world and mapping it to an understandable computerised system. In OO for example, a person object would be an encapsulation of state with methods that act upon the persons state. In Functional programming, a person would be broken down into data parts and functions that act upon the person data, with the added proviso that data should be created once and only once and be immutable.I must admit though coming from an OO background, that in most OO languages when dealing with Multi-core chips, I have gone the functional route, mainly by core programming design  structures (such as threads and delegates) and pass pseudo data objects around. This has led me to question the techniques of OO programming as it does not seem to map well to this threaded design. "},
{"body": "Suppose I have a method  but you don't know whether it will return you a string or a null, because it comes from Java. Is there an easier way to treat this in Scala instead of  ? Maybe some magic apply like  and then I can treat it in Scala way like The  companion object's  method serves as a conversion function from nullable references:The  object has an method that does exactly that:"},
{"body": "When asked about Dependency Injection in Scala, quite a lot of answers point to the using the Reader Monad, either the one from Scalaz or just rolling your own. There are a number of very clear articles describing the basics of the approach (e.g. , ), but I didn't manage to find a more complete example, and I fail to see the advantages of that approach over e.g. a more traditional \"manual\" DI (see ). Most probably I'm missing some important point, hence the question.Just as an example, let's imagine we have these classes:Here I'm modelling things using classes and constructor parameters, which plays very nicely with \"traditional\" DI approaches, however this design has a couple of good sides:How could this be modelled with the Reader monad? It would be good to retain the characteristics above, so that it is clear what kind of dependencies each functionality needs, and hide dependencies of one functionality from another. Note that using es is more of an implementation detail; maybe the \"correct\" solution using the Reader monad would use something else.I did find a  which suggests either:However, apart from being (but that's subjective) a bit too complex as for such a simple thing, in all of these solutions e.g. the  method (which calls , which calls  to find the inactive users) would need to know about the  dependency, to be able to properly call the nested functions - or am I wrong?In what aspects would using the Reader Monad for such a \"business application\" be better than just using constructor parameters? I'm not sure if this  be modelled with the Reader, yet it can be by:Just right before the start I need to tell you about small sample code adjustments that I felt beneficial for this answer.\nFirst change is about  method. I let it return  so the list of addresses can be used\nin  method. I've also added simple implementations to methods. Finally, the sample will use a \nfollowing hand-rolled version of Reader monad:Maybe that's optional, I'm not sure, but later it makes the for comprehension look better.\nNote, that resulting function is curried. It also takes former constructor argument(s) as their first parameter (parameter list).\nThat waybecomesKeep in mind that each of , ,  types can be completely arbitrary: a tuple, a function or a simple type.Here's the sample code after the initial adjustments, transformed into functions:One thing to notice here is that particular functions don't depend on the whole objects, but only on the directly used parts.\nWhere in OOP version  instance would call  here it just calls \n- a function passed to it in the first parameter.Please note, that the code exhibits the three desirable properties from the question:Reader monad lets you only compose functions that all depend on the same type. This is often not a case. In our example\n depends on  and  on . To solve that problem\none could introduce a new type (often referred to as Config) that contains all of the dependencies, then change \nthe functions so they all depend on it and only take from it the relevant data. \nThat obviously is wrong from dependency management perspective because that way you make these functions also dependent \non types that they shouldn't know about in the first place.Fortunately it turns out, that there exist a way to make the function work with  even if it accepts only some part of it as a parameter.\nIt's a method called , defined in Reader. It needs to be provided with a way to extract the relevant part from the .This knowledge applied to the example at hand would look like that:I hope that by preparing this answer I made it easier to judge for yourself in what aspects would it beat plain constructors.\nYet if I were to enumerate these, here's my list. Disclaimer: I have OOP background and I may not appreciate Reader and Kleisli \nfully as I don't use them.I would also like to tell what I don't like in Reader.You want. You technically  avoid that, but just look what would happen if I didn't convert  class to object. The respective line of for comprehension would look like:which is not that readable, is that? The point is that Reader operates on functions, so if you don't have them already, you need to construct them inline, which often isn't that pretty.I think the main difference is that in your example you are injecting all dependencies when objects are instantiated. The Reader monad basically builds a more and more complex functions to call given the dependencies, wich are then returned to the highest layers. In this case, the injection happens when the function is finally called. One immediate advantage is flexibility, especially if you can construct your monad once and then want to use it with different injected dependencies. One disadvantage is, as you say, potentially less clarity. In both cases, the intermediate layer only need to know about their immediate dependencies, so they both work as advertised for DI."},
{"body": "Lets say I have the following code:How do I fix the last line of this code?  Basically, I just want to do what, in a C-like language would be done:I figured this out myself.  There are two solutions:1) Do the explicit cast:2) Use pattern matching to cast it for you, this also catches errors:"},
{"body": "How can I convert an RDD () to a Dataframe . I converted a dataframe to rdd using . After processing it I want it back in dataframe. How can I do this ? has a number of  methods that create a  given an . I imagine one of these will work for your context.For example:Assuming your RDD[row] is called rdd, you can use:Suppose you have a  and you want to do some modification on the fields data by converting it to .To convert back to  from  we need to define the  of the .If the datatype was   then it will become as  in structure.If  then  in structure.Now you can convert the RDD to DataFrame using the  method.  To create a DataFrame from an RDD of Rows, there are two main options: As already pointed out, you could use  which can be imported by . However, this approach only works for the following types of RDDs:(source:  of the  object)The last signature actually means that it can work for an RDD of tuples or an RDD of case classes (because tuples and case classes are subclasses of ).So, to use this approach for an , you have to map it to an . This can be done by mapping each row to a custom case class or to a tuple, as in the following code snippets:orThe main drawback of this approach (in my opinion) is that you have to explicitly set the schema of the resulting DataFrame in the map function, column by column. Maybe this can be done programatically if you don't know the schema in advance, but things can get a little messy there. So, alternatively, there is another option: You can use  as in the accepted answer, which is available in the  object. Example for converting an RDD of an old DataFrame:Note that there is no need to explicitly set any schema column. We reuse the old DF's schema, which is of  class and can be easily extended. However, this approach sometimes is not possible, and in some cases can be less efficient than the first one.This code works perfectly from Import necessary classesCreate SparkSession ObjectLet's an  to make it Using . Using  and specifying column names. This way requires the input  should be of type .create the schema Now apply both  and  to Here is a simple example of converting your List into Spark RDD and then converting that Spark RDD into Dataframe.Please note that I have used Spark-shell's scala REPL to execute following code, Here sc is an instance of SparkContext which is implicitly available in Spark-shell. Hope it answer your question.Method 1: (Scala)Method 2: (Scala)Method 1: (Python)Method 2: (Python)Extracted the value from the row object and then applied the case class to convert rdd to DFThere are 4 ways to create dataframes.Simple way you can use toDF() method, before apply toDF(), you have to make structured format, than only apply toDF() method. I think this Video will help you  Assuming val spark is a product of a SparkSession.builder...Same steps, but with fewer val declarations:"},
{"body": "Suppose I'm doing something like:but I really wanted the  as  (and perhaps transform some other columns).The best I could come up with iswhich is a bit convoluted.I'm coming from R, and I'm used to being able to write, e.g.I'm likely missing something, since there should be a better way to do this in spark/scala...[EDIT: March 2016: thanks for the votes! Though really, this is not the best answer, I think the solutions based on ,  and  put forward by msemelman, Martin Senne and others are simpler and cleaner].I think your approach is ok, recall that a Spark  is an (immutable) RDD of Rows, so we're never really  a column, just creating new  each time with a new schema.Assuming you have an original df with the following schema: And some UDF's defined on one or several columns: Changing column types or even building a new DataFrame from another can be written like this:which yields: This is pretty close to your own solution. Simply, keeping the type changes and other transformations as separate s make the code more readable and re-usable.Since Spark version 1.4 you can apply the cast method with DataType on the column:If you are using sql expressions you can also do:For more info check the docs:\nAs the  operation is available for Spark s (and as I personally do not favour s as proposed by Svend at this point), how aboutto cast to the requested type? As a neat side effect, values not castable / \"convertable\" in that sense, will become .In case you need this as , usewhich is used likethe Scala code most similar to R that I can achieve :Though the length is a little longer than R's.\nNote that the  is a function for R data frame , so Scala is very good enough in expressive power given without using a special function . ( is surprisingly a Array[String] instead of Array[Column], maybe they want it look like Python pandas's dataframe.)To convert the year from string to int, you can add the following option to the csv reader: \"inferSchema\" -> \"true\", see You can use  to make it a little cleaner:So this only really works if your having issues saving to a jdbc driver like sqlserver, but it's really helpful for errors you will run into with syntax and types.the answers suggesting to use cast, FYI, the cast method in spark 1.4.1 is broken.for example, a dataframe with a string column having value \"8182175552014127960\" when casted to bigint has value \"8182175552014128100\"We had to face a lot of issue before finding this bug because we had bigint columns in production.Java code for modifying the datatype of the DataFrame from String to IntegerIt will simply cast the existing(String datatype) to Integer.One can change data type of a column by using cast in spark sql.\ntable name is table and it has two columns only column1 and column2 and column1 data type is to be changed.\nex-spark.sql(\"select cast(column1 as Double) column1NewName,column2 from table\")\nIn the place of double write your data type.This method will drop the old column and create new columns with same values and new datatype. My original datatypes when the DataFrame was created were:-After this I ran following code to change the datatype:-After this my result came out to be:-"},
{"body": "Given the following Scala List:How can I get:Since zip can only be used to combine two Lists, I think you would need to iterate/reduce the main List somehow. Not surprisingly, the following doesn't work:Any suggestions one how to do this? I think I'm missing a very simple way to do it. I'm looking for a solution that can take a List of N Lists with M elements each and create a List of M TupleNs. As it turns out it is better for my specific use-case to have a list of lists, rather than a list of tuples, so I am accepting pumpkin's response. It is also the simplest, as it uses a native method.I don't believe it's possible to generate a list of tuples of arbitrary size, but the  does exactly what you need if you don't mind getting a list of lists instead.For future reference.So this piece of code won't answer the needs of the OP, and not only because this is a four year old thread, but it does answer the title question, and perhaps someone may even find it useful.To zip 3 collections:Yes, with .Scala treats all of its different tuple sizes as different classes (, , , ,...,) while they do all inherit from the  trait, that trait doesn't carry enough information to actually use the data values from the different sizes of tuples if they could all be returned by the same function. (And scala's generics aren't powerful enough to handle this case either.)Your best bet is to write overloads of the zip function for all 22 Tuple sizes. A code generator would probably help you with this.I don't believe that's possible without being repetitive. For one simple reason: you can't define the returning type of the function you are asking for.For instance, if your input was List(List(1,2), List(3,4)), then the return type would be List[Tuple2[Int]]. If it had three elements, the return type would be List[Tuple3[Int]], and so on.You could return List[AnyRef], or even List[Product], and then make a bunch of cases, one for each condition.As for general List transposition, this works: does the trick. A possible algorithm is:For example:The answer is truncated to the size of the shortest list in the input.If you don't want to go down the applicative scalaz/cats/(insert your favourite functional lib here) route, pattern matching is the way to go, although the  syntax is a bit awkward with nesting, so let's change it:The  is an arbitrary choice here, anything that looks nice infix should do it. You'll likely get a few raised eyebrows during code review, though.It should also work with anything you can  (e.g. s) has a operation up to arity 22.With Scalaz:For more than 5:"},
{"body": "What's the difference between an   and  method? And does  behave like  or like ? Thanks.(edit)\ni.e. what is the difference (either semantically or in terms of execution) betweenAnd:The method  converts each  of the source RDD into a single element of the result RDD by applying a function.  converts each  of the source RDD into multiple elements of the result (possibly none).Neither,  works on a single element (as ) and produces multiple elements of the result (as ).Yes. please see example 2 of .. its self explanatory.  if we have 100K elements in a particular  partition then we will fire off the function being used by the mapping transformation 100K times when we use .  Conversely, if we use  then we will only call the particular function one time, but we will pass in all 100K records and get back all responses in one function call.Example :Example 1Example 2The above program can also be written using flatMap as follows.Example 2 using flatmap transformation is faster than  since it calls your function once/partition, not once/element.. :  "},
{"body": "How do I escape a dollar sign in string interpolation?Just double it"},
{"body": "What exactly does  initialize a value to? Is this a typed null? Thanks. is a compile error. For example:What does work is  (note  instead of ). As Chuck says in his answer, this initialises the variable to a default value. From the Scala Language Specification:It initializes  to the default value of the type . For example, the default value of an Int is 0 and the default value of a reference type is null."},
{"body": "Is there a way to build tests with SBT without running them?My own use case is to run static analysis on the test code by using a scalac plugin. Another possible use case is to run some or all of the test code using a separate runner than the one built into SBT.Ideally there would be a solution to this problem that applies to any SBT project. For example, Maven has a test-compile command that can be used just to compile the tests without running them. It would be great if SBT had the same thing.Less ideal, but still very helpful, would be solutions that involve modifying the project's build files.Just use the  command. works for compiling your unit tests.To compile integration tests you can use .Another hint to continuously compile on every file change: "},
{"body": "In Scala one can write (curried?) functions like thisWhat is the difference between the above  function definition with two parameters lists and functions with multiple parameters in a single parameter list: From a mathematical point of view this is  and  but I can write  and the same will be I know only one difference - this is partially applied functions. But both ways are equivalent for me.Are there any other differences?Strictly speaking, this is not a curried function, but a method with multiple argument lists, although admittedly it looks like a function.As you said, the multiple arguments lists allow the method to be used in the place of a partially applied function. (Sorry for the generally silly examples I use)Another benefit is that you can use curly braces instead of parenthesis which looks nice if the second argument list consists of a single function, or thunk. E.g.versusOr for the thunk:Another advantage is, you can refer to arguments of a previous argument list for defining default argument values (although you could also say it's a disadvantage that you cannot do that in single list :)Finally, there are three other application in an answer to related post  . I will just copy them here, but the credit goes to Knut Arne Vedaa, Kevin Wright, and extempore.First: you can have multiple var args:...which would not be possible in a single argument list.Second, it aids the type inference:And last, this is the only way you can have implicit and non implicit args, as  is a modifier for a whole argument list:There's another difference that was not covered by 0__'s excellent : default parameters. A parameter from one parameter list can be used when computing the default in another parameter list, but not in the same one. For example:That's the whole point, is that the curried and uncurried forms are equivalent! As others have pointed out, one or the other form can be  more convenient to work with depending on the situation, and that is the only reason to prefer one over the other.It's important to understand that even if Scala didn't have special syntax for declaring curried functions, you could still construct them; this is just a mathematical inevitability once you have the ability to create functions which return functions. To demonstrate this, imagine that the  syntax didn't exist. Then you could still achieve the exact same thing like so: . Like many features in Scala, this is just a syntactic convenience for doing something that would be possible anyway, but with slightly more verbosity.The two forms are isomorphic. The main difference is that curried functions are easier to apply partially, while non-curried functions have slightly nicer syntax, at least in Scala."},
{"body": "What are package objects, not so much the concept but their usage?I've tried to get an example working and the only form I got to work was as follows:Observations I've made so far are:is disallowed (which is reasonable),is also disallowed.It seems that a package object must be declared in the immediate parent package and, if written as above, the brace delimited package declaration form is required.Are they in common use? If so, how?Normally you would put your package object in a separate file called  in the package that it corresponds to. You can also use the nested package syntax but that is quite unusual.The main use case for package objects is when you need definitions in various places inside your package as well as outside the package when you use the API defined by the package. Here is an example:Now the definitions inside that package object are available inside the whole package . Furthermore the definitions get imported when someone outside of that package imports .This way you can prevent to require the API client to issue additional imports to use your library effectively - e.g. in scala-swing you need to writeto have all the goodness like  and implicit conversions from  to .While Moritz's answer is spot on, one additional thing to note is that package objects are objects.  Among other things, this means you can build them up from traits, using mix-in inheritance.  Moritz's example could be written as Here Versioning is an abstract trait, which says that the package object must have a \"version\" method, while JodaAliases and JavaAliases are concrete traits containing handy type aliases.   All of these traits can be reused by many different package objects.You could do worse than to go straight to the source. :)"},
{"body": "I'm new to Play! Framework 2.1 (java version) and have no experience with scala, so sorry for my noob question. I don't understand what are and what does  and  mean in Build.scala. I googled about them but couldn't find their meaning.In my Build.scala file I have:Why the first line uses a single % symbol and the second one uses two percent symbols ?\nWhat are they for?Thanks for your help!From the official documentation:As you can see above, if you use , you don't have to specify the\nversion.This is part of SBT which play uses as a build tool.  Specifically this is an import statement. The percent symbol  is a actually a method used to build dependencies. The double percent sign  injects the current Scala version - this allows you to get the correct library for the version of scala you are running. This is to avoid having to change your build file when you update Scala. More information "},
{"body": "Sorry for the newbie question. I realize that there are several questions on here about  currying and partially applied functions are, but I'm asking about how they are different. As a simple example, here is a curried function for finding even numbers:So you could write the following to use this:which returns: .  But I've found that I can do the same thing this way:which also returns: . So my question is, what's the main difference between the two, and when would you use one over the other? Is this just too simplistic of an example to show why one would be used over the other? Thanks so much for your help.The semantic difference has been explained fairly well in .In terms of functionality, there doesn't seem much of a difference, though. Let's look at some examples to verify that. First, a normal function:So we get a partially applied  that takes an , because we've already given it the first integer. So far so good. Now to currying:With this notation, you'd naively expect the following to work:So the  notation doesn't really seem to create a curried function right away (assumingly to avoid unnecessary overhead) but waits for you to explicitly state that you want it curried (the notation has some  as well):Which is exactly the same thing we got before, so no difference here, except for notation. Another example:This demonstrates how partially applying a \"normal\" function results in a function that takes all parameters, whereas partially applying a function with multiple parameter lists creates a chain of functions,  which, all return a new function:As you can see, because the first parameter list of  has two parameters, the first function in the curried chain has two parameters. In summary, partially applied functions aren't really different form curried functions in terms of functionality. This is easily verified given that you can convert any function to a curried one: The reason that your example  works without the underscore after  seems to be that the Scala compiler simply assumes that underscore as a convenience for the programmer. As @asflierl has correctly pointed out, Scala doesn't seem to be able to infer the type when partially applying \"normal\" functions:Whereas that information is available for functions written using multiple parameter list notation: shows how this can be very useful.Currying is to do with tuples: . Remembering this is the key to distinguishing curry vs partial application, even in languages that don't cleanly support currying.Partial application is the ability to .It is easy to remember if you just think currying is the transformation to do with tuples.In languages that are curried by default (such as Haskell) the difference is clear -- you have to actually do something to pass arguments in a tuple. But most other languages, including Scala, are uncurried by default -- all args are passed as tuples, so curry/uncurry is far less useful, and less obvious. And people even end up thinking that partial application and currying are the same thing -- just because they can't represent curried functions easily!Multivariable function:Currying (or the curried function):So it's not partially applied function that's comparable to currying. It's the multivariable function.\nWhat's comparable to partially applied function is the invocation result of a curried function, which is a function with the same parameter list that the partially applied function has.Just to clarify on the last point Scala can infer types if all parameters are wildcards but not when some of them are specified and some of them not."},
{"body": "I am trying to avoid constructs like this:Ok, in this example the  and  branch are simple, but you can image complex ones.\nI built the following:Defined that, I can replace the above simple example with:But how can I get rid of the ? I want something like that:I guess the compiler needs the extra stuff to infer types.We can combine  with the answer to  to getIs this adequate for your needs?From :I hear this question a lot. Yes it does. Instead of , it is written .This may not be preferable. Perhaps you\u2019d like to write it using the same syntax as Java. Sadly, you can\u2019t. This is because  is not a valid identifier. Fear not,  is! Would you settle for this?Then you\u2019ll need the following code. Notice the call-by-name () annotations on the arguments. This evaluation strategy is required to correctly rewrite Java\u2019s ternary operator. This cannot be done in Java itself.Here is an example using the new operator that we just defined:Have fun ;) expressed in basic Scala:although I\u2019m not sure what part of the if\u2013else construct you want to optimise.Since : by itself won't be a valid operator unless you are ok with always escaping it with back ticks , you could go with another character, e.g. \"|\" as in one of the answers above. But how about elvis with a goatee ?::Of course this again won't work if you values are lists, since they have :: operator themselves."},
{"body": "I think there is  annotation to ensure the compiler will optimize a tail recursive function. Do you just put it in front of the declaration? Does it also work if Scala is used in scripting mode (for instance using  under REPL)?From the \"\" blog post:Example:And it works from the REPL (example from the ):The Scala compiler will automatically optimize any truly tail-recursive method.  If you annotate a method that you believe is tail-recursive with the  annotation, then the compiler will warn you if the method is actually not tail-recursive.  This makes the  annotation a good idea, both to ensure that a method is currently optimizable and that it remains optimizable as it is modified.Note that Scala does not consider a method to be tail-recursive if it can be overridden.  Thus the method must either be private, final, on an object (as opposed to a class or trait), or inside another method to be optimized.  The annotation is . It triggers a compiler error if the method can't be tail call optimized, which happens if:It is placed just before the  in a method definition. It works in the REPL.Here we import the annotation, and try to mark a method as .Oops! The last invocation is , not ! Let's reformulate the method:Note that  is automatically private because it is defined in the scope of another method."},
{"body": "I have this Scala method with below error. Cannot convert into a Scala list.type mismatch; found :  required: \n  will do implicit conversion for you; e.g.:You can simply convert the List using Scala's :"},
{"body": "I just read and enjoyed . However, to my mind, one of the key reasons to use dependency injection is that you can vary the components being used by either an XML file or command-line arguments.How is that aspect of DI handled with the Cake pattern? The examples I've seen all involve mixing traits in statically.Since mixing in traits is done statically in Scala, if you want to vary the traits mixed in to an object, create different objects based on some condition. Let's take a canonical cake pattern example.  Your modules are defined as traits, and your application is constructed as a simple Object with a bunch of functionality mixed inNow all of those modules have nice self-type declarations which define their inter-module dependencies, so that line only compiles if your all inter-module dependencies exist, are unique, and well-typed.  In particular, the Persistence module has a self-type which says that anything implementing Persistence must also implement DataSource, an abstract module trait.  Since ProductionDataSource inherits from DataSource, everything's great, and that application construction line compiles.  But what if you want to use a different DataSource, pointing at some local database for testing purposes?  Assume further that you can't just reuse ProductionDataSource with different configuration parameters, loaded from some properties file.   What you would do in that case is define a new trait TestDataSource which extends DataSource, and mix it in instead.  You could even do so dynamically based on a command line flag.Now that looks a bit more verbose than we would like, particularly if your application needs to vary its construction on multiple axes.  On the plus side, you usually you only have one chunk of conditional construction logic like that in an application (or at worst once per identifiable component lifecycle), so at least the pain is minimized and fenced off from the rest of your logic.  Scala is also a script language. So your configuration XML can be a Scala script. It is type-safe and not-a-different-language.Simply look at startup:is not so different than:You can always use DI, but you have one more tool.The short answer is that Scala doesn't currently have any built-in support for dynamic mixins.I am working on the autoproxy-plugin to support this, although it's currently on hold until the 2.9 release, when the compiler will have new features making it a much easier task.In the meantime, the best way to achieve almost exactly the same functionality is by implementing your dynamically added behavior as a wrapper class, then adding an implicit conversion back to the wrapped member.Until the AutoProxy plugin becomes available, one way to achieve the effect is to use delegation:But beware, the downside of this is that it's more verbose, and you have to be careful about the initialization order if you use s inside a trait. Another downside is that if there are path dependent types within  above, you won't be able to use delegation that easily.But if there is a large number of different implementations that can be varied, it will probably cost you less code than listing cases with all possible combinations.Lift has something along those lines built in. It's mostly in scala code, but you have some runtime control. "},
{"body": "I usually end up trying every combination until it compiles. Can somebody explain what I should use where?I'll disagree with   in one regard. The classes ,  and   classes. But they don't appear as classes in bytecode, because of intrinsic limitations of the JVM.This arises out of the fact that not everything in Java is an object. In addition to objects, there are primitives. All objects in Java are descendant from , but primitives are set apart and, presently, not extensible by a programmer. Note also that primitives have \"operators\", not methods.In Scala, on the other hand, everything  an object, all objects belong to a class, and they interact through methods. The JVM bytecode generated does not reflect this, but that doesn't make them any less so, just as Java has generics, even though the bytecode doesn't have them.So, in Scala, all objects are descendant from , and that includes both what Java considers objects and what Java considers primitives. There's no equivalent in Java because there is no such unification.Everything that is considered a primitive in Java is descendant from  in Scala. Until Scala 2.10.0,  was sealed, and programmers were unable to extend it. It should be interesting to see what will happen with Scala on .Net, since interoperability alone calls for Scala to at least recognize user-defined \"primitives\".Also extending  is , which is equivalent to  (on the JVM at any rate).Up to Scala 2.9.x, a user could not extend  or , nor reference them from Java, but there  other uses they could be put to in Scala. Specifically, type signatures:What each means should be obvious from the class hierarchy. Of note, however, is that  and  will auto-box, but  will not. That is a bit the opposite of what Java does, in that  and  cannot be specified, and  (defined with ) would cause auto-boxing.Starting with Scala 2.10.0, though, the user can extend  or , with the following semantics:PS: In my own view, Java is likely to follow C# in allowing \"struct\" primitives, and maybe typedefs, as parallelism without resorting to them is proving difficult to accomplish with good performance.Seen this?  The text of the page has some java interoperability remarks.\n and  are, I believe, part of the scala  and  (in the same way that  is a type, not a class). You cannot use them explicitly from within Java code.Howwever, in Java/Scala interoperation, a method which accepts a Java  will expect a scala  / . "},
{"body": "How can I define a function with multiple implicit arguments.They must all go in one parameter list, and this list must be the last one."},
{"body": "I know this question has come up many times in different ways. But it is still not clear to me. Is there a way to achieve the following.It's a two step procedure.  First turn foo into a function,  then call tupled on it to make it a function of a tuple.@dave-griffith is dead on.You can also call:If you want to wander into \"way more information than I asked for\" territory, there are also methods built into partially applied functions (and on ) for currying. A few input/output examples:Wherein the curried version is invoked with multiple argument lists:Finally, you can also uncurry/untuple if needed.   has builtins for this: or the one suggested by Dave.\n\nTo respond to your comment:In that case, this trick won't work.You can write a factory method in the companion object of your class and then obtain the tupled version of its  method using one of the aforementioned techniques.With es you get a companion object with an  method for free, and thus this technique is more convenient to use with es.I know that's a lot of code duplication but alas... we don't have macros (yet)! ;)I appreciate some of the other answers which were closer to what you asked for, but I found it easier for a current project to add another function which converts tuple parameters into the split parameters:Now, you can implement foo and make it take a param of the Tuple2 class like so."},
{"body": "I have a list of simple scala case class instances and I want to print them in predictable, lexicographical order using , but receive \"No implicit Ordering defined for ...\".Is there exist an implicit that provides lexicographical ordering for case classes?Is there simple idiomatic way to mix-in lexicographical ordering into case class?I am not happy with a 'hack':My personal favorite method is to make use of the provided implicit ordering for Tuples, as it is clear, concise, and correct:This works because the  defines an implicit conversion from  to  which is in scope for any class implementing . The existence of implicit s for s enables a conversion from  to  provided an implicit  exists for all elements  of the tuple, which should always be the case because it makes no sense to sort on a data type with no .The implicit ordering for Tuples is your go-to for any sorting scenario involving a composite sort key:As this answer has proven popular I would like to expand on it, noting that a solution resembling the following could under some circumstances be considered enterprise-grade\u2122:Given ,  will sort by name, and  will sort by id. This has a few benefits:This has the benefit that it is updated automatically whenever A changes. But, A's fields need to be placed in the order by which the ordering will use them.To summarize, there are three ways to do this: Answering your question There is a set of , e.g. for String, tuples up to 9 arity and so on.No such thing exists for case classes, since it is not easy thing to roll off, given that field names are not known a-priori (at least without macros magic) and you can't access case class fields in a way other than by name/using product iterator.The sortBy method would be one typical way of doing this, eg (sort on  field):Since you used a  you could extend with  like such:The  method of the companion object provides a conversion from your case class to an , where the  is the tuple corresponding to the first argument list of the case class. In other words:"},
{"body": "How to convert Scala's  into Java's ?Scala List and Java List are two different beasts, because the former is immutable and the latter is mutable. So, to get from one to another, you first have to convert the Scala List into a mutable collection.On Scala 2.7:From Scala 2.8 onwards:However,  in that example is not necessary if the type expected is a Java , as the conversion is implicit, as demonstrated by the last line.Not sure why this hasn't been mentioned before but I think the most intuitive way is to invoke the  decorator method of  directly on the Scala list:Assuming we have the following :If you want to be  and  what you want to convert:If you  and let compiler make  work for you:It's up to you how you want to control your code.with scala 2.9.2:resultPretty old questions, though I will answer, given but most of suggestions are deprecated.  For single invocations, doing it by hand might be the simplest solution:I didn't measure performance. Just doing as proposed above produces immutable list even on Java side.\nThe only working solution I've found is this:Since Scala 2.12.0 So the simplest solution for me was :"},
{"body": "I've been doing dev in F# for a while and I like it.  However one buzzword I know doesn't exist in F# is higher-kinded types.  I've read material on higher-kinded types, and I think I understand their definition.  I'm just not sure why they're useful.  Can someone provide some examples of what higher-kinded types make easy in Scala or Haskell, that require workarounds in F#?  Also for these examples, what would the workarounds be without higher-kinded types (or vice-versa in F#)?  Maybe I'm just so used to working around it that I don't notice the absence of that feature.(I think) I get that instead of  or  higher kinded types allow you to simply write  and it'll return a .  That's great (assuming it's correct), but seems kind of petty?  (And couldn't it be done simply by allowing function overloading?)  I usually convert to  anyway and then I can convert to whatever I want afterwards.  Again, maybe I'm just too used to working around it. But is there any example where higher-kinded types  saves you either in keystrokes or in type safety?So the kind of a type is its simple type. For instance  has kind  which means it's a base type and can be instantiated by values. By some loose definition of higher-kinded type (and I'm not sure where F# draws the line, so let's just include it)  are a great example of a higher-kinded type.The type constructor  has kind  which means that it must be passed a concrete type in order to result in a concrete type:  can have inhabitants like  but  itself cannot.I'm going to assume that the benefits of polymorphic containers are obvious, but more useful kind  types exist than just the containers. For instance, the relationsor parsersboth also have kind .We can take this further in Haskell, however, by having types with even higher-order kinds. For instance we could look for a type with kind . A simple example of this might be  which tries to fill a container of kind .This is useful for characterizing s in Haskell, for instance, as they can always be divided into their shape and contents.As another example, let's consider a tree that's parameterized on the kind of branch it has. For instance, a normal tree might beBut we can see that the branch type contains a  of s and so we can extract that piece out of the type parametricallyThis  type constructor has kind . We can use it to make interesting other variations like a Or pathological ones like a Or a Another place this shows up is in \"algebras of functors\". If we drop a few layers of abstractness this might be better considered as a fold, such as . Algebras are parameterized over the  and the . The  has kind  and the carrier kind  so altogetherhas kind .  useful because of its relation to datatypes and recursion schemes built atop them.Finally, though they're theoretically possible, I've never see an  higher-kinded type constructor. We sometimes see functions of that type such as , but I think you'll have to dig into type prolog or dependently typed literature to see that level of complexity in types.Consider the  type class in Haskell, where  is a higher-kinded type variable:What this type signature says is that fmap changes the type parameter of an  from  to , but leaves  as it was.  So if you use  over a list you get a list, if you use it over a parser you get a parser, and so on.  And these are , compile-time guarantees.I don't know F#, but let's consider what happens if we try to express the  abstraction in a language like Java or C#, with inheritance and generics, but no higher-kinded generics.  First try:The problem with this first try is that an implementation of the interface is allowed to return  class that implements .  Somebody could write a  whose  method returns a different kind of collection, or even something else that's not a collection at all but is still a .  Also, when you use the  method you can't invoke any subtype-specific methods on the result unless you downcast it to the type that you're actually expecting.  So we have two problems:There are other, more complicated ways you can try, but none of them really works.  For example, you could try augment the first try by defining subtypes of  that restrict the result type:This helps to forbid implementers of those narrower interfaces from returning the wrong type of  from the  method, but since there is no limit to how many  implementations you can have, there is no limit to how many narrower interfaces you'll need.( And note that this only works because  appears as the result type, and so the child interfaces can narrow it.  So AFAIK we can't narrow both uses of  in the following interface:In Haskell, with higher-rank type variables, this is .)Yet another try is to use recursive generics to try and have the interface restrict the result type of the subtype to the subtype itself.  Toy example:But this sort of technique (which is rather arcane to your run-of-the-mill OOP developer, heck to your run-of-the-mill functional developer as well) still can't express the desired  constraint either:The problem here is this doesn't restrict  to have the same  as \u2014so that when you declare a type , the  method can  return a .Final try, in Java, using raw types (unparametrized containers):Here  will get instantiated to unparametrized types like just  or .  This guarantees that a  can only return a \u2014but you've abandoned the use of type variables to track the element types of the lists.The heart of the problem here is that languages like Java and C# don't allow type parameters to have parameters.  In Java, if  is a type variable, you can write  and , but not .  Higher-kinded types remove this restriction, so that you could have something like this (not fully thought out):And addressing this bit in particular:There are many languages that generalize the idea of the  function this way, by modeling it as if, at heart, mapping is about sequences.  This remark of yours is in that spirit: if you have a type that supports conversion to and from , you get the map operation \"for free\" by reusing .In Haskell, however, the  class is more general than that; it isn't tied to the notion of sequences.  You can implement  for types that have no good mapping to sequences, like  actions, parser combinators, functions, etc.:The concept of \"mapping\" really isn't tied to sequences.  It's best to understand the functor laws:Very informally:This is why you want  to preserve the type\u2014because as soon as you get  operations that produce a different result type, it becomes much, much harder to make guarantees like this.I don't want to repeat information in some excellent answers already here, but there's a key point I'd like to add.You usually don't need higher-kinded types to implement any one particular monad, or functor (or applicative functor, or arrow, or ...). But doing so is mostly missing the point.In general I've found that when people don't see the usefulness of functors/monads/whatevers, it's often because they're thinking of these things . Functor/monad/etc operations really add nothing to any one instance (instead of calling bind, fmap, etc I could just call whatever operations I used to  bind, fmap, etc). What you really want these abstractions for is so you can have code that works generically with  functor/monad/etc.In a context where such generic code is widely used, this means any time you write a new monad instance your type immediately gains access to a large number of useful operations .  the point of seeing monads (and functors, and ...) everywhere; not so that I can use  rather than  and  to implement  (which gains me nothing in itself), but rather so that when I come to need  and  I can re-use the code I originally saw in terms of lists because it's actually monad-generic.But to abstract across a parameterised type like a monad , you need higher-kinded types (as well explained in other answers here).The most-used example of higher-kinded type polymorphism in Haskell is the  interface.  and  are higher-kinded in the same way, so I'll show  in order to show something concise.Now, examine that definition, looking at how the type variable  is used.  You'll see that  can't mean a type that has value.  You can identify values in that type signature because they're arguments to and results of a functions. So the type variables  and  are types that can have values.  So are the type expressions  and .  But not  itself.   is an example of a higher-kinded type variable.  Given that  is the kind of types that can have values,  must have the kind .  That is, it takes a type that can have values, because we know from previous examination that  and  must have values.  And we also know that  and  must have values, so it returns a type that must have values.This makes the  used in the definition of  a higher-kinded type variable.The  and  interfaces add more, but they're compatible. This means that they work on type variables with kind  as well.Working on higher-kinded types introduces an additional level of abstraction - you aren't restricted to just creating abstractions over basic types.  You can also create abstractions over types that modify other types.For a more .NET-specific perspective, I wrote a  about this a while back.  The crux of it is, with higher-kinded types, you could potentially reuse the same LINQ blocks between  and , but without higher-kinded types this is impossible. The closest you could get (I figured out after posting the blog) is to make your own  and  and extended them both from an .  This would allow you to reuse your LINQ blocks if they're denoted , but then it's no longer typesafe because it allows you to mix-and-match  and  within the same block, which while it may sound intriguing to enable this, you'd basically just get some undefined behavior.I wrote a  on how Haskell makes this easy.  (A no-op, really--restricting a block to a certain kind of monad requires code; enabling reuse is the default)."},
{"body": "My application does large data arrays processing and needs more memory than JVM gives by default. I know in Java it's specified by \"-Xmx\" option. How do I set SBT up to use particular \"-Xmx\" value to run an application with \"run\" action?Try this: To modify the java options for forked processes you need to specify them in the Build.scala (or whatever you've named your build) like this: This will give you the proper options without modifying JAVA_OPTS globally,  it will put custom JAVA_OPTS in an For  processes it's most convenient to set the config via  or  depending on your sbt version.Since sbt 0.13.6 . Modify  along these lines:You can also create an .sbtopts file in the root of your SBT project using the same syntax as in the /usr/local/etc/sbtopts file. This makes the project self-contained. you could set the options in .sbtconfig for  processes:If you want to set SBT_OPTS only for the current run of sbt you can use  as suggested by Googol Shan. Or you can use the option added in Sbt 12: . This gets unwieldy for longer lists of options, but it might help if you have different projects with different needs.Note that CMSClassUnloadingEnabled in concert with UseConcMarkSweepGC helps keep the PermGen space clean, but depending on what frameworks you use you might have an actual leak on PermGen, which eventually forces a restart. In sbt version 12 onwards there is an option for this:If you run sbt on linux shell, you can use:This is my usually used command to run my sbt project. is deprecated starting with SBT . Instead, I configured these options in  in the following way:There's one way I know of. Set the environment variable JAVA_OPTS.I have not found a way to do this as a command parameter.Use JAVA_OPTS for setting with environment variable. Use -J-X options to sbt for individual options, e.g. -J-Xmx2048 -J-XX:MaxPermSize=512Newer versions of sbt have a \"-mem\" option. The  in our build.sbt as referenced by @iwein above worked for us when we were seeing a java.lang.OutOfMemoryError thrown while running Specs2 tests through sbt.The environment variable is _JAVA_OPTIONS, which needs to be set.\nOnce you set _JAVA_OPTIONS, and when you sbt, sbt will show the message using JAVA_OPTIONS and the values.Alternatively you could set javaOption in the sbt or .scala file\ne.g From sbt shell you could run show javaOptions to see the values that are set.This sets the JVM options for tests. Works also with jvm forking ()."},
{"body": "Can you implement a Matrix class and an * operator that will work on two matrices?:and just so people don't say that it's hard, here is the Scala implementation (courtesy of ): I understood that strictly speaking the answer is No: overloading * is not possible without side-effects of defining also a + and others or special tricks. The  describes it best:It'll be perfectly safe with a smart constructor and stored dimensions. Of course there are no natural implementations for the operations  and  (or maybe a diagonal matrix would be fine for the latter).Test it in ghci:Since my  instance is generic, it even works for complex matrices out of the box:Oh, you want to just override the  function without any  stuff. That's possible to o but you'll have to remember that the Haskell standard library has reserved  for use in the  class.Testing in ghci:There. Now if a third module wants to use both the  version of  and the  version of  it'll have to of course import one or the other qualified. But that's just business as usual.I could've done all of this without the  type class but this implementation leaves our new shiny  open for extension in other modules.Alright, there's a lot of confusion about what's happening here floating around, and it's not being helped by the fact that the Haskell term \"class\" does  line up with the OO term \"class\" in any meaningful way. So let's try to make a careful answer. This answer starts with Haskell's module system.In Haskell, when you import a module , it creates a new set of bindings. For each variable  exported by the module , you get a new name . In addition, you may:Furthermore, names may be . For example, if  and  both export the variable , and I import both modules without qualification, then the name  refers to   and . If the name  is never used in the resulting module, this clash is ignored; otherwise, a compiler error asks you to provide more qualification.Finally, if none of your imports mention the module , the following implicit import is added:This imports the  without qualification, with no additional prefix, and without hiding any names. So it defines \"bare\" names and names prefixed by , and nothing more.Here ends the bare basics you need to understand about the module system. Now let's discuss the bare basics you need to understand about typeclasses.A typeclass includes a class name, a list of type variables bound by that class, and a collection of variables with type signatures that refer to the bound variables. Here's an example:The class name is , the bound type variable is , and there is only one variable in the collection, namely , with type signature . This class declares that some types have a binary operation, named , which computes an . Any type may later (even in another module) be declared to be an  of this class: this involves defining the binary operation above, where the bound type variable  is substituted with the type you are creating an instance for. As an example, we might implement this for integers by the instance:Here ends the bare basics you need to understand about typeclasses. We may now answer your question. The only reason the question is tricky is because it falls smack dab on the intersection between two name management techniques: modules and typeclasses. Below I discuss what this means for your specific question.The module  defines a typeclass named , which includes in its collection of variables a variable named . Therefore, we have several options for the name :You might alternately choose to leave the implicit import of , and use a qualified  everywhere you use it. Other solutions are also possible.The main point I want you to take away from this is: . It is just a name defined by the , and all of the tools we have available for namespace control are available.You can implement * as matrix multiplication by defining an instance of Num class for Matrix. But the code won't be type-safe:  * (and other arithmetic operations) on matrices as you define them is not total, because of size mismatch or in case of '/' non-existence of inverse matrices.As for 'the hierarchy is not defined precisely' - there is also  type class, exactly for the cases when only one operation is defined.There are too many things to be 'added', sometimes in rather exotic ways (think of permutation groups). Haskell designers designed to reserve arithmetical operations for different representations of numbers, and use other names for more exotic cases."},
{"body": "I have 2 datasets. \nOne is a dataframe with a bunch of data, one column has comments (a string). \nThe other is a list of words.If a comment contains a word in the list, I want to replace the word in the comment with @@@@@ and return the comment in full with the replaced words.Here's some sample data:CommentSample.txtProfanitySample.txtHere's my code so far:It compiles, it runs, but the words are not replaced.\nI don't know how to debug in scala.\nI'm totally new! This is my first project ever!Many thanks for any pointers.\n-MFirst, I think the usecase you describe here won't benefit much from the use of DataFrames - it's simpler to implement using RDDs only (DataFrames are mostly convenient when your transformations can easily be described using SQL, which isn't the case here). So - here's a possible implementation using RDDs. This  the list of profanities isn't too large (i.e. up to ~thousands), so we can collect it into non-distributed memory. If that's not the case, a different approach (involving a join) might be needed.Note that the case-insensitivity and the \"words only\" limitations are implemented in the regex itself: . "},
{"body": "How do I \"join\" an iterable of strings by another string in Scala?I want this code to output  (join the elements by \",\").How about  ?A variant exists in which you can specify a prefix and suffix too.See  for an implementation using , which is much more verbose, but perhaps worth looking at for education's sake."},
{"body": "I've been programming in Scala for a while and I like it but one thing I'm annoyed by is the time it takes to compile programs.  It's seems like a small thing but with Java I could make small changes to my program, click the run button in netbeans, and BOOM, it's running, and over time compiling in scala seems to consume a lot of time.  I hear that with many large projects a scripting language becomes very important because of the time compiling takes, a need that I didn't see arising when I was using Java.But I'm coming from Java which as I understand it, is faster than any other compiled language, and is fast because of the reasons I switched to Scala(It's a very simple language).  So I wanted to ask, can I make Scala compile faster and will scalac ever be as fast as javac.The Scala compiler is more sophisticated than Java's, providing type inference, implicit conversion, and a much more powerful type system.  These features don't come for free, so I wouldn't expect scalac to ever be as fast as javac.  This reflects a trade-off between the programmer doing the work and the compiler doing the work.That said, compile times have already improved noticeably going from Scala 2.7 to Scala 2.8, and I expect the improvements to continue now that the dust has settled on 2.8.  documents some of the ongoing efforts and ideas to improve the performance of the Scala compiler.There are two aspects to the (lack of) speed for the Scala compiler.You should be aware that Scala compilation takes at least an order of magnitude longer than Java to compile. The reasons for this are as follows:The best way to do Scala is with IDEA and SBT. Set up an elementary SBT project (which it'll do for you, if you like) and run it in automatic compile mode (command ) and when you save your project, SBT will recompile it.You can also use the SBT plug-in for IDEA and attach an SBT action to each of your Run Configurations. The SBT plug-in also gives you an interactive SBT console within IDEA.Either way (SBT running externally or SBT plug-in), SBT stays running and thus all the classes used in building your project get \"warmed up\" and JIT-ed and the start-up overhead is eliminated. Additionally, SBT compiles only source files that need it. It is by far the most efficient way to build Scala programs.The latest revisions of  (Eclipse) are much better atmanaging incremental compilation.See \"\" for more.The other solution is to integrate  - (as illustrated in this ) as a builder in your IDE.But not in  Eclipse though, as  mentions in the comments:Finally, as  reminds me in the comments: also include some kind of \"incremental\" compilation (through ), even though it , and enhanced incremental compilation is in the work for the upcoming 0.9 sbt version.Use  - it is a fast scala compiler that sits as a background task and does not need loading all the time. It can reuse previous compiler instance.I'm not sure if Netbeans scala plugin supports fsc (documentation says so), but I couldn't make it work. Try nightly builds of the plugin.You can use the JRebel plugin which is free for Scala. So you can kind of \"develop in the debugger\" and JRebel would always reload the changed class on the spot. I read some statement somewhere by Martin Odersky himself where he is saying that the searches for implicits (the compiler must make sure there is not more than one single implicit for the same conversion to rule out ambiguities) can keep the compiler busy. So it might be a good idea to handle implicits with care.If it doesn't have to be 100% Scala, but also something similar, you might give  a try.-- OliverI'm sure this will be down-voted, but extremely rapid turn-around is not always conducive to quality or productivity.Take time to think more carefully and execute fewer development micro-cycles. Good Scala code is denser and more essential (i.e., free from incidental details and complexity). It demands more thought and that takes time (at least at first). You can progress well with fewer code / test / debug cycles that are individually a little longer and still improve your productivity and the quality of your work.In short: Seek an optimum working pattern better suited to Scala."},
{"body": "One way that has  to deal with double definitions of overloaded methods is to replace overloading with pattern matching:This approach requires that we surrender static type checking on the arguments to . It would be much nicer to be able to writeI can get close with , but it gets ugly fast with more than two types:It looks like a general (elegant, efficient) solution would require defining , , ....  Does anyone know of an alternate solution to achieve the same end? To my knowledge, Scala does not have built-in \"type disjunction\". Also, are the implicit conversions defined above lurking in the standard library somewhere so that I can just import them?Well, in the specific case of , this trick below won't work, as it will not accept mixed types. However, since mixed types wouldn't work with overloading either, this may be what you want.First, declare a class with the types you wish to accept as below:Next, declare  like this:And that's it. You can call  or , and it will work, but try  and it will fail. This could be side-stepped by the client code by creating a , unless, as noted by  below, you make  a  class.It works because  means there's an implicit parameter of type , and because Scala looks inside companion objects of a type to see if there are implicits there to make code asking for that type work.Miles Sabin describes a very nice way to get union type in his recent blog post :He first defines negation of types asusing De Morgan's law this allows him to define union typesWith the following auxiliary constructs you can write union types as follows:Here is the Rex Kerr way to encode union types. Straight and simple! Comment #27 under  excellent blog post by Miles Sabin which provides another way of encoding union types in Scala., a new experimental Scala compiler, supports union types (written ), so you can do exactly what you wanted:It's possible to generalize  as follows:The main drawbacks of this approach are Mitch Blevins   and shows how to generalize it to more than two types, dubbing it the \"stuttering or\".A type class solution is probably the nicest way to go here, using implicits.\nThis is similar to the monoid approach mentioned in the Odersky/Spoon/Venners book:If you then run this in the REPL:I have sort of stumbled on a relatively clean implementation of n-ary union types by combining the notion of type lists with a simplification of , which someone mentions in another answer.Given type  which is contravariant on , by definition given  we can write\n, inverting the ordering of types.Given types , , and , we want to express .\nApplying contravariance, we get . This can in turn\nbe expressed as  in which one of  or  must be a supertype of  or  itself (think about function arguments).I did spend some time trying to combine this idea with an upper bound on member types as seen in the s of , however the implementation of  with type bounds has thus far proved challenging.There is also this :See .You might take a look at , which has something called . I get the impression that this doesn't work well with  statements but that you can simulate matching using higher-order functions. Take a look at , for instance, but note that the \"simulated matching\" part is commented out, maybe because it doesn't quite work yet.Now for some editorializing: I don't think there's anything egregious about defining Either3, Either4, etc. as you describe. This is essentially dual to the standard 22 tuple types built in to Scala. It'd certainly be nice if Scala had built-in disjunctive types, and perhaps some nice syntax for them like .I am thinking that the first class disjoint type is a sealed supertype, with the alternate subtypes, and implicit conversions to/from the desired types of the disjunction to these alternative subtypes.I assume this addresses  - 36 of Miles Sabin's solution, so the first class type that can be employed at the use site, but I didn't test it.One problem is Scala will not employ in case matching context, an implicit conversion from  to  (and  to ), so must define extractors and use  instead of .ADD: I responded to Miles Sabin at his blog as follows. Perhaps there are several improvements over Either:UPDATE: Logical negation of the disjunction for the above pattern follows, and I .ANOTHER UPDATE: Regarding comments 23 and 35 of , here is a way to declare a union type at the use site. Note it is unboxed after the first level, i.e. it has the advantage being , whereas  needs nested boxing and the paradigm in my prior comment 41 was not extensible. In other words, a  is assignable to (i.e. is a subtype of) a .Apparently the Scala compiler has three bugs.3.The get method isn't constrained properly on input type, because the compiler won't allow  in the covariant position. One might argue that is a bug because all we want is evidence, we don't ever access the evidence in the function. And I made the choice not to test for  in the  method, so I wouldn't have to unbox an  in the  in .March 05, 2012: The prior update needs an improvement.  worked correctly with subtyping.My prior update's proposal (for near first-class union type) broke subtyping.The problem is that  in  appears in both the covariant (return type) and contravariant (function input, or in this case a return value of function which is a function input) positions, thus substitutions can only be invariant.Note that  is necessary only because we want  in the contravariant position, so that supertypes of   of  nor  (). Since we only need double contravariance, we can achieve equivalent to Miles' solution even if we can discard the  and .So the complete fix is.Note the prior 2 bugs in Scala remain, but the 3rd one is avoided as  is now constrained to be subtype of .We can confirm the subtyping works.I have been thinking that first-class intersection types are very important, both for the , and because instead of  to  which means unboxing with a  on expected types can generate a runtime error, the unboxing of a ( containing a) disjunction can be type checked (Scala has to fix the bugs I noted). Unions are more straightforward than the  the experimental  of  for heterogeneous collections.We\u2019d like a type operator  that can be used to constrain a type parameters  in such a way that either  or . Here's a definition that comes about as close as we can get:Here is how it's used:This uses a few Scala type tricks. The main one is the use of . Given types  and , the Scala compiler provides a class called  (and an implicit object of that class) if and only if the Scala compiler can prove that  is a subtype of . Here\u2019s a simpler example using generalized type constraints that works for some cases:This example works when  an instance of class , a , or has a type that is neither a supertype nor a subtype of  or . In the first two cases, it\u2019s true by the definition of the  keyword that  and , so Scala will provide an implicit object that will be passed in as : the Scala compiler will correctly accept  and . In the last case, I\u2019m relying on the fact that if , then  or . It seems intuitively true, and I\u2019m simply assuming it. It\u2019s clear from this assumption why this simple example fails when  is a supertype or subtype of either  or : for example, in the example above,  is incorrectly accepted and  is incorrectly rejected. Again, what we want is some kind of type expression on the variables , , and  that is true exactly when  or .Scala\u2019s notion of contravariance can help here. Remember the trait ? Because it is contravariant in its type parameter ,  if and only if . That means that we can replace the example above with one that actually will work: That\u2019s because the expression  is true, by the same assumption above, exactly when  or , and by the definition of contravariance, this is true exactly when  or .It\u2019s possible to make things a little more reusable by declaring a parametrizable type  and using it as follows:Scala will now attempt to construct the type  for every  that  is called with, and the type will be constructed precisely when  is a subtype of either  or . That works, and there is a shorthand notation. The syntax below is equivalent (except that  must now be referenced in the method body as  rather than simply ) and uses  as a :What we\u2019d really like is a flexible way to create a type context bound. A type context must be a parametrizable type, and we want a parametrizable way to create one. That sounds like we\u2019re trying to curry functions on types just like we curry functions on values. In other words, we\u2019d like something like the following:That\u2019s  in Scala, but there is a trick we can use to get pretty close. That brings us to the definition of  above:Here we use  and Scala\u2019s  to create a structural type  that is guaranteed to have one internal type. This is a strange beast. To give some context, the function  must be called with subclasses of  that have a type  defined in them:Using the pound operator allows us to refer to the inner type , and using  for the type operator , we arrive at our original definition of :We can use the fact that function types are contravariant in their first type parameter in order to avoid defining the trait :There is another way which is slightly easier to understand if you do not grok Curry-Howard:I use similar Well, that's all very clever, but I'm pretty sure you know already that the answers to your leading questions are various varieties of \"No\".  Scala handles overloading differently and, it must be admitted, somewhat less elegantly than you describe.  Some of that's due to Java interoperability,  some of that is due to not wanting to hit edged cases of the type inferencing algorithm, and some of that's due to it simply not being Haskell.  Adding to the already great answers here. Here's a gist that builds on Miles Sabin union types (and Josh's ideas) but also makes them recursively defined, so you can have >2 types in the union ()NB: I should add that after playing around with the above for a project, I ended up going back to plain-old-sum-types (i.e. sealed trait with subclasses). Miles Sabin union types are great for restricting the type parameter, but if you need to return a union type then it doesn't offer much.From , with the addition of :Regarding the  part:"},
{"body": "What are the hidden features of Scala that every Scala developer should be aware of?One hidden feature per answer, please.Okay, I had to add one more.  Every  object in Scala has an extractor (see answer from oxbox_lakes above) that gives you access to the match groups.  So you can do something like:The second line looks confusing if you're not used to using pattern matching and extractors.  Whenever you define a  or , what comes after the keyword is not simply an identifier but rather a pattern.  That's why this works:The right hand expression creates a  which can match the pattern .Most of the time your patterns use extractors that are members of singleton objects.  For example, if you write a pattern likethen you're implicitly calling the extractor .But you can also use class instances in patterns, and that is what's happening here.  The val regex is an instance of , and when you use it in a pattern, you're implicitly calling  ( versus  is beyond the scope of this answer), which extracts the match groups into a , the elements of which are assigned in order to the variables year, month, and day. type definitions - i.e. a type described by what methods it supports. For example:Notice that the  of the parameter  is not defined other than it has a  methodWithout this feature you can, for example, express the idea of mapping a function over a list to return another list, or mapping a function over a tree to return another tree. But you can't express this idea  without higher kinds.With higher kinds, you can capture the idea of  that's parameterised with another type. A type constructor that takes one parameter is said to be of kind . For example, . A type constructor that returns another type constructor is said to be of kind . For example, . But in Scala, we have  kinds, so we can have type constructors that are parameterised with other type constructors. So they're of kinds like .For example:Now, if you have a , you can map over lists. If you have a , you can map over trees. But more importantly, if you have  , you can map a function over . which allow you to replace messy  style code with patterns. I know that these are not exactly  but I've been using Scala for a few months without really understanding the power of them. For (a long) example I can replace:With this, which is  clearer in my opinionI have to do a bit of legwork in the background...But the legwork is worth it for the fact that it separates a piece of business logic into a sensible place. I can implement my  methods as follows.. which are a sort of way at getting the type information at runtime, as if Scala had reified types.In scala 2.8 you can have tail-recursive methods by using the package scala.util.control.TailCalls (in fact it's trampolining).An example:Case classes automatically mixin the Product trait, providing untyped, indexed access to the fields without any reflection:This feature also provides a simplified way to alter the output of the  method:It's not exactly hidden, but certainly a under advertised feature: . As a illustration of the use consider the following source:Compiling this with  outputs:Notice , which is a the application of the  present in Predef.scala. will print the syntax tree after some compiler phase. To see the available phases use .This is a great way to learn what is going on behind the scenes. Try with  using the  phase to really feel how useful it is.You can define your own control structures. It's really just functions and objects and some syntactic sugar, but they look and behave like the real thing.For example, the following code defines  and :Now you can do the following: annotation in Scala 2.8:Example:Dunno if this is really hidden, but I find it quite nice.Typeconstructors that take 2 type parameters can be written in infix notationScala 2.8 introduced default and named arguments, which made possible the addition of a new \"copy\" method that Scala adds to case classes.  If you define this:and you want to create a new Foo that's like an existing Foo, only with a different \"n\" value, then you can just say:in scala 2.8 you can add @specialized to your generic classes/methods. This will create special versions of the class for primitive types (extending AnyVal) and save the cost of un-necessary boxing/unboxing :\nYou can select a subset of AnyVals :\nExtending the language. I always wanted to do something like this in Java (couldn't). But in Scala I can have:and then write:and getYou can designate a call-by-name parameter (EDITED: this is different then a lazy parameter!) to a function and it will not be evaluated until used by the function (EDIT: in fact, it will be reevaluated every time it is used).  See  for detailsYou can use  to introduce a local block without causing semicolon inference issues. is defined in \"Predef.scala\" as:Being inline, it does not impose any additional overhead.Output:You can compose structural types with the 'with' keywordFrom The Scala Language Specification:From :Using this you could do something like:Implicit definitions, particularly conversions.For example, assume a function which will format an input string to fit to a size, by replacing the middle of it with \"...\":You can use that with any String, and, of course, use the toString method to convert anything. But you could also write it like this:And then, you could pass classes of other types by doing this:Now you can call that function passing a double:The last argument is implicit, and is being passed automatically because of the implicit de declaration. Furthermore, \"s\" is being  like a String inside sizeBoundedString because there is an implicit conversion from it to String.Implicits of this type are better defined for uncommon types to avoid unexpected conversions. You can also explictly pass a conversion, and it will still be implicitly used inside sizeBoundedString:You can also have multiple implicit arguments, but then you must either pass all of them, or not pass any of them. There is also a shortcut syntax for implicit conversions:This is used exactly the same way.Implicits can have any value. They can be used, for instance, to hide library information. Take the following example, for instance:In this example, calling \"f\" in an Y object will send the log to the default daemon, and on an instance of X to the Daemon X daemon. But calling g on an instance of X will send the log to the explicitly given DefaultDaemon.While this simple example can be re-written with overload and private state, implicits do not require private state, and can be brought into context with imports.Maybe not too hidden, but I think this is useful:This will automatically generate a getter and setter for the field that matches bean convention.Further description at Implicit arguments in closures.A function argument can be marked as implicit just as with methods. Within the scope of the body of the function the implicit parameter is visible and eligible for implicit resolution:Build infinite data structures with Scala's s :\nResult types are dependent on implicit resolution.   This can give you a form of multiple dispatch:Scala allows you to create an anonymous subclass with the body of the class (the constructor) containing statements to initialize the instance of that class. This pattern is very useful when building component-based user interfaces (for example Swing , Vaadin) as it allows to create UI components and declare their properties more concisely.See  for more information.Here is an example of creating a Vaadin button:Suppose you want to use a  that contains a  and a  method, but you only want to use the one for error messages, and keep the good old  for standard output. You could do this:but if  also contains another twelve methods that you would like to import and use, it becomes inconvenient to list them. You could instead try:but this still \"pollutes\" the list of imported members. Enter the \u00fcber-powerful wildcard:and that will do \u2122. method (defined in ) that allow you to define additional function constraints that would be checked during run-time. Imagine that you developing yet another twitter client and you need to limit tweet length up to 140 symbols. Moreover you can't post empty tweet.Now calling post with inappropriate length argument will cause an exception:You can write multiple requirements or even add description to each:Now exceptions are verbose: One more example is .You can perform an action every time requirement fails:Traits with  methods are a feature in Scala that is as not widely advertised as many others. The intend of methods with the  modifier is to do some operations and delegating the call to . Then these traits have to be mixed-in with concrete implementations of their  methods.While my example is really not much more than a poor mans AOP, I used these  much to my liking to build Scala interpreter instances with predefined imports, custom bindings and classpathes. The  made it possible to create my factory along the lines of  and then have useful imports and scope varibles for the users scripts."},
{"body": "As i understand it, in Scala, a function may be called eitherFor example, given the following declarations, do we know how the function will be called?Declaration:CallWhat are the rules please?The example you have given only uses call-by-value, so I will give a new, simpler, example that shows the difference.First, let's assume we have a function with a side-effect.  This function prints something out and then returns an .Now we are going to define two function that accept  arguments that are exactly the same except that one takes the argument in a call-by-value style () and the other in a call-by-name style ().Now what happens when we call them with our side-effecting function?So you can see that in the call-by-value version, the side-effect of the passed-in function call () only happened once.  However, in the call-by-name version, the side-effect happened twice.This is because call-by-value functions compute the passed-in expression's value before calling the function, thus the  value is accessed every time.  However, call-by-name functions  the passed-in expression's value every time it is accessed.Here is an example from Martin Odersky:We want to examine the evaluation strategy and determine which one is faster (less steps) in these conditions:call by value: test(2,3) -> 2*2 -> 4\ncall by name:  test(2,3) -> 2*2 -> 4\nHere the result is reached with the same number of steps.call by value: test (7,8) -> 7*7 -> 49\ncall by name: (3+4) (3+4)-> 7*7 ->49\nHere call by value is faster.call by value: test(7,8) -> 7*7 -> 49\ncall by name: 7 *  7 -> 49\nHere call by name is fastercall by value: test(7,2*4) -> test(7, 8) -> 7*7 -> 49\ncall by name: (3+4)(3+4) -> 7*7 ->  49\nThe result is reached within the same steps.  In the case of your example all the parameters will be evaluated  it's called in the function , as you're only defining them .\nIf you want to define your parameters  you should pass a code block:This way the parameter  will not be evaluated  it's called in the function.This  here explains this nicely too.**I will try to explain by a simple use case rather than by just providing an example **Imagine you want to build a  that will Nag you every time since time last you got nagged.**Examine the following implementations: ****In the above implementation the nagger will work only when passing by name \nthe reason is that, when passing by value it will re-used and therefore the value will not be re-evaluated while when passing by name the value will be re-evaluated every time the variables is accessed\n**To iteratate @Ben's point in the above comments, I think it's best to think of \"call-by-name\" as just syntactic sugar. The parser just wraps the expressions in anonymous functions, so that they can be called at a later point, when they are used.In effect, instead of definingand running:You could also write:And run it as follows for the same effect:As i assume, the  function as discuss above pass just the values to the function. According to  It is a Evaluation strategy follow by a Scala that play the important role in function evaluation. But, Make it simple to . its like a pass the function as a argument to the method also know as . When the method access the value of passed parameter, it call the implementation of passed functions. as Below: According to @dhg example, create the method first as: This function contain one  statement and return an integer value. Create the function, who have arguments as a : This function parameter, is define an anonymous function who have return one integer value. In this  contain an definition of function who have  passed arguments but return  value and our  function contain same signature. When we call the function, we pass the function as a argument to . But in the case of  its only pass the integer value to the function. We call the function as below: In this our  method called twice, because when we access the value of  in  method, its call to the defintion of  method. Typically, parameters to functions are by-value parameters; that is, the value of the parameter is determined before it is passed to the function. But what if we need to write a function that accepts as a parameter an expression that we don't want evaluated until it's called within our function? For this circumstance, Scala offers call-by-name parameters.A call-by-name mechanism passes a code block to the callee and each time the callee accesses the parameter, the code block is executed and the value is calculated.Parameters are usually pass by value, which means that they'll be evaluated before being substituted in the function body.You can force a parameter to be call by name by using the double arrow when defining the function.  is invoked when used and  is invoked whenever the statement is encountered. For example:- I have a infinite loop i.e. if you execute this function we will never get  prompt.a  function takes above  method as an argument and it is never used inside its body.On execution of  method we don't find any problem ( we get  prompt back ) as we are no where using the loop function inside  function.a  function takes above  method as a parameter as a result inside function or expression is evaluated before executing outer function there by  function executed recursively and we never get  prompt back.I don't think all the answers here do the correct justification:In call by value the arguments are computed just once:you can see above that all the arguments are evaluated whether needed are not, normally  can be fast but not always like in this case.If the evaluation strategy was  then the decomposition would have been:as you can see above we never needed to evaluate  and hence saved a bit of computation which may be beneficial sometimes.See this:y: => Int is call by name. What is passed as call by name is add(2, 1). This will be evaluated lazily. So output on console will be \"mul\" followed by \"add\", although add seems to be called first. Call by name acts as kind of passing a function pointer.\nNow change from y: => Int to y: Int. Console will show \"add\" followed by \"mul\"! Usual way of evaluation."},
{"body": "Let's say I have a case class that represents personas, people on different social networks. Instances of that class are fully immutable, and are held in immutable collections, to be eventually modified by an Akka actor.Now, I have a case class with many fields, and I receive a message that says I must update one of the fields, something like this:Notice I have to specify all fields, even though only one changes. Is there a way to clone existingPersona and replace only one field, without specifying all the fields that don't change? Can I write that as a trait and use it for all my case classes?If Persona was a Map-like instance, it would be easy to do.comes with a  method that is dedicated exactly to this usage:Since 2.8, Scala case classes have a  method that takes advantage of named/default params to work its magic:You can also create a method on  to simplify usage:then"},
{"body": "I can see in the API docs for Predef that they're subclasses of a generic function type (From) => To, but that's all it says.  Um, what?  Maybe there's documentation somewhere, but search engines don't handle \"names\" like \"<:<\" very well, so I haven't been able to find it.Follow-up question: when should I use these funky symbols/classes, and why?These are called . They allow you, from within a type-parameterized class or trait, to  one of its type parameters. Here's an example:The implicit argument  is supplied by the compiler, iff  is . You can think of it as a  that  is --the argument itself isn't important, only knowing that it exists. Now I can use it like so:But if I tried use it with a  containing something other than a :You can read that error as \"could not find evidence that Int == String\"... that's as it should be!  is imposing  on the type of  than what  in general requires; namely, you can only invoke  on a . This constraint is enforced at compile-time, which is cool! and  work similarly, but with slight variations: by @retronym is a good explanation of how this sort of thing used to be accomplished and how generalized type constraints make it easier now.To answer your follow-up question, admittedly the example I gave is pretty contrived and not obviously useful. But imagine using it to define something like a  method, which adds up a list of integers. You don't want to allow this method to be invoked on any old , just a . However the  type constructor can't be so constrainted; you still want to be able to have lists of strings, foos, bars, and whatnots. So by placing a generalized type constraint on , you can ensure that  has an additional constraint that it can only be used on a . Essentially you're writing special-case code for certain kinds of lists.Not a complete answer (others have already answered this), I just wanted to note the following, which maybe helps to understand the syntax better: The way you normally use these \"operators\", as for example in pelotom's example:makes use of Scala's alternative .So,  is the same as  (and  is just a class or trait with a fancy-looking name). Note that this syntax also works with \"regular\" classes, for example you can write:like this:It's similar to the two syntaxes for method calls, the \"normal\" with  and  and the operator syntax.Read the other answers to understand what these constructs are. Here is  you should use them. You use them when you need to constrain a method for specific types only.Here is an example. Suppose you want to define a homogeneous Pair, like this:Now you want to add a method , like this:That only works if  is ordered. You could restrict the entire class:But that seems a shame--there could be uses for the class when  isn't ordered. With a type constraint, you can still define the  method:It's ok to instantiate, say, a ,   on it.In the case of , the implementors wanted an  method, even though it doesn't make sense for . By using a type constraint, all is well. You can use  on an , and you can form an  and use it, as long as you don't call  on it. If you try , you get the charming messageIt depends on where they are being used. Most often, when used while declaring types of implicit parameters, they are classes. They can be objects too in rare instances. Finally, they can be operators on  objects. They are defined inside  in the first two cases, though not particularly well documented.They are meant to provide a way to test the relationship between the classes, just like  and  do, in situations when the latter cannot be used.As for the question \"when should I use them?\", the answer is you shouldn't, unless you know you should. :-) : Ok, ok, here are some examples from the library. On , you have:On , you have:You'll find some other examples on the collections."},
{"body": "Is there any difference between case object and object in scala?Case classes differ from regular classes in that they get:Pattern matching, equals and hashCode don't matter much for singletons (unless you do something really degenerate), so you're pretty much just getting serialization, a nice , and some methods you probably won't ever use.Here's one difference - case objects extend the  trait, so they can be serialized. Regular objects cannot by default:It's similar with  and  ,we just use  instead of  when there isn't any fields representing additional state information."},
{"body": "In , there are two very similar objects  and .  provide a series of implicit methods that convert between a Java collection and the closest corresponding Scala collection, and vice versa. This is done by creating wrappers that implement either the Scala interface and forward the calls to the underlying Java collection, or the Java interface, forwarding the calls to the underlying Scala collection. uses the pimp-my-library pattern to \u201cadd\u201d the  method to the Java collections and the  method to the Scala collections, which return the appropriate wrappers discussed above. It is newer (since version 2.8.1) than  (since 2.8) and makes the conversion between Scala and Java collection explicit. Contrary to what David writes in his answer, I'd recommend you make it a habit to use  as you'll be much less likely to write code that makes a lot of implicit conversions, as you can control the only spot where that will happen: where you write  or .Here's the conversion methods that  provide:To use the conversions directly from Java, though, you're better off calling methods from  directly; e.g.:For anyone landing on this question since Scala 2.12.x,  is now deprecated and  is the preferred method.As explained in the API (http://www.scala-lang.org/api/current/index.html#scala.collection.JavaConversions$),  is a set of implicit conversions that transforms java collections into related scala collection.You can use it with an . When necessary, the compiler will automatically transform the java collection into the right scala type. are a set of decorator (http://www.scala-lang.org/api/current/index.html#scala.collection.JavaConverters$)that helps transform java or scala collections to scala or java collections using  or  methods that will be implicitly added to the collection that you want to transform. In order to use these converters, you need to import :You should prefer  as it's generally easier to use (no need to use  or )."},
{"body": "Newbie question of Akka - I'm reading over Akka Essentials, could someone please explain the difference between Akka Stop/Poison Pill vs. Kill ? The book offers just a small explaination \"Kill is synchronous vs. Poison pill is asynchronous.\" But in what way? Does the calling actor thread lock during this time? Are the children actors notified during kill, post-stop envoked, etc? Example uses of one concept vs. the other?Many thanks!Both  and  will terminate the actor and stop the message queue.  They will cause the actor to cease processing messages, send a stop call to all its children, wait for them to terminate, then call its  hook.  All further messages are sent to the dead letters mailbox.The difference is in which messages get processed before this sequence starts.  In the case of the  call, the message currently being processed is completed first, with all others discarded.  When sending a , this is simply another message in the queue, so the sequence will start when the  is received.  All messages that are ahead of it in the queue will be processed first.By contrast, the  message causes the actor to throw an  which gets handled using the normal supervisor mechanism.  So the behaviour here depends on what you've defined in your supervisor strategy. The default is to stop the actor. But the mailbox persists, so when the actor restarts it will still have the old messages except for the one that caused the failure.Also see the 'Stopping an Actor', 'Killing an Actor' section in the docs:And more on supervision strategies:PoisonPill asynchronously stops the actor after it\u2019s done with all messages that were received into mailbox, prior to PoisonPill."},
{"body": "I noticed that Scala provide . But I don't get what they do.The  shows that  is a , but how is it different from a normal ?The difference between them is, that a  is executed when it is defined whereas a  is executed when it is accessed the first time.In contrast to a method (defined with ) a  is executed once and then never again. This can be useful when an operation takes long time to complete and when it is not sure if it is later used.Here, when the values  and  are never used, only  unnecessarily wasting resources. If we suppose that  has no side effects and that we do not know how often it is accessed (never, once, thousands of times) it is useless to declare it as  since we don't want to execute it several times.If you want to know how  are implemented, see this .This feature helps not only delaying expensive calculations, but is also useful to construct mutual dependent or cyclic structures. E.g. this leads to an stack overflow:But with lazy vals it works fineAlso  is useful without cyclic dependencies, as in the following code:Accessing  will now throw null pointer exception, because  is not yet initialized.\nThe following, however, works fine:EDIT: the following will also work: This is called an \"early initializer\". See  for more details. A lazy val is most easily understood as a \" def\".Like a def, a lazy val is not evaluated until it is invoked.  But the result is saved so that subsequent invocations return the saved value.  The memoized result takes up space in your data structure, like a val.As others have mentioned, the use cases for a lazy val are to defer expensive computations until they are needed and store their results, and to solve certain circular dependencies between values.Lazy vals are in fact implemented more or less as memoized defs.  You can read about the details of their implementation here:I understand that the answer is given but I wrote a simple example to make it easy to understand for beginners like me:Output of above code is:As it can be seen, x is printed when it's initialized, but y is not printed when it's initialized in same way (I have taken x as var intentionally here - to explain when y gets initialized). Next when y is called, it's initialized as well as value of last 'x' is taken into consideration but not the old one.Hope this helps."},
{"body": "I have just started learning scala .I want to print $ using String InterpolationInput is 2.7255 I get output as .You owe 2.73 while i want it as You owe $2.73\nany pointers will be of a great helpJust double itResult: "},
{"body": "How does case class work in Scala?Is is some magic that goes behind this..Here is the answer to the question..When we create a case class saywhat is that goes behind this case class?Case classes are special because Scala automatically creates a companion object for them: a singleton object that contains not only an apply method for creating new instances of the case class, but also an unapply method The the method that needs to be implemented by an object in order for it to be an extractor.When u create a case class a extractor method apply and unapply are created in the companion object.For example when u create a case class say Module constructed with a json objectIf your extractor is supposed to only extract a single parameter from a given object, the signature of an unapply method looks like this:This expands as followsNow when we match this everything that has an unapply will always workSo in this scenario the Module.unapply method is called and the module object is passed to this unapply method that checks whether this is the the instance of Module and the case evaluates.}"},
{"body": "Given a String, how to get the last word in Scala, assuming words divided by space?Example:Expected output: I would go by a . But keep in mind that other blank chars may be used instead of spaceNah, just kidding. Just do this:Or this:Or this: Or maybe: OrEtc."},
{"body": "I want to return the union of two sets in the following way:I get the error \"Cannot resolve symbol ++\". What happened? defines  to mean a function from  to . There is no  method on functions. That's it.You need to specify the type parameter of  if you are using Scala Set:"},
{"body": "I am trying to execute the simple wordcount using scala in Spark. But i get these two errors. I am relatively new to scala and am unable to figure it out. The code i am trying to run is,I haven't been able to figure it out. I did check on a few docs for scala syntax but do not understand the issue there. Thanks to balaji, I did solve that issue.Just wrap the spark master URL inside double quotes as its just a string.\nHence it should look like,"},
{"body": "I am learning Scala language in university, and as homework for \"Functions as data\" topic, he asked us to write a function plus(x,y) \u2261 x + y without the use of operation +.How do I need to start thinking to solve this task?I am not sure what your professor intended, but a simple way to do it is just to subtract the negative of :You could always make passes through the bits:1) AND X and Y together into a different variable (c):2) Bit shift (c) one bit to the (left/right) depending on your endian. Check for 0 value.3) XOR X and Y together into another variable (d). If step 2 was a zero value, this is your correct answer.If step 2 was not a zero value:4) Repeat 1-3 on (c) and (d).This is very recursive, and it can't handle numbers which approach the limit of an integer, nor can it handle negative numbers without modification. It's not a perfect solution, but at least it's enough to help you start thinking in terms outside of strict mathematics."},
{"body": "How to join two lists such that:and result will be Ok. So first you will need to convert first list to map."},
{"body": "I am suprised by this error : \nHow should I transform my timestamp string into a TimeStamp object ? Looking for current timestamp here : ?\nI got the value 1485783591 =  30/1/2017  14:39:51   But my java program doesn't : output : why ?? (a subclass of ) uses milliseconds for time while the Unix timestamp counts seconds.  If you have a Unix timestamp you need to multiply it by 1000 (and divide by 1000 to get a Unix timestamp from a Java Date).As the page says, it considers the timestamp to be the number of  since midnight January 1 1970. Just looking at  tells you  expects the number of -seconds. So multiply it by 1000.  is also unlikely to be the type you want to use: it's SQL-specific. Consider  types instead."},
{"body": "In the functional programming world, when I want to design an API, i will encounter the word algebra api. \nCould someone please describe, what an algebra is in FP in context of designing API.Which components build an algebra api? Laws, operations, etc..?There is a word primitive, what is a primitive exactly? Please with show me an example. I think what you are referring to is .A common class of ADT is the product type.  As an example, a \"user\" can be described as a combination of \"name\", \"email address\", and \"age\":This is called a \"product\" type because we can count the number of possible distinct Users using multiplication:The other common ADT class is the sum type.  As an example, a user can either be a common user or an adminstrator:This is called the sume type because we can count the number of possible distinct Users using addition:"},
{"body": "What does mean this line in java:Thanks.You can get this done in java using any reader. e.g- BufferedReader, FileReader anything like this. If you are using a list use Iterator.  is perhaps an iterable collection of  objects. The code that you've posted accesses the next element in sequence and converts it to an integer.If you can post the surrounding code as well, we could get more context on the same."},
{"body": "A detailed information on what does the above statement (the one enclosed in quotes) does in Scala would be greatly appreciated.That is a recursive method that just calls itself. It creates an infinite loop."},
{"body": "I can use Array to make the List of the Songs.Because Array supports Random Access.i.e I can play the songs in different index in the same time.In the same way I need the real life examples of List,Tuple,Set and Map.You can use a List to store the steps necessary to cook a chicken, because Lists support sequential access and you can access the steps in order.You can use a Tuple to store the latitude and longitude of your home, because a tuple always has a predefined number of elements (in this specific example, two). The same Tuple type can be used to store the coordinates of other locations.You can use a Set to store passport numbers, because a Set enforces uniqueness among its elements. Passport numbers are always unique and two people can't have the same one.You can use a Map to convert a list of Imperial/US measurements to their Metric (SI) equivalents because a Map takes a list and applies an operation to each member of the list and returns a new list ( )"},
{"body": "I understand that RDD is the base of Spark's HighAvailability concept.  In how many different ways we can create this 'RDD' in Spark using Scala.I may be NOT reaching you by asking this question!  But all I wanted to know is what are the methods (or any) result in giving RDD.It is not about an operation/action on RDD is again giving RDD.  I understand that one on the ways is with sc.textFile(\"fileName\")There are 2 waysYou can read their documentation and can practice some exercises described here  (Full disclosure: That is my Spark notes I prepared while learning it myself.)"},
{"body": "ouput :\n               List(List('a, 'a, 'a, 'a), List('b, 'b, 'b), List('c, 'c, 'c, \n               'c), List('d), List('e, 'e, 'e), List('f), List('a))Can anyone explain this code how it works.span the list here is the ls parameter and ls.head is the first item of the list\nThe two lists are then assigned to packed (containing the longest prefix) and next which contains the rest. if next is empty the prefix list is returned otherwise there's a recursive call concatenating the prefix list with the result of the recurse"},
{"body": "Complier showsi think  the problem is"},
{"body": "iam having small problem with my code, i make the code at java and its works but for some reason it wont work in Scala. any idea ?You should break the line after  or use . Otherwise, you are simply calling the member  on  which is of type ."},
{"body": "In scala, the following generic type, is replaceable with the polymorphic definitionWhen should we prefer the 1st style and 2nd style? Any scenarios?Consider the following method:Now the precise type of  will be preserved in the return type. In the other style you would always have  as the return type, not something more specific.So basically you need the first style if you need access to the most precise type of  known to the compiler. \nAnother example:"},
{"body": "NOTE: I know we can do it in simpleway \nBut i want to know why above code is not workingLet us first start with how will you do this in any other language,JavaJavaScript,And you can do it the more or less same way in Scala with . (Not to be confused with the concept of )If you want to use  then,As for \"why your code does not work ?\". The reason is that you are trying to use for-comprehension without really understanding these, which lead to a very strange looking and incorrect code. in Scala works by using combinations of  /  /  /  in various ways depending on the usage.In this case, the above for-comprehension version is equivalent to the following  based version,"},
{"body": "I am doing the first assignment of Big Data Analysis with Scala and Spark on Coursera. I got stuck at implementing the following method:And the List is defined earlier in the program:My understanding is that for each element in the list, I should pass it to the RDD so that it can do the filtering and count the number of occurrence of programming languages appearin the Wikipedia articles. I am thinking to do a for loop / foreach in the RDD to loop over the list. Is it doable? If not, could you suggest what to do in order to achieve the functionality? Here is the snapshot of the problem description: \n My implementation of occurrencesOfLang is as follows:Thank you for help!You are looking for this answer.I tnik you are looking for something like this:     "},
{"body": "I have multiples dataframes loaded from csv files i would like to join them based on a column , here what i did .i took just i would like to generelize and make it automatic.The problem it took just two of them not all them .The result is joining two of them\nThanksHere is a solution for the answer"},
{"body": "Assume below are directory structures in a list, how to find lowest common ancestor for these.  Below function helped!!"},
{"body": "Here is an example of what I mean.I got two multidimensional arrays w and z.\nz is a 3x3 matrix.\nNow I want to controll if z is part of the bigger multidimensional array w.\nLike z= (1,2,3),\n        (1,2,3),\n        (1,2,3)w= (4,7,1,2,3)\n   (7,8,1,2,3)\n   (9,0,1,2,3)\n   (8,1,5,3,6)\n   (4,6,6,6,6)In that case my function would give me a true.\nI know how to acess all elements and single rows and columns.\nCould someone give me a little push in the right direction?\nMy tryHere is the function that return 1 if sub matrix of order 3x3 exist , else returns 0.    "},
{"body": "I have a spark (scala) dataframe \"Marketing\" with approx 17 columns with 1 of them as \"Balance\". The data type of this column is Int. I need to find the median Balance. I can do upto arranging it in ascending order, but how to proceed after that? I have a given hint that the percentile function of scala can be used. I don't have any idea about this percentile function. Can anyone help?Median is the same thing as the 50th percentile. If you do not mind using hive functions you can do one of the following:If you do not need an exact figure you can look into using percentile_approx() instead. Documentation for both functions is located ."},
{"body": "I am searching in an rdd textfile a word say \"Scala\":Works fine.But:Does not work, It freezes the program without error.It seems to be because of a local variable but can't figure out why.Thanks Jasper-M,worked just fine. But I am very surprised this is not better known at least from all searches I did.I would like in fact to do something like the following (not sure of the syntax) which does not work:But final does't seem to work for a list. I will look more closely at the link provided, but a direct answer would quite ease my pain.Also sorry for my ignorance, but I am interested also in an \"scala\" answer in the fact that I don't see how we can refactor or write a code without passing variables in lambda expression. I am still learning... but I'm curious to see how this list example can be solved.Many thanks,\nEz"},
{"body": "Going over a course in Scala. WhIle reading  functions, it says we can convert methods to a function object. Like :Where  is a method. \nMethods exist in a class. Functions are independent objects. I am trying think. Of situations  where a method has to be assigned to a function object. Please believe me. I Googled before I am posting this.\nThanksYou didn't pose an actual question. Is this what you're looking for?The result of all this is, as expected, 3.14....You might find my blog article helpful: .The most complete and understandable post about methods vs functions I ever read was this one: hope it can help you"},
{"body": "here iam getting illegal start of statement. plz give me ans why it is "},
{"body": "I am reading Scala for the Impatient and now I am on chapter 4, I got a feeling that this book only introduce programming syntax of scala instead of focusing on functional programming paradigm, or what is functional programming.I could be wrong or the author has mixed functional programming syntax in the content but I could not tell because I am 100% new to functional programming.Here is the table of content:It looks like scala is very similar to Java in terms of what it provides and scala has more. But I heard that we could do both object-oriented or functional programming and I want to know what features of scala is functional. I know scala is base on Java.I want to jump to the chapter of this book which is related to scala's functional programming instead of going through all the chapter one by one."},
{"body": "I try to compile a scala example in the book: \"Play for Scala\" but get a following compilation error on Play console:sbt start: \n               [info] Compiling 8 Scala sources and 1 Java source to \n                C:\\Users\\karthik.a.arumugam\\kt\\target\\scala-2.10\\classes...\n               [error] \n               C:\\Users\\karthik.a.arumugam\\kt\\conf\\routes:8: \n                 object Products is not a member of package \n                controllers\n                 [error] GET     /prod\n                    controllers.Products.list\n          [error] C:\\Users\\karthik.a.arumugam\\kt\\conf\\routes:8: \n            object Products is not a member of package \n                 controllers\n                     [error] GET     /prod\n                controllers.Products.list\n                     [error] `enter code File: controllers->Products.scalaFile :models->Product.scalaFile: views -> list.scala.htmlFile : views- > main.scala.htmlFile: Routes: \n                   GET     /prod                       controllers.Products.list"},
{"body": "trying to pass a dynamic variable based on the input to an object.input : table1getting the below error :Your function  requires parameter of following types\n but you are trying to passIn case of (this is a string) pass a valid  According to your example you need to pass  or  as which table are you reading."},
{"body": "Here is what I would like to doThe above is a sort of pseudocode not respecting the Spark API but I think it gives an idea of what should be the outcome, running the aggregations asynchronously in parallel producing a list of Futures and combining the results of each future with df.unionAll to have the result dataframe.Do you have any idea on how to implement that?Thanks"},
{"body": "I am trying to define a custom input format to spark's dataframereader object: In PySpark:Spark currently supports csv and a few other file types as an argument to the format function. How can I add another my custom file type to the types of files it can read? Specifically I would like: "},
{"body": "I have a class called Piece that have two attributes: x, y and color. These x and y attributes are integers representing indexes in a matrix. So I want to set those coordinates in a decent way.\nThat's the pattern:\n "},
{"body": "I want to define a function in Scala in which I can pass my training and test datasets and then it perform a simple machine learning algorithm and returns some statistics. How should do that? What will be the parameters data type?Imagine, you need to define a function which by taking training and test datasets performs a simple classification algorithm and then return the accuracy.What I expect to have is like as follow:I need just the declaration of the functions and not the code that produce the results1, results2, and results3.Thanks.In case you're using supervised learning you should opt for . Excerpt from mllib doc: And example is:"},
{"body": "I'm looking for the exact implementation not just references to libraries so I posted the answer by myself. I'm coding in Scala but at first I asked for Java to increase the chance of getting an answer as quickly as possible, so the implementation is in Scala language.Thanks to the comments, here is the Scala implementation which works exactly the same as the above PHP code:It uses bouncycastle which can be added to build.sbt:"},
{"body": "When playing with something in Scala, I typically spend a bunch of time trying combinations of dependency versions, Scala versions,  vs , etc. And when it starts working, I am not quite sure why, or for how long...It would be great if someone could explain the Scala ecosystem's way(s) of dealing with versions of sbt, scala, and libraries. Or perhaps point me to some documentation. I struggled with this extensively when i started out. These days i start every project with a boiler-plate  with just scalaVersion and whatever sbt is currently on my machine:Pick the latest 2.10 or 2.11, dependening on your need. Most libraries of note are cross-published into both.Now, as you find libraries you want to use, head over to  and search for them there. Look for a  or  postfix (depending on your version). If it has neither, you are likely fine.Once you find your library and the version you want, mavenrepository even provides you the sbt link you need to use in its  tab like this:And from there you can even explore the dependencies that library will bring along with it. This should cover most of your day to day needs."},
{"body": "I have a specific requirement where I have to write few hadoop MR code in scala and then to fire those codes using a web-app and then finally show the results in a web page.Is this possible ?If yes is there any framework which I can make use of ?Regards,RahulYour question is not clear. If you mean Hadoop MR jobs then you should look at   and . How big is your data? You should be able to do MR on normal or Parallel Scala collections if the size of your data it not big. "},
{"body": "i am new in Scala and i would like to ask what is the MOST EFFICIENT way to implement this problem. Imagine that we have a stadium, which has X sectors and each sector has Y rows and each row has Z seats. Imagine that we have random number of occupied seats. a) Given a row, return the amount of occupied seats. b) Given a sector, return the amount of occupied seats. c) Given a sector and a row, make sure the chair is vacant x (1 <= x <= n). d) Given a row, returns an array with the positions of the vacant chairs. e) Given a sector, return the row with the greatest amount of vacant chairs. f) Return the amount of occupied stadium chairs. g) Suppose the entrance to the stadium has two values: $ 75.00 for the two closest rows to the field and $ 50.00 for the other rows . Write a function that returns the income of a game. Thank for your solutions in advanceMost efficiently can mean in terms of your effort writing the code or in terms of runtime speed. In the former case, I would use something like a nested . In the latter case, probably a flat  would perform best.Accordingly for three or more dimensions.Unless this is for your exercise, I would use an existing optimised library such as .I think that most efficient way to map s is  it comes in two flavors:  and . Immutable are little bit more efficient, but it's efficiency is disappearing in any concurrent\\distributed situation.\nHere is the implementation :So. If you like to define your stadium like:Income with standard price $50 will be calculated easilyAnd could be increased for first two rows"},
{"body": "While reading from InputStream, how do I convert the InputStream to a java/scala object? An example use case being, receive a CSV file as stream and parse the CSV row by row, on the fly.For example: I haveand a sample CSV file's single row is (Andy, Morgan, Male). Now suppose I receive this CSV InputStream and this CSV has millions of rows and can't be held into memory. Is it possible to cast the InputStream to the above mentioned case class, use it for my purpose, discard the instance of this case class and repeat this process for the entire stream.A vague example would be on the lines of:I want to understand the internals, so I'd be very thankful to a solution in native java/scala without any 3rd party libraries.There's actually a Java inputstream called  that you could use for this exact purpose of casting as a class.Of course this assumes that your inputStream is streaming these  objects, otherwise you'd have to go about this differently. If the file is to big to fit in memory to begin with, you may want to consider streaming these  objects rather than to do this on the receiving end.I did, however find this , which seems fairly versatile that has constructors like so (among many others):There is a method to read in by line ( which returns a ) which may help you in converting to the object.Hope this helps!"},
{"body": "When I create an Spark context using scala, this trace is shown:I read anything about netty version conflict, but I cannot resolve this topic.This is my set of dependencies:Sorry but I cannot be more verbose because I am totally missing with this topic.If anything knows what happened with this ...I am just initializing an spark context with cassandra support :There was a dependency problem. Avro-Tools jar file was being imported to the project and it caused the ERROR. Thanks to everybody.Had a similar problem but using maven instead of sbt. Since i'm including avro-ipc as one of my dependencies, i need to exclude netty, thus, it looked like this."},
{"body": "I have a string  that I need to parse into an associative array like .  What is the best way in Scala to parse this text?Your question is unclear because you ask for, as commented by @heenenee, a map, but show something that is nothing in scala.A very dirty way to have a List or Array of Arrays, as you're showing, would be:(replace toList by toArray if you prefer an array...)"},
{"body": "I am using Spark scala API. \nprods_grpd has this type: String, mutable.HashSet[String] val prods_grpd = all_meds.aggregateByKey(initialSet)(addToSet, mergePartitionSets)\nprods_grpd.saveAsTextFile(\"scratch/prods_grpdby_users.tsv\")When I save this rdd, I get this o/p. The first value is key and then I get a set of keys.(8635214,Set(2013-01-01))(3580112,Set(2013-01-01))(146086,Set(2010-01-01, 2012-01-01))(112220,Set(2013-01-01))(2020,Set(2013-01-01))(24218,Set(2013-01-01))However, I want o/p like:(8635214, 2013-01-01)(3580112, 2013-01-01)(146086, 2010-01-01, 2012-01-01)(112220, 2013-01-01)(2020, 2013-01-01)(24218, 2013-01-01)I which like to know how do  I unnest/flatten the 2nd parameter of RDD. You cannot simply convert  to  because tuples are not collections and don't support arbitrary number of elements. Instead you can map entries to strings with desired format:"},
{"body": "I want to use the   in an SBT project. I'm on Linux. How can I do that?In your build script you probably need to specify an artifact explicitly for the dependency (the module id parameters are jus):import sbt._\nimport Keys._"},
{"body": "I want to implement an async rest via scala dispatch but i get an error, and i don't know why.\nAny ideas?Thanks in advanceCompiler:[error] file.scala:11: not found: object dispatch[error] import dispatch._[error] file.scala:47: not found: value host[error]     val request = host(s)[error]  A friend of mine helped me out, \nthx to anyone who gave it a thought.The problem was, that my intelliJ had the library dependencies in its build.sbt, but not imported.I just imported the lib, and the problem was solved. "},
{"body": "I am struggling to join two tables in mongodb.  I understood from google search that there is no joins concept in mongo.  However, we can achieve this by doing map reduce.  I am also new to scala.\nI have two tables in mongodb.I want to combine data from both tables to represent like belowSample Data will beCan anyone please help to add some sample scala code here to achieve this map reduce functionality?I tried this below code which works good, but I have millions of records and finding one by one is timing out.  First fetch userid from user table and then try to find the corresponding roleid.  Instead of querying on scala side, i followed this approach to resolve.This may not the trivial solution, but this is one of the approach suggested forthis kind of issues."},
{"body": "I have a scala code snippet given below: is expected to collate all nameToPlaces from all countries.\nHowever, while running this code, i am getting a classcastexception I have tried adding  but of no use. I am also importing \nAny help will be appreciated.NOTE: I don't see this question as a duplicate of a generic classcast exception as noted below: Please revise your judgement.You have several java collections in your outer map, you have to make sure to convert them all. As far as I can see, the following should work for you:"},
{"body": "I have problem parsing JSon into RDD{\"data\":\"{\\\"orderID\\\":\\\"3\\\",\\\"products\\\":[{\\\"productID\\\":10028,\\\"category\\\":\\\"342\\\",\\\"name\\\":\\\"Kids Coats\\\",\\\"gender\\\":\\\"Kids\\\",\\\"sport\\\":\\\"Basketball\\\",\\\"color\\\":\\\"Blue\\\",\\\"retailPrice\\\":268.0,\\\"sellPrice\\\":268.0,\\\"sellQuantity\\\":1,\\\"taxablePrice\\\":268.0,\\\"brand\\\":\\\"Inno Fashion\\\",\\\"stockQuantity\\\":999,\\\"subTotal\\\":268.0,\\\"ancesstorCategories\\\":[\\\"2426\\\",\\\"2454\\\",\\\"241\\\",\\\"342\\\",\\\"24\\\",\\\"34\\\",\\\"2439\\\",\\\"21\\\",\\\"3\\\",\\\"2\\\",\\\"1\\\",\\\"2412\\\",\\\"2430\\\",\\\"2503\\\"]},{\\\"productID\\\":10031,\\\"category\\\":\\\"334\\\",\\\"name\\\":\\\"Kids Tshirt\\\",\\\"gender\\\":\\\"Kids\\\",\\\"sport\\\":\\\"Cycling\\\",\\\"color\\\":\\\"Blue\\\",\\\"retailPrice\\\":59.0,\\\"sellPrice\\\":59.0,\\\"sellQuantity\\\":6,\\\"taxablePrice\\\":59.0,\\\"brand\\\":\\\"361 Sports\\\",\\\"stockQuantity\\\":994,\\\"subTotal\\\":354.0,\\\"ancesstorCategories\\\":[\\\"2426\\\",\\\"241\\\",\\\"33\\\",\\\"24\\\",\\\"2429\\\",\\\"334\\\",\\\"2439\\\",\\\"21\\\",\\\"3\\\",\\\"2\\\",\\\"1\\\",\\\"2412\\\",\\\"2503\\\",\\\"2451\\\"]}}When I read this information into RDD,I got an error at line 9, when processing category\nI would like to get the 'ancestorCategories' of JSON into the 'category' or RDD like a list\nList(2426, 2454, 241, 342, 24, 34, 2439, 21, 3, 2, 1, 2412, 2430, 2503)Error: found   : List[org.json4s.JsonAST.JValue]\n[ERROR] [Console$] [error]  required: Array[String]Can anyone help me to convert from List[org.json4s.JsonAST.JValue] to List[String]? \nThank you very much.you could use render to get raw String out of Valuethus you could:I can not run it locally now ,hope it helps.I think this might works , as you can do what ever you can in the x=>{} bolock"},
{"body": "I created an  wtih the following format using Scala : How can I get the list of the  from this ?The data for the first data line is:Assuming you have something like this (just paste into a ):then you get the first array usingwhich means the first row (which is a Tuple2), then the 2nd element of the tuple (which is again a Tuple2), then the 1st element.Using pattern matchingIf you want to get it of , just use :which puts the results of all rows into an array.Another option is to use pattern matching in :"},
{"body": "I learn Scala programming language and \nI have interest in machine learning.Which ML toolboxes are popular in Scala programming?you can use spark which is part of Hadoop and has a scala API and machine Learning Library (ML LIB).\n"},
{"body": "So the problem that I am having is that I don't seem to be able to create a sparkcontext. And I have no idea why not.Here is my code:And here is the result that I am getting:Any thoughts?Your scala version is too new and spark-core version is too old . I am using scala 2.11.8 and spark-core_2.11:2.0.1\uff0cyou can try it!"},
{"body": "I'm a complete newbie to Play framework as well as Scala. I'm stuck at the following scenario.My frontend is coded in html. The input from a user and then reads the data in scala using HashMaps. In html, there's a textbox by the id=\"replayZkUrlActual\"Scala Code on calling this page so as to read the text.I get the values correctly and I store the value from the textfield successfully. However, I need to display this value again whenever I click on submit button of my form in the same textbox (kind of autofill the previous value). I need \"replayZkUrlActual\" to display its value after clicking the submit button. \nThe same page opens on clicking the submit button.\nCan anyone please help.\nThanksJust follow these documentation to get rid of it\nI hope it will solve your problem"},
{"body": "I am building HTTP service with scala-play. I want to serve the JSON response.\nTo serve JSON response, I created case classes and created instance of Writes[T] class which is similar to followingI can server response like: I followed above approach from play documentation.Why do I need to define class and create Writes[T] instance for  serve JSON response.\nIf I have to create JSON for dozens of classes, do I have to define all classes and create instances for Writes[T]?I can easily convert any class data to JSON with following method:All I need do is create Map[String, String] for any class to serve JSON.Why play documentation recommends to define case class and create instance of Writes[T] to create JSON representation?Is there any other way to serve create JSON in scala? I want to keep response time minimal so that JSON creation must be lightening fast.You don't need to create the Writes instance. Play already have a default formatter for all the regular types (Integer/String/Boolean..)You just need to write the case class and add an implicit formatter in the companion object.See json.scala format[A] description from play:Back to your example, you only need:to get the json text just do:val json = Point.fmtJson.writes(Point(10, 20)).toString()will result: json = {\"X\":10,\"Y\":20}"},
{"body": "Env: Spark 1.6, ScalaHi\nI need to run to process parallel. First one, for receiving data and second one for transformation and saving in Hive table. I want to repeat first process with a interval of 1 min and second process with interval of 2 min.  How can I run this two process parallel and with desired interval in Scala?\nThanks\nHossainWrite two separate Spark applications.Then use cron to execute each app per your desired schedule. Alternatively, you can use  for scheduling your Spark apps.Refer to the following question for how to use cron with Spark: "},
{"body": "I get an  when I try to compile and package my code with  and Java 1.8.0_112.My code compiles well with Java 7.I tried to set  and tried with 1G as well but always get the same error.The code is written in Scala and I am using these versions in my pom.xml:I tried also with Scala version 2.12.0 because 2.10.x is not compatible with Java 8.part of the error stack: One more thing, stackoverflow error is usually a runtime execution error, how is it possible to get it during compilation ?[Added 1]   I found that this error is caused by scala classes with at least 150 properties [Added 2] The function throwing this error is equals function, i developed it like this: Thank you.I solved the problem by splitting the logical operation in manyThe approach to solve this type of problems is to compile the code gradually till finding the code bloc that throws the errorI hope this will help someone "},
{"body": "I am a scala beginner. Now I have to convert some codes I wrote in Pyspark to scala. The codes are just to extract fields for modeling. \nCould someone point out to me how to write the following code into scala? At least where and how I could get the quick answer. Thanks so much!!!Here are my previous codes...It goes like this:There may be a direct method for above logic.Test:"},
{"body": "I am creating a Companion Objects, How do i traverse these objects?, i have written but not working, error thrownPlease help hereHow do i traverse these objects?, i have written but not working, error thrownThe declaration of  variable is not correct.That's the correct way to declare a variable in Scala. Or you can just ignore the type and let Scala interpret it."},
{"body": "I filter a dataset to get a list of datasets which I then want to persist in parallel.Code:Currently the  runs sequentially. I can convert the  to   but then it won't be using spark for the parallelisation How can I do this with spark?The question is regarding nested parallization in spark. The following link answers it."},
{"body": "Im very new to spark... \nI have set up a standalone cluster using 3 centos vms...\nnow i want to develope some simple scala program and run it in the cluster...\nI am working on windows 7 station without network connection...\nI want to use eclis with scala plugin\nI have on this computer:scala.msi sbt.msi spark.gz hadoop.gzwhat do i do next?\ncan somwone please instruct me how to start a new simple project in eclipse and create a scala program i can send to the cluster \ni need a detaild instruction from what type of project to create and what refereces to add and how\nwhat project structure to use and how to set it\nand how to send it to the clustershowing me how to do that with both maven and without maven (so i deceide what best fit for me since i working without internet) will be most helpfullThanksIf you have set up of cluster then for starting point I suggest you to place text file in hdfs of your cluster and use simple app in order to read words and counts them.\nFor built you can use Maven-pom or SBT. In order to have less troubles use  with scala plugin. The dependency would be org.apache.spark:spark-core_2.10:{your_version_of_spark}. After packaging your app launch with required params with --master . More details which parameters can be used I am using IntelliJ and SBT on Windows to build my project but I think the key aspect here is to use a plugin like  for you (jar containing all dependencies except spark). Then all you need is to copy that jar to your CentOS cluster and from there run spark-submit with whatever mode (standalone, yarn, client) you need. See also  and Regarding the use of Maven vs. SBT, my recommendation is use whatever you're familiar with. Personally I find SBT quite concise and easy to use. For your first project just follow the recommendations in www.scala-sbt.org.As you are already have eclipse with scala plugin , then you can start directly creating maven project . The easiest way to go ahead is to download any sample spark maven project (in scala) from github and import in your eclipse as an existing maven project. \nInstead of running the code in cluster, you can run and check in eclipse itself if you provide .setmaster(\"local\") in spark configuration. This way testing and learning will be more easier . \nComing to deploying on cluster, you need to create a jar file out of your project (Right click on project->export as jar) and then copy this on to your cluster . Then use spark-submit to submit spark application by specifying master and other configuration settings . "},
{"body": "I cannot find documentation for json4s Scala library.\nI've found  or  but it's not exhaustive doc.\nI would expect complete documentation like Does anyone know where to find it? Google did not help me.\nThanks.You might be looking for this: "},
{"body": "I've seen my people stuck on this problem. And so as I. And many people said using sbt. And currently I've already downloaded sbt. But I don't know where the directories are, such as dir \"/src/main/scala\" or dir \n\"/src/test/scala\", could anybody tell me where are the directories? Thanks"},
{"body": "I want to setup a Spark-Scala-Sbt dev environment on UbuntuSo i have installed SBT, Scala Seperately before installing IntelliJ.But after installing intelliJ i have installed the  plugin for intelliJ as well.Now how to avoid conflicts between the 2 sbt's which one to use and how to setup properly to avoid conflict.Also wanted to know HOW .. the installed Scala & SBT is different from the scala & sbt plugin that comes with itelliJ IDEAThe standalone sbt plugin is obsolete with IntelliJ Scala Plugin 2017.1, which includes an  and supports building through that shell.To avoid any conflicts between different instances of sbt shells, it is best to have only one running at a time - either from the terminal or from IDEA. Other than that you don't need any special setup."},
{"body": "this is part of my code:I need to write a new file into my bucket using Scala. \nI get this error:When I try to save it locally, it works:How could I solve it?\nThanks in advanceS3 being an object store, you cannot write strings. You can probably write the strings to a local file and periodically upload the file to S3.S3A being a Hadoop Filesystem client, you can't use the java File API to work with it."},
{"body": "I have a requirement where I have to pull parquet file from S3 process it and convert into another object format and store it in S3 in json and Parquet format.I have done the below changes for this problem statement, but the Spark job is taking too much time when collect statement is called Please Let me know how this can be optimized, Below is the complete Code which reads Parquet file from S3 and process it and store it to S3. I am very new to Spark and BigData technologyFirst, you really should improve your questions, with a minimal code example. It's really hard to see whats going on in your code...Collect retrieves all elements of your RDD into a single RDD on the driver. If your RDD is large, then this will of course take a lot of time (and maybe cause an  if the content does not fit into the driver's main memory).You can directly  the content of a / using parquet. This will surely be much faster and more scalable.Use s3a:// URLs . S3n// has a bug which really kills ORC/Parquet performance, and has been superceded by s3a now"},
{"body": "How to write a function can take  Array[Array[T]] or List[List[T]] or List[Array[T]] or SeqLike[SeqLike[T]] as parameter? Some function like:matrix is List[Array[String]], but matrix can be anyof Array[Array[T]] or List[List[T]] or List[Array[T]] or SeqLike[SeqLike[T]]or in another way, very similar as  method of  trait of scala libraryBTW : right now I got error \"Error:(48, 26) type mismatch;\n found   )\" during runningYou could use higher kinds I guess, and play around with the lower bound to get the semantics you want. You can also enable  in  by explicitly passing  to ."},
{"body": "How do I complete the declaration of the val:This is to help understand how to iterate over it, with say .Edit: I restated the same question in a different way.Seems that's what you're asking for. "},
{"body": "I am new to scala. I have a case class. The code is given below.I also have a function which returns an  of objects of the class. This is what is being returned.Now I want to read values from the object. I have looked at some resources in google. This are what I have tried. BTW,  is the  of obejcts. None are working. I don't know what to do. Please help me. I am stuck for hours.Your question is pretty vague.  Do you just mean this?prints:"},
{"body": "now I have two rdds ,the first is  like this 1,23,45,6 and the other one is like this 7,8 9,10 11,12 now I want to union these two rdds like this 1,2,7,8 3,4,9,10 5,6,11,12 how can I do this? the rdd.union can't get this resultIf you can warranty that the two RDDs have the same number of elements and partitions, you can achieve the desired result with , and then re-shaping the resulting pairs:If the two RDDs differ in the number of elements or partitions, you will need some key to join them. Indexing them is not very efficient but would serve the purpose, although a domain-specific solution (if existing) will be much better :"},
{"body": "I want to change the format of my data, from RDD(Label:String,(ID:String,Data:Array[Double])) to an RDD Object with the label, id and data as components.\nBut when I print my RDD consecutively twice, the references of objects change :I think that explains why the  method doesn't work. So can I use  with objects as values, or do I return to my classic model ?Unless you specify otherwise, objects in Scala (and Java) are compared using reference equality (i.e. their memory address). They are also printed out according to this address, hence the  and so on.Using reference equality means that two  instances with identical properties will NOT compare as equal unless they are exactly the same object. Also, their references will change from one run to the next.  Particularly in a distributed system like Spark, this is no good - we need to be able to tell whether two objects in two  JVMs are the same or not, according to their properties. Until this is fixed, RDD operations like  will not give the results you expect.Fortunately, this is usually easy to fix in Scala/Spark - define your class as a . This automatically generates  and  and  methods derived from all of the properties of the class. For example:If you want to compare your objects according to only  of the properties, you'll have to define your own  and  methods, though. See , for example."},
{"body": "I am familiar with \"\"I also have read  which states that a programmer is able to produce only so many lines of code per day and therefore a language with a \"better\" syntax can only help in this direction.As there is nothing perfect in the world, the syntax of Scala has its  (of course someone can see these as pure advantages). I am talking about stuff like:If one has Java background (or even C++, C#, etc.) it is kind of weird to have import statements in the middle of a function, to have nested functions and I am not even going into other language constructs.I am a bit sceptical about the real productivity gain coming with Scala. How do you experience this? Do you apply all (as much as possible) of the coding conventions and is this a real benefit? Is there a better way to wrap ones head around this style of code?P.S. maybe this whole post can be moved to Programmers or similar...I can share only my experience and I don't know whether other people's experiences are similar. It certainly depends on personal preferences. I came to Scala from Java, so my preference in general is to have all imports at the top of the file. This makes it easier for me to faster see the dependencies of the files. Sometimes however I do use local import in places where I want to import a name only locally and not to pollute the namespace of the whole file.After few weeks in Scala I really enjoy the the lack of mandatory semicolons. In Scala they are mostly not needed, but they can be helpful when you want to write more short statements on one line. Scala actually changes my attitude towards semicolons and now in new projects I don't use semicolons even in JavaScript.This is one of the best features of Scala: Functional programming. It allows to write much simpler code consisting of small easily understandable functions working with immutable data. This decreases the complexity of the code, reduces bugs and increases the productivity.Again, lack of return statements feels just natural. The function returns the value of its expression, where the value of a sequence of statements/expressions is the last one of them. Return statements just feel very unnatural in Scala.Scala has a learning curve and of course when I have started my productivity had a dent caused by my lack of familiarity with the language and the standard library. Now, after few months of Java and Scala work, I can say that I am in more productive in Scala than in Java. Even though I consider myself a Java expert but only a Scala novice.I'll go step by step, although some points could be subject to personal opinion:Using such imports for implicits is useful in many cases.For this usage, importing makes code safer and more readable.Second case would be ambiguous class and package names (can be solved via aliases). Showing explicite which one is using can be helpful.In Scala you can do many things in many ways, a little bit like with C++. You have more responsibility for your coding style. Watch this: Nested methods are great for recurrence and when such method should not be used even as private at class level. With nesting you can also reduce amount of passed arguments.If you can you then you should place you nested methods on the top of method body. Giving comments on where is constructor body at big class is also helpful.Scala gives to community the ability to develop the language, scala can change in fundamental ways due to our experiments, good practices."},
{"body": "As per this example will the calculation for someValue be ran only once for each instance of the class? Would the same rules apply to an If you have an immutable case class (and of course objects are immutable by virtue of being a single instance) then there is no sense in re-running calculations that have been ran, even if they use values in the class itself (I'm thinking of custom hashCode calculations for example). Is the compiler smart enough to handle that or would it be better to place the val outside of the def? (Making it lazy so it only gets called when needed...)  The literal is inlined at compile time:Generally, the Scala compiler doesn't optimize such cases when there are no literals involved:When you write , then you mean , i.e. every time you call the method everything in it is evaluated. Anything that should be cached needs to be done manually on your side.In the above example however, it is very likely that the JVM itself will inline  and , with the result that the first and the second example will be optimized to the same machine code.Simply put: No. is only evaluated once. is evaluated every time.That is the difference between those keywords in the Scala language.As  has no parameters and is not affected by side-effects, it should be changed to  or as you say  so it is only ran when needed.It is more productive to program with intent rather than rely on compiler optimisation."},
{"body": "Why this  line shows compile time error??Im pretty sure it should be , since Member is a class, not a function"},
{"body": "My two scala lists are as below. The two lists may have different lengths.andnow I want to filter like this:but when I execute above code it returns me as:The result mixes the tuples, and it returns wrong results. Actually the result should be the empty list because both lists of tuples contain the same data. If I check for  like  it returns expected results but for  it mixes data.Does anyone know the exact problem here? What I am missing? Another problem is that every time both lists not same check this new lists with less data .and \n and expected output as Basically I want to find out  Trying testing with a simpler and smaller data set so it's easier to see what's going on.You're mapping over the product of the two lists. It sounds like what you want instead is the zip of the lists.Add back the filter, and we're done:You need to zip your  and  list firstly and  with condition:"},
{"body": "I am implementing my own  collection using a generic abstract class . I want to add a  function which changes the list according to a given function :I am getting the following error in : Tt first you can read architecture of scala collection\nImplementation of map in TraversableLike:With  you are trying to match your own  against Scala standard library . If you want to use pattern matching, you need to define your own extractor. You can read more about infix extractors at , paragraph .Another nice explanation can be found at :"},
{"body": "Using Spark 1.4 I have two dataframes of different schemas. Let's say:DF1: stringA, stringB\nDF2: stringCI'd like to merge/combine the two into a single dataframe like:DF3: stringA, stringB, stringCI can't seem to find a way to combine the two when they have no key for me to use.You are looking for "},
{"body": "I want to initialize a java object within scala class, so this works:But this one doesn't work:My question is, how I can pass arguments to a java object declared inside a scala class?Your code is correct, You can check it by the next one code:Java class  takes argument from the scala. You definitely have problem with the class . Check if the class correct and if you have a correct class version of the  in your class path"},
{"body": "I want to create a  function which can later be used by three different RDD data sets.\nFunction takes key and value and converts to seq[String]when I tried to apply by one data set its ok because it has one key with  6 values example:-but I tried to apply on another data set I need to we write the function because other data set contains 7 values with one key this makes to re write the function with same logic but different name.Is there a way to write one function for both which accepts 6 or 7 values of string with just one key ? or can I extend my function ? classes inherit from , so I would define the function like this:Note that  classes have a  that I'm using here to create the string (I found your way somewhat verbose and more difficult to read) and I'm also delaying the  call until after converting the values, so the map operation is run in parallel.Finially, I have changed the name of the function, since it converts to a  and not a .Yep go the answer need to use data type of any"},
{"body": "I want to replace all the consecutive underscore with a single space character. I am able to do so with the following  code. But I am not able to do so with . Below is given my  code.Below is given my scala codeI am given the following error.What am I doing wrong?In Scala, as in Java, a single backslash is an escape character in a (standard) string literal, so you need to double it to put an actual backslash in the string to be used to initialise the Regex:Alternatively, you can 'triple-quote' the string, to prevent the usual escaping behaviour:"},
{"body": "I have a piece of code as such (simplified): \n    import scala.concurrent.FutureIt works fine, but when I have a huge \"BigCase\" class with 15 fields, it would be nice to be able to write , instead of , especially during the stub-out phase. But once I did , id became type \"Nothing\" , which makes further steps hard to compose. Is there a way around it? I am not sure how your code is supposed to work but you are defining your  instance for the first time within your comprehension. There is no way to shorten the declaration itself but I think your code makes more sense if the  instance is defined before the for comprehension:Assuming I understand the question correctly,  will compile and work everywhere you want a . If you want to use the placeholder in multiple places, you can use . In fact, given the type of ,  should already work fine, so it isn't clear what you mean in this comment:If you just want arbitrary int fields,Then go back later and fix the number of fields?or, more arbitrarily,"},
{"body": "I have the two following objects (in scala and using spark):\n1. The main object2. The object odbscanWhen I execute this code I obtain an infinite loop, until I change:filter(_._2.size >= param_user_minimal_rating_count)to filter(_._2.size >= 1)or any other numerical value, in this case the code work, and I have my result displayed What I  is happening here is that Spark serializes functions to send them over the wire. And that because your function (the one you're passing to ) calls the accessor  of object , the entire object  will need to get serialized and sent along with it. Deserializing and then using that deserialized object will cause the code in its body to get executed again which causes an infinite loop of serializing-->sending-->deserializing-->executing-->serializing-->...Probably the easiest thing to do here is changing that  to  so the compiler will inline the value. But note that this will only be a solution for literal constants. For more information see  and .An other and better solution would be to refactor your code so that no instance variables are used in lambda expressions. Referencing vals that are defined in an object or class will get the whole object serialized. So try to only refer to vals that are local (to a method). And most importantly don't execute your business logic from within a constructor/the body of an object or class.The only difference between the 2 snippets is the definition of  outside of the  which does not change at all the control flow of your code.Please post more context so we can help."},
{"body": "I have an rdd which has 50 million elements all of which are strings. I have to filter and make a new rdd which has any of the following terms: SO, if a string in original rdd contains any of the string from above array, it should be in the filtered rdd.\nWhat's the efficient way to do it in scala?\nIs there an one line way of doing it or do I have to traverse through each element and find out?Something like:"},
{"body": "I am learning Scala and running below code .I knew functions, that do not return anything is procedures in Scala but when running below code why extra () is coming in output. Here in procedure i am just printing the value of 'value'.\nCan someone explain about this.The so-called \"procedure syntax\" is just \"syntactic sugar\" for a method that returns  (what you would call  in Java).Is semantically equivalent (and gets actually translated) to:Notice the explicit type and the equal sign right after the method signature.The type  has a single value which is written  (and read unit, just like it's type). That's what you see: the method  prints  and then produces  of type , which you then move on to print on the screen itself.As noted in a comment, the \"procedure syntax\" is deprecated and will removed in a future version of the language.Procedure syntax compiles to a method that returns unit.calling toString on  produces You are printing out the result of test (which is ) so you see its string representation,  in the output."},
{"body": "I have a type of set and union function as followWhen i use the union operation,It should return me another set which contains all the elements of both sets  and I am sorry i was not clear on my question, i have my own implementation of the iterator for both Set xs and ysBut i was not satisfied with this implementation and found that you can implement the condition with the function (after some googling) but i was unable to get it to work in my eclipse worksheet. I am guessing it returns a function. so my questions,\n1. is it possible to implement as described above in the edited section? if so, then what am i missing to get it working?\n2. I don't understand how the element  in  is iterating over the elements in both the sets Look at your  type: . So it takes an  and returns a . What that means is that it is  a collection that you can iterate over to retrieve all its values, because it actually contains no values.If you want to know what  values return  then you have to iterate over the entire range of possible inputs (or some subset thereof) and filter for the condition you're looking for.Your confusion stems from the fact that you've named your function after an existing collection in the standard library.  works because  is  an example of your , it is a  from the standard library with all the associated methods. Rename your type alias to something like  and you'll see what I mean."},
{"body": "... is not the same thing as ...For one thing, the underscore, , represents a passed parameter only once. If you encounter something like  it does not mean the parameter is added to itself, it represents the addition of two different passed parameters.So what is ? It is an anonymous function that takes one parameter of type  and attempts to find the index where the sub-string \"123\" can be found. In this case the underscore is unrelated to the parameter sent to the  lambda."},
{"body": "How can I remove empty Array?expected Output isYou can just use . If you have a list:If you have a RDD:"},
{"body": "I used  to combine the source DF (with negative weights) and the target DF (with positive weights) into a node DF. Then I perform  to sum all the weights of the same nodes, but i don't know why  didn't work for the unioned DF at all. Did anyone face the same problem ?:You're simply ignoring the result of the  operation: just like all DataFrame transformations,  doesn't  the original DataFrame (), it produces a new one. I suspect that if you actually use the return value from  you'll see the result you're looking for:"},
{"body": "So, if I convert this ListBuffer to a list, it is not retaining the type.Is there a way to do this? is a   from your declaration When you convert buffer to a list, the same \"inner type\" is maintained, meaning you get a  from a scala \"native\" collections don't support collections of unrelated types (String, Int, Option[X]), you get to lose important type informationA solution would be to use a tuple instead, if your variable is of finite length (a tuple of 2 elements has a different type of a tuple with 3 elements for instance)Another solution is to use  HList.  is very good to explain what is an HList, why would you want one, and how to use"},
{"body": "Why None: Option[A] in the first parameter in headOptionViaFoldRight?Why Stream[A] in the first parameter in takeWhileViaFoldRight?Why true in the first parameter in forAll?Just confused on what to use in the first parameter in foldRight?The initial value: what you'd get on an empty sequence. It is by convention, and historical precedent, that   on an empty collection returns  while  on an empty collection returns ."},
{"body": "The server will be processing large amounts of data daily and needs to be fast. The best language is the language that you, and your team, know the best - you will do a better job writing your server in a language you are an expert in than a language you just picked up.It also depends wildly on the details of your problem.  You mentioned \"lots of data\", but not what that means.  That could be 100MB a day, 1TB a day, 15PB a day?Is this big batch processing, or lots of little interactive operations?Of the three specific languages you tagged in, Java, Scala, and Ruby, all three are reasonable choices that real companies have been successful in the market.JVM hosted languages are generally going to be \"faster\" than traditional MRI Ruby, and both Java and Scala are generally \"faster\" than JRuby, when it comes to raw CPU capabilities.Of the three, Scala probably offers the least familiar, most challenging paradigm for developers unfamiliar with it, but also has very good parallel and concurrent programming tools available in the core compared to the other options.Finally, your end results will be influenced as substantially, if not more so, by the rest of the ecosystem you choose - the web or message handling stack, the data storage engine, and other concerns.  They, perhaps, should dictate your language more than your language dictates your ecosystem."},
{"body": "i have a problem in my projecthow do I retrieve data from the value: [some (test)] and only take the value of (test) only. and using the get method?orhow do i change the form of value: [some (test)] to \"test\" justBasically, don't use get() ever.  There's no situation in which it is a better choice than the alternatives.  Option is a way of indicating in the type system that a function might not return what you want.  You might think of it as a collection containing 0 or 1 things.Take a look at the  which show several different ways to handle Option; as a collection using map/flatMap, getOrElse, for comprehensions, and pattern matching.e.g. map:If maybeData is None, nothing happens.  If it is Some() you will get back an Option containing the result of doSomethingWithAString().  Note that this will be Option[B] where B is the return type of doSomethingWithAString.e.g. getOrElse:If data is Some, result is \"test\", otherwise it is \"N/A\".e.g. pattern matching:You get the picture (note this assumes doSomethingWithAString returns String).If you use get() and maybeData is None, you get the joy of handling the equivalent of nullpointers.  Nobody wants that."},
{"body": " can be instantiated with or without parentheses:Which form is (more) idiomatic? Should the parentheses be included since, as I understand, creating an object is a , similarly to a ? I wrote a similar answer here:\nIt's especially relevant that constructors are not like methods with respect to parens.A constructor always has a non-implicit parameter list.  If you omit it in the definition, then it is supplied. is idiomatic. (If there's a need to rationalize it, consider the instance as a result.  If instantiation is a side effect, nothing non-primitive could be pure.) requires the parens, so they are useful syntactically.You must supply parens if an implicit follows; see the link."},
{"body": "I'm trying to call my Scala  object from Java code:Here's the compile-time error:From reading these Java to Scala Collections , I am using a  Set rather than the default, immutable Set:But, I don't understand the error message. By using a  (boxed ) in my Java code, why is a  found?Demonstrating what the commenter said:Working with  and type aliases from  (not the converse, as  showed :) is going to be at least nasty, most likely very painful, and quite possibly impossible. If you're able to modify the Scala side of the API, I'd recommend adding a Java-friendlier API to it.  If not, I guess you could build an adapter layer in Scala that proxies Java clients through to the native Scala API.So, something like:"},
{"body": "I have a json data as from above., I am interested only in data I extracted it as a Map,}I got output as followsFrom Here, How can I  bind a Map to scala properties, Thanks in advanceThere are many different ways of doing this, here is a straightforward one using . For more ways of handling JSON in Scala, see  and Note: You probably want to handle date and time in a slightly more sophisticated manner, e.g., using locales, etc.If you don't mind the fields of  being all of type  (which is what the JSON delivers them as), you can also use the simpler versionand then first parse the whole JSONand access and decode the relevant part"},
{"body": "I'm trying to define an array of numbers with the last being a function. It'll probably be better to give an example:Is this possible using Scala? If so, how?Thanks in advance.Your question as stated is nonsensical, because a function is not a number. But if you meant an array of numbers and also a function, then you can do:or if you prefer efficiency over  type safety:But it would really make more sense to do:Replace  and  with appropriate types in the above.Yes it is possible but you cannot use an ; you need something like a .  This is further answered here: You can define  with an element that is a function as long as the function returns an  since all elements in a Scala  must have the same type."},
{"body": "i'm writing a method that take a  ,  and a  parameters and return a List[(String, Int)]\nThe  parameter represents the input list, the  element represents the element to add to the list, the  represents the maximum dimension of the list.The method has the following behavior:I've implemented the method as the following:Exist a way to express this behavior in a more clean way??First, a corrected version of what you posted (but it appears, not what you intended)And some testsNeater version (but still not meeting the spec as mentioned in a comment from the OP)Another version that always removes the minimum, if that's less than x.and its test resultsbut as I say in a comment, better to use a method that processes the whole sequence of Xs and returns the top N.I would separate the ordering from the method itself:What do you want to do if the list contains more than one element less than x? If you're happy with replacing all of them, you could use  rather than your handwritten loop:If you only want to replace the smallest element of the list, maybe it's neater to use ?"},
{"body": "first Listsecond ListThe above are my two lists now i have to combine these two lists like below.Expected OutPut =>)The similar question "},
{"body": "I'm trying to insert data into a table called users.\nI'm only passing a value for the name field and this exception pops up.I'm not even passing any Timestamp in the parameter.The data still gets inserted into the database even if this happens. Why though?Here is the error I'm getting: Here's the code:joined is a field of data type timestamp with time zone.My scala version is 2.11.1java version is 1.8.0_25My postgres jdbc driver is 9.3-1102-jdbc41I guess the pk returned by INSERT is a timestamp, and you ask it to be parsed as ."},
{"body": "I am considering using the Blue eyes framework in order to build HTTPS RESTful webservices in Scala for a project. However, as the documentation is pretty poor, I can't figure out if we can configure it in order to work with SSL or not?Many thanks for the answers!Blueeyes is unmaintained; I would not recommend it for a new project. I'm a big fan of Spray myself, but if you want something that's very similar to Blueeyes you might like to use Redeyes."},
{"body": "I have a Scala project built with SBT.This is my project structure:I want to read the content of  in . How should I write code in parser.scala or I should do more setting in build.sbt?Just getResource should be good"},
{"body": "I have a class called Rational:I've written this function, which is a subfunction of a bigger one. I'm just debugging it.\nAssume b is the Rational 3/4 and we pass the Rational 2/4 into iter.And here's denomDec: The problem is that the iter function does not stop and runs into Stack Overflow. Here's what happens inside iter(). It gets 2/4 as argument. Then itake 2/4 + 2/3 + when it reaches 2/2, it calls denomDec to get 2/1 after 2/2 because the latter fails to be smaller than 3/4. Then denomDec gets to 2/0 and here it is supposed to return new Rational(0,0) but it fails to do so which in its turn causes Stack Overflow.My question is that why doesn't the function stop when recurring when it hits the base case which is For clarification: example iter(2/4) should do this: 2/4 + 2/3 + 0/0. \n2/2 and 2/1 are skipped as they are both greater than 3/4.maybe your problem is out of the presented scope;\nI have tested code and I do not get error:Rational(0,0) is not zero. When you do math on it you are getting bad results. Consider the case Rational(0,0) < Rational(1,2). It should be true, but your < operator evaluates it like this: 0*2 < 1*0, which is the same as 0 < 0 which is false.In iter() our second if statement does not work correct when it evaluates Rational(0,0) and the Rational(0,0) gets passed to the denonDec and its denominator goes negative. Once that happens you're sunk.I would put a guard in the Rational constructor to prevent denoms less than 1. I would use Rational(0,1) as zero. Remember from basic arithmetic that the fraction 0/0 is undefined.I figured out what is wrong with my code above. \nInstead of  I should do  so that I actually call the  function again with a smaller argument.\nAnother mistake of mine was trying to add 0/0 to another . The problem is that if you look at my code for add  you can see that adding 0/0 to any  will always return 0/0. Hence, I need to return something other than  when the base case is reached."},
{"body": "I'm having a situation like this:I have a sequence that I need to match. Actually, in the \"case\" I only need to match against a sequence whose elements are of tuple (String, Seq[String])..., but I couldn't find a way to do that..., so I resorted to the technique I read on web: decapitate the seq, match against the first element, and re-attach inside the block to get the original seq.The problem with that approach is: type erasure.The resulting seq from the expression \"head +: rest\" is a Seq[Any] instead of Seq[(String, Seq[String])]That's why the \"tuple_.1\" gives compile error (line 153 in the attached image).How to work around this situation?It was my bad, there are a couple of coding errors in the above screenshot; from the case block of sequence matching I should have added another line to the end: complexSeqReconstructed.Apart from that, there's a little detail: from the \"case Nil\", I also have to return an empty Seq of type Seq[(String, Seq[String])] ... that matches the return from the other case. That way Scala will correctly perform type inference. Otherwise the line hhh.map wouldn't compile (it will say, Object hhh doesn't have map method).So here's the updated (working) code:"},
{"body": "I am working on a web project using scala on play framework. I will need to import an external package to use the classes defined in it. I have no problem to do it in my .scala file. But I don't find a way to import that package in view/..scala.html file. As a result, when it is built, it reports error not found the definition of that class. I tried to import that package like the below way. @import myPackage._ But it doesn't seems to work. How can I do that? if you want to use any method of model or controller or any of your package you can tryi have not tried your way"},
{"body": "The following template displays a PNG image by taking the image as a byte array and encoding it into Base64:The template works for a Play project that uses Scala as its main language:However, if a project uses Java as its main language:The following error is seen when running the project:This is with Scala 2.11.4 and Play 2.3.6Why does the template expect a Scala Byte instead of a Java Byte here, even though the Play project is configured to use Java?Twirl, the Play template compiler, is Scala based. It reads in the template and generates Scala source code. It doesn't matter that you enabled PlayJava, that doesn't change what the Twirl compiler generates.  I suggest you look at the source code under target/scala_2.X/twirl to see what Twirl generated and what the Scala compiler is attempting to compile. Twirl has likely interpreted  in your template as  and output a fully qualified class name. You will need to be more specific in your template if you meant to use the java version of Byte."},
{"body": "So I have  key value pairs. From this, I can easily create  key value pairs as well as  key value pairs. However, I require  key value pairs for persons who share the same surnames. The way I solve this now is to pass a function to the  function that takes  as input and return  by iterating through the list of firstnames against a surname. However, I notice that spark is not properly able to parallelize my program with this. I want to know if this result is possible only by using  and  functions, that is, without requiring us to write a special function for this with ?In other words, this is an example of my inputfor which, we should have the following output to  pairs, then ?"},
{"body": "Below is some really rudimentary sample code to illustrate the question.This will reliably return the same first value always. Every time getRandom is called.However,This will return a different random value every time.I suspect this is because by re-initializing rand every single time (as in the first one) we end up getting the first number for that seed. Since the seed guarantees that we will get the same \"set\" of random numbers each time this is expected behavior. By moving the generator outside of the function it will exist as long as the class does, and as a result continue generating numbers in that \"set\". Is this the correct way to think about this?It's not a \"set\" so much as a \"stream\", but yes, that's it."},
{"body": "I am relatively new to Spark and scala programming.I was trying to execute the simple pagerank algorithm using scala. But I encountered this error when compiling.I have attached the code I am using.foreach is, as Ryan pointed out, solely for side-effects. It returns Unit and not the List itself. Ergo no chaining. Now what you are actually doing is the following: is not a member of  and you get your error messageYou should be doing:or "},
{"body": "If I have the following tuple:Is it possible using distinct()?, with the first element by tupleYou need to introduce some concept of \"first\": an RDD is a (distributed) set, not an ordered list. So given a function like: You can simply: Yes, it's possible. However it's not , but .(Assuming you use Scala for your spark job)\nOnce you get an array of these tuples, create RDD with  and call  method of resulting RDD. At reduceByKey, you can specify how you want to prioritize the values in second element of these tuples. For other RDD related operations, you can refer .With spark-shell you can interactively try your function.I formatted the last output. Hope this helps."},
{"body": "i build tests on my Scala Spark App, but i get the exception below on Intellij while running the test.  Other tests, without SparkContext are running fine. If i run the test on the terminal with \"sbt test-only\" the tests with sparkcontext works? Need i to specially configure intellij for tests with sparkcontext?The most likely problem is that spark-core version doesn't match.Check your sbt file to make sure using the corresponding spark core version you have."},
{"body": "I'm a tester and fairly new to both play and scala. I have some source code to test, which I've managed to compile and have installed elastic search, which is being used as my data source. I'm getting an error when I try to run the code:I've seen other threads on this but they all refer to checking the elastic search nodes are configured in . However, the app I'm testing has no such file. Is there anywhere else the node could be configured? I'm at a loss to where the elasticsearch node is defined. I have elasticsearch 1.4.1 installed. Any help would be much appreciated, I have exhausted all avenues within my current knowledge! Basically you need to tell Play where (IP and port) of your ElasticSearch instance. This is typically done in the . Here are a few things you can try: "},
{"body": "and this is my insert statement:There are two main modifications you need:It's hard to tell from the code you posted exactly what you have in mind. It would be helpful next time to simplify your example first. I've assumed your model is something like this:I've left out one of the byte arrays and the created and updated fields as they don't seem relevant to the question. In other words, I've simplified.The  seems ok.  I'm ignoring the foreign key as that doesn't seem relevant. Oh, the  are now deprecated (in Slick 3 at least). You can leave them off because your columns are not  values.What we need is the table query, which I'd add inside :You could use  to insert data. But you've asked for the IDs back, so you want to use .Putting it all together (again, using Slick 3 here):Produces: "},
{"body": "I'm stepping through my first Scala project, and looking at parser combinators in particular. I'm having trouble getting a simple unit test scenario to work, and trying to understand what I'm missing.I'm stuck on pattern matching a ParseResult into the case classes of Success, Failure and Error. I can't get Scala to resolve the case classes. There's a few examples around of this, but they all seem to be using them inside something that extends of of the parser classes. For example the  are inside the same package. The example  is inside a class extending a parser.The test i'm trying to write looks like:and the method I'm calling is designed to let me exersize each of my parsers on my SUT:The fundamental issue is getting the case classes resolved in my test scope. Based on the  for the Parsers class, how can I get them defined? Can I resolve this with some additional import statements, or are they only accessible via inheritance? I've tried all the combinations that should resolve this issue, but I'm obviously missing something here.Right hand column FTW! I stumbled on an answer in this . The problem was identifying the case classes as nested classes of Parsers."},
{"body": "I am trying to run some scala code in eclipse but i got stuck with a weird problem. If no errors exist, it runs fine, otherwise it just gives the error: could not find or load main class ....\nWhat may cause such an annoying problem?this snippet outputs:Error: Could not find or load main class p.MainI want to see something like this:Error:(15, 5) not found: value --try omitting the .  If that works, then it's the capture into In my estimation, the odds are high that you have a case mismatch.  You defined , which is  but your error is for not finding .  Many systems are case sensitive, so if you are used to Windows style case insensitivity, you'll have to be vigilant."},
{"body": "I'm admittedly new to Scala and Android programming and in all my searching I haven't been able to find answer to help me understand and resolve my problem.Here's a gist of my two scala classes I'm running through the tutorial found here: which I'm trying to adapt to scala code (I have need for understanding Scala for work and a personal desire to mess around with Android development so I figured I'd try and combine these efforts).The problem is seen in DisplayMessageActivity.scala in which the IDE reports that it cannot resolve MyActivity in this line:I feel like this should work. I can get it to resolve if I change MyActivity to an object, but then that breaks other pieces of the application that expects MyActivity to be a class.An help in getting me to understand my problem would be appreciated.You cannot reference the  field from  as though it was a 'static' field (in Java terminology). To access  in your other activity you will need to either obtain an instance of  that you can then de-reference, or add a companion object (defined using the  keyword in the same file in which the class is defined) for  and define the field there:then your call as above will work."},
{"body": "I'm trying to get Slick 3.0 running with MySql. I've made the following changes to the hello-slick-3.0 activator project:It compiles fine, but running gives the error  coming from the line .How can I get Slick 3.0 running with MySql? Am I missing something simple here, or are there any other working examples? Thanks.Fixed it. The URL in  was in the wrong format. Should be where you've already created  in your database. (That fixes the db access issue that I was asking about, but you'll still get a key-spec error. To fix that you need to remove the  entry from  for , which is described here: .)"},
{"body": "I've created a cookie in Scala, which I would like Javascript to be able to delete and/or modify.Here is how I created the cookie in Scala ensuring the httpOnly param is set to false: (\n\n)I used the following method to delete the cookie in JavaScript, but the cookie does not delete.\n(  )Aside from attempting to delete the cookie, it doesn't seem like I can modify the contents of the cookie either.How can I ensure the JavaScript can modify and delete the cookie created in Scala?I fixed the issue. \nI had to ensure that both the cookie created in Scala and the one deleted/modified in JavaScript both had the same path.For example, in Scala:In JavaScript:Notice the path in both examples used \"/\". Once I used the same path, I was able to delete/modify the cookies in JavaScript. Before this I hadn't explicitly set the path in the JavaScript code."},
{"body": "When I am using play framework, every time I've changed the code, it will take effect automatically by re-compile the code.However, when I'm using sbt to run a project without play-plugin, it won't take effect.I'm wondering if there were a way to make sbt project hot swap the changed code.My build.sbt is as below:And what I have configured in intellij idea is like below as a sbt task(I can't post images by now):However, every time I've changed the code in backend, It won't take effect after I've call backend from the frontend.I'm wondering how I can solve the problem. Thanks for your guys' help.You can have sbt automatically recompiling any changes by invoking it like this:\n    If you use ~run, on every change the changeed classes will be compiled and project rerun again. \nIf it does not work, you might explain more about your project and structure."},
{"body": "I'm pretty new into development and I'm going to be building a Proxy server for work. I'm not really sure what goes into a building a Proxy server and anything I can find is just telling me to install something and set one up; but I want to be able to build my own. I'm going to be working in Scala so what exactly goes into making one and what does it do?There are two major kinds of proxies:Both kinds of proxies may or may not have the following value-added capabilities (this is a sample and nowhere near exhaustive):I'm going to assume you want to build an HTTP/HTTPS forward or reverse proxy.Is it a reverse proxy?An HTTP load balancer in front one or more application servers is a reverse proxy.  In this case the backend server is either fixed, selected based on headers (Host is a popular one), or selected from a pool when load balancing.   The backend may use the same protocol or may use a custom load balancing protocol.  For your case I'd recommend using the same protocol unless there's a very compelling reason not to.Is it a forward proxy?An HTTP proxy between end-users and the Internet is a forward proxy.  In this case the proxy establishes a new connection to the requested domain and sends its own HTTP request possibly directly copying the headers and content of the user.  The proxy may choose to allow or deny requests based on domain names, URLs, IP addresses, or content.Both kinds of proxies do essentially the same thing: take an inbound request and make it to the destination on the requestors behalf.The basic procedure for a simple proxy is as follows:To make that work for a reverse proxy the destination needs to be a parameter of the proxy process (configuration, code, etc.).To make that work for a forward proxy the requestor needs to have a protocol for expressing the destination.  There are many protocols for this: HTTP. HTTP CONNECT, and SOCKS being the major ones.  In the simplest case the client connects to the proxy server over HTTP and sends the same request they would normally send directly to the destination server.A novel (as far as I know) thing to enable would be a reverse proxy that accepts connections from backends.  This would allow a DMZ that can't be easily used to attack the backends in the secure zone.Kate Madsudaira provides  in the book ."},
{"body": "I recently downloaded source code of com.github.nscala_time package version 2.11,\nAfter set up dependency in Maven, I got lots of errors, I checked one file\ncom.github.nscala_time.time.DurationBuilder, it got a line like:There are no class or type named \"Super\" in the same package or imported packages. I am wondering scala has a type called \"Super\"? the Eclipse scala 2.11 compiler complains about cannot find type \"Super\"I think you should find it in the object time in package com.github.nscala_time. Try to add this import:the solution I have is to add a trait named \"Super\"        `package com.github.nscala_time.timetrait Super {}`It seems to me the author of the library forgot to upload the class file"},
{"body": "I am trying to create a controller that extracts a file on the fly from an archive and then then renders it as a static asset with the default Asset controllerI get these errors at compileYou either have to make sure your action returns a :Or change it so it's not  anymore:"},
{"body": "Using geotools version 11.2Ended up using this function. You can see something similar for maxX and minY in the source code for GridGeometry2D.java worldToGrid function"},
{"body": "Given:The following call takes roughly 15 seconds to run (5 seconds per 's application to each element).I can use  to, as I incompletely understand, parallelize it. If I use it, then the execution time shrinks to roughly 5 seconds.Is this parallelization the recommended way in Scala? What are alternatives?  creates an already completed future and to do it it must evaluate its argument. What you want is slightly different and consists in running each future and then wait for completion of each oneAnd then"},
{"body": "I am trying this:This is not giving expected output, how can I achieve it in fastest way?You can just group by both the columns instead of grouping by a single column and then filtering. Because I am not fluent enough in Scala, below is the code snippet in Python. Note, I have changed your col names from \"stream\" and \"class\" to \"dept\" and \"name\" to avoid name conflicts with Spark's \"stream\" and \"class\" types.This results in the following output -It is not exactly clear what is the expected output but I guess you want something like this:If you don't care about names and have only one group column you can simply use :"},
{"body": "I tried to add a ChangeListener to a TextField but I get an error. Here is the Example I triedAnd these are the packages I've importedBut I get the following error in eclipse:The reason might be the  and  but I'm not sure about this.In Java,  means  (if there are no imports saying otherwise). In Scala, it means . , being a JavaFX method, obviously can't know anything about ."},
{"body": "I have parquet file locally saved,\nIt is loaded...val catDF = sqlContext.read.parquet(\"data.lzo.parquet\") It recognize the schema but each query or actions return the same error...parquet.io.ParquetDecodingException: Can not read value at 0 in block -1Thanks in advanceLoading a parquet file as dataframe is as simple as you've statedYour above code should work. Please check whether the parquet location is correctLZO (parquet) compression issues, resolved adding depend. to sbt config:libraryDependencies ++= Seq(\n  \"org.anarres.lzo\"   %  \"lzo-hadoop\"              % \"1.0.0\"\n)Hope will be usefull for someone."},
{"body": "I am using node.js for writing restful service. I want to trigger clustering algorithm written in scala from this service.How can i call scala function from nodejs apiIt's a broad question to answer it. \nBut one way to achieve it is using a web service on top of your clustering algorithm. You can check  as a lightweigt webservice toolkit. It has embedded web server so you can start your application standalone. Implement a simple rest service and make a http request to it from your node.js application.   "},
{"body": "How do I detect if a method is called by unit test in ScalaTest?Edit: sorry, I was expressing wrongly the thing I wanted. I have a code block in a method which takes very long to finish (I cannot mock it) and it does not affect any logic. I want to skip that code block in the unit test. So I want to know whether it is called by unit test or normal running. If it is called by unit test, I skip it, otherwise, I let it runs normally.I have a simple workaround by adding a trait like this: use it in the places where I needs to check whether it is in debug mode:Use a code coverage library such as  for instance. It will generate reports that indicate you which parts of your code are used by unit tests."},
{"body": "I am using a fastutill ObjectArrayList[object] instead of ArrayBuffer in scala in my spark streamign applicatiuon (inside a mapwithstae function)I use the lates version..using below sbt However, it throws an exceptionOk..just added as jars parameter in spark submit and it resolved that.."},
{"body": "I have looked at a number of questions online, but they don't seem to do what I'm trying to achieve.I'm using Apache Spark 2.0.2 with Scala.I have a dataframe:which I want to transpose to I've tried using  but I couldn't get to the right answer. I ended up looping through my  columns, and pivoting each as per below, but this is proving to be very slow.Then using  on each iteration of  to my first dataframe.Is there a more efficient way of a transpose where I do not want to aggregate data?Thanks :)Unfortunately there is no case when:You have to remember that , as implemented in Spark, is a distributed collection of rows and each row is stored and processed on a single node.You could express transposition on a  as :but it is merely a toy code with no practical applications. In practice it is not better than collecting data:For  defined as:both would you give you the desired result:That being said if you need an efficient transpositions on distributed data structure you'll have to look somewhere else. There is a number of structures, including core  and , which can distribute data across both dimensions and can be transposed. "},
{"body": "For a while I was looking for a fast and simple solution for a microservice framework. I am quite new to all Lightbend products & scala but since its looks very interesting I decided to give it a try. 1)  I don\u2019t understand why there is a need for the new framework Lagom ? If play can already give me the same solution (to serve as microservice) then why there is a need for another framework ? 2)  With play I manage to create an \u201cHello World\u201d project very fast and also the deployment was very easy and straight forward(via dist). I like the fact that I can merge all in one ZIP and run it via script . In Lagom from what I understand I need to use ConductR. For my current needs its look like a big overhead. Is there a simple why to deploy it something like in play ?Thank you allLagom is built on top of Play. Whereas Play is intended to be a general-purpose (asynchronous) web framework, Lagom's more specific goal is to add some tools/opinions focussed on deploying your app as microservices.A couple of examples that Lagom provides that helps you realise a microservices style architecture (that Play does not):-One thing it adds, for example, is an API for -based persistence on top of the persistence support Play currently provides - this (if you do not know) is a pattern which helps you realize a microservices architecture by decoupling queries and commands.Let's say you have a Play application that has 25 different microservices - which is probably a conservative number for say even a relatively small corporate application - how do you manage the deployment/orchestration of all those JVM's? Well containers are all the rage. How do you manage all those containers? ConductR is a tool which takes some of the pain away from that task, and Lagom provides you integration tools for ConductR to make it easier for you to use it with your Lagom project - something you do not get with Play on it's own.Ok, there are loads of SBT modules that you could use in your Play project to help you realise the same thing but then you need to choose what tools you need, figure out which of the many modules available are right for your project, configure and wire them as necessary - this is one of Lagom's goals - to take these decisions and configuration tasks away from you so you can focus on writing your application logic.If my application was small, maybe just 5 services, then you could argue quite convincingly that you really don't need Lagom (or any other microservice framework for that matter). However, if your application is likely to grow, then Play on it's own will cost you more time in the long run.There are obviously many more considerations when designing microservices but you get the jist of Play vs Lagom."},
{"body": "The code below prints '1' and never prints '2', as a result the browser hangs when it requests the page served by the  method. The future is never invoked. If the  statement is replaced with  the code works correctly. What is the problem?First of all you shouldn't create db connection on each http request. \nSecond your finally clause executes probably before you future has chance to be executed and this may be the reason of your problem.Other than that, it's looking good. Try to initialize resource on application startup or via DI, close db on application stop and then see if problem still occurs."},
{"body": "my problem is creating a sqrt function for integer via Scalaf.e.: sqrt ( 26 ) = 6 , s\u00b2 >= n \nsqrt (0) = 0, sqrt (1) = 1, sqrt (2) = 2, sqrt (3) = 2, sqrt (4) = 2, sqrt (5) to sqrt (9) = 3 .... here is my code i came up with:however it doesn't work with all the numbers n >= 0. \nf.e.: for 6 its 2, how do i tweak my formula?Try change type to Double. Result should be obvious.here is an easy and nice solution. I have added minor improvements to Pavel's Code:  function does not really add up and it gives the wrong results. As square root values will mostly contain decimals, I have limited the decimal by 2 digits. "},
{"body": "I am writing a simple function to  compute the value of a bag of coins.  It consumes four numbers: the number of pennies, nickels, dimes, and quarters in the bag; it produces the amount of money in the bag.Here is the function:Why does the result look like this?I expect the result should be 0.41This is trivial Double \"problem\"For exact numbers, like money, use BigDecimal, as Mr D propose. Literals , , ,  are floating point  by default, you should use  instead. The following implementation uses instead."},
{"body": "Suppose I have two RDDs: rdd1=(Double,Int,String), rdd2=(Double,String) and a function:fun1 wrote by myself and it would take both rdd1 and rdd2 as its inputs. how could I get the result like rdd1.fun1( val1) or rdd1.fun1( rdd2)?For example,\nAnd fun1 would return a new rdd3 from rdd1 where each element in rdd2 replaced each corresponding parameter in rdd1,the expected output is as follows,Maybe one way to call fun1 is rdd2.fun1(rdd1) or some other calling methods.    I'v tried \"join\", but it doesn't work for my problem, because \"join\" only return those pairs with the same key. \nBut I don't know how to make fun1 work when rdd1 and rdd2 are the inputs. You can do this with implicit conversions or implicit classes.Here's an example with integers:"},
{"body": "How can I create Dataframe with all my json files, when after reading each file I need to add fileName as field in dataframe? it seems Variable in for loop is not recognized outside loop. How to overcome this issue?Thank in advance\nHossainI think you are new to programming itself.\nAnyways here you go.Basically you specify the type and initialise it before loop.Ok you are completely new to programming, even loops.Suppose fileArray is having values as [1.json, 2.json, 3.json, 4.json]So, the loop actually created 4 dataframe, by reading 4 json files.\nWhich one you want to register as temp table.If all of them,And reason for df being empty before this update is, your fileArray is empty or Spark failed to read that file. Print it and check.To query any of those registered LandingTable\nQuestion has changed to making a single dataFrame from all the json files.Insure, that fileArray is not empty and all json files in fileArray are having same schema."},
{"body": "I just started learning Spark and Scala. I wish to know how to use this conversion method from Unix timestamp to data in spark-shellIt only shows top 20 rows. Why does spark-shell only show 20 rows? Is it possible to remove this limitation?One more question, how can I save the result to my home directory and HDFS location? By default the  method is  , where the first argument is the number of rows to display and second argument truncates the output to 20 rows. So you can simply use  or  to display all of the rows.\nTo save the  you can use\n  or\nits not spark-shell limit you can call show with number of rows you want like belowand to save the result in you home directory you can simply convert dataframe into rdd and call saveAsTextFile action on it as belowyou can also save file in hdfs using saveAsTextFile actionYou can use This should work to convert Milliseconds to Date"},
{"body": "I'm attempting the Kaggle  using SparkML and Scala.  I'm attempting to load the first training file but I am running into a strange error:The file is a  so I'm not sure why its expecting a Parquet file.Here is my code:You're missing input format. Either:orIt is expecting a parquet file because that is what the .If you are using Spark < 2.0, you will need to use . Otherwise if you are using Spark 2.0+ you will be able to use the  by using  instead of .You have to add a dependency jar from databricks into your pom . Lower version spark doesn't provide api to read csv. Once you download you can write something like below.. Ref url: I seem to have had a conflict with Scala versions in my  NOT my original code.  My  had multiple Scala versions seemingly causing issues.  I updated all dependencies that used Scala to the same version using a dynamic property  and that fixed the problem."},
{"body": "In our application , most of our code is just apply  ,  and  operations on  and save the DF to Cassandra database.Like the below code, we have several methods which do the same kind of operations[] on different number of fields and returns an DF and that will be saved to Cassandra tables.Since i am calling the action by saving the DF to Cassandra, i hope i need to handle the exception only on that line as per  thread.If i get any exception, i can see the exception in the Spark detailed log by default.Do i have to really surround the filter, group by code with  or I don't see any example on Spark SQL DataFrame API examples with exception handling.How do i use the  on  method? it returns You don't really need to surround the ,  code with  or  , . Since, all of these operations are transformations, they don't get execute until an  is performed on them, like  in your case.However, if an error occurs while ,  or  the dataframe, the catch clause in  function will log it as action is being performed there."},
{"body": "I'm trying to select the last element from a map and compare it against the other last elements in the map to say if the selected element is higher or lower.So far I can get it to select the element from user input but cannot work out how to loop through the map and get it to compare against the other elements:Here is the map which the elements are coming from:for example I select H8 and its last element is 6I then want to compare it to all last elements and say if it is higher or lower than each of the last elements.Thanks in advanceI'm not sure if I understand you correctly. You can access last element of the list by calling last.Result:Is this what you need ?Note that I still learn Scala so there's probably better way of doing this.\nI also don't know if any list in mapdata can be empty so use Option if necessary.// EDIT\nIf you want the letters you may try sth like this:You just need to remember that when you iterate through mapdata you get entry. entry._1 is the key (your string), entry._2 is the list."},
{"body": "I'm using the Scala language.\nLibraries used are: Akka, LWJGL (includes GLFW)When testing at high fps, like 4000 for example, everything works as expected. But as I lower the fps to 30 by adding 100k cubes, the key polling of glfw seems to get issues.When I release a key, the callback gets called instantly with a new key_pressed event, and then a new keyrelease a few seconds later. Please see this video here for a detailled example:I made sure that there is only 1 callback active, and there is only 1 glPollEvents() call.\nIf that is not weird enough, calling glPollEvents() 20 times per update loops seems to reduce the time between the key release and the second key_released event...Thanks!I didn't exactly found what was causing it, but the issue is gone. From what I understood, running glfw and opengl in the same thread on a multithreaded game running at lower than 60 fps causes weird behaviour in the glfw polling internal threading. To fix it, I created a subthread under the main window thread and put a fast ticking loop that calls glfwPollKeys at a faster rate than the window refresh rate. The problem is now gone, at the price of a loop and a thread."},
{"body": "Help ,I have two RDDs, i want to merge to one RDD.This is my code.Just use union:Documentation is Shotcut in Scala is:You need the These don't join on a key. Union doesn't really do anything itself, so it is low overhead. Note that the combined RDD will have all the partitions of the original RDDs, so you may want to coalesce after the union.APIReturn the union of this RDD and another one.Return the union of this RDD and another one. Any identical elements will appear multiple times (use .distinct() to eliminate them)."},
{"body": "I'm trying to save an rdd by as below,\ndata.coalesce(1).saveAsTextFile(outputPath)but i'm getting java.lang.OutOfMemoryError: Unable to acquire 76 bytes of memory, got 0Has anyone faced the similar issue, if so, i would like to learn how did you fix itcan you please provide of more details on you are getting OOM on driver of executor ?By the code you posted coalesce(1) will force all executor to send data to a single executor and  if you are data size is huge , you will start seeing failures . coalesce is resulting in shuffle.  (All mapperTask sending data to a single Task).Follow  for getting a in-depth understanding "},
{"body": "I've got a collection of projects that all have a large third party dependency in common, it seems like a waste of space to copy this jar to all the projects during the build, is it possible to have maven just create a hard or soft link to a single cached copy?This is not a duplicate of  which relates to how to manage common dependencies from a pom perspective. This is about how to avoid copying the same dependent files to the target of multiple projects and just creating links to a single instance to save space.Simpler version of the question is: is there an equivalent to the maven-dependency-plugin's copy-dependencies goal that creates links vs. copying the files. Yes, Maven already does. It puts all your dependencies in your .m2/repository/... folder. In Windows it's c: \\ Users \\ [your_name] \\ .m2 \\ repository. So there is no need for linking it with simlinks or st like that. Just have your maven project search for the dependencies in your maven repository and your dependencies will be reused by all projects which need these dependencies. If you use the same maven all the time, the Maven settings are the same and for every project your repository folder is the same, so you're done already.Al different jars are saved here, so every single version you depend on. If more pojects depend on the same dependency, with the same version, it's that one jar being used for all your projects.You can remove all dependencies and re\u00efmport your dependencies, if you used an old version before, and you use a newer version now, you just deleted the old version, and the new one is back where it's needed."},
{"body": "I just started learning scala, I installed it through \"brew install scala\" and when I type scala MYSCRIPT.scala or scalac MYSCRIPT.scala I get this error:I've tried looking for a solution to this but found nothing,Please help.OK I got it I have to write \"scala\" keyword outside of the scala terminal while trying to compile.So instead of what I was doing which was typing \"scala\" to bring up the scala interpreter and then again writing scala MYSCRIPT.scala I have to skip the first step which was opening the scala interpreter.Post can be removed."},
{"body": "Simple problem:\nFor the following RDD I want to print out an output text file with the following format and header (UserID,MovieID,Pred_rating )Simple enough. right? so I am using this function:The above function is not working with the following errorWith your code the FileWriter object would be send to all nodes and executed in parallel, which does not work with the reference to the local file. Therefore you get the NotSerializableException.You would typically save the RDD to a file by saveAsTextFile:This writes out the file in multiple parts. You could add the header and combine the parts manually later.This worked like sweet lords will:"},
{"body": "Hey I'm new to the language and trying to make a map that contains values used alot within the code, but currently having trouble calling the map into a parameter. heres and example of what it looks likethat part is fine but when i want to add them all to the parameter it says there is too many arguments?had to addto the end of the strings and ints, for example"},
{"body": "I am writing parsing a JSON Strings and splitting them with a comma, but the Free also contains commas.I am unable to get my Pattern working for this. Tried various combinations but could not succeed.\n        I am using this as part of the Spark streaming, I could not use the JSON parsers for the reason these data come randomly from 750+ random sources, so splitting and getting the text as key-value pairs was a reason to use splitting.here is my String(Shortened for Stackoverflow use and readability).Can someone throw some help on this, please!This answer appears to have a valid regex for parsing JSON:"},
{"body": "After I change the function name from something else to addOptitrans and recompile, I get the following error:I have confirmed that the updated class is produced in the expected place in the classpath. What could be going wrong?Delete temporary files with . Then rerun the command."},
{"body": "I want to convert a rdd to DataFrame, it gave me type mismatch error, but actually the type is defined in the case class. how to fix it ?Error: is of type  so the map operation fails to convert a  to a  or a .Why not make  an  already, in order to have a strongly typed object?The problem is because your array has many different datatypes in it. The first value is a string, then two numbers and another string. So when it cant match it with one data type, by default it takes it as \"Any\".This can typically be forced to take up a specific datatype by saying something like,var data: Array[String] = new Array(\"abc\",\"123\")But this wouldnt work for you since you have values of different datatype in your array. Either pass them all as strings and then typecase them inside the class, or use something more sophisticated than an array which can only hold one datatype."},
{"body": "I have a rdd:what I need is: now I'm trying ???is something I have difficulty with, I don't know how can I get access to the tuple inside iterable and only take each of them to a array. Or did I do something wrong from choosing mapValue method?If you don't need an Array as a result, and it is ok with any type of Seq, maybe it is ok with something like this:Given this example of data set:You can use a mapValue like this:That it is equivalent to: is the iterable object and It's in the  where you take the second part of the tuple.In this point, you now how to access and manipulate the iterable. If you need, force an Array, you can do it:"},
{"body": "I would like to generate unique date range between current date to say like 2050.Not sure how can i create a function for it. Any inputs here please. The difference between the start and end dates can be anything.Unique date range means the function would never return me a date range which it has already returned.I have this solution in mind:We will use the  and  imports to achieve this:"},
{"body": "Listing below is my code Here the plain text(\"E:\\plaintext.txt\") I'd like to encrypt is about 1 Mb.  The max length of RSA keys is 4096 bits and the max size it can encrypt is 501 bytes. I decided to divide my text into 2093 arrays(1024*1024/501=2092.1)I changed the plain text(E:\\plaintext.txt) and the amount of the plain text changed as well. But I got the same amount of cipher with two different amounts of plain text.Anyone who can help me? Use symmetric encryption such as AES to encrypt data.If you need a public/private key pair encrypt the symmetric (AES) key with asymmetric encryption (RSA). Then package the AES encrypted data and RSA encrypted key as a unit.  This is commonly known as hybrid encryption and is in essence how HTTPs works."},
{"body": "I need to build Spark DataTypes manually from string depends on DataType of the column.\nI've tried it in different ways like:But can't find the way to do it correctly. Is this possible? \nI need to compare value given me as string with the value in the column.As I understand, you already have a data structure but want a conversion of the data types according to Spark's . I assume, there are no nested sequences or arrays."},
{"body": "I have a dataframe with N lines in it and it has only one column. I want to copy it N times and concatenate. So at the end I want to create another dataframe with N^2 lines.How can I achieve that simply using Scala? I do not want to use for loops etc. because N is large.You can do this using a fold function:The only constraint is that N must be less than Int.Max_Value. Hope this helps :)You can test with a small df:"},
{"body": "I am reading from a CSV file and I want only certain fields from the entire CSV my commands are as follows:Now I want to change the type of column 15 from String to Double. So i did this,Now i need to take only top 10 values from column 15, so i have used takeOrdered funtionBut When I run the last command i get ArrayIndexOutOfBound Exception 17.Please help me out.\nThanks.The second line is equivalent to That says, your Tuple has 7 fields.Your problem is here.you are expecting tuple fields  which does not exist  on   that means it got just 7 fields but you are using upto .I'm wondering, how you did not get any compile time error for   "},
{"body": "I want to be able to easily replace one Spark Estimator with another.\nI've created method that uses generic estimator:Here's one of the classes that returns generic estimator that should be passed into above-mentioned method:The last line gives this error:How that can be fixed?The problem appears to have nothing to do with your higher-kinded type.  Your problem is that  declares that it will return an , but you are returning a  instead. You need to return an instance of  to match the declaration. This is a Scala issue--not Spark--and it's independent of the  function you want to use later."},
{"body": "I am new to Scala and this is the first time I'm using it. I want to read in a textfile with with two columns of numbers and store each column items in a separate list or array that will have to be cast as integer. For example the textfile looks like this:I want to separate the two columns so that each column is stored in its on list or array.Lets say you named the file as \"columns\", this would be a solution:Here is one possible way of doing this:"},
{"body": "I'm using Spark and Graphx on IntelliJ IDEA for the first time. I'm trying to create a graph and running queries on it but I'm getting the following error: java.lang.IllegalAccessError: tried to access class org.apache.spark.util.collection.Sorter from class org.apache.spark.graphx.impl.EdgePartitionBuilderHere's my code:The problem was with the scala version I was using, 2.10.6, because I had read from an installation guide that spark worked well only with 2.10.x versions. But after installing the latest version, 2.11.x, everything worked fine"},
{"body": "I am new to Scala (Akka Actors. I am aware there are benefits of avoiding mutable state in actors, but a found a solution to incrementing a var total\nsimilar to that given by the SO question:    My code is below.The solution would appear simple: my Calculator actor maintains a local var total, and the appropriate match on a case class in receive()  => increments this total with an amount extracted from the matching case class.My case statements are working. A println shows that each time my input matches the case statement, the amount to be added to the total (amt) is yielded by the match. I.e. this works as expected:case MyCaseClass(_, amt) => println(amt)But this line:  case MyCaseClass(_, amt) => total += amtfails. The local var total is never incremented. I tried using a list, and adding the new amt as a new member to a var List, but this fails also. In each case increment of the local var, be it type Double or List[Double] fails. Why is this? and what can I do to get the increment to work inside receive()? Code:The  from your  is a different actor than the  from the . Each  creates a new instance of this actor, each with its own state. You send the  messages to one actor instance and ask a completely different instance for the state, which hasn't seen any  messages at all.You can modify the RoomActor to take an actorRef as a parameter and pass the  to the Props, this way you're using the same actor instance for the state.Also, since everything is asynchronous, the 'total' message might be lost if you shut down the system immediately after sending the message."},
{"body": "This one is not duplicate I have new question. I tried to write thisCan I use na or IsNull in  to check whether or not that  is null?Code resultSomething like that:"},
{"body": "I'm working with  and when i compile my proyect, show me the next errors:The lines 90 and 120 are are in bold:How can i fix this?Just add parentheses to  method. This will be work. "},
{"body": "I'm reading a book \"Scala in action\" to learn scala. On page 59, the author provide following code. But when I run it, it doesn't compile.It reports:This book use scala 2.10. I'm using scala 2.11.Does scala change the usage of setter method in 2.11?I'm not sure what the context of  or what the author is trying to demonstrate with that line. Parameters of the constructor are public by default so removing  allows you to use the setter for free."},
{"body": "I have the following RDDAn example would be:And I want to insert into my Cassandra table with the following schemaIn the end, for the given example, I will have the following in my DBWhat is the Scala code which does this? I tried to use , but my input isn't in the form of . Should I first flatten it?All you need here is a :"},
{"body": "This was query written in slick using a for loop. Here  is table name and I am trying to update firstName, lastName of a user . When update completes successfully I am trying to return userId, firstName, lastName, false to user.I am trying to get equivalent query without using a for comprehension. Can anyone help me through this?I found equivalent query which works same as previous query without using for comprehension."},
{"body": "This question is there on stackoverflow, the solution is in Python. Please help me with a solution in SCALA.Quoting the query below:So starting with a list of strings, as belowI want to remove any element from the list that is a substring of another element, giving the result for instance...How to achieve this in SCALA?You can do it the following way:"},
{"body": "I am using Scala on Spark and need some help splitting a sequence of sets based on specific values within the sets.Here is an example:I am trying to split each sequence within the array based off the criteria that the first integer within each set is between the two integers within the other sets in the same sequence and return the character value of the sets grouped together. So for the first sequence you can see that we have the integers 15 and 20 in the first set and in the second set 17 and 21. So those sets would be grouped together because 17 is between 15 and 20 and the third set would not be left alone.In the second sequence I have 15 and 20 overlaps with 17 and 21. Also 17 and 21 will overlap with 21 and 23 and then the last set would be left alone.Essentially I would like to have it return , , , ,  I realize this is not great phrasing but if someone could give me a hand that would be very appreciated.As noted by the zero323,  should probably not be a set. I suggest converting it to a case class:With this class, if you described your problem correctly it could be solved like this:"},
{"body": "I have this code :The return value of this function is (Long, String, String, String), I am trying to save the result of this function to a text file using saveAsTextFile and I cant since the result is not an RDD,any idea how to implement this ?You can convert the return value to an  and then call  on it."},
{"body": "Find below the imperative style code:     Here I am interested in knowing how to convert changes made to offset within the loop in a functional way, how to do early break from the loop etc.This is a nice demonstration of one of Scala's great strengths - gradual, easy migration from imperative Java code to functional code.To convert to Scala, you don't need to go 100% functional straight away.  You can easily convert what you have to Scala as follows:Note we already have some small improvements in verbosity: no more s, no redundant type signatures, mutable variables converted to constants where possible, eliminated return statements and function brackets.  Note I changed the argument type of  to a scala list, you'll need to use the  at the call site to make this compile.Next, we can start making this more functional... however, this function probably isn't the place you should start, as you are calling state-mutating functions in several places.  For example, you could start with ChessLayout:These methods mutate ChessLayout, which is not functional style.   You could modify  to return a new  with the piece removed, and  could return a tuple of .  Once you have stopped any functions called from  from mutating state, you can then convert it to functional style."},
{"body": "How can I have my custom exceptions include a text message when thrown?\nThis code snippet demonstrates a wishful intent:But running in sbt, the custom text is not shown.\nWhat might be the idiomatic way to accomplish that? Of course I could generically catch any exception and print its details from that catch code, but that would be a little tedious.Technically speaking this is just , the superclass being java's  in this case. "},
{"body": "I am trying to create my first project using Play2 and Scala.Unfortunately, my controller does not see my model and my view.Controller:Model:View:I am getting the following errors in my Controller.\nWhere I access the model:Where I access the view:\nI changed the signature of wikiArticle.queryApi()You call without parameters:while you have defined queryApi as function with parameter:Also, note upper/lower case"},
{"body": "I just started playing with scala and i cross the following issue. I want to simply return a Map with Int as key and List of Tuples for values. That is my method:However my IDE shows error as:Does it mean that  creates  instead of Map? If so, how can i make it return Map?// editI'm trying to write a bot to javascript game. I'm mapping a board of tiles. In the mentioned method I am trying to find all \"open tiles\" (tiles which are not fully surounded by other tiles, thus can be moved) and in the return i would like to have a Map where key is a tile number with coordinates as values. In next step i want to find if it is possible to find path between \"open\" tiles with the same number. I think the problem is the lineYou should try this:Your version assigns the type  to the value ."},
{"body": "I have an RDD as a tuple where its key as an integer and value as List of integers:Data contains userId, the list of items the user have bought.I want to convert this to string such that:I did the following: But when I look at the part files the structure was completely different, which was: Try this:"},
{"body": "Here is a code structure:Now, a method in another class needs to refer to the method makeACopy.I get a complying error for invoking the makeACopy method although I already specify the parameter as a sub-type of Foo and Intellij IDEA doesn't complain the line:Why? And how to solve it?Also, I am not sure why I have to have a full path for returned data type of makeACopy in Bar. Otherwise, the IDE will complain returned type mismatch.You're defining  separate type parameters named , and only one of them as a subclass of Foo:A method defined within a trait (or class) with a type parameter  shouldn't declare  again, so you probably want to change this to: Then, wherever you call / override , you don't need to specify the type, you already did when you created / extended , e.g.: "},
{"body": "It's possible that this question has been answered before, but I have not found it via search engine or this site's search.I'm trying to implement soft deletes in my DAO object. My table has a column, , which is either null or a timestamp. If it's a timestamp it has been soft deleted.I'm trying to write the query that will perform the soft delete, and this is what I came up with (note that it doesn't work):The error I get is , which I suspect is for the update, why isn't this valid, and what's the correct version of what I'm trying to do here?You do not post the table definition but I assume that is Option[Timestamp] if is the case you need to do: Some(new Timestamp(System.currentTimeMillis))"},
{"body": "I have simple Scala Play Framework and Angular application. I tried to render JSON data on play's \"xxx.scala.html\" template but don't know what is the problem it is not rendering as expeted.My route entryScala Controller:When I am calling my page using urlI was getting response on page as below,Please can you explain what am I doing wrong here?Thanks !!There are some problems in the code. The  one is this:What has changed: is what you would expect:I have corrected code as you mentioned above but still I can't see response on my UI as below,Still don't know what am I doing wrong here."},
{"body": "In my current environment,  doesn't work. I would like to make sbt print \"This doesn't work, do the following instead: [...]\" until we get  sorted out. How do I do this?You really should have read the sbt docs for this. "},
{"body": "I have 2 rddCompare the two rdd and once which match the with the id will be updated with the value from the rdd2 to the rdd1.\nI understand that rdd are immutable so I consider that the new rdd will be made.\nThe output rdd will look something like thisIts a basic thing but, I am new to spark and scala and trying things.Use  to match two RDDs by key, then use  to choose the \"new value\" (from ) if it exists, or keep the \"old\" one otherwise:"},
{"body": "How to solve the error  for the following case. is in the format , where the first  is actually .I want to save  into the text file, which works fine for non-vector parameters. However, when I add , it stops working.UPDATE:The command  gives the following output:Ok after a small discussion with you in chat, the issue lies in the way you are actually working with your parameters grid.So, first, I'll re-defined my gridParams as an object and not a MapThen I can compute the evaluations with the parameters : If you want to convert the list to an RDD, go ahead, it seems rather small, so you can actually use plain Scala, as for the format now that we have error with, you can save them doing the following :"},
{"body": "I'm trying to concat all distinct values of a Spark RDD, separating them with comma. This is my code:But it returns to me just , why? Which is my mistake?It works if I write  and iterate on that. Why?Running a similar example on PySpark, I get \"lambda cannot contain assignment\", so I assume Scala would work the same. You should be able to  the RDD, and then do the comma-join. That is essentially what you are doing anyway. The reason you probably can't do assignment is because Spark can't pass your string to all the worker nodes and concatenate it with the diffrent partitions of data, then accumulate the result to pass it back to the running code.  "},
{"body": "I have the following arrays:I want to remove Arrays having different Strings. E.g:  needs to be removed.  How can I do it?  You could simply use:Instead of distinct, you can use simple custom function, that will be little bit more efficient, since it should not traverse whole array, if duplicate is in the beginning of an array:"},
{"body": "To be more specific, how can i convert a  to a  ?I have an RDD of \nand i want this to be converted into an  of , so that i can apply a reduceByKey function to the internal .e.g\ni have an RDD where key is 2-lettered prefix of a person's name and the value is List of pairs of Person name and hours that they spent in an eventmy RDD is :i need the List to be converted to RDD so that i can use accumulate each person's total hours spent. Applying reduceByKey and make the result as \nBut i counldn't find any such transformation function. How can i do this ? Thanks in advance.You can achieve this by using a  and . Something like this:"},
{"body": "I am trying to define a function in scala to iterate on it with Spark.\nHere is my code :With this code I get this error : I don't know what means this error...You are not showing your imports, but you are probably importing Scala standard collections' (this one takes a type parameter, e.g. ) instead of the SparkML , which is a different type and you should import like this:"},
{"body": "I want to call value of '' variable. How do I do that? \nIf i pass static value like \"actionId\":1368201 to myActionID then it works, but If I use \"actionId\" : ${actionIdd} it gives error.Here's the relevant code:Everything works fine If I put value 13682002351 instead of myActionID. Thanks in Advanced. While executing this script in Gatling I am Getting this errorScala has various mechanisms for String Interpolation (see ), which can be used to embed variables in strings. All of them can be used in conjunction with the triple quotes  used to create multi-line strings.In this case, you can use:Notice the  prepended to the string literal, and the dollar sign prepended to the variable names. "},
{"body": "I got this far:but get  as a result (with no error). The file is  How do I go on from here?After consulting  I triedand gotI see that you tried the Learning Spark examples.\nHere the reference to the complete code \n \nE.Instead of you can try this(i write it in java cuz i don't know scala but the API is the same):spark will read your json file and convert it as a dataframe. If your json file is nested, you could use for example, see my answer Hope this help you."},
{"body": "How to navigate in a Map[String, Any] in Scala ?e.g.: Which will be equivalent of .But what i want to do is  and navigate in the Map.Is this possible?P.S.: I have this made in Java, but it's like  cast to Map, get the new value and recursive call the  with remaining tokens.For the record, I think, this is a very wrong thing to do, but, if you insist, here you go:"},
{"body": "Two sets, s1 and s2, of Foo objects, whereThe equals and hashCode methods are override for a data comparison.Now, I need to find out all objects in the s1, but not in the s2. I can't usedue to the equals/hashCode method override. It likely shall use the filteror creating a map with the ID field as its key and its object as the value for the first set and remove any entries in the second set.What is a good approach to solve this problem in the Scala way(s)?Update:I was thinking of something like the followings:The second line of code doesn't work, however."},
{"body": "I have installed the spark in cluster mode. 1 master and 2 workers.And When I start spark shell in master node it is countinously running without getting the scala shell.\nBut when I run spark-shell on a worker node I am getting scala shell.And I am able to do the jobs.And for this I got the output.So My doubt is actually where to run the spark jobs.\nIs it in worker nodes?Based on the , you need to connect your  to the master node with the following command : . This url can be retrieved from the master's UI or log file. You should be able to launch the spark-shell on the master node (machine), make sure to check out the UI to see if the  is effectively running and that the prompt is shown (you might need to press enter on your keyboard after issuing spark-shell).Please note that when you are using  in  mode, the driver will be submitted directly from one of the worker nodes, contrary to  mode where it will run as a client process. Refer to the documentation for more details. "},
{"body": "I have an array, for example:and I want to create a new array containing just the integers called .I tried something like:I hope the idea is clear, but of course that didn't work.You are looking for A good resource for answering questions like this in the future: "},
{"body": "I have a Scala/Spark project (built with Maven) in which I want to do the following.I would like to modify , so I've copied the source code of that file into my own package: . However, it relies on some classes like , which are declared as .I had hoped that by putting my code inside of , it would be able to access the private members. But it is complaining, perhaps because Spark is imported via a Maven module? How can I work around this?Here is how the pom.xml imports spark-mllib:Here is an example of an error message I run into:Looking in the source of org.apache.spark.SparkContext, I see that checkpointDir is declared as private to org.apache.spark:Why is it that I can't access this member even though my code is in org.apache.spark.mllib.myrecommendation.ALS, a child of org.apache.spark?It works for me.Since packages are open, adding to them in the way you describe is perfectly fine (modulo loading from sealed jar files).A trivial sbt project with ALS.scala modified to reside in package  (and fixing a couple of ) and importing .build.sbt:ThenPlease do not try to violate visibility and accessibility rules, in any programming language. If you really need to change something in Spark source code where it was not designed for, the safe approach is to :Spark tests are executed as a part of the build and hopefully if your modifications introduce bugs, you will discover it because a test fail.  "},
{"body": "I am getting a compile error at \n.\nIt says . Any idea about this issue?\nAlso is this the correct way to use Scala generics? is a map whose values are of type \"some unknown subtype of \". So you can't put  into such a map, because  might be a different subtype of  from the particular subtype that that map uses.You probably want a , though to be honest most of your generics look unnecessary - if you just want to store and retrieve s, why not just keep them as that? If you want a cache of some specific subtype of  then  should be a type parameter on the class, not on the individual methods. What is it you're trying to achieve?"},
{"body": "I'm getting this error:I don't get error when I do  instead of just  , but in this case, it is shadowing the variable. Please help, how to resolve this.I get error When you say , you're saying that the type parameter  of  must have a lower type bound of . That means that  can be  or any super-type of it, which would break the type bounds that .For example , so  in this hypothetical situation could be , but  is not also not true, so the type bounds break.When you say , then  has a  and an . The upper bound of  for  is important, because the contained type of  also has an upper bound of . Without that, it would try to allow  (or some other super-type that isn't ), which of course it can't.How can you solve this? You have you pick one way to do it. You can't have  and  at the same time. It just won't work, because the type bounds conflict."},
{"body": "What does :: mean here ?Specially I don't understand the \"\" operator.Its the list  operator. It creates a new list whose head is first argument and whose tail is contents of the second argument."},
{"body": "I am trying to sort an Array of Strings in alphabetical order using scala. The Strings are passed in an array when the method is called. The method must also return an array with the sorted Strings. How would I do this? ThanksMikeYou just need to call ."},
{"body": "I am trying to install SBT for scala developing. I follow the guid in The errors show as followsThe link \"\" cannot be accessed even in the browser. Are there any solutions?As the commenter said, it looks like you are just having a network connection problem.However, if you are not using a package manager, I recommend to use the \"sbt-extras\" script from . I.e."},
{"body": "How can I make the following function tail recursive and fully streaming?The goal process the stream with some lookahead. As an example let's say we have a stream of sequential numbers: . Now I'd like to calculate the sum of 5 sequential elements of this stream beginning at every element. With the function above it would look like this: , producing the stream of . But this will break the stack on large input. How can I optimize this function?Why do you say this will break the stack on larger input? Have you tried this? Please refer to , where Ken Bloom points out that streams don't need tail recursion. In fact, I ran this on a take 1,000,000 and it returned in < 1 second."},
{"body": "Am trying to send the table query object from local actor to remote actor,\nbut akka gives serialization exceptionRemote Actor looks like this and am using active slick pattern hereLocal Actor looks like this. Local Actor builds TableWithIdQuery and passes it to the Remote Actor mentioned above.It's not safe to assume that any thing like a Query is serializable.  You really ought to run the query in a local context, map the results to a set of serializable types (e.g. immutable collections, case classes), and send that."},
{"body": "I know there is two ways to run a scala code in Apache-Spark:Is there any other way to run a scala code in Apache-Spark? for example, can I run a scala object (ex: object.scala) in Apache-Spark directly?ThanksScala version:for more detail refer to ."},
{"body": "Everytime I change code in the project I would like to run two separate sbt actions in this order:I thought it is possible with activator/sbt commands like , but it sticks to \"run\" and don't do any testing until I break sequence with Ctrl-D.Is this possible to run \"test\" action before \"run\" in triggered execution mode?Because of the nature of  it won't re-run  on a code change. What perhaps might work for you is to have 2 sbt shells open, one with  and one with .A note of warning though, multiple sbt shells, especially in trigger execution,  trip over itself because there's limited locking on the relevant files - which is one of the things sbt server hopes to solve."},
{"body": "Here are the codes (I'm using Slick 2.1):I tried to create two mappings  and  in the Table class, however, Slick complains about this:I saw a  about this, but it looks quite complex and has no explanation..Does anyone have ideas about this? Thanks!Slick expects Username to be Option[String] in the 'profile' projectionClearly Slick here doesn't know how to map the columns with the case class ."},
{"body": "In scala, if we have a class say,would give an error saying, when compile with  Everything is fine till here.However, if I have a java class :first compile it with ,\nthen use it in scala with :compile  with  and run with . No error occurs.Does it break intention of  in scala?If I want to, when using java class in scala, maintain the fixed value of instance , what can I do?Or there actually is some documentation mentioning this behavior? Thank you everyone for your reply.While I am well aware that by making the field final:\n, immutability can be achieved.I'm not hoping to solve this problem by modifying java code.\nImagine that I'm using third party java libraries that I can no way modify their code, yet I want making sure my code won't accidentally modify third-party's internal state, hence come the .In addition, by disassembling the  file using , it can be seen clearly that scala is not making  immutable by inserting  to class . Or maybe someone explaining the \"magic\" behind  would be helpfulClearly in your Java class , the member  is both public and mutable. To treat a Java variable as Scala , you can make it  in your Java class.Also remember that Scala too has  which has essentially same behaviour as Java variables ( except for  ones ).Define  like so:And then check the compiled class definitionThing to note here is that for member 'c' you will have  but for member  there is no such method. In fact in Scala, the assignment turns out to be a method call.I hope that makes it clearer.Your confusion comes from line  in . It means that you cannot reassign value to your variable  but it's completely ok to reassign any field of . This can be limited by marking a filed be finalTry making   in your java class as following It will enforce that field  of class  cannot be reassigned. "},
{"body": "I have the following scala script:When I am trying to compile it, I am getting the following errors:I have a file simple.sbt, which lists the dependencies:Does anyone know how I can fix this? Thanks in advance!After fixing the quote problem, as correctly pointed out, I am getting this error:TryIt looks like you have the wrong kind of quotation mark in the offending line ( instead of )."},
{"body": "I have to make a connect-four game using scala. I have attached the code but everytime the game runs and gets to row 3 it just continues to change the second rows entry instead of going to the next row. Any help would be appreciated. I found this code on  on here and couldn't figure out how to get it to work:I've tried to make your code readable and compiling and also tried to fix some logic.However, I've never worked with Scala so this is just a first sketch where you might want to continue ...Some functions can be merged and the currentRow needed a fix. See here:"},
{"body": "So basically I'm starting scala and I'm trying to build a simple RPG script to get accustomed to the language. The first problem I encountered is how do I declare a class Character and a class Ennemy, knowing each have methods using instances of the other class as arguments. I haven't encountered this problem in other languages (Or they have a way of saying: Hey I'm using this other class but don't instantiate anything unless you're called), here in Scala I have the following error: not found: type Ennemy. I guess there is a key word to use somewhere but I just can't find it.ThanksEdit: Sorry I didn't incorporate the code, BUt basically I stripped it down to this:And the code still doesn't compile and return the mentioned error.If you're in the scala repl, you need to define them at the same time to avoid this. This is because each expression is run one at a time, and if you can't reference something that isn't defined yet.Note that this is not a problem for classes defined in .scala source files.You can use  to define multiple things at once.Other notes:This does mean that characters and enemies can kill their own kind. There's ways you can define it differently to avoid this, but I'll omit them since I think it's more complicated than you need right now."},
{"body": "I am converting my Scala code to pyspark like below, but got different counts for the final RDD.My Scala code:where I then tried to convert them to pyspark code as below:where I have:and and BTW,  is converted from a data frame. i.e. However, I got different counts for  and . I checked the codes many times but couldn't figure out what I missed. Does anyone have any ideas? Thanks!Consider this:and this:"},
{"body": "How can I define this function recursively?Somehow k has to stay the same, but as we can't use a j, we have to change k for the next recursion stepAs usual for recursive functions you have to pass ,  and  as well with every call. This is very logical especially if you think about one particular function execution. It only calculates one step of the incremental product. As a consequence the function signature looks as follows:You have to increment the j after every particular calculation to pass the next  to the following execution. Furthermore you have to add a termination condition which looks like the following:If you have to do this as a recursive function, try something like:It is worth checking searching for foldLeft and foldRight and reading about the differences."},
{"body": "I have a dataframe and I'd like to add an extra column to it based on a simple condition which basically says whether the value sof another column is equal to a given string or not. I know I can create an UDF and register it and use it then, however I think there must be an easier way of doing it. This is the psuedocode of what I'm about to doYou are pretty much there:Note that you will need  and  imported for the above to work."},
{"body": "Just learning Scala.Tried this:def ! : Int => Boolean = (p : Int => Boolean) => !pGetting a compiler error:error is highlighted for \"!p\"Shouldn't the compiler automatically figure out that the result of p is a Boolean ?Thanks in advanceEDIT: Based on comments, tried the following also. Have accomplished my task using other methods, nonetheless, am trying to learn how to define an unary operatordef unary_! : Int => Boolean = (p : Int => Boolean) => !(p(_))still getting a compiler error at \"!(p(_))\"Maybe you intend something like:"},
{"body": "I am trying to get avg of ratings of all json objects in a file. I loaded the file and converted to data frame but getting error while parsing for avg. \nSample Request :so for this json, US avg rating will be  (2.3 + 3.3)/2  = 2.8for this avg for us= (3.3 +1.3)/2 = 2.3so over all, the average rating will be :  (2.8 + 2.3)/2 = 2.55    (only two requests have 'US' in their visited list)My schema :When doing : Can someone tell me what I am doing wrong here ?Assuming  is a  (your code example isn't comprehensible... you create a  variable and query an  variable), you shouldn't call  in order to select from it: would call a function on each record (of type ). That is useful mostly for invoking some side-effect (e.g. print to console / send to external service etc.). Mostly, you wouldn't use it for selecting / transforming Dataframes.:As for the original question of how to calculate the average of US-only visits:"},
{"body": "I am creating a web application which will accept some inputs from user (like name, age, address etc) and generate some predefined forms with filled information for user to download and print.For example, an Application Form for driving license or something along those lines. The backend will have the format information about the document to be generated and other information will be gathered from user from front-end.I am going to use  for this and  as programming language. But right now I am not aware if there are any free libraries/APIs that I can use to achieve this document generation.I should be able to manipulate the  some other basic stuff. And I need documents in format that are widely supported for editing and printing purposes. Like  for example. DOCX is preferred so user can edit something after downloading the document before taking a print out.I have used the apache POI library  to parse and create ms word documents (including docx) files:It's not amazing but it's the best I've found :)I have used  which simply converts xhtml to docx.What you can do for your requirement is save your format information as xhtml template and place input from form (like name,age,address etc) into the template at runtime.This is a sample code to refer from this "},
{"body": "I have the following data in a CSV file(actually, my real data is larger but this is a good simplification):I am reading it the following way, where  is the location of the file:For reading, I am using Then, I am applying a map on it:However, when I am referencing the column like this:  I am getting a type mismatch:Why is this occurring? Shouldn't every row of  be a Map?when you reference a perticular column in datafram's row, you have several methods to do this. \nif you are using apply method then you need to pass the index of column.\nor if you want to get a column by name you need to use getAs[T] function of Row.so you can use :or hope it will help you."},
{"body": "I've got and I want to get it zipped. Is there any succinct way to do it instead of accessing each element of main list by index and zip it together with  command?This works, under the assumption that the outer list has exacly two inner lists.In order to take into account the possibility of failure, you can wrap the result into an ."},
{"body": "Hi i am new to apache spark and scala. I am trying to convert \n to Could you please help me in conversion.In the absence of concrete types in the question I have made an assumption that the values in your array are Char and Int tuples respectively. Here is how we can transform to the desired output:Basically we can traverse the elements in a Tuple using a productIterator. We also need to map each value to an Integer so so to compute the sum.Also have a look at this question: "},
{"body": "i am new to spark and to its relevant concepts, so please be kind with me and help me to clear up my doubts, i'll give you an example to help you to understand my question.i have one javaPairRDD \"rdd\" which contains tuples likelets assume that String[].length =3, means it contains 3 elements besides the key,what i want to do is to update each element of the vector using 3 RDDs and 3 operations,\"R1\" and \"operation1\" is used to modify the first element,\"R2\" and \"operation2\" is used to modify the second element and \"R3\" and \"operation3\" is used to modify the third element,i know that spark devides the data (in this example is \"rdd\") into many partitions, but what i am asking about : is it possible to do different operations in the same partition and at the same time?according to my example,and because i have 3 operations, it means that i can take 3 tuples at the same time instead of taking only one to operate it:the treatment that i want it is :(t refers the time)at t=1:at t=2:After finish updating the 3 first tuples, i take others (3 tuples) from the same partion to treat them, and so on..Spark doesn't guarantee the order of execution.You decide how individual elements of RDD should be transformed and Spark is responsible for applying the transformation to all elements in a way that it decides is the most efficient.Depending on how many executors (i.e. thread or servers or both) are available in your environment Spark will actually process as many tuples as possible at the same time.First of all, welcome to the Spark community.To add to @Tomasz B\u0142achut answer, Spark's execution context does not identify nodes (e.g. one computing PC) as individual processing units but instead their cores. Therefore, one job may be assigned to two cores on a 22-core Xeon instead of the whole node.Spark EC does consider nodes as computing units when it comes to their efficiency and performance, though; as this is relevant for dividing bigger jobs among nodes of varying performance or blacklisting them if they are slow or fail often."},
{"body": "I want to build a recommendation application using spark mllib and the ALS algorithm in collaborative filtering technique. My data set has the user and product features in string form like :But the  method seems to accept only int values for both user and product features. Does that mean I will have to build a separate dictionary to map each string to an int? My dataset will have duplicate entries for both user and product.Is there a built-in solution for this in the mllib library itself?Thanks and any help appreciated!Edit: No, this is not a duplicate as the answer in that question doesn't seem to fit my scenario.  library doesn't seem to support String values for  or . I need this support.Let me try. Assuming that That should do it. Basically, you just create a mapping from string to long and then convert long to int."},
{"body": "I have created a unlabeled Dataset which has some columns. The values in one of the Column are France,Germany,France and UKI know how to filter and count using below code.However, I am not sure how to count values other than France.I tried below code but it is giving me wrong resultPS: My question is a bit similar to  but I am looking for some simpler answer.You are trying to filter those elements which is equal to \"France\". \nTry thisTo cricket_007 's point, should be something like thisI am not sure what column your data is in, so the row._1 would change to the correct number.  You can run the following to see all of your columns:"},
{"body": "I have seq i want to do the iterate automaticYou could use  with . Here is a simplified example:This code would return . If you just care about the correct result and don't care whether or not all futures get executed, you could chain them with :This would give you a  that would complete with  with my example predicate .As I found myself with some extra time, I took the liberty to combine some comments and suggestions in other answers (especially from ) with a  approach:You can now use this function with any kind of futures you want and provide an operation that should be executed with the first valid result or the defaultValue (though IMHO in case of no valid result I'd prefer a  with proper error handling myself):As an extra tip, if you want to speed up the function you can have the iterator lazily evaluate two functions at a time with . This way, while one future result is being evaluated, the iterator will automatically start the next future.Method  will return the first  that matches the condition specified. The resulting  will be a  if none of the input s match. To return some default value otherwise, you would simply do the code here demonstrates basic principles and does not distinguish between planned or unplanned exceptions. this code is not tail-recursive.This will give you Iam not sure if there is another approach but for my understanding if you want to decide based on the result of the preceeding future you need to wait.\nHere is a recursive approach. But in my opinion the one with the iterator is good.It's tail recursive therefore your stack won't overflow that easy."},
{"body": "I am trying to dig scala deeper but scaladocs is not straight as javadocs. While doing some mathematical operation I visited to scala.math  package.\nI am not able to understand value member in the docs.please suggest some guide line to decipher scaladocs. It may be a silly question but as a newbie please suggest something.See :So - the Scaladoc for  package divides members of this package to  (classes, just like in Java) and  (s, s, s and s). You can see these members in the source code at .In this package all member classes and  are  members,  and s are  members."},
{"body": "I have the following case classso, I can do the followingand I would like to able to call parts of the LeftHandSide by name too, e.g:And then:Create an additional case class called :And use that in :And then:"},
{"body": "I am trying to install SBT following the instructions mentioned in:\nBut I am getting an error while running command:The error is:I referred  with similar issue but I am not able to figure out what is the problem.I don't think it's really an issue with the filename. I was able to use that same command without a problem. If the filename was an issue, you could always use this to save it as a different filename: The other option is to use bitly and get a URL if the URL is just too long.But it could be a space issue. Do you have enough disk space? Check with  to see your available space."},
{"body": "So suppose I have the following data (only the first few rows, this data covers an entire year) - I would like to filter the data so that I only see what the names are for a specific date (say, 2014-09-05). I've tried doing this using the filter function in Scala but I keep receiving the following error - Is there another way of doing this? The  method takes a function, called a predicate, that takes as parameter an element of your (I'm assuming) , and returns a .The returned  will keep only the rows for which the predicate evaluates to .In your case, it seems that what you want is something likeI presume from the tag your question is in the context of Spark and not pure Scala. Given that, you could filter a dataframe on a date and get the associated name(s) like this:Note that the  above is .Here's a function that takes a date, a list of datetime-name pairs, and returns a list of names for the date:..."},
{"body": "Is there a way to get the dataframe that union dataframe in loop?this is a sample codeThanks againSteffen Schmitz's answer is the most concise one I believe. \nBelow is  a more detailed answer if you are looking for more customization (of field types, etc):references:In a for loop:you can first create a sequence and then use  to create .Well... I think your question is a bit mis-guided.As per my limited understanding of whatever you are trying to do, you should be doing following,And this should be sufficient.You could created a sequence of s and then use :"},
{"body": "I was playing around with dependent types and run into this error:using this code:Could someone decrypt what value element stands for?The first problem is that you're using an abstract type member for no conceivable reason. It's not directly related to your error, but alas.Ok now for the actual failure, it's pretty obvious and straightforward, you have not provided an implementation for . Your  class , so any concrete implementor must provide an implementation for  by your very contract.A  or  can have abstract members, a normal  cannot. Ergo, kaboom, compiler is telling you to get your OOP up to scratch.Or better yet:"},
{"body": "How can I convert type  to type  in scala?Right now I have this val:I am trying to extract the  objects associated with  and  and pass these maps into a function.example, following is  of to cast  to Then pass to the function. This way, you can handle a case when the result somehow is not . ()Example below"},
{"body": "I have a  as mentioned below:I want to create a new , which will have only unique , but as  and  columns are different for same customer in data, so I want to pick those records which has highest priority for the same , so my final outcome should be:Can anyone please help me to achieve it using Spark scala. Any help will be appericiated.You basically want to select rows with extreme values in a column. This is a really common issue, so there's even a whole tag . Also see this question  which has a nice answer.Here's an example for your specific case.This example is in , but it should be straightforward to translate to To create  you have to first order  by  and then find unique customers by . Like this:It would give you expected output:Corey beat me to it, but here's the Scala version:You will have to use   on  column  the  by  and then  the  with the  and  the required columns. you should have the desired result"},
{"body": "My problem is that I have to take a result permutation on Factorial.\n Print out all n! permutations of the n letters starting at a (26 letters). A permutation of n elements is one of the n! possible orderings of the elements. AS an example, when n = 3 you should get the following output. For Example: abc bac acb cab cba bcaIn scala 2.9, there is a method called permutations of Seq.Or just implement it yourself:"},
{"body": "Why does the following code works:But not this one:Which fails with the following error:It seems to work if you help the type inferencer when calling the constructor for :So it doesn't look like a fundamental limitation. But the error message which you didn't include is very unhelpful . "},
{"body": "When trying to hit an environment with improperly configured SSL certificates, I get the following error: I would like to ignore the certificates entirely. : I understand the technical concerns regarding improperly configured SSL certs and the issue isn't with our boxes but a service we're using. It happens mostly on test boxes rather than prod/stg so we're investigating but needed something to test the APIs. You can't 'ignore the certificates entirely' for the following reasons:The following was able to allow unsafe SSL certs. "},
{"body": "Here is my code:It doesn't work. It just to totally skips over the if (values.get(\"x_card_num\") == None) and else statement, and card_num ends up as an empty string, as defined before the if else statement.Why would it totally ignore the if else statement?And how do I check to see whether the key exists in the array \"values\"?The problem here is that you are  the outer scope. That is, you are declaring  value called . In scala, , so you can simply do this:That is, the  itself evaluates to a value (as long as there is an )the other option is to use  as opposed to , but this is not idiomatic (you should avoid s):You could use  to simplify the test"},
{"body": "I'm compiling the following code in scala:but it gives me the following error:And this:I think the problem is that I need some packages.I need help and I tell me please the following:Repository that I can download the packages that I needThe package versions:In my sbt I have the following packages:The problem is with: draft10, not where to get the package.Thanks for your attention, I hope you can help me.I'll point you to the javadoc: It says that the package draft10 is available since version 1.4. So update your sbt script to use version 1.4 instead:Also, make sure that you don't try to use several versions of the same library. The script that you posted refers both to version 1.10.0-beta and versiom 1.3.1-alpha of google-api-client.the solution is:\n\u00a0\nfor SBTand the repository:"},
{"body": "I'm building several different web services, hosted on different machines, but I want a single set of users to be able to access all of these services, yet also have varying read/write permissions between the services. What is the best way to accomplish a secure authentication scheme for these multiple services?This is a very general question, so here's a general answer.What you are looking for is something with a single authentication scheme for all sites combined with different authorization and access control schemes on the different sites.The first part is pretty much what a \"single sign-on\" (SSO) system gives you, though you could do this without SSO.The second part can be achieved by separating the access rights management from the authorization scheme.  For example:(These are just examples ... not recommendations.  I'm not going to make specific recommendations because it is next to impossible to do objectively, especially since your requirements are so high level.  For instance, a lot depends on the technologies used to implement the websites.)Currently i'm building a registration system for our projects, similar to what you want. Although we have a different stack: Scala-Akka-Spray, authentication system is based on  and  with some custom internal logic for more REST style session managment. Strompath will give you a good user managment system with quite flexible group settings and Shiro, if you don't use Akka, gives you goog session managemant."},
{"body": "Please let me know how to make the following bit of code work as intended.  The problem is that the Scala compiler doesn't understand that my factory is returning a concrete class, so my object can't be used later.  Can TypeTags or type parameters help?  Or do I need to refactor the code some other way?  I'm (obviously) new to Scala.The last line of code fails because the compiler thinks  is an , not a .You should create companion objects with  method for each subclass of  instaed of  trait. Also, it is considered a bad practice to use mutable field like you did with .You can do that, without changing anything else :But, i don't think it's a very good idea...I don't want to sound rude but the compiler is right about assuming that  is an  because that's what the  method returns. As already pointed out you could force the type of d with an explicit cast but it simply wouldn't be type safe. It would be leveraging your knowledge about the method implementation as a programmer, which will eventually become a source of bugs as your codebase grows and you possibly change previous code in unexpected ways. If you need to call a  method then you would better use a factory method that creates  objects, or at least check the object type before doing the type cast, usingOr better still, using pattern matching"},
{"body": "I have a simple Jackson parser, that is supposed to return me values but instead am getting only  values. Any ideas will be appreciated? Sample Json data:Code:   Output:My Java code looks like this and works perfectly fine:"},
{"body": "How can I parse a date string with  datetime which uses the correct timezone WITH daylight saving time?As an example in scala I try to parse the string \"2014-04-07 01:00:00.000\" (without timezone information). This date is coming from MySQL and is supposed to be in tz Europe/Berlin +01:00. What I like to have is a joda date time according to 2014-04-07 00:00:00+01:00 which is the timezone Europe/Berlin currently not on DST (GMT +1).Unfortunately Joda-Time parses the date to 2014-04-07T01:00:00.000+02:00 which is currently the wrong offset (02:00 instead if 01:00)Any ideas how to make Joda-Time parse the date with the correct DST offset?Joda-Time is correct. Your assumption of +01:00 for Berlin is incorrect. You did not account for Daylight Saving Time.According to  at TimeZoneConverter.com for the time zone \"Europe/Berlin\",  (DST) began on Sun 30-Mar-2014 at 02:00:00 A.M. when local clocks were set forward 1 hour. According to the Wikipedia list of , that means Berlin shifted from being one hour ahead of UTC (+01:00) to two hours ahead (+02:00)."},
{"body": "If my code is:}It works, @main is my another template.\nBut if I want to do:}the compile get me error: How to passing variable string in scala template in this situation?Just use  instead of It is because u are using '' multiple times in a single row, answer is given by @serejja above"},
{"body": "There used to be a helpful Scala Option cheatsheet on , but it seems to have been taken down. Has anyone saved it?You could get full version (with introduction) on archive.org.Note: , which includes the introduction and the cheatsheet (which is also below in this answer). (I'm unaffiliated). The ever helpful cheatsheet:flatMap:flatten:map:foreach:isDefined:isEmpty:forall:exists:orElse:getOrElse:toList:coflatMap*:duplicate*:*: Unfortunately coflatMap and duplicate is not part of the standard library. You will need to write it yourself or use Scalaz.End of excerpt"},
{"body": "I have the following:But the assertions fail. Surely that is correct, but it says implementation is missing when running it.I think the implementation you are looking for is:What does the method  mean? It takes an  and returns a function .\nThe function  takes another  and returns an .So we want to return with a function that takes a  , that's why we need: .Type inference works nicely here, you don't need to write (but can): this returns Int while expecting Int => Intit should be"},
{"body": "I want to print set in the order (8,9,5,2,6,4,3).But instead of printing in this order it gets printed in this order (9, 5, 2, 6, 3, 4, 8).Why does it so?Sets are traditionally unordered, so the insertion order of elements is usually lost. List are ordered, but differ from Sets in that they can have duplicate elements.You'll want to use a  if you wish to use a Set that maintains insertion order when iterating over its elements.Warning: LinkedHashSet is mutable,  is an immutable option that stores the reverse of the insertion ordering.Sets are not sorted inherently, but can you use a ? There is some decent documentation on sets Set is an unordered data type. Use an ordered set - such as SortedSet or TreeSet - and pass it a customized sorting algorithm to sort based on when it was added to the set.You will have to create your own data type which holds the integer value you're trying to store and a date attribute representing the entry time.\nYou could use an ArrayList or some other list type. And instead of directly adding an element, call your own add method which checks to see if the element is there already with the contains method.Hope this helps."},
{"body": "Say I have a  and I want to get A.nameHow to write it shortly?Use :If you want to do nothing if it's , use updated:you can learn more usage of  in the API doc: "},
{"body": "How can I make a function that is given a function as input and returns a function with the value tripled. Here is some pseudo code for what I'm looking for. Concrete examples in Python or Scala would be appreciated. In Scala:You can use the Python  syntax to do this;  is equivalent to assigning , allowing an arbitrary wrapper function to be applied:Note the  syntax to handle arbitrary arguments (positional and keyword, respectively). You can also create the  decorator directly, e.g.:or even apply it without the decorator syntax at all:"},
{"body": "I have two list as given following-  I wants output as following:\n    o/p\n    (C:,46552070,433057), (E:,5435,1728)How do I get desired output using scala??Using  like this,Other similar approaches includeAnswering @Yogesh in case the data exists as a list within a list:Similar to Shyamendra but using exists operator on a list."},
{"body": "I have to return the index of the first element in the list that contains the \u2019?\u2019 character.Why does it come up as -1 when it should be 8. Does indexOf not work on characters? Should I use indexWhere or will that have the same result?When I make the val a string it works correctlyTry  \"Finds index of first occurrence of some value in this list.\" Therefore it finds string that equals \"?\" in the list. works on Stringso tryfor each element of list you can use"},
{"body": "The codes are pasted below:I know the first parentheses  means the parameter list of , and the second part  means a . But what does  mean? Is it a decorator to the superclass  or the defined class . Is there any document about this?It just calls the constructor of .Table's constructor takes a Tag and a String. extends Table( tag, \"suppliers ) just wires the required values into the constructor (optionally with added type ascription). This is a standard Scala mechanism.It is a constructor equivalent to something like below:"},
{"body": "How in scala check that variable: Any is list?I need something like You can simply use  method\n"},
{"body": "Doesn't the Play framework have any lenses support included by default?  Something like  or those with Scalaz. Maybe there are recommended alternative approaches (along with their rationale) that I'm not aware of.Scenario: I have an immutable, deeply nested object of a case class, and I want to copy it with a field changed somewhere in the object.I'm looking specifically to do this very simply without needing much boilerplate, much like what lenses libraries like Monocole do.I'm just surprised that Play framework doesn't have any support built in.  Have I missed something?  If not, is there any known plan for this support?Play doesn't include type unions and heterogeneous lists ().Play doesn't include automatic resource management ().Play doesn't include lenses ( or ).It's a MVC web framework, and limits itself to that problem space. However, any of these libraries would be easy to add to a Play project. Play uses SBT, so just add to the  of your build.sbt / Build.scala appropriately."},
{"body": "Reading \"\" it states \" all messages sent over the wire must be serializable\" which in turn requires all messages sent to actor's are serializable. Why is it required that the actor be serializable ? Is the reason that messages are sent between actors using serialization and deserialization ? Reason messages are sent/received using serialization is that potentially message could be sent to actors that reside on different JVM's ?Messages don't technically need to be serializable unless they are actually sent across a process boundary. Best practice is that all messages should be serializable in order to maintain location transparency.To enforce serializability for testing purposes, you can use the configuration , which will cause akka to always attempt to serialize messages even when sent locally."},
{"body": "I want to define , but my IDE (IntelliJ IDEA) complains that \"notify  cannot override final member\".I know that  is a member method of  and the IDE may confuse variable name for method name. I need a  field, how can I do that?This isn't a problem with the IDE.  A Scala class can't have conflicting symbols, whether or not they are methods or fields.  For example, the following won't compile: is defined on , and furthermore it is , so you don't really have any great options here!  You can name the variable something different, of course:If you insist on the variable name, you can also extends , but only if you have exactly one  parameter:You might be looking for the feature that allows you to escape keywords with the grave accent:This would allow you to use a keyword (and some other normally illegal variable names) as a variable name.  This doesn't apply to your example, since  isn't a keyword \u2013 it's an already used symbol on the class!You can also come up with a solution where  is private to the class, but that won't work with case classes or most reasonable use cases.Ignoring your use case, a field called notify:"},
{"body": "I got some strange results when running the following code:Why does type inference in mutable and immutable sets behave so differently? The  part is most puzzling, why didn't we at least get a ?The problem can be simplified to something like this:Your use of  doesn't reveal what type it actually is. In fact, it doesn't even compile as-is. I'm operating under the assumption that you have some implicit  in scope you're not showing, where .On to the question:The simple answer is that you cannot have a  without an . If I try to combine  a  with a , their most common type is . A  is a type of , but how can we sort a set of ? By default, we can't, because it doesn't really make sense to sort a set of .I could contrive an  if I really wanted to, and I would end up with a :But this doesn't really make any sense.The technical answer is that in order to concatenate two s,  and , we need an implicit .There are  s generated by , but notice how they require an implicit . Because the  cannot be found, the compiler looks for something more generic, and finds . This makes sense, because if we put elements into a sorted set that we don't know how to sort, then we no longer have a sorted set. What is left is just a plain ."},
{"body": "Lets say I have an array of vertices and I want to create edges from them in a way that each vertex connects to the next x vertices.\nx could have any integer value.\nIs there a way to do that with Spark?This is what I have with Scala so far:where vertices variable is an array of (Long, String). But the whole process is of course sequential.:For example, if I have vertices as such: , , ,  . I need the following edges: , , , ,  -> , , , , , , , , and so on.Do you mean something like this?If you want to run it on a RDD you simply adjust an initial step like this:and of course drop . If you want circular connections (tail connected to head) you can replace  with  and drop .So, I think this will get you, what you want:First off, I define a little helper function (note that I have set edge data here to the vertex names so it's easier to inspect visually):I do a  on your array to get a key, and then convert the array to an RDD:And then to generate the edges with :This will generate the following output:"},
{"body": "I have the following array:I don't have experience with Scala but I believe this array is mutable because I am instantiating it with .However, when I try to assign a value to one of its elements(where  is an integer):I am getting the following error:In Scala, you cannot access or assign array elements using the  syntax. Instead,  has to be used: is solely used for type arguments:"},
{"body": "I have the following Scala Class:I instantiate it like this:Then, I try to access one of its fields:At this point however, I am getting an error:Why is the field not recognized?Assuming  is declared as an array, you cannot access array elements using . Use  instead:The error is shown because Scala thinks you are trying to parameterize a  method, with the type parameter being . This would be valid if there was such a method and  had an inner type named ."},
{"body": "I'm a Performance QC engineer, so far i used Visual Studio Ultimate to run load test bug now I'm going to change to gatling. So I'm a newbie on gatling and scala. I'm defining the simulation with step-load scenario here: Meaning: start with 5 users > after 10 seconds increase 5 users: repeat until maximum 100 user and run the test in 10 minutes.I tried some code and other injects but the result is not as expected: Could you please help me to simulate the step load on gatling?Can you be more specific about the result not being as expected?According to the documentation your situation should be:If test duration is the target then have a look at Throttling in the Gatling documentation."},
{"body": "I'm a new Spark user and I'm trying to process a large file set of XML files sitting on a HDFS file system. There are about 150k files, totalling about 28GB, on a \"development\" cluster of 1 machine (actually a VM).The files are organised into a directory structure in HDFS such that there are about a hundred subdirectories under a single parent directory. Each \"child\" directory contains anything between a couple of hundred and a couple of thousand XML files. My task is to parse each XML file, extract a few values using XPath expressions, and save the result to HBase. I'm trying to do this with Apache Spark, and I'm not having much luck. My problem appears to be a combination of the Spark API, and the way RDDs work. At this point it might be prudent to share some pseudocode to express what I'm trying to do: So, discounting the part where I write to HBase for a moment, lets focus on the above. I cannot load a file from within the RDD map() call. I have tried this a number of different ways, and all have failed:Alternative approaches have included attempting to use  so I don't have to do the file load from within the map() call, fails because the program runs out of memory. This is presumably because it loads the files eagerly. Has anyone attempted anything similar in their own work, and if so, what approach did you use?Try using a wildcard to read the whole directory.\nActually, spark can read a whole hfs directory, quote from spark "},
{"body": "I'm trying to include a JS script within my mustache template file.It is located in . Also I have a JS script in . A few words about backend. I'm using  framework with support of Mustache out of the box:So here is my question. How can I add a JS script to my Mustache template?I also tried to use But my app still doesn't see the script. (thx to @nuc)I didn't find any good solution but only this one:I placed all assets to And in my view it should be You need to have an additional route to serve your js file, named to the prefix/convention you're using.See "},
{"body": "Installed Scala Language : 2.11.7\nScala SBT : 0.13.9I started scala project in a folder MySample and created  and given below commands And then created folder Project and created  file and added below code And then in the same Project Folder  created  file and added below codeand ran the below command in command window I tried that command with 2.5.0 , 2.3.0 etc trial and error but couldn't figure that outAll answers are sayting sbteclipse ect Do I need to insatll separately SBTECLIPSE plugin? If so provide any URL Try upgrading to the latest and greatest:See  for up to date version information."},
{"body": "I am trying to start writing test for mongodb in my play application.It seems that I can not get it working because the connection is shutdown before the save is executed.Here are the resultsThis is the code of the test, which is not testing anything, but I am trying first to write the database.And the dao implementationWhat could be that I am doing wrong?Thank youAs says in the comments, it turns out that the mistake was that I was not waiting for the future completion. I decided to use ScalaTest specific function to make tests wait for the futures completionsHere is the codeand just in case, I override the default behaviour because the default time was not enoughThank you"},
{"body": "My :My  (at the project root):My root :When I run  in SBT console, I get Have you put key=value in application.conf?? just define the key inside the application.conf because it search value of key inside application.conf\nlike example"},
{"body": "I have an arrayI need to calculate all subtractions to get a new list It is very easy to implement if you are using  and But what If I want to calculate this via MR? Is it possible? As for me, I don't see any possibility to do that. Maybe there is some other ideas?Something like this, perhaps:You can also do it with : But may I ask what is it that you are really trying to do here?Or, if you insist on only using  and  for some obscure reason (what would that be), something like this will work:This really is cheating though, because the function you give to  must be commutative and associative, and this case it is neither. \nIt just happens to work in toy applications, because there is no parallelism, but do not attempt to use this in real life, because it will probably break very badly ... and also because there is no reason to do that."},
{"body": "I am trying to make a basic http client posting some data to a REST API with Akka HTTP but I cannot make the following code work :I got the following errorHere is the code for the dataIngestFlow Here is the code on server side :I tried another simple client, it builds well but the ingestion stop after 160 events, the server doesn't receive anymore events.The first problem I see with your example is that  does not have a map function.  Therefore, the following lineshould not compile.Further, if  is actually a Future (which I infer from the map) then  is actually of type  because the stream materializes into its own Future.  To solve this problem you can use  instead of map.  Namely:This is the  within Futures."},
{"body": "If I do something like this in Scala Play:Can I get an guarantee that the expression I'm passing won't get executed unless the warn log level is set? I see that Logger.warn is defined as pass by name, but does that mean the log message expression isn't executed until needed?It will be parsed/executed because the string has to be parsed when being passed as an argument to the  level. Here is a code that proves that:This codes will print:In other words, the string interpolation was executed."},
{"body": "I am building an API service using Play Framework in Scala and I am using HBase as my database. I want to use HBase coprocessors and since my HBase version is 1.0.0 I have to implement my service to be compatible with Protocol Buffer. I am following cloudera documentation for endpoint coprocessor. I need to create a proto file and then use a protocol buffer compiler to generate the service interface that needs to be implemented by client. I could not find any Scala protocol buffer compiler that can generate scala code from a proto file that has (not only messages).  you can see an example of a proto file with service that is used for a coprocessor. What can I do?There is Scala implementation for . Google ."},
{"body": "Code:I want to make  private and change all the reference to it to , is there a way to do this safely and hassle-free in intellij?I soon realized that for this specific case I could have handled it in a better way (by recursion):But I am still interested in finding out how to refactor the old problematic code because it might be useful in the future. I don't know if the fact you use it for Scala changes anything, but you should be able to right-click on a function, and  to the one you want (so you wouldn't replace function1 by function2 per se, but rather modify f1 to match f2).Edit: This won't work as per Ben comment. Leaving anyway for the sake of mentioning the functionality."},
{"body": "I am trying to write a helper function that will allow my dashboard to check if MongoDB is running on not. Based on the result, it could warn admins on the visual dashboard. I read through the reactive mongo docs  and I have the following function so far but unfortunately its not working.UPDATE 1:I got it done this way:"},
{"body": "I am reading JSON data from kafka and parsing the data  using spark. But I end up  with JSON parser issue. Code shown below:error details:\nerror: not found: value JSON\n[INFO]     val lineJson = lines.map(JSON.parseFull(_))Could you please help me to which maven dependency should use to sort out the error.Thanks.I think you are looking for this:ANd adding Maven:"},
{"body": "Will there be any problems if you use GUICE with Akka Cluster with regard to that will run multiple JVMs? What problems may occur if actively to use GUICE? Or better get rid of it and try to do everything the built-in tools?Why are you concerned about Guice with Akka Cluster? Guice is for Dependency Injection; as long as your actors are injected correctly, using Guice or Spring, it does not matter. When you obtain your remote ActorRef the executing actor can reside in any of the nodes in the cluster."},
{"body": "I've just been learning about floating point numbers and am having difficulty understanding how I am supposed to find 3 double values (let's say a, b, and c) so that:(a + b) + c == 1.0 and a + (b + c) == 0.0I've read several web pages and watched videos about what floating points are but I have no idea how I should find something like this. I tried different numbers but only get much smaller differences in my outcomes. e.g.(1.1 - 0.2) + 0.1 = 1.00000000000000021.1 +(-0.2 + 0.1) = 1.0In the general case, you can only compare the results of floating operations with respect to an error margin you trust.For example, using the  library:If you don't want this, you need to use  precise maths."},
{"body": "I've refactored my code for  of advent of code by using monocle, a lens library in scala.Is it possible to improve this code :And here is another try, separating the modification of each fieldOne more question : is there a way to use  with monocle ?The full code is here : You might want to use the second approach, because it separates handling of two fields.\nHowever, the functions shouldn't be interpreted in sequence (), but rather they should be combined as s with .  (after the Yann Moisan comment)The execution may not terminate in case of infinite loop in user's program. So instead of the  we need some recursive function that will extract the next instruction by pointer:(The last line of  should be replaced with the above code)"},
{"body": "I am building an Akka cluster and want to use Akka-HTTP server as an API server inside. How does one do that?I would imagine it will be a cluster singleton, as it is an access point to the cluster, but making it an actor seems weird, as it will need to have a receive() method, which will do nothing (i guess). Simplest example:To stop:"},
{"body": "I'm trying to incorporate a class that I extracted from another Scala project into a class library that I can reference from multiple projects. However, I cannot seem to reference the class from inside my client project.  I'm newish to IntelliJ 15.0.6 and my background in Scala is limited to scripts.Any ideas how I can fix this?ThanksSince you are using SBT, I would suggest you to use it to manage you library dependencies. SBT is a build tool which will download the jar file and make it available in your project for you.To add a library dependency (in this case apache derby) add the following line towards the end of your \"build.sbt\" fileYou can find your dependency on this site - . Search for your library name, choose a version and copy the script from the sbt tab.I solved the JAR reference problem by creating src/main/scala/lib and copying the JAR to that folder. might need todo a project refresh (Different to a project rebuild), or build it via sbt instead. I've noticed intellij can be a bit sluggish on this. "},
{"body": "I'm using the following code to generate association rules in FP Growth algorithm.But whenever i'm trying to run the algorithm on a big data table with 100 million records, it fails with java heap space error.What is the alternative of using the collect() method for executing the FP growth algorithm on big data datasets?I'm using spark 1.6.2 with scala 2.10Solution codeTry to increase driver memory if applicable. If you are running your app on yarn then it would be better to configure memory for driver according container memory which driver heap memory + memory over head (15% of heap memory) = (should be) yarn container memory"},
{"body": "I'm using certain external library that has a method which is overloaded several times with different arguments, something like:And a certain case class I'm using whose data I want to pass onto said methods, say:Right now I'm using the library method like:But since I'm always using the index of the value in the case classes I figured that maybe I could could save up on some lines of code (around 10) with something like the following: because I'd be iterating a  and therefore the compiler complains that I'm not passing the correct argument type to  since  is not What would be the correct way of handling this? Thanks in advanceTry this. I am using Scala 2.9.3. In newer versions, you can use TypeTag and ClassTag. Check  So now you have the class type information. You can devise some mechanism to map class type value as string to  instance and then use  to cast it."},
{"body": "I was looking at ActorLogging  and came across this syntax:What does this syntax  mean? I know that in scala  create a block of statements and the last line is assigned the variable. The comments explain it as :Is there a technical term for it so that I can learn its usage more?\nNote: I know what a partial function is. From the comments and answers I understood that LoggingReceive returns a partial function and the syntax of apply.In akka the  method has to have  as the result type. So here they're using an object  which has an  method defined like this:In Scala we have a syntax sugar construct so instead of calling:You can simply write: There's another thing - in Scala we can use  brackets instead of  parentheses, so the above expression can be write as:So at the end they're just simply wrapping the  method with the  method of  which calls the  method."},
{"body": "I am new to spark. I need to change the date format in a spark dataframe which is in String. I need to remove / as well as 00:00:00. I tried to use     The format is changing as expected but with some random date. I found the issue is with \n  The dataframe is imported from CSV file. I got a solution using regex but it would be better to know why it is not working. Code used: (The code may not be exact but similar)Try something like this:See  for details. "},
{"body": "I am on Spark 1.x, and attempting to read csv files. If I need to specify some data types, as per the , I need to import the types defined in the package .This works fine when I use this interactively in spark-shell, but as I want to run this thru spark-submit, I wrote some Scala code to do this. But, when I attempt to compile my Scala code, it gives me an error saying it could NOT find org.apache.spark.sql.types. I looked up the jar contents of , but couldn't find these types defined in there.So, which jar has org.apache.spark.sql.types?I looked at the  at GitHub to realize that these types can be found in the  jar. That didn't seem intuitive.Also, since  has this codewe end up with another dependent jar - json4s-core."},
{"body": "In my development and qa environments, I will be hitting a rest endpoint using internally signed certs. The policy where I work is to put internal certs in a separate bundle on our Linux servers.The following works perfectly well in curl:My actual client for this endpoint is written in Scala, however. Currently I'm making my call using :I would like to figure out how, in my dev and qa environments, to use our internal bundle. Anyone doing that in Scala or Java?You need to configure the \"truststore\" used by the JVM, with the \"\" option when you launch java, i.e.( will take the same  argument if you are using SBT to launch your app)You'll need to get your CA certs into JKS format.See:I would entirely recommend you to use Gatling for this kind of things. Gatling is a really cool framework for load-testing and it provides support for many protocols like jms, jdbc, and of course http among others. Please take a look on it here  (This framework is build on Scala) and it provides support for the things that you are searching for"},
{"body": "I have two dataframes DF1 and DF2. My task is to select the data which is present is only in DF1 but not present in DF2. Could any1 please help. \nI'm using Spark 1.6 shell. You can use this:Note that the two dataframes need to have the same structure (same columns)In Spark 2.0 you can do . Assume your records are identified by the common column  : How about..  where col1 is the column to join."},
{"body": "I want to conditionally create a Binding of HTML node. However the code does not compile.You need an  block with an empty content, usually a HTML comment:"},
{"body": "I am trying to write a function that returns a future. Within this future 3 other future functions are called, the third depends on the results of the first two. My instinct is to use a for comphrension, but when I try to match on the tuple I get this error:Which refers to the match of  below It can be solved by making whatever is under  return "},
{"body": "Relevant request URI:How can I grab this query string ? You have to create a custom binder this is a Java implementation but it follows the same principle:"},
{"body": "Good afternoon,I am trying to follow , however when running the command:I get the following error:As stated in the setup part of the tutorial I have checked my java version:My javac version:And my sbt version:I have trying running the command with my sbt version as the parameter:But it still throws the no template error, could you please point out what am I doing wrong?Which comply with the prerequisites for the tutorialTurns out I had to remove the -Dsbt.version=0.13.13 parameter of the commmand.Running it as:Removes the error and it works correctly.The problem was not the extra flag. The documentation is correct. You have to use it before the  command"},
{"body": "I have a Future lazy val that obtains some object and a function which submits operations in the Future. The problem is when Future is completed it executes operations in reverse order than they have been submited. So for example if I execute sequentialy I get after Future completionThis order is important for me. Is there a way to preserve this order?Of course I can use an actor who asks for future and stashing strings while future is not ready, but it seems redundant for me.You can use  to ensure the order of execution.Something like this.. Im not sure how your func has a reference to the correct futureC, so I moved it inside."},
{"body": "The following code:Does not compile. I get the following error:I don't really understand this as according to the  a  extends the trait . That is, .I thought it might be something to do with the contravariant type  but if I use  instead of , it still doesn't compile. What am I missing? The solution is to do:but this seems inelegant to me.OK - this has been figured out with the help of the scala console:So the type inference is inferring m's type as being an immutable map. The following code will compile OK:However, this is not much help as the map can not be added to in such a way as to return a . I think that they should have overridden the  method inherited from  with  as the return type.Could anyone comment on this? What use a ?I'm curious why you want m to be var rather than val - if you're not trying to reassign the Map to itself, things seem to work OK, as shown in the repl, and this is more in keeping with the scala philosophy of preferring immutability where possible:I see that the question is kind of old, it's possible that the behaviour you're seeing is different in Scala 2.8 given the major refactoring of the collections classes."},
{"body": "I am trying to understand the IO provided by akka. I started another  for this issue.\nI found a  on how to use akka IO. I reimplemented it and started it. Now I am wondering why only the first message of the session is printed. \nCan somebody please explain what  is doing and how I could sent a message to another  of the  instead?I figured out, that the actor just takes a single input and process it by just printing it to the console. That leads me to another question: How can I make the actor take multiple inputs and process them? I connected to the server via putty and if I hit enter one time it processes the input as expected, but if I send another line of input it only stores the input. Where do I have to run , so it processes the next line of input? I tried it inside of the  case, but it didn't work.The example you are trying to understand is a bit more advanced, mainly because it is dealing with socket based communication. If you are actually interested in that kind of stuff, I suggest you read  where the example from the blog seems to originate from.In any case, before you do that, a good read would be  which explains the fundamentals of Akka actors and gives you some actors to play with."},
{"body": "Why do I get a pattern matcher warning regarding type erasure with the following Scala Swing code:The warning is:Is this just horrible design of Scala Swing, or am I doing something wrong?This issue has disappeared with the new Scala 2.10 pattern matcher."},
{"body": "I see lots of documents referencing the \"Within\" method on the TestKit.\nThere appears to be something wrong with my tooling because I'm not able to import that method.Examples of usage : \n\nShould be available as of at least 2.0\n"},
{"body": "At my current place of work, we're considering using the 'Play' framework for our next project. I'm actually an advocate for sticking with the 'devil we know' (Java EE / Spring / Hibernate) but I'm rapidly losing faith due to the technical pot-holes in that tech-stack. I dread to think of the unknown pitfalls awaiting us in the all-new Play framework. I'd really appreciate feedback from anyone who is a  Java EE / Spring / Servlet / ORM engineer who has experience using Play. How does it compare? What are the major booby-traps awaiting us? If you're a newbie web hacker who's infinitely optimistic about 'Play', yet utterly naive about real-world software development and maintenance, I don't need your input thanks! ;-)Edit: I was asked to identify some of the technical-pot-holes I'm referring to. I don't know what these might be in the Play framework - which is why I'm asking the question. Some J2EE gotchas I suspect might have equivalents in Play could be: "},
{"body": "Using Squeryl ORM and Scala. For the first time I did a JOIN statement that used grouping. Admittedly, I can't figure out how to start iterating through the contents.Here's the JOIN:How do I print out its contents?I tried:But got the compiler error:Max the great was able to help out on the mailing list. I almost had the syntax right.replace :by:then :Source: "},
{"body": "So I'm writing a mini timeout library in scala, it looks very similar to the code here: The function I execute is either going to complete successfully, or block forever, so I need to make sure that on a timeout the executing thread is cancelled.Thus my question is: One alternative that I'm considering is to use the java Future library to do this as there is an explicit cancel() method one can call.[Disclaimer - I'm new to Scala actors myself]\nAs I read it,  waits until the list of futures are all resolved OR until the timeout.  It will not , , or otherwise attempt to terminate a ; you get to come back later and wait some more.The  may be suitable, however be aware that your code may need to participate in effecting the  operation - it doesn't necessarily come for free.   cancels a task that is scheduled, but not yet started.  It interrupts a running thread [setting a flag that can be checked]... which may or may not acknowledge the interrupt.  Review  and .  Your long-running task would normally check to see if it's being interrupted (your code), and self-terminate.  Various methods (i.e. ,  and others) respond to the interrupt by throwing .  You need to review & understand that mechanism to ensure your code will meet your needs within those constraints.  See ."},
{"body": "I found a thread here that looks like it: But my version is relatively new already. How come it's still happening?Here comes my lift setting in maven:I don't have any clue here :("},
{"body": "Could someone please help. I have Play2 project in which I need to test some DAO code.\nI used documentaion from  .\nThe test is very simple:And when I run 'sbt test' I get strange error:StackOverflow/Google knows nothing about this exception. Thanks in advance.The stacktrace makes me think that a library is incorrect or missing in your classpath. This is why you are seeing \"Helpers$.\" traces where the class constructor seems to be failing. You can validate this by writing a small app in your test directory, without specs2 but using Play2's helper classes and see what happens.I found solution - :\n should be added to actualy start Play app so DAO calls can work in test."},
{"body": "I am coming from the Java EE world. New to scala and Play. Looking at some sample applications. I see scala code in html files. I am able to understand the framework but I could not get scala.html files. Do I need to learn scala to use Play framework. Any work around.You can use  without any scala skills. If you don't want to use scala.html templates you can get some module providing Java templates for Play20. Modules list can be found .Some examples with samples:"},
{"body": "I have got a pogoplug an want to run an play2.0 application on this device. Pogoplug has 128mb ram und a dualcore processor. What is the best way to do this?What is the OS and connectivity options?I think the easiest way is to use  command to prepare zipped application, push it to the server and start there.  and  commands builds application from source and you probably don't want ot do that on 128m ram.Of course you need JRE and play installed there and HTTP server in front.This is quite a broad question, but here are some things you should consider:"},
{"body": "In the following scenario, by-name parameters cause conflict with functions.Given some serialization infrastructure:And an example type along with its serializer:Now imagine a wrapper called  which deals with the recursive/mutual connectivity:And its serializer:Then the following fails:How can this be fixed without changing by-name parameters to functions (as far as possible)?It seems one can write the type :"},
{"body": "I have an intensive task that can distributed but whose end result needs to be accumulated and passed on to another method down the line.For concreteness, let's say I'm doing word counts for individual files in a large collection of text files. The version I've worked out so far looks like this:I can think of several homebrewed methods to make the main method wait for collector to finish and then deal with the \"bin\" object.But what is the proper scala way to retrieve \"bin\" above and hand it back to main or what have you?With an actor, you don't \"retrieve\" anything as calling actor methods can be dangerous.Instead, you get the actor to send the answer out. You could add a case for a  object, or in your case you could have it send the  somewhere in the handler for .You could ping an actor using method  to create a future result. The actors receiving the message can fill in the result by using .However, you save a lot of management work by using executors instead, which are better for  tasks (you don't really have a  situation here where actors would be particularly useful).The following is based on Scala 2.10 (forthcoming, you can use 2.10.0-M6 right now), because it contains the improved concurrent framework:Example:"},
{"body": "Using Scala, JodaTime, and Squeryl for ORM. There's an annoying problem where once the application starts up, a Timestamp generated using JodaTime doesn't re-initialize every time it's called. Instead it sets the time once and annoyingly doesn't re-initialize every time the SQL is called.Code below. First, the time parameter:And the Squeryl JOIN:The strange part is that if I generate the  timestamp  JodaTime, then it re-initializes every time. So what is JodaTime doing differently?Found the problem: apparently the thread managing the JOIN was never successfully being shutdown, and was being re-referenced inside Akka. This meant that the  variable had never been re-initialized.So the take-home lesson is: manage your threads.As I have further learned, the original object holding the time values were set as . As it turns out, they need to be .Bad:Good:"},
{"body": "When I define a function inside the Scala REPL, I can see the created function's type, like this:How can I see the function  when I'm not in REPL? Say, I want to know a function type a library uses at runtime so I can use the same type.Thanks before."},
{"body": "I am using the play-salat plugin with an app I am making with Play! 2.0 scala.I get the Duplicate Key error, and the compiler points here to the error:I have read that this can be fixed in PHP  doing an unset, please tell me how to do this with Play.I am able to post in my database the once, when I try again I get the error.I resolved this issue on my own. I am pretty new to Scala and the Play Framework.I changed my Application.scala file like this:I also changed my Post.scala file:"},
{"body": "I'm trying to find the correct method of deleting a file from mongodb gridfs.Would the standard query suffice?Or is there a specific way of doing it properly?Thanks in advance, any help much appreciated :)GridFS doesn't store files as records. GridFS gives you handle to stored file that spans more than one document (in order to exceed document size limits). To delete file in GridFS you have to use its API. So in order to delete file you have to find it, for example using this method:\n)\nThen when you have reference to file you can delete it. Example code might look like:gridFsId is ObjectId stored in Lift record that keeps handle to file in GridFS.\nI hope that helps."},
{"body": "We use jenkins for build automation of a Visual Studio 2008 C++ Project.\nIn essence it's a series of calls of:which works fine in general.To run everything out of a dos box I wrote a scala wrapper, which does the same with ProcessBuilder. It works, but I have the problem, there is not output on the console.My guess: devenv starts a bunch of processes to compile and linke the projects in parallel. My scala programm only outputs the stdout and stderr of the devenv process, which is none. All the other output of the subprocesses is shot into nirvana.for the sake of completness the source snippet:The List used to start the process looks like:Is there a way to record/output the output of the subprocesses?"},
{"body": "I want to get the data from a database table using a regular expression.This is what i did:But i get nothing in result. \nWhat I'm doing wrong?PS: I'm using Well I succeded in getting a result. It gives me all the user that contains the inserted . I have just deleted  and  that were surrounding . Here is the code.  "},
{"body": "I'm working on a homework assignment, modifying code created by my professor.  Unfortunately, he's not available to me currently so I'm reaching out to the stackoverflow crowd.This snippet is from a file \"Peer.scala\", which communicates with an actor from another class \"RemoteActorChat.scala\".  I have only included the snippet that I believe is the cause of my issue, for the sake of brevity.  Should the rest of the code be useful in finding a solution, I'll gladly post it.My intent is to have the Peer capture input from the console in a continuous loop, while simultaneously reacting to any messages received from the RemoteChatActor....}...When the inputText is matched to _ the post function is called, does it's thing and I get another prompt.  I can post messages all day long like this if I want.  However, when inputText is matched to \"Unsubscribe\", \"Subscribe\", or \"?\", the behavior is different.  The statements in the case are executed IE) the unsubscribe or subscribe functions are called and do their thing as expected. However, I don't get the prompt back to continue sending input from the console.  The peer basically just hangs at this point.  I expect another prompt but the console window is just empty and doesn't take any additional input.Clearly my understanding of how this should work is flawed.\nWhat am I misunderstanding, and how can I make this do what I intend?The actor is waiting for a  inside .  It won't continue until it gets one.  Since you only send one in one case, only that case gets through.If you want to make the other cases work, either have them also send a message that the  block can receive (at least as a  inside the react block), or move the react block so it will only be entered if there will in fact be a  coming back its way at some point."},
{"body": "I am going through the Scala book i bought and am trying to get TextMate to cooperateI got the point where:Don't know why it gives you two run script options, but scala REPL is an interactive scala console. You can execute any scala statements on the fly with it. Option 4 will just start a console, option 5 I suppose with preload everything you have in your file, so you don't have to import everything yourself. 6 will start a console and let you execute whatever you selected.Here's more about REPL: "},
{"body": "I'm trying to get the following Scala code to compile:The idea here is that I need a way to make doMath operate on any Integer and thus a type with an addition operator.  I want it to be agnostic of large number like BigInt or a very small number like Byte.When I try to compile this I get an error:Any ideas?Rex is right.  Since x : Long means x.asInstanceOf[Short] is invalid the routine fails when trying to cast initializer to T when T is short.  Yes, that seems legitimate.  Thanks Rex!  Wish you had answered instead of commented so I could give you a +1.I do have some control over the type of initializer and given all that Byte is its logical type but I need to think about this a bit to make sure that's the only issue because the actual implementation of doMath uses a parameter to get a lazy value type with a bunch of options controlling the output.  But above is in its simplest form and I think you hit the nail on the head.  Thank you Rex Kerr!Implicit object is no good.Try using case class insteadE.G:I'll explain:\nIn the above example doMath takes a single value of type NumericValue[T] (i.e. NumericValue[Long]) but if the received value is of type Long the compiler will look for implicit value for it, the case class.Note I don't have my computer on me so I can't test this but it did with in the past when I tried to initialize a variable in that way."},
{"body": "Is it possible to configure Akka to use blocking or non blocking i/o?Thanks.Akka is an actor system for concurrency and parallel execution. As such, you pass event-based messages into actors to make them do things. When you send a message, it goes into the actor's input queue, leaving the sender to go and do something else. But the sender can hold a future instead and later block when it wants to collect a result.The actor itself may possibly use blocking i/o, but this doesn't block the actor's clients.Akka IO is non-blocking, please refer to the documentation: "},
{"body": "I am a newbie to scala and I am writing scala code to implement pastry protocol. The protocol itself does not matter. There are nodes and each node has a routing table which I want to populate.\nHere is the part of the code:The problem is when the function call to getMatchingNode takes place then the actor dies suddenly by itself. 'list' is the list of all nodes. (list of node objects) \nAlso this behaviour is not consistent. The call to getMatchingNode should take place 15 times for each actor (for 10 nodes).\nBut while debugging the actor kills itself in the getMatchingNode function call after one call or sometimes after 3-4 calls.\nThe scala library code which gets executed is this :Eclipse shows that this code has been called from the for loop in the getMatchingNode functionThe strange thing is that sometimes the loop behaves normally and sometimes it goes to the scala code which kills the actor.Any inputs what wrong with the code??Any help would be appreciated.Got the error..\nThe 'continue' clause in the for loop caused the trouble.\nI thought we could use continue in Scala as we do in C++/Java but it does not seem so.\nRemoving the continue solved the issue.From the book: \"Programming in Scala 2ed\" by M.OderskyI really suggest reading the book if you want to learn scalaYour code is based on tons of nested , which can be more often than not be rewritten using the  available on the most appropriate .You can rewrite you function like the following [I'm trying to make it approachable for newcomers]:The potential to easily transform or operate on the 's elements is what makes , and  in general, less used in ScalaYou can convert the row iteration in a similar way, but I would suggest that if you need to work a lot with the collection's indexes, you want to use an  or one of its implementations, like ."},
{"body": "I have written a couple of tests, very identical but different in only one way. One test has an international address and the other has domestic address: .scala & .scala.scala extends another class .scala that has a  method. Here I am validating that .scala has valid ship methods present. .scala does  extend .scala.Both test classes (.scala and .scala) have different users; the only similarity is that the address is stored in a val named 'address'. When I run these two tests some times (and only some times) the test fails for .scala because I see an international address in there.  My testng xml is preserving the order of tests, so this is more confusing that a race condition could occur especially since I am not sharing any resources among the tests.Why does your ShipMethods have a @Test annotation on it? Is it a test class? If it's not a test class (as opposed to a class which you want to test), then it shouldn't have a @Test annotation on it.Your classes under test shouldn't have test methods inside them."},
{"body": "I'm trying to tie in an API for the St. Louis Federal Reserve data (FRED) into a Play2 web app. You can see the API here:  under java.I put the FREDAPI.jar into the scala/play classpath and I'm able to import com.uniservsolutions.stlouisfed.fred without a problem.But the API needs the API key to instantiate a 'session' and that is put into a file .properties (I have fred.properties). API docs instruct me to declare the following at the commandline:This will read the contents of fred.properties:But I'm not familiar how to do this in Play. I have found info on JAVA_OPTS but I'm unsure where to declare or if indeed that will work. Finally, when I deploy this (currently staging on Heroku) where will I declare this (heroku's Procfile?).I discovered quite by accident that you can declare java command line args for play e.g.:"},
{"body": "I have the following build structure:in , I have the following declaration for Scala because my Scala depends upon external JAR's from , as well as it needs to compile first because my Java compilation is dependent on my scala code (not the other way around):When I run:I get compilation issues with  not finding the classes in someDependantJar-1.0.jar from .When I run:I get the following list of dependencies:So I dont see someDependantJar-1.0.jar and was hoping that someone can shed light on this issue so I dont have to define the transitive dependencies in subProjectA and subProjectB (duplicate)you havn't defined a compile dependency in subProjectB.\nNot sure if i got your setup correctly, I think you need to add the dependency to someDependantJar? an you try adding a dependency in projectB on projectAcheers,\nRen\u00e9"},
{"body": "In my Play 2.0 app, I am calling a Java API which depends on certain environment variables being set to work properly. However, this can only be checked at runtime and what I would like to do is to throw a meaningful error message at the start of the application but unsure how to handle it. A possible approach is to use require likeThe problem with the above approach is that I get a  and stack trace (in dev mode) without the ability to display a meaningful error message. Anyone knows a better way to handle this?You can create a  which gets called automatically during startup.  See the "},
{"body": "I am using ~;container:start; container:reload / with xsbt-web-plugin. Reloading works well, only it does not happen when I change static resources, pages, etc.How do I include them to change monitor, any ideas?Thanks.watchSources didn't work for some reason, even when directory is successfully added therewhat worked for me is unmanagedResourceDirectories in Compile <+= (baseDirectory) { _ / \"src/main/webapp\" }"},
{"body": "I'm trying to port my maven-based docbook projects to sbt... but so far it is a nightmare :-(Given the standard project directory:... build.sbt looks like this:... and finally project/plugins.sbt looks like this:... I always get the following error:Any idea? Any help would be really appreciated. I'll try a bit further and then I'll give up.Thanks,\nj3dThe  seems not to be maintained anymore and it uses 0.10.0 which maybe outdated for you.There are  out there, this is the .But must build yourself and publish it local.By the way ."},
{"body": "I want to write code that has a generic, possibly slow, implementation of a type parameterized method that works for all classes implementing a certain interface but still allow for subclasses to specialize this method with more efficient implementations if they so desire.However, the following code does not quite workbecause it treats A in class C as a type parameter not as something that overrides the function for the case where the type parameter is only A.Is there a way to write the code that I want without using match at runtime?What you're going for is this:But as the type system tells you, this is . You are violating the  here. What you can instead do is make  a class level type parameter or an abstract type member. Then you can constrain it in the subclasses as shown below:You haven't provided more information so I don't know if this will work in your particular case or not. Here's one way to speciailize things: using typeclasses. Code: (This turned out to be more convoluted than expected.)"},
{"body": "I need to execute the same task many times as much as possible within given milliseconds. So I made it using scala.actors.threadpool.Executors as following code.But this is a bit complex. Instead, I think, Broker of Twitter and some Executor which connects with the Broker and sends a finished task to the Broker, if exists, could make it easier. The following is a pseudo codeIs there any Executor like this?If I am understanding you correctly, you may be able to use  like this:"},
{"body": "I'm running into a compiler problem when using Scala7-M9 and json4s 3.0.1 with compiler versions 2.10.0 and 2.10.1When compiling the short snippet of JSON parsing code, I get the following compiler error:from this code:Changing the imports to:(in my real project I only need the / monad for some validation) fixes the issue. Just wondering if anyone from the Scalaz/compiler world would have an idea what was causing the compiler error when I import the whole scalaz namespace and if it's a BUG I need to file.Thanks."},
{"body": "I am pretty new to Scala/Lift and ran into following problem:I get following error message:Sample JSON:I tried several things, but it seams I stuck here. Can any one point me in the right direction? BTW. I am using scala 2.7.7!Cheers, enzoIf you remove the  hint from your JSON input, it should parse. Something like:Otherwise, you can try adding  around your input like:I think it is looking for a collection since the  attribute is used to help determine which subtype to deserialize the current item to. There is probably a way of getting it to work with a single element, but I am not sure what that is.The reason I asked about the scala version is, 2.7.7 is pretty old at this point (2.10 is the latest) and a newer version may make finding things a bit easier.  "},
{"body": "I've setup Emacs + Ensime for scala.I'm able to start sbt console inside emacs using C-c C-v sIf i start scala console inside emacs using C-c C-v z, I get the following errorWhat is the fix for this error? How do i get scala console running inside Emacs?I've ran into a similar error recently, but here's how I worked around it (but I don't know how to fix it, so this is only half the answer). What I did was to customize the  variable to have the value: . Which will indeed start the interactive Scala environment.EDIT:Here's relevant parts from , but I'm not sure it will matter / will be the same in every install:I've installed  from MELPA. Installed  version 0.12.0 by downloading an RPM from their site. I had previously JRE and JDK installed, the active version is OpenJDK 1.7. Scala installed is 2.9.2. I don't know how to identify Ensime's version :|"},
{"body": "Is there anything out there that's similar to Scala worksheets? If not, I want to implement it, and would like advice on where to start.  "},
{"body": "For my development machine, I'd like to disable SSL hostname checks for HTTPS connections. The exception that comes out of the dispatch code is this:I'm looking for a way to set the default hostname verifier in a way that will be used by dispatch. There's  to set the default hostname verifier, but that whole class is apparently deprecated. Is there a current correct way to do this?It's  that was deprecated in favor of ; the latter isn't deprecated.As to the ways to bypass this check, you could:"},
{"body": "I am using BigQuery from Scala. I tried the sample Scala code to call Google bigQuery API Scala: Above BQ returns:Please help me parse the values from above the results in JSON Format or change the query to return the result of the format like this:I like .Look at the lotto example, it's straight forward with case classes"},
{"body": "I'm using Play! framework to build my application. I have such dependency:Somewhere in the code, I'd like to get the version of the superLib. But is it possible to do with Play! tools? My code is dependent from the version of superLib. For example:As the version of superLib is changing quite often, I don't like to change the version of it in the build file and in the code. \nI could put it in the configuration file and read it from there both in build file and in the code, as well as parse the build.scala file in the code and get the version by extracting it from the build file. Is there any prettier way of doing this? I was trying to play around with play.api.Play, but didn't succeeded. Currently, I decided to get maven property file from classloader.Thanks in advance!In case someone will try to do something similar:"},
{"body": "... for each article.Get Image name and Image (reactivemongo.bson.BSONValue) and put it in a form select?Does anyone know how to do this?The application is based on Play 2.1"},
{"body": "I have the latest Scala IDE 3.0 installed on Eclipse 4.2.2I installed the latest SBT using When I run SBT and type the command  this is what I getWhere did the Scala REPL 2.9.2 come from?"},
{"body": "I'm trying to use a regexp to parse a string of the form:\"a=10:30:00:b=2:c=3\"to yieldThe closest i've gotten is \n    (\\w+)=(\\S+?)(?=:|$), \nbut that chops off a's :30:00:Code is in scala:Modified the regex based on \n to handle:Or based the regex you quoted:"},
{"body": "I am trying to write function that writes data to MongoDB using Casbah toolkit:I get the following errors:What's wrong?I'm not sure without full imports, but try to change  to  or add "},
{"body": "I'm trying to use sbt-scalabuff to generate scala case class from proto file. I have added the plugin reference into plugins.sbt file of the Play project, added runtime dependencies too.If I do not add any settings to Build.scala referring to sbt-scalabuff then I assume that will go with default settings, expecting to find the files in src/main/protobuf.Do you know any Build.scala sample of proper settings of sbt-scalabuff?Thanks,\nGabiI haven't actually tested sbt-scalabuff much myself, but you can try raising an issue on the sbt-scalabuff github page."},
{"body": "I made an eclipse project for my application using eclipsify. Then I added the code at  appHome/target/scala-2.9.1/src_managed/main to the dependency list and everything works fine except for the code that calls modules. It seems I am missing some classes that belong to them. Where can I find them? What is the place where Play places the code generated for those modules?I am currently running Play 2.0.2Jose!\nYou should not change any code at target folders. This folders are generated automatically from your source folders. \nSo, just try to remove target folders from the project, and then use compile and eclipsify again.\nHope it would help.  "},
{"body": "I have a scala project (based around unfiltered and jetty) which uses bval. I've tried everything I can think of to get custom validation messages to load from a properties file, but it just doesn't work. I have a scala class like this:and a ValidationMessages.properties file containing the following:But the output from the validation results is  not matter what I try. I have tried placing the properties file in the project root, in a resource folder which is exported to the classpath by the run configurations. I've also tried placing it in the  folder and then running the project with ."},
{"body": "I am trying to use pattern matching when querying a mysql database from a play2 module using anorm. Code looks like this:But the name:String is not matching anything (already tried to match just the integer and it works fine). On my db the entity table \"name\" column type is varchar(45).Anything I am missing?You can try to match  on an  and a named wild card.If name is nullable then this matching should work:"},
{"body": "I have a play-template with javascript code in it. Everything works but it has two disadvantages:On the other hand, the advantage I have with his solution is:Any suggestion to improve the code is welcome! ThanksThe best way for correct implementation JS is ... checking the HTML code in the browser, most probably you are including jQuery at the end of doc, therefore it's not available. Create  stable HTML file and work on JS there, when you'll debug and finish it then move it to the view (place parts in proper places).Also consider moving JS scripts into separate  file ."},
{"body": "I am experimenting with existential types.I was playing with a function that expects a sequence where the elements of that seq are all the same type. I had .. Where ...I then came across the \"forSome\" syntax and found I could achieve the same constraint with it.I wrote the following for comparison purposes ...What I am trying to understand is - am I missing something? \nWhat is the benefit of one signature over the other.One obvious diff is that the compilation errors are different.The difference is that in  you may not refer to type , whereas in  you can:"},
{"body": "Can someone help me understand the compile error message below?  I've been looking at this for a while now and I don't understand what's wrong.Error message:EDIT:\nClass declarations look like:Works for me on scala 2.10.0then paste in the code you supply above.   compiles without error and seems to work in the intended way:"},
{"body": "I'm making a little web project with scala, scalate and jade templates. The problem is when i'm changing .scala or .conf files, sbt automatically recompiles them and reloads project, but it doesn't do it when i'm changing my .jade files. All my templates lies under the  folder in src/main and it is added on the project classpath, also i've added this line to  file in  folder:Just had a similar problem. You should change your line in the scala build file on this one:This should solve your problem."},
{"body": "I have a given (and impossible to change Java class from external Jar) for this question we will call it: I also have Java class which will be used as a Type Parameter: Now I want to create a  class called  And we also have our main method in  object.Hrmmm.. I'm trying to picture your code and what it's doing. It's reminding me of using Hibernate with generic DAOs which I've done quite a bit so I would say you should look at  which would print SomeClass the way you set it up. There is also TypeTools here  I  this should get you on the right path if I'm understanding your problem correctly."},
{"body": "I'm creating an authentication action that wraps other actions, using the Play Zentasks sample app as a template. One of the things that this needs to be able to do, is hit a webservice in certain circumstances, in order to retrieve a user's details. I want to do this in a non-blocking fashion, but I don't want to have to pass a future back to the action that I'm wrapping.The only way that I can think of doing this is by using Enumerator.fromStream() with an InputStream pulled from a URL object. I'm guessing this isn't the best way though, since it seems like a duplication of efforts (considering the ws object). The async ws api (and underlying asynchttpclient) returns a Future for everything however. I don't suppose anyone has tackled this issue before and could point me in another direction? Is there something that I'm missing? Also, would using a Enumerator.fromStream() as I've suggested definitely not block?Thanks in advance,SucheYou can use the async WS api. When it returns a future, you can call map on that, and pass the value to the action you're wrapping. Now you have a future containing the result of your wrapped action. Turn that into an AsyncResult or just wrap the whole thing in an Async{} block and it should work."},
{"body": "In below code the filter code is extracted to a local val : I can pass this then into a filter function like so : But if the .filter function is within a method and the filter is defined outside the method I need something like this :  is part of the method signature of  method. Above code does not compile because type A is not set. How can the code be updated so that method  accepts a filter definition as one of its parameters and then runs this code within  method ?Edit : When I make the change : I receive the error on line   :The error is : def myMethod[A](p: A => Boolean)As already said in comments: If you know the type of collection you're running filter against, why don't you define p type as ?"},
{"body": "I want to create a maven scala spring hibernate project . Im following this tutorial : \nNow I'm stuck in the 4th step : Start the Webapp with Maven using the Jetty Plugin. This is what the browser is displaying : And this is the log im having during the initialisation :first here s pom.xml:Here is my web.xml : This the web-context.xml :My JSP is containing the following code : My scala class :Can you help me please ?Actually when i input this url : localhost:8080/ i have this display :when clicking on web-inf I have an HTTP ERROR 404 Not Found"},
{"body": "I am trying to call a implicit function inside a test case but seems eclipse UI is not able to recognize it, I am getting compilation error, using Eclipse - Build id: 2.1-M2-20121018-1623-Typesafe Eclipse SDK Version: 3.7.2Am I missing something ?Getting compilation error at .brokeragePricing\nI tried cleaning the project & restarting eclipse but all in vain.The problem is that you are importing piece by piece of the  object. You can fix it by importing all elements of the object:Or by importing the missing element:"},
{"body": "The above is creating a column family meant to be used as a counter in Cassandra.  For whatever reason, compilation is failing with the following:Astyanax ColumnFamily syntax with description Now in your case, you have mentioned your rowkey as Long type and column as String and while updating the counter you are using key as your string, that's why this ."},
{"body": "I'm writing a Play 2 application with a Postgres backend.My code has a sql string which is a concatenation of two inserts and a select. Then when I try to execute my query, anorm blows up and says me no results have been returned. I could break it up into two separate queries but then I'd be making an unnecessary trip to the database?The sql string is pretty simple.Then the code that throws the exception just tries to get the gameid from query. This should work right?Here is the error As the other comments have said, this is caused by issuing a composite statement to JDBC.  You have a couple options.This would reduce your round trips to some extent."},
{"body": "I'm having trouble with an HTTP Request from my scala code.giveswhich is what I want.When I try to do this from my code like:givesWhich is not what I want.\nI'm tired and dont know what am I doing wrong here? Should I go for some other solution? I'm stuck and would really appreciate some answers for my problem. \nBTW. I'm doing my request from local, maybe this is my problem?\nThanks!When I ran this code, I got an exception indicating that the access token was not valid (expected).  If you plug in a valid token it should work:"},
{"body": "I need something like this:"},
{"body": "Given is the following constructHow do I append to the linked list?If I use  or  the list is not appended but a new list containing the value is returned. Does HashMap only return a copy of the associated value instead of a reference to the value? To answer myself after trying around a lot: it worked using a  instead of a .Using your second solution it works for me:although, better than  iswhich won't throw an exception if it can't find the value."},
{"body": "Could anyone please advise me what is the best framework/library for web browser automatisation? The task is to open web browsers page, sign in, perform some long searches, and save gathered information to excel. Now I'm using IE references in C#, but at work I could use only IE8. If I've upgraded it to IE9, but some scripts on target sites started working with errors. I tried to use awesomium, but I couldn't parse page with help of it, as I understand. Are there any variants to do this with high speed? Size of libs - doesn't matter.If there are any solutions compatible with Scala it would be great.As om-nom-nom hinted already, your best bet is probably a webdriver implementation like . It has bindings for c# and java and can use IE, FF, Chrome, phantomjs (great if you want to go headless) and others.Note that it might be not the best idea to do also the gathering of information directly with the webdriver, especially if the site content is changing fast. In such cases it might be useful to save the html page source with webdriver and then switch to some more efficient library for static content, like ."},
{"body": "I am having a clean closed type class system with serialization (detached from POJO serialization woes). For example:But in situations I need to capture a closure. For example:Now, I had solved this once with POJO serialization ( etc.) for the . I got badly bitten in the feet, because I couldn't read in Scala 2.10 what I had serialised in 2.9. And in this case, I really need to make sure I can get my stuff back independent of the Scala version.So... I have been thinking that I could use a macro to make a \"backup\" of the source code, so that if POJO deserialisation fails, I can regenerate the function from source (using an in-place compiler/interpreter).My idea would be(My guess is the def-macro needs to be already in the  function of ).Text replacement macros are awesome at this kind of thing.  Scala doesn't come with them, but consider writing your own!  Transforming e.g.toshould be pretty easy; then you just have to preprocess before you compile.  If you want your IDE to not get confused by different line lengths, you can store the strings in a separate object, e.g.goes to (For best results, base-64 encode the captured text.  Nesting will work fine as long as you work recursively and are aware that nesting might happen.)"},
{"body": "Recently trying to upgrade some oldish Scala code to use the Scala ARM library, in particular I like the continuations approach.My code is throwing a java.lang.NoSuchMethodError on the resource.ManagedResource.managed object.Here is my code:Where ConnectionPool.getConnection returns a java.sql.Connection to my database, the connection pool is based on BoneCP and works fine without using managed.This line of code returns the errors:Any help much appreciated. It's starting to seem I should just create my own ARM library as it's obviously very easy. However I do prefer to use community projects for my team's ease of future maitenance.I should add that BoneCP is not the cause and the same error is returned when I create the connection manually.Thanks\nRich"},
{"body": "In a Play 2.0 application, I need to deserialize some JSON from a source which I don't control which uses single-quotes around strings -- where the JSON spec calls for double-quotes.The solution using Jackson is here:\nBut trying to implement this solution in play2.0 I hit a wall of static objects and private classes... it should be enough to replace object JerksonJson with one implementing the solution linked above at initialization, but because it is a static object it can't be extended, and id I try to copy it into my code I need to drag along classes PlaySerializers, PlayDeserializers, JsValueDeserializer,... I stopped here, as it looked like too much.Is there a clean solution?How about trying to fix the invalid json string by replacing every ' in it with a \"?That would work if 's are only used for specifying strings.I realize that this may not help too much with Play framework part, but perhaps you could use  instead of Jerkson? Doing that should make it easier to just use , with Scala module registered, instead of having to use Jerkson-specific handlers."},
{"body": "I have a simple mongo application that happens to be async (using Akka).\nI send a message to an actor, which in turn write 3 records to a database.\nI'm using WriteConcern.SAFE because I want to be sure the write happened (also tried WriteConcern.FSYNC_SAFE).I pause for a second to let the writes happen then do a read--and get nothing.So my write code might be:then in my test code (running outside the actor--in another thread) I print out the # of records I find:My output looks like this:Indeed if I look in the database I see no records.   and the test works.  Async code can be tricky and it's possible the test code is being hit before the writes happen, so I also tried printing out timestamps to ensure these are being executed in the order presented--they are.  The data should be there.  Sample output below w/timestamps:So the 3 saves clearly happened (and should have written) a full 2 seconds before the read was attempted.I understand for more liberal WriteConcerns you could get this behavior, but I thought the two safest ones would assure me the write actually happened before proceeding.Subtle but simple problem.  I was using a def to create my connection... which I then proceeded to call twice as if it was a val.  So I actually had 2 different writers so that explained the sometimes-difference in my results.  Refactored to a val and all was predictable.  Agonizing to identify, easy to understand/fix."},
{"body": "Pretty simple scenario here.  I've got:... (I have Json.read and Json.writes defined for both classes, and the necessary apply/unapply methods (which do the straightforward thing - just as if they were case classes.)In a controller, in a post I have:I can post JSON like this:or any other subclass, but Jerkson always gives me a straight Thing back.Also, when Things are serialized out, @type is not included, making it impossible for the client to instantiate the correct type of object on the other end.Note that virtually the exact same code works fine with Jackson under a Java EE server.  Even older versions of Jackson.(I am trying to justify a move to all-Scala & Play from Java EE. For the most part it's gone wonderful but things like this have to be made to work.  It would really be a shame for us to have to handle our own polymorphic JSON serialization - there must be a way to do this but I can't find it easily.  And I'm puzzled as to why this doesn't simply work, as Jackson is Jackson.)Well, I was able to solve the problem, by using the Jackson API directly:It hit me that Play's JSON libraries are probably not using Jackson in a way that I am familiar with - if they were a no-arg ctor would be necessary, instead of the apply/unapply that Json reads/writes make use of.I don't want to use this mechanism because makes immutable objects difficult or impossible, so I would still appreciate someone demonstrating to me how polymorphic JSON de-serialization can be achieved using Play's JSON library directly."},
{"body": "I'm pretty new to scala and programming in general (just for fun) and I'm trying to understand tail-recursion and collections but debugg is really hard.I've 2 lists :ex : \"quoters\" quote \"quoted\" and \"quoted\" also quote \"quoters\".In the example, :the output should be something like :1/I think I'm misusing collections (it seems too much Json like...)2/I can get intersection with just List of Lists with :but not with the structure of List of Maps.3/I tried this for the recursion but it's not working because ... well i don't really know :I hope I'm clear enough :/Thank you in advance !FelixI think you have one extra layer in your data structures.  And I would probably use the name \"authors\" instead of \"quoted\", to avoid confusion because of the assonance between quoter / quoted:With this in place, and a small function like this:You can then start experimenting with different ways of collecting and reporting who has quoted which author and where, e.g.:which prints output like this:Now, think a bit about what you are mapping with these associative arrays.  You can effectively building the adjacency matrix of a graph.  How would you go about finding all circles in a directed graph?"},
{"body": "I have a quite heavy project with many modules aka sbt subprojects each of each has a big company name prefix. I've tried to find  command task key and override it, but didn't success. Is there any way to do this?The best thing to do is to create a dedicated object, with its own Scala file, that would define a def Project.That way you could have something like::in the same folder as your  file:That way you could create a  task that would call  with your company's prefix."},
{"body": "I'm trying to generate a sql statement like this:using Scala's slick 1.0.1 and postgres.This is my code:but it's generating queries like the following:\"rows\" seems to be generated as expected:rowCount sure is odd.e is an unholy beast as well.I'm not quite certain, but it seems to me I don't quite grok aggregate/grouping functions(such as count(*)) and Scala slick.  Does anyone have a suggestion?"},
{"body": "I'm working on getting a test case working with Play 2.1.1 and Specs2 and am running into an interesting problem. I create a FakeRequest to send to a Controller, but the function in the controller never gets called.Here's the test code (slightly simplified, but with all the moving parts):And here's the main function in the controller (well, at least just the start of it):This code never hits main. Strangely enough, though, if I make the FakeHeader with no parameters, and remove AnyContentAsXml, just sending the Xml Element to the controller, then it works:Does anyone have any idea why this would happen?Here is your test little modified showing two ways testing the controller. The reason AnyContentAsXml is not working for you because your testing is mixing two approaches together.}In the second approach Play is take care of unpacking the xml for the BodyParser to use. And here is the controller:"},
{"body": "I have a special code to execute when a pool thread start to execute and another when it finished.I mean, A need to call an initialize() before a thread start to execute actors code, and a cleanup() after it, in order to initialize thread specific resources (Database connections as an example) and cleanup (Close any already open connection)It will be great to do it in a thread scope. I'm thinking of doing in a trait with all actors mixing, but in this scope, the initialization is by actor. I think I'll have a better performance if I make it by thread.Any suggestion will be appreciated!ThanksEspecially for your cleanup code you will have trouble because there is no hook which you could use. I would recommend using the Actor life-cycle to model your resource life-cycle, i.e. create one DB connection when you start the actor and close it in postStop. Then instead of using a ThreadLocal database handle you send your DB queries to the (pool of) actors. Do not worry about threads yourself, that is Akka\u2019s job. "},
{"body": "If I'm using Play! to build a scala web app, how can I connect to a remote instance, run a map reduce job there? I have the scripts for the map reduce job, just not sure how I can connect to the instance within play and run the map reduce job on the instance using parameter data I get from the user through the web app? "},
{"body": "I am writing scala and ssp files in Intellij, i want Intellij to auto-Indent my code. While it is working for scala files but it doesn't work for ssp files.Is there any way we can configure IntelliJ to auto-indent these files also.or is there any ssp editor available online ?Currently no support available for scalate in Intellij.For now you can use Textmate with . Working for me."},
{"body": "I have a webservice running on jboss server. I can't change it to netty because i'm using other features of jboss. But i want to use finagles futures from the client. Is there a way ?The  class used in Finagle is part of , which is open source.  is usable on its own within any project that adds  as a dependency.You can always use a finagle client to make calls to an HTTP [or other RPC protocol] webservice. It doesn't matter how the service is implemented as along as it uses the protocol correctly. If you are using Java, this link should give you details on how to build a finagle client for an HTTP service: Here's some sample code to for a more elaborate finagle HTTP client: "},
{"body": "Given the Scala code below: What info about the sender (actor1) can actor2 get ?How can it tell which actor the message came from if more than one actor\n can send the same message to actor2 ?  Is there metadata in sender that\n can be queried ?Am a newcomer to Scala. Thanks for your patience....Ken RYou can actually send the actor that sent the original message as part of the message. It should be noted, though, that you should only use this to send a message to the original sender. Calling a function directly on the actor could cause all sorts of terrible things to happen!I actually just read about this in Programming in Scala today, so if you want to look up more about it, I highly recommend the book."},
{"body": "I have little previous experience with deprecated Scala actor, recently I started to learn Akka actor, they seem to be quite different.I completed this tutorial Next, I am trying to write an alternative Hello World, using a paradigm similar to deprecated Scala actor. But I have some difficulties (see in-line comments):There are a couple of things wrong with the code.1) You cannot create an actor by simply instantiating its class. You need to obtain an ActorRef using actorOf. To do this you need to start up an ActorSystem that will manage the creation of Actors.  2) Your receive method is wrong too. An pattern of type  will match everything including . Try something like this:Here a message is printed only if it is not  in which case it does not match the first case and falls of to the second one."},
{"body": "I'm working on a Scala project that is managed with Maven, all on OS X. When I build the project, I will get errors that look like this:Sometimes they will be switched, e.g.Running \"mvn clean\" and then rebuilding the project is the only way I've been able to get around this. I would like to avoid doing that (and of course, inexplicable build errors are not good either :P)."},
{"body": "I was wondering whether anyone could shed some light on this peculiar problem. I am using STS 3.2 (i.e. Eclipse 4.3 Kepler) with the Scala IDE and the Eclipse Maven plugin. The version of Maven I am using is Maven 3.0.5. I am using the maven surefire plugin version 2.4.3 and Scala version 2.10.2 . I have followed the instructions such as those provided here  and  however, however when I set break points in my JUnit scala tests, the break points are not being hit. I am not sure why this happening. Is there a problem with Eclipse Scala Maven and Maven Surefire??Thanks If you have flexibility in your choice of IDEs then I'd switch to IntelliJI've had issues like this in the past with maven-eclipse and while they are solvable, Intellij supports maven natively.  "},
{"body": "i wanted to use the , which is written in , in Scala. But there are several problems that i cannot solve.In , if i wanted to search for a label with a Wrapper, i can do it in that way: If i want to do that in Scala, i get no working solution. I have tried it already in that way:But then scala tells me: - inferred type arguments  do not conform to method as's type parameter bounds The method \"as\":The method \"lookup\":The Class \"Parent\":So i don't get it, how i can use these interfaces in scala. Is there any solution, or are there limitations of the use between scala and java?I know nothing about JemmyFX, but here's my 2 cents.\nIt's not an interoperability issue between Scala and Java. It's an issue of correctly typing your call to the  method.The  method signature says  which means that the INTERFACE type must be parametrized exactly with the same type than TYPE. Because of this, you cannot use  in the first argument and  in the second argument: you must use either  or  twice.I've tried to reproduce the issue with the following Scala code:Note that if the  method had been defined like this:the last line in the example above would have compiled."},
{"body": "I am having trouble using a left outer join in Slick. I'll start with some code : PS : The  &  are the objects extending  in opposition to  &  which are simple case classes.This is the error, I am getting when compiling the part above :I don't really understand this error. I know it has something to do with transformation, composition of queries but I have no idea how to change/fix it. Could some shed some light on this? The fix is explained here:  .  is not a query operation at the moment, use  instead."},
{"body": "How can I ignore all strings in these grammar rules using correct placement of ~> or <~ operators?I found the solution, I should break  to 3 None terminal rules as below"},
{"body": "I try to reproduce the Monoid examples from: I tried with Shapeless-2.0.0-SNAPSHOT and Shapeless-2.0.0-M1. Do i need some other things? Thanks \nFabianThe problem was that i missed the Monoid part of the example..."},
{"body": "Given the JSON...Represented with case classes...I attempt the following which fails with .What is the correct way to parse this?For a strange reason I did not find an elegant solution to read a json model with only one value. For 2 and more values you may write:For a json with 1 field try using something like that:This should work but doesn't look nice :("},
{"body": "I try to fill a form with the values of Systembut some of 'system' values are java.math.BigDecimal and it says :how can i handle this?Looks like problem in anorm or somewhere near. As you may see from error description  and  are s in query result, but declared as  in form.I am not familiar with anorm, but solution is to declare these ids as long in anorm, or declare them as  in form mapping or convert from  to  in your query."},
{"body": "Given some object that has the ability to query its fields, is it possible to iterate over the fields to generate a list of mappings and to use the list of mappings to create a form?For example:Background:\nI need to build a basic crud interface for our data model which consists of a 100+ tables, some with up to a 100 columns. Refactoring the data model is not an option since this crud interface is just one of many other users of the data. We generate the model classes so it is possible to also generate the form classes but I am hoping that I can get around the 18 / 22 field limit by using a list of fields instead."},
{"body": "I am using jetty 2.1.6 and sbt 0.7.7.Currently my app is running on http, but I want to run with httpsI tried following approach to run it with https :\nThen i set below code in build.scala file.But I did not get my work done. Please let me know , if i am doing anything wrong.Sorry, i mentioned it wrong earlier.Now I have successfully upgraded my jetty server to jetty-8.0.0.M3Now i am getting below error when I am writing above code in my build.scala file :[info] == jetty-run ==\n[error] Error running jetty-run: Error running Jetty: java.lang.NoSuchMethodError: org.eclipse.jetty.xml.XmlConfiguration.configure(Ljava/lang/Object;)V is a a pretty old version of SBT. I understand you may have reasons not to upgrade, but if you can there is the  SBT plugin that makes  in Jetty super easy."},
{"body": "I installed , then I installed SVN. After restarting I was asked to choose my SVN Connector (I choose ). At this point, everything is working.Then I installed the , and everything is still working.Then I installed the , and suddenly after restarting Eclipse my SVN has stopped working. It was installed etc., but it isn't working.I figured, I needed to install the Scala plugin first. Then I installed SVN but after restarting I didn't get the message about choosing an SVN Connector, so I installed it manually. Still doesn't work.Is this a bug? How do I fix it?"},
{"body": "I'm using Play 2.1.2 and I'm having some trouble with an  and I'm seeking ideas on how to debug this.I'm trying to stream some S3 data through my server.  I can get an  from the Amazon SDK for my S3 file ().  I then turn that  into an  using .All this type checks and on my local development machine it all works perfectly.  When I formulate my  in Play, I just return .The problem comes when I deploy this to a production server.  The very first time I request the file, it works just fine and I get the whole file.  But subsequent times it frequently gets part way through (different amounts each time) and then gets \"stuck\".  I wrapped the  as follows to be able to log whether the enumeration completed:As expected, on my development machine (and the first time through on the production machines), I get the message \"Contents fully enumerated\".  But after that, the production machine will start the download of the file, but it doesn't finish (in both the HTTP sense and in the  sense).I'm not sure how to debug this.  Obviously,  does some magic and I don't know how to figure out what is happening between chunks.  I thought this might be a thread pool issue so I wrapped the whole response in a  block, but it didn't appear to make any difference.I'm trying to avoid the hassle of creating a local temporary file from S3 and then building my  from that.  Using the  to create an  seemed like the elegant way to do this...if it worked.Suggestions?OK, so I think I figured this out.  It turns out that on the Play side, things appear to be working.  I tried all kinds of variations (different ways of constructing the enumerator, creating temporary files, etc).  It didn't really matter.What  matter was the proxy I was using.  I'm using  and if I make a request on the server behind the proxy, I get the correct response (directly from Play).  If I make the request on the server outside the proxy, I get an incorrect (empty) response.  So it looks like the proxy is \"dropping\" the response.It appears the issue is that the response is chunked by the  call and this is causing the problem with the proxy.  If I reformulate my response to be:Then play uses the  to construct a complete response (not streamed) and things work again.  Of course, it is stupid to have to form the complete response in this case, but it does work.  Hopefully I can find a better solution than this in the long term, but this seems to work for now."},
{"body": "I have an application that takes Photo with photo overlay and then resize it, here is my code\n,is it possible to set size to the camera to avoid the resizing process after taking the picture, since after taking the picture I have some delay..View_Camera_OverlayCamera Overlay Camera activity would you please help me in this implementationThanks in advancedid you check  in Camera.Parameters\nAlso make sure that the camera supports the picture size. you can use  for that. "},
{"body": "I would like to know if it is possible to merge JsResult objects, something similar to ~> operator in play 2.1+. In the following code I want to validate two inputs and then update user information accordingly. The  operator below between two validate method calls is not valid. Is there a way in play to combine two JsResult objects together in the following scenario ?You can use flatmap to combine the JsResults.This is how you can join two JsResult into one: and  is part of the  package."},
{"body": "I'm building a java/Scala app which would be deployed in hundreds of nodes in cluster.  I thought of the following idea.  Instead of building rest api for my services where nodes would communicate and query and do operations one on each other.  I'll simply use akka actors in this way one actor can send messages to others and this would save me from the whole process of managing rest api's and even discovery of nodes load balancing etc (although in don't have to be asynchronous).Note I'm aware of the motivation to use actors as means for async programming instead of locks I just want to know if the use case I suggested is actually a good use case for Hakka actors or am I missing something thanks."},
{"body": "I have the following architecture at the moment:Load(Play app with basic interface for load tests) -> Gateway(Spray application with REST interface for incoming messages) -> Processor(akka app that works with MongoDB) -> MongoDBEverything works fine as long as number of messages I am pushing through is low. However when I try to push 10000 events, that will eventully end up at MongoDB as documents, it stops at random places, for example on 742 message or 982 message and does nothing after.What would be the best way to debug such situations? On the load side I am just pushing hard into the REST service:and then in the workerRouterOn the spray side:On the processor side it's just basically an insert into a collection. Any suggestions on where to start from?Update:\nI tried to move the logic of creating messages to the Gatewat, did a cycle of 1 to 10000 and it works just fine. However if spray and play are involed in a pipeline it interrupts and random places. Any suggestions on how to debug in this case?In a distributed and parallel environment it is next to impossible to create a system that work reliably. Whatever debugging method you use it will only allow you to find a few bugs that will happen during the debug session.Once our team spent 3 months(!) while tuning an application for a robust 24/7 working. And still there were bugs. Then we applied a method of  (). Within a couple of weeks we implemented a model that allowed us to get a robust application. However,  requires a bit different way of thinking and it can be difficult to start.I moved the load test app to Spray framework and now it works like a charm. So I suppose the problem was somewhere in the way that I used WS API in Play framework:The problem is resovled but not solved, won't work on a solution based on Play. "},
{"body": "I'm on the road to learn Scala and I'm having a hard time understanding contravariants, covariants, invariance, etc. From  I have learned how functions can be considered subtypes of another function. (Really useful to know!)The code below is what I believe are the important pieces to solving my puzzle. I have extracted parts that I think would add unneeded complexity to the problem. According to the example I have a Student object that will act as a factory to generate functions. Functions will take in types or subtyes of AnyVal (Int, Double, Long, etc.), and the return output will be of the same input type. To achieve this the student class takes in a generic (A) that is a subtype of AnyVal. The abstract class is there so that I can refer a list of these students by doing something like List[Master[AnyVal]]( Student.func1). The problem is that I cannot have the line \"val function: List[A] => A\" as I get error \"covariant type A occurs in contravariant position in type => List[A] => A of value function\". I don't know why the return type must be the contravariant of A. I can somewhat accept this fact for name based on the Function1 trait. So how would I define my function in the abstract Master class so that the return type is a contravariant of type A? I found an example of how to define this with function definitions (Ex. def function[B >: A](v: B): List[B]), but how would I implement this with an anonymous function? Remember \"A\" in the master abstract class must be covariant since there will be a list of function that take in all AnyVal types (Int, Double, etc.)Really appreciate the help! Let me know if any of my terminology is off. - Scala LearnerIt is very difficult to create generic classes or traits parameterised with a covariant type.  Once you add the variance annotation, you cannot use the type as a parameter for any of the class methods.  This is because functions are contra-variant in their parameters and covariant in their return types.  There is a very good explanation in  accepted answer.If you really want the type to be covariant, you have to write all the methods to take parameters of type [B >: A] - that is, to take supertypes of A as their parameter type.  This can be quite challenging; the online Scala documentation has an  of the acrobatics that might be required."},
{"body": "I have a java project that I am adding some scala code to. Previously I used ant to build the project and am moving to using sbt. In the ant build file was an rmic task to build the RMI Stub classes in the project. This is a task that comes directly with ant. Has anyone used a similar task in sbt or do they know how I could define one?Thanks\nDes"},
{"body": "I am converting some Matlab codes to Scala. In Matlab, there is the  function that is used to define the number of cores to use and \"open\" the core for parallel computations (and thereafter use for instance  instead of  to run loops in parallel). (\"opens\" 4 cores)(\"closes\" the cores for parallel processing)What is the equivalent of  in Scala?The Scala compiler generates regular JVM bytecode, so in runtime Scala code has the same capabilities that Java code does. The JVM provides parallelism through light-weight threads (see Thread class and Runnable interface), which will use as many cores as they can (no more than one per thread, off course). See: Twitter's Scala School has a concurrency page () which indeed shows how to use Java concurrency primitives from Scala.That said, there are facilities for parallel collections in Scala, which leverage the fork/join Java capabilities from JVM 6 onwards. See: "}